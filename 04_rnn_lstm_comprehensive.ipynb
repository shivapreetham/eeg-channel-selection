{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN and LSTM Comprehensive Guide: Sequential Learning Mastery\n",
    "\n",
    "This notebook provides a complete understanding of Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) networks.\n",
    "\n",
    "## Complete Learning Objectives:\n",
    "1. **Sequential Data**: Understanding time series and sequence problems\n",
    "2. **Vanilla RNN**: Architecture, forward pass, backpropagation through time\n",
    "3. **Vanishing Gradients**: Why vanilla RNNs fail on long sequences\n",
    "4. **LSTM Architecture**: Gates, cell state, hidden state mechanics\n",
    "5. **GRU**: Simplified alternative to LSTM\n",
    "6. **Applications**: Text, time series, sequence-to-sequence tasks\n",
    "7. **Implementation**: From scratch understanding with real projects\n",
    "\n",
    "**Prerequisites**: Complete foundational notebooks (01, 02, 03)\n",
    "\n",
    "**Why RNNs Matter for GNNs**: Graph Neural Networks use similar message-passing concepts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Comprehensive RNN/LSTM Environment Setup\n",
    "\"\"\"\n",
    "SEQUENTIAL LEARNING LIBRARY ECOSYSTEM:\n",
    "\n",
    "Core Deep Learning:\n",
    "- tensorflow: Excellent RNN/LSTM support with tf.keras.layers.LSTM, GRU\n",
    "- tensorflow.keras.preprocessing: Text and sequence preprocessing\n",
    "- tensorflow.keras.utils: Sequence utilities and data generators\n",
    "\n",
    "Text Processing:\n",
    "- nltk: Natural Language Toolkit for text preprocessing\n",
    "- re: Regular expressions for text cleaning\n",
    "- string: String manipulation utilities\n",
    "\n",
    "Time Series:\n",
    "- pandas: Time series data manipulation and analysis\n",
    "- datetime: Date and time handling\n",
    "- numpy: Numerical operations for sequence data\n",
    "\n",
    "Visualization:\n",
    "- matplotlib: Time series plots, sequence visualizations\n",
    "- seaborn: Statistical plots for sequence analysis\n",
    "- plotly: Interactive time series plots\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, optimizers, callbacks\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import re\n",
    "import string\n",
    "import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For text data\n",
    "try:\n",
    "    import nltk\n",
    "    # Download required NLTK data\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    NLTK_AVAILABLE = True\n",
    "except ImportError:\n",
    "    NLTK_AVAILABLE = False\n",
    "    print(\"NLTK not available - will use basic text processing\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "\n",
    "print(f\"🔧 RNN/LSTM ENVIRONMENT SETUP\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Keras version: {keras.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"GPU available: {len(tf.config.list_physical_devices('GPU')) > 0}\")\n",
    "print(f\"NLTK available: {NLTK_AVAILABLE}\")\n",
    "print(f\"Random seed: {RANDOM_SEED}\")\n",
    "\n",
    "# Configure plotting for sequence visualization\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (15, 10)\n",
    "plt.rcParams['font.size'] = 11\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams['grid.alpha'] = 0.3\n",
    "\n",
    "print(\"\\n✅ All RNN/LSTM libraries imported and configured successfully!\")\n",
    "print(\"🎯 Ready for sequential learning exploration!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding Sequential Data: The Foundation\n",
    "\n",
    "**What Makes Data Sequential?**\n",
    "- **Order matters**: Past influences present and future\n",
    "- **Temporal dependencies**: Information flows through time\n",
    "- **Variable length**: Sequences can have different lengths\n",
    "- **Context dependency**: Meaning depends on surrounding elements\n",
    "\n",
    "**Types of Sequential Data:**\n",
    "\n",
    "1. **Time Series**: Stock prices, weather, sensor readings\n",
    "   - Fixed time intervals\n",
    "   - Continuous values\n",
    "   - Trend and seasonality patterns\n",
    "\n",
    "2. **Natural Language**: Text, speech, conversations\n",
    "   - Discrete tokens (words, characters)\n",
    "   - Grammar and syntax dependencies\n",
    "   - Long-range dependencies\n",
    "\n",
    "3. **Biological Sequences**: DNA, proteins, gene expressions\n",
    "   - Discrete alphabet (A,C,G,T)\n",
    "   - Functional dependencies\n",
    "   - Pattern recognition\n",
    "\n",
    "**Key Challenges:**\n",
    "- **Variable lengths**: How to batch different sequence lengths?\n",
    "- **Long dependencies**: Information from far past affecting present\n",
    "- **Vanishing gradients**: Difficulty learning long-term patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Sequential Data Analysis and Preparation\n",
    "\n",
    "print(\"=== SEQUENTIAL DATA UNDERSTANDING ===\")\n",
    "\n",
    "# 1. TIME SERIES DATA EXAMPLE\n",
    "print(\"\\n📈 TIME SERIES DATA EXAMPLE:\")\n",
    "\n",
    "def create_synthetic_time_series(n_points=1000, trend=0.001, seasonality=True, noise_level=0.1):\n",
    "    \"\"\"\n",
    "    Create synthetic time series with trend, seasonality, and noise\n",
    "    \"\"\"\n",
    "    t = np.arange(n_points)\n",
    "    \n",
    "    # Trend component\n",
    "    trend_component = trend * t\n",
    "    \n",
    "    # Seasonal component\n",
    "    if seasonality:\n",
    "        seasonal_component = (\n",
    "            0.5 * np.sin(2 * np.pi * t / 50) +  # Short-term cycle\n",
    "            0.3 * np.sin(2 * np.pi * t / 200)   # Long-term cycle\n",
    "        )\n",
    "    else:\n",
    "        seasonal_component = 0\n",
    "    \n",
    "    # Noise component\n",
    "    noise_component = np.random.normal(0, noise_level, n_points)\n",
    "    \n",
    "    # Combine components\n",
    "    time_series = trend_component + seasonal_component + noise_component\n",
    "    \n",
    "    return time_series, {'trend': trend_component, 'seasonal': seasonal_component, 'noise': noise_component}\n",
    "\n",
    "# Generate example time series\n",
    "ts_data, ts_components = create_synthetic_time_series(n_points=500)\n",
    "\n",
    "print(f\"Time series characteristics:\")\n",
    "print(f\"  Length: {len(ts_data)} time points\")\n",
    "print(f\"  Value range: [{ts_data.min():.3f}, {ts_data.max():.3f}]\")\n",
    "print(f\"  Mean: {ts_data.mean():.3f}, Std: {ts_data.std():.3f}\")\n",
    "print(f\"  Components: Trend + Seasonality + Noise\")\n",
    "\n",
    "# 2. TEXT SEQUENCE DATA EXAMPLE\n",
    "print(\"\\n📚 TEXT SEQUENCE DATA EXAMPLE:\")\n",
    "\n",
    "# Sample text data for sequence learning\n",
    "sample_texts = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Machine learning is a subset of artificial intelligence.\",\n",
    "    \"Neural networks learn patterns from data through training.\",\n",
    "    \"Deep learning uses multiple layers to extract features.\",\n",
    "    \"Recurrent networks process sequential information effectively.\",\n",
    "    \"Long short-term memory networks solve vanishing gradients.\",\n",
    "    \"Natural language processing enables computers to understand text.\",\n",
    "    \"Time series forecasting predicts future values from past data.\"\n",
    "]\n",
    "\n",
    "def analyze_text_sequences(texts):\n",
    "    \"\"\"\n",
    "    Analyze text sequence characteristics\n",
    "    \"\"\"\n",
    "    # Basic statistics\n",
    "    word_counts = [len(text.split()) for text in texts]\n",
    "    char_counts = [len(text) for text in texts]\n",
    "    \n",
    "    # Vocabulary analysis\n",
    "    all_words = ' '.join(texts).lower().split()\n",
    "    unique_words = set(all_words)\n",
    "    word_freq = {}\n",
    "    for word in all_words:\n",
    "        word_freq[word] = word_freq.get(word, 0) + 1\n",
    "    \n",
    "    return {\n",
    "        'num_sequences': len(texts),\n",
    "        'avg_words': np.mean(word_counts),\n",
    "        'avg_chars': np.mean(char_counts),\n",
    "        'vocabulary_size': len(unique_words),\n",
    "        'total_words': len(all_words),\n",
    "        'most_common': sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "    }\n",
    "\n",
    "text_stats = analyze_text_sequences(sample_texts)\n",
    "\n",
    "print(f\"Text sequence characteristics:\")\n",
    "print(f\"  Number of sequences: {text_stats['num_sequences']}\")\n",
    "print(f\"  Average words per sequence: {text_stats['avg_words']:.1f}\")\n",
    "print(f\"  Average characters per sequence: {text_stats['avg_chars']:.1f}\")\n",
    "print(f\"  Vocabulary size: {text_stats['vocabulary_size']}\")\n",
    "print(f\"  Total words: {text_stats['total_words']}\")\n",
    "print(f\"  Most common words: {text_stats['most_common']}\")\n",
    "\n",
    "# 3. SEQUENCE PREPROCESSING CHALLENGES\n",
    "print(\"\\n🔧 SEQUENCE PREPROCESSING CHALLENGES:\")\n",
    "\n",
    "def demonstrate_sequence_challenges():\n",
    "    \"\"\"\n",
    "    Demonstrate common sequence preprocessing challenges\n",
    "    \"\"\"\n",
    "    \n",
    "    # Variable length sequences\n",
    "    sequences = [\n",
    "        [1, 2, 3],\n",
    "        [4, 5, 6, 7, 8],\n",
    "        [9, 10],\n",
    "        [11, 12, 13, 14, 15, 16, 17]\n",
    "    ]\n",
    "    \n",
    "    lengths = [len(seq) for seq in sequences]\n",
    "    \n",
    "    print(f\"Challenge 1: Variable Length Sequences\")\n",
    "    print(f\"  Sequence lengths: {lengths}\")\n",
    "    print(f\"  Min length: {min(lengths)}, Max length: {max(lengths)}\")\n",
    "    print(f\"  Problem: Can't batch variable lengths directly\")\n",
    "    \n",
    "    # Padding solution\n",
    "    max_len = max(lengths)\n",
    "    padded_sequences = []\n",
    "    for seq in sequences:\n",
    "        padded = seq + [0] * (max_len - len(seq))  # Pad with zeros\n",
    "        padded_sequences.append(padded)\n",
    "    \n",
    "    print(f\"  Solution: Padding to max length ({max_len})\")\n",
    "    print(f\"  Padded sequences: {padded_sequences}\")\n",
    "    \n",
    "    # Long sequences memory challenge\n",
    "    long_sequence = list(range(10000))\n",
    "    memory_estimate = len(long_sequence) * 4 / 1024  # Float32 in KB\n",
    "    \n",
    "    print(f\"\\nChallenge 2: Long Sequences Memory\")\n",
    "    print(f\"  Sequence length: {len(long_sequence):,}\")\n",
    "    print(f\"  Memory per sequence: {memory_estimate:.1f} KB\")\n",
    "    print(f\"  Batch of 32: {memory_estimate * 32:.1f} KB\")\n",
    "    print(f\"  Problem: Memory grows quadratically with sequence length\")\n",
    "    \n",
    "    # Truncation solution\n",
    "    max_sequence_length = 100\n",
    "    truncated = long_sequence[:max_sequence_length]\n",
    "    print(f\"  Solution: Truncate to max length ({max_sequence_length})\")\n",
    "    print(f\"  Information loss: {len(long_sequence) - len(truncated):,} elements\")\n",
    "\n",
    "demonstrate_sequence_challenges()\n",
    "\n",
    "# 4. VISUALIZE SEQUENTIAL DATA PATTERNS\n",
    "print(\"\\n📊 SEQUENTIAL DATA VISUALIZATION:\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# Plot 1: Time series components\n",
    "t = np.arange(len(ts_data))\n",
    "axes[0, 0].plot(t, ts_data, 'b-', linewidth=1, label='Combined Signal')\n",
    "axes[0, 0].plot(t, ts_components['trend'], 'r--', linewidth=2, label='Trend')\n",
    "axes[0, 0].set_title('Time Series: Trend Component')\n",
    "axes[0, 0].set_xlabel('Time')\n",
    "axes[0, 0].set_ylabel('Value')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Seasonal component\n",
    "axes[0, 1].plot(t, ts_components['seasonal'], 'g-', linewidth=2, label='Seasonal')\n",
    "axes[0, 1].set_title('Time Series: Seasonal Component')\n",
    "axes[0, 1].set_xlabel('Time')\n",
    "axes[0, 1].set_ylabel('Value')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Noise component\n",
    "axes[0, 2].plot(t, ts_components['noise'], 'orange', linewidth=1, alpha=0.7, label='Noise')\n",
    "axes[0, 2].set_title('Time Series: Noise Component')\n",
    "axes[0, 2].set_xlabel('Time')\n",
    "axes[0, 2].set_ylabel('Value')\n",
    "axes[0, 2].legend()\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Sequence length distribution\n",
    "text_lengths = [len(text.split()) for text in sample_texts]\n",
    "axes[1, 0].hist(text_lengths, bins=5, color='skyblue', alpha=0.7, edgecolor='black')\n",
    "axes[1, 0].set_title('Text Sequence Length Distribution')\n",
    "axes[1, 0].set_xlabel('Number of Words')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 5: Word frequency distribution\n",
    "word_freqs = list(text_stats['most_common'])\n",
    "words, freqs = zip(*word_freqs)\n",
    "axes[1, 1].bar(words, freqs, color='lightgreen', alpha=0.7)\n",
    "axes[1, 1].set_title('Most Common Words')\n",
    "axes[1, 1].set_xlabel('Words')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Plot 6: Sequence preprocessing visualization\n",
    "original_lengths = [3, 5, 2, 7]\n",
    "padded_lengths = [7] * 4  # All padded to max length\n",
    "\n",
    "x = np.arange(len(original_lengths))\n",
    "width = 0.35\n",
    "\n",
    "axes[1, 2].bar(x - width/2, original_lengths, width, label='Original', color='lightcoral', alpha=0.7)\n",
    "axes[1, 2].bar(x + width/2, padded_lengths, width, label='Padded', color='lightblue', alpha=0.7)\n",
    "axes[1, 2].set_title('Sequence Padding Effect')\n",
    "axes[1, 2].set_xlabel('Sequence Index')\n",
    "axes[1, 2].set_ylabel('Length')\n",
    "axes[1, 2].set_xticks(x)\n",
    "axes[1, 2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n💡 SEQUENTIAL DATA KEY INSIGHTS:\")\n",
    "print(f\"\\n1. TEMPORAL DEPENDENCIES:\")\n",
    "print(f\"   • Past information influences future predictions\")\n",
    "print(f\"   • Order matters: [A,B,C] ≠ [C,B,A]\")\n",
    "print(f\"   • Context provides meaning: 'bank' in finance vs river context\")\n",
    "\n",
    "print(f\"\\n2. PREPROCESSING CHALLENGES:\")\n",
    "print(f\"   • Variable lengths → Padding or truncation needed\")\n",
    "print(f\"   • Long sequences → Memory and computational constraints\")\n",
    "print(f\"   • Vocabulary size → Embedding dimensionality trade-offs\")\n",
    "\n",
    "print(f\"\\n3. MODELING IMPLICATIONS:\")\n",
    "print(f\"   • Need memory to store past information\")\n",
    "print(f\"   • Gradients must flow through time\")\n",
    "print(f\"   • Different sequence lengths need special handling\")\n",
    "\n",
    "print(f\"\\n4. APPLICATIONS:\")\n",
    "print(f\"   • Time series: Stock prediction, weather forecasting\")\n",
    "print(f\"   • NLP: Language modeling, machine translation\")\n",
    "print(f\"   • Speech: Recognition, synthesis\")\n",
    "print(f\"   • Biology: Gene sequence analysis, protein folding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Vanilla RNN: The Foundation of Sequential Learning\n",
    "\n",
    "**RNN Core Concept:**\n",
    "- **Hidden state**: Memory that carries information through time\n",
    "- **Recurrent connection**: Hidden state fed back to next time step\n",
    "- **Parameter sharing**: Same weights used at each time step\n",
    "\n",
    "**RNN Mathematics:**\n",
    "```\n",
    "h_t = tanh(W_hh * h_{t-1} + W_xh * x_t + b_h)\n",
    "y_t = W_hy * h_t + b_y\n",
    "```\n",
    "\n",
    "**Where:**\n",
    "- `h_t`: Hidden state at time t\n",
    "- `x_t`: Input at time t\n",
    "- `y_t`: Output at time t\n",
    "- `W_hh`: Hidden-to-hidden weights\n",
    "- `W_xh`: Input-to-hidden weights\n",
    "- `W_hy`: Hidden-to-output weights\n",
    "\n",
    "**Key Properties:**\n",
    "1. **Memory**: Hidden state acts as memory\n",
    "2. **Parameter sharing**: Efficiency across time steps\n",
    "3. **Variable length**: Can handle sequences of any length\n",
    "4. **Sequential processing**: Information flows step by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Vanilla RNN Implementation and Analysis\n",
    "\n",
    "print(\"=== VANILLA RNN DEEP DIVE ===\")\n",
    "\n",
    "class SimpleRNN:\n",
    "    \"\"\"\n",
    "    Educational implementation of a simple RNN cell\n",
    "    This helps understand the mathematics behind RNNs\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # Initialize weights with small random values\n",
    "        # Xavier initialization for better gradient flow\n",
    "        self.W_xh = np.random.randn(input_size, hidden_size) * np.sqrt(2.0 / input_size)\n",
    "        self.W_hh = np.random.randn(hidden_size, hidden_size) * np.sqrt(2.0 / hidden_size) \n",
    "        self.W_hy = np.random.randn(hidden_size, output_size) * np.sqrt(2.0 / hidden_size)\n",
    "        \n",
    "        # Initialize biases to zero\n",
    "        self.b_h = np.zeros((hidden_size,))\n",
    "        self.b_y = np.zeros((output_size,))\n",
    "        \n",
    "        # Store activations for analysis\n",
    "        self.hidden_states = []\n",
    "        self.outputs = []\n",
    "    \n",
    "    def forward_step(self, x_t, h_prev):\n",
    "        \"\"\"\n",
    "        Single forward step of RNN\n",
    "        \n",
    "        Args:\n",
    "            x_t: Input at time t, shape (input_size,)\n",
    "            h_prev: Previous hidden state, shape (hidden_size,)\n",
    "            \n",
    "        Returns:\n",
    "            h_t: New hidden state, shape (hidden_size,)\n",
    "            y_t: Output at time t, shape (output_size,)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Compute new hidden state\n",
    "        # h_t = tanh(x_t @ W_xh + h_prev @ W_hh + b_h)\n",
    "        linear_combination = (\n",
    "            np.dot(x_t, self.W_xh) +     # Input contribution\n",
    "            np.dot(h_prev, self.W_hh) +  # Previous hidden state contribution\n",
    "            self.b_h                     # Bias\n",
    "        )\n",
    "        h_t = np.tanh(linear_combination)\n",
    "        \n",
    "        # Compute output\n",
    "        # y_t = h_t @ W_hy + b_y\n",
    "        y_t = np.dot(h_t, self.W_hy) + self.b_y\n",
    "        \n",
    "        return h_t, y_t\n",
    "    \n",
    "    def forward_sequence(self, X, initial_hidden=None):\n",
    "        \"\"\"\n",
    "        Forward pass through entire sequence\n",
    "        \n",
    "        Args:\n",
    "            X: Input sequence, shape (sequence_length, input_size)\n",
    "            initial_hidden: Initial hidden state, shape (hidden_size,)\n",
    "            \n",
    "        Returns:\n",
    "            hidden_states: List of hidden states\n",
    "            outputs: List of outputs\n",
    "        \"\"\"\n",
    "        \n",
    "        sequence_length = X.shape[0]\n",
    "        \n",
    "        # Initialize hidden state if not provided\n",
    "        if initial_hidden is None:\n",
    "            h_t = np.zeros((self.hidden_size,))\n",
    "        else:\n",
    "            h_t = initial_hidden.copy()\n",
    "        \n",
    "        # Store results\n",
    "        hidden_states = []\n",
    "        outputs = []\n",
    "        \n",
    "        # Process each time step\n",
    "        for t in range(sequence_length):\n",
    "            x_t = X[t]\n",
    "            h_t, y_t = self.forward_step(x_t, h_t)\n",
    "            \n",
    "            hidden_states.append(h_t.copy())\n",
    "            outputs.append(y_t.copy())\n",
    "        \n",
    "        # Store for analysis\n",
    "        self.hidden_states = hidden_states\n",
    "        self.outputs = outputs\n",
    "        \n",
    "        return hidden_states, outputs\n",
    "    \n",
    "    def get_parameter_count(self):\n",
    "        \"\"\"Calculate total number of parameters\"\"\"\n",
    "        params = (\n",
    "            self.W_xh.size +\n",
    "            self.W_hh.size +\n",
    "            self.W_hy.size +\n",
    "            self.b_h.size +\n",
    "            self.b_y.size\n",
    "        )\n",
    "        return params\n",
    "\n",
    "# Demonstrate RNN with simple sequence\n",
    "print(f\"\\n🧠 SIMPLE RNN DEMONSTRATION:\")\n",
    "\n",
    "# Create a simple RNN\n",
    "input_size = 3\n",
    "hidden_size = 4\n",
    "output_size = 2\n",
    "\n",
    "rnn = SimpleRNN(input_size, hidden_size, output_size)\n",
    "\n",
    "print(f\"RNN Architecture:\")\n",
    "print(f\"  Input size: {input_size}\")\n",
    "print(f\"  Hidden size: {hidden_size}\")\n",
    "print(f\"  Output size: {output_size}\")\n",
    "print(f\"  Total parameters: {rnn.get_parameter_count()}\")\n",
    "\n",
    "# Parameter breakdown\n",
    "print(f\"\\nParameter breakdown:\")\n",
    "print(f\"  W_xh (input→hidden): {rnn.W_xh.shape} = {rnn.W_xh.size} params\")\n",
    "print(f\"  W_hh (hidden→hidden): {rnn.W_hh.shape} = {rnn.W_hh.size} params\")\n",
    "print(f\"  W_hy (hidden→output): {rnn.W_hy.shape} = {rnn.W_hy.size} params\")\n",
    "print(f\"  b_h (hidden bias): {rnn.b_h.shape} = {rnn.b_h.size} params\")\n",
    "print(f\"  b_y (output bias): {rnn.b_y.shape} = {rnn.b_y.size} params\")\n",
    "\n",
    "# Create example input sequence\n",
    "sequence_length = 5\n",
    "X_example = np.random.randn(sequence_length, input_size)\n",
    "\n",
    "print(f\"\\nExample input sequence:\")\n",
    "print(f\"  Shape: {X_example.shape} (time_steps, features)\")\n",
    "print(f\"  Values:\\n{X_example}\")\n",
    "\n",
    "# Forward pass through sequence\n",
    "hidden_states, outputs = rnn.forward_sequence(X_example)\n",
    "\n",
    "print(f\"\\nRNN Processing Results:\")\n",
    "for t in range(sequence_length):\n",
    "    print(f\"  Time step {t+1}:\")\n",
    "    print(f\"    Input: {X_example[t]}\")\n",
    "    print(f\"    Hidden state: {hidden_states[t]}\")\n",
    "    print(f\"    Output: {outputs[t]}\")\n",
    "    print()\n",
    "\n",
    "# Analyze hidden state evolution\n",
    "print(f\"\\n📊 HIDDEN STATE ANALYSIS:\")\n",
    "\n",
    "hidden_states_array = np.array(hidden_states)  # Shape: (time_steps, hidden_size)\n",
    "outputs_array = np.array(outputs)              # Shape: (time_steps, output_size)\n",
    "\n",
    "print(f\"Hidden state statistics:\")\n",
    "for t in range(sequence_length):\n",
    "    h_t = hidden_states_array[t]\n",
    "    print(f\"  Step {t+1}: Mean={h_t.mean():.3f}, Std={h_t.std():.3f}, Range=[{h_t.min():.3f}, {h_t.max():.3f}]\")\n",
    "\n",
    "# Demonstrate RNN memory\n",
    "print(f\"\\n🧠 RNN MEMORY DEMONSTRATION:\")\n",
    "\n",
    "def test_rnn_memory():\n",
    "    \"\"\"Test how RNN remembers information from early time steps\"\"\"\n",
    "    \n",
    "    # Create two similar sequences with different first elements\n",
    "    base_sequence = np.ones((5, 2))  # All ones\n",
    "    \n",
    "    sequence_A = base_sequence.copy()\n",
    "    sequence_A[0] = [5.0, 0.0]  # Strong signal at beginning\n",
    "    \n",
    "    sequence_B = base_sequence.copy() \n",
    "    sequence_B[0] = [-5.0, 0.0]  # Opposite signal at beginning\n",
    "    \n",
    "    # Create RNN for this test\n",
    "    test_rnn = SimpleRNN(2, 8, 1)\n",
    "    \n",
    "    # Process both sequences\n",
    "    _, outputs_A = test_rnn.forward_sequence(sequence_A)\n",
    "    _, outputs_B = test_rnn.forward_sequence(sequence_B)\n",
    "    \n",
    "    print(f\"Memory test results:\")\n",
    "    print(f\"  Sequence A (starts with [5,0]):\")\n",
    "    for i, out in enumerate(outputs_A):\n",
    "        print(f\"    Step {i+1}: {out[0]:.3f}\")\n",
    "    \n",
    "    print(f\"  Sequence B (starts with [-5,0]):\")\n",
    "    for i, out in enumerate(outputs_B):\n",
    "        print(f\"    Step {i+1}: {out[0]:.3f}\")\n",
    "    \n",
    "    # Check if difference persists\n",
    "    final_diff = abs(outputs_A[-1][0] - outputs_B[-1][0])\n",
    "    print(f\"  Final output difference: {final_diff:.3f}\")\n",
    "    \n",
    "    if final_diff > 0.1:\n",
    "        print(f\"  ✅ RNN remembers early information!\")\n",
    "    else:\n",
    "        print(f\"  ❌ RNN forgot early information\")\n",
    "\n",
    "test_rnn_memory()\n",
    "\n",
    "# Visualize RNN behavior\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# Plot 1: Input sequence\n",
    "for i in range(input_size):\n",
    "    axes[0, 0].plot(range(sequence_length), X_example[:, i], \n",
    "                   marker='o', label=f'Feature {i+1}', linewidth=2)\n",
    "axes[0, 0].set_title('Input Sequence')\n",
    "axes[0, 0].set_xlabel('Time Step')\n",
    "axes[0, 0].set_ylabel('Input Value')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Hidden state evolution\n",
    "for i in range(hidden_size):\n",
    "    axes[0, 1].plot(range(sequence_length), hidden_states_array[:, i], \n",
    "                   marker='s', alpha=0.7, label=f'Hidden {i+1}' if i < 3 else \"\")\n",
    "axes[0, 1].set_title('Hidden State Evolution')\n",
    "axes[0, 1].set_xlabel('Time Step')\n",
    "axes[0, 1].set_ylabel('Hidden Value')\n",
    "if hidden_size <= 3:\n",
    "    axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Output sequence\n",
    "for i in range(output_size):\n",
    "    axes[0, 2].plot(range(sequence_length), outputs_array[:, i], \n",
    "                   marker='^', label=f'Output {i+1}', linewidth=2)\n",
    "axes[0, 2].set_title('Output Sequence')\n",
    "axes[0, 2].set_xlabel('Time Step')\n",
    "axes[0, 2].set_ylabel('Output Value')\n",
    "axes[0, 2].legend()\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Weight matrices visualization\n",
    "im1 = axes[1, 0].imshow(rnn.W_xh, cmap='RdBu', aspect='auto')\n",
    "axes[1, 0].set_title('Input-to-Hidden Weights (W_xh)')\n",
    "axes[1, 0].set_xlabel('Hidden Units')\n",
    "axes[1, 0].set_ylabel('Input Features')\n",
    "plt.colorbar(im1, ax=axes[1, 0])\n",
    "\n",
    "# Plot 5: Recurrent weights\n",
    "im2 = axes[1, 1].imshow(rnn.W_hh, cmap='RdBu', aspect='auto')\n",
    "axes[1, 1].set_title('Hidden-to-Hidden Weights (W_hh)')\n",
    "axes[1, 1].set_xlabel('Hidden Units (t)')\n",
    "axes[1, 1].set_ylabel('Hidden Units (t-1)')\n",
    "plt.colorbar(im2, ax=axes[1, 1])\n",
    "\n",
    "# Plot 6: Information flow diagram\n",
    "axes[1, 2].axis('off')\n",
    "info_text = f\"\"\"\n",
    "RNN INFORMATION FLOW:\n",
    "\n",
    "At each time step t:\n",
    "1. Receive input x_t\n",
    "2. Combine with previous hidden h_{t-1}\n",
    "3. Apply transformation: \n",
    "   h_t = tanh(x_t·W_xh + h_{t-1}·W_hh + b_h)\n",
    "4. Produce output:\n",
    "   y_t = h_t·W_hy + b_y\n",
    "5. Pass h_t to next time step\n",
    "\n",
    "KEY PROPERTIES:\n",
    "• Parameter sharing across time\n",
    "• Sequential processing\n",
    "• Memory through hidden state\n",
    "• Variable sequence lengths\n",
    "\n",
    "LIMITATIONS:\n",
    "• Vanishing gradients\n",
    "• Short-term memory\n",
    "• Sequential bottleneck\n",
    "\"\"\"\n",
    "\n",
    "axes[1, 2].text(0.05, 0.95, info_text, transform=axes[1, 2].transAxes,\n",
    "               verticalalignment='top', fontsize=10, fontfamily='monospace')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n💡 VANILLA RNN KEY INSIGHTS:\")\n",
    "print(f\"\\n1. ARCHITECTURE:\")\n",
    "print(f\"   • Hidden state h_t carries information through time\")\n",
    "print(f\"   • Same parameters used at each time step (parameter sharing)\")\n",
    "print(f\"   • Recurrent connection: h_t depends on h_{t-1}\")\n",
    "\n",
    "print(f\"\\n2. MATHEMATICAL FORMULATION:\")\n",
    "print(f\"   • h_t = tanh(x_t @ W_xh + h_{t-1} @ W_hh + b_h)\")\n",
    "print(f\"   • y_t = h_t @ W_hy + b_y\")\n",
    "print(f\"   • Tanh activation keeps hidden states bounded [-1, 1]\")\n",
    "\n",
    "print(f\"\\n3. MEMORY MECHANISM:\")\n",
    "print(f\"   • Information from early time steps can influence later outputs\")\n",
    "print(f\"   • Memory capacity limited by hidden state size\")\n",
    "print(f\"   • Gradual information decay through time steps\")\n",
    "\n",
    "print(f\"\\n4. LIMITATIONS:\")\n",
    "print(f\"   • Vanishing gradients: Difficulty learning long-term dependencies\")\n",
    "print(f\"   • Sequential processing: Cannot parallelize across time\")\n",
    "print(f\"   • Information bottleneck: All info must pass through hidden state\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Vanishing Gradient Problem: Why Vanilla RNNs Struggle\n",
    "\n",
    "**The Problem:**\n",
    "- **Backpropagation Through Time (BPTT)**: Gradients flow backward through each time step\n",
    "- **Gradient multiplication**: Gradients multiply by weights at each step\n",
    "- **Vanishing**: If weights < 1, gradients → 0 exponentially\n",
    "- **Exploding**: If weights > 1, gradients → ∞ exponentially\n",
    "\n",
    "**Mathematical Analysis:**\n",
    "```\n",
    "∂L/∂h_1 = ∂L/∂h_T * ∏(t=2 to T) ∂h_t/∂h_{t-1}\n",
    "```\n",
    "\n",
    "**Where each term:**\n",
    "```\n",
    "∂h_t/∂h_{t-1} = diag(tanh'(·)) * W_hh\n",
    "```\n",
    "\n",
    "**Impact:**\n",
    "- **Short memory**: Can't learn dependencies > 5-10 time steps\n",
    "- **Training difficulties**: Slow convergence, unstable gradients\n",
    "- **Limited applications**: Poor for long sequences\n",
    "\n",
    "**Solutions:**\n",
    "1. **LSTM/GRU**: Gating mechanisms\n",
    "2. **Gradient clipping**: Prevent exploding gradients\n",
    "3. **Better initialization**: Careful weight initialization\n",
    "4. **Residual connections**: Skip connections through time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Vanishing Gradient Problem Demonstration\n",
    "\n",
    "print(\"=== VANISHING GRADIENT PROBLEM ANALYSIS ===\")\n",
    "\n",
    "def analyze_gradient_flow():\n",
    "    \"\"\"\n",
    "    Demonstrate vanishing gradient problem through gradient magnitude analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n🔍 GRADIENT FLOW ANALYSIS:\")\n",
    "    \n",
    "    # Create RNNs with different weight scales\n",
    "    weight_scales = [0.5, 0.9, 1.0, 1.1, 1.5]\n",
    "    sequence_length = 20\n",
    "    \n",
    "    gradient_magnitudes = {}\n",
    "    \n",
    "    for scale in weight_scales:\n",
    "        print(f\"\\nAnalyzing weight scale: {scale}\")\n",
    "        \n",
    "        # Simulate gradient flow (simplified analysis)\n",
    "        # Gradient at time t flows back through (W_hh)^(T-t) path\n",
    "        \n",
    "        # Assume tanh derivative ≈ 0.5 on average\n",
    "        tanh_derivative = 0.5\n",
    "        \n",
    "        gradients = []\n",
    "        for t in range(sequence_length):\n",
    "            # Gradient magnitude after flowing back t steps\n",
    "            steps_back = sequence_length - 1 - t\n",
    "            gradient_magnitude = (scale * tanh_derivative) ** steps_back\n",
    "            gradients.append(gradient_magnitude)\n",
    "        \n",
    "        gradient_magnitudes[scale] = gradients\n",
    "        \n",
    "        final_gradient = gradients[0]  # Gradient at first time step\n",
    "        print(f\"  Final gradient magnitude: {final_gradient:.6f}\")\n",
    "        \n",
    "        if final_gradient < 1e-5:\n",
    "            print(f\"  ❌ Vanishing gradients detected!\")\n",
    "        elif final_gradient > 1e5:\n",
    "            print(f\"  💥 Exploding gradients detected!\")\n",
    "        else:\n",
    "            print(f\"  ✅ Stable gradients\")\n",
    "    \n",
    "    return gradient_magnitudes\n",
    "\n",
    "def demonstrate_memory_limitations():\n",
    "    \"\"\"\n",
    "    Demonstrate how vanilla RNNs struggle with long-term dependencies\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n🧠 MEMORY LIMITATION DEMONSTRATION:\")\n",
    "    \n",
    "    # Create sequences with dependencies at different distances\n",
    "    def create_memory_task(sequence_length, dependency_distance):\n",
    "        \"\"\"\n",
    "        Create a sequence where early information must be remembered\n",
    "        \"\"\"\n",
    "        # Sequence of mostly zeros with signal at beginning and end\n",
    "        sequence = np.zeros((sequence_length, 2))\n",
    "        \n",
    "        # Important signal at the beginning\n",
    "        sequence[0, 0] = 1.0  # Signal to remember\n",
    "        \n",
    "        # Query at the end\n",
    "        sequence[-1, 1] = 1.0  # Query: \"what was the signal?\"\n",
    "        \n",
    "        # Target: 1 if signal was present, 0 otherwise\n",
    "        target = 1.0\n",
    "        \n",
    "        return sequence, target\n",
    "    \n",
    "    # Test different dependency distances\n",
    "    distances = [5, 10, 20, 50]\n",
    "    \n",
    "    print(f\"Memory task results (simplified analysis):\")\n",
    "    for distance in distances:\n",
    "        # Create task\n",
    "        sequence, target = create_memory_task(distance, distance-1)\n",
    "        \n",
    "        # Estimate gradient strength for this distance\n",
    "        # This is a simplified calculation\n",
    "        gradient_strength = (0.5 * 0.9) ** (distance - 1)  # Rough estimate\n",
    "        \n",
    "        print(f\"  Distance {distance:2d}: Gradient strength ≈ {gradient_strength:.6f}\")\n",
    "        \n",
    "        if gradient_strength < 1e-3:\n",
    "            difficulty = \"Very Hard\"\n",
    "        elif gradient_strength < 1e-2:\n",
    "            difficulty = \"Hard\"\n",
    "        elif gradient_strength < 1e-1:\n",
    "            difficulty = \"Moderate\"\n",
    "        else:\n",
    "            difficulty = \"Easy\"\n",
    "        \n",
    "        print(f\"              Learning difficulty: {difficulty}\")\n",
    "\n",
    "# Analyze gradient flow\n",
    "gradient_data = analyze_gradient_flow()\n",
    "\n",
    "# Demonstrate memory limitations\n",
    "demonstrate_memory_limitations()\n",
    "\n",
    "# Create comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# Plot 1: Gradient flow for different weight scales\n",
    "for scale, gradients in gradient_data.items():\n",
    "    time_steps = range(len(gradients))\n",
    "    axes[0, 0].semilogy(time_steps, gradients, marker='o', \n",
    "                       label=f'Scale {scale}', linewidth=2)\n",
    "\n",
    "axes[0, 0].set_title('Gradient Magnitude vs Time Step')\n",
    "axes[0, 0].set_xlabel('Time Step')\n",
    "axes[0, 0].set_ylabel('Gradient Magnitude (log scale)')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "axes[0, 0].axhline(y=1e-5, color='red', linestyle='--', alpha=0.7, label='Vanishing threshold')\n",
    "\n",
    "# Plot 2: Effect of sequence length on gradient\n",
    "seq_lengths = range(1, 31)\n",
    "stable_gradients = [(0.9 * 0.5) ** (l-1) for l in seq_lengths]\n",
    "vanishing_gradients = [(0.5 * 0.5) ** (l-1) for l in seq_lengths]\n",
    "exploding_gradients = [(1.5 * 0.5) ** (l-1) for l in seq_lengths]\n",
    "\n",
    "axes[0, 1].semilogy(seq_lengths, stable_gradients, 'g-', label='Stable (0.9)', linewidth=2)\n",
    "axes[0, 1].semilogy(seq_lengths, vanishing_gradients, 'r-', label='Vanishing (0.5)', linewidth=2)\n",
    "axes[0, 1].semilogy(seq_lengths[:15], exploding_gradients[:15], 'b-', label='Exploding (1.5)', linewidth=2)\n",
    "\n",
    "axes[0, 1].set_title('Gradient vs Sequence Length')\n",
    "axes[0, 1].set_xlabel('Sequence Length')\n",
    "axes[0, 1].set_ylabel('Final Gradient Magnitude')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Memory task difficulty\n",
    "distances = [5, 10, 15, 20, 25, 30]\n",
    "difficulties = [(0.5 * 0.9) ** (d-1) for d in distances]\n",
    "\n",
    "axes[0, 2].semilogy(distances, difficulties, 'ro-', linewidth=2, markersize=8)\n",
    "axes[0, 2].set_title('Memory Task Difficulty')\n",
    "axes[0, 2].set_xlabel('Dependency Distance')\n",
    "axes[0, 2].set_ylabel('Learning Signal Strength')\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "axes[0, 2].axhline(y=1e-3, color='red', linestyle='--', alpha=0.7, label='Difficulty threshold')\n",
    "axes[0, 2].legend()\n",
    "\n",
    "# Plot 4: Activation function derivatives\n",
    "x = np.linspace(-3, 3, 100)\n",
    "tanh_vals = np.tanh(x)\n",
    "tanh_derivs = 1 - tanh_vals**2\n",
    "\n",
    "axes[1, 0].plot(x, tanh_vals, 'b-', linewidth=2, label='tanh(x)')\n",
    "axes[1, 0].plot(x, tanh_derivs, 'r--', linewidth=2, label=\"tanh'(x)\")\n",
    "axes[1, 0].set_title('Tanh and its Derivative')\n",
    "axes[1, 0].set_xlabel('x')\n",
    "axes[1, 0].set_ylabel('Value')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "axes[1, 0].axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "\n",
    "# Plot 5: Weight initialization impact\n",
    "init_methods = ['Small\\n(0.1)', 'Xavier\\n(√2/n)', 'Large\\n(2.0)']\n",
    "gradient_survival = [0.01, 0.1, 10.0]  # Rough estimates\n",
    "\n",
    "bars = axes[1, 1].bar(init_methods, gradient_survival, \n",
    "                     color=['lightcoral', 'lightgreen', 'gold'], alpha=0.7)\n",
    "axes[1, 1].set_title('Weight Initialization Impact')\n",
    "axes[1, 1].set_ylabel('Gradient Survival Rate')\n",
    "axes[1, 1].set_yscale('log')\n",
    "\n",
    "# Add value labels\n",
    "for bar, val in zip(bars, gradient_survival):\n",
    "    axes[1, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height(),\n",
    "                   f'{val}', ha='center', va='bottom')\n",
    "\n",
    "# Plot 6: Solutions overview\n",
    "axes[1, 2].axis('off')\n",
    "solutions_text = \"\"\"\n",
    "VANISHING GRADIENT SOLUTIONS:\n",
    "\n",
    "1. LSTM/GRU Networks:\n",
    "   • Gating mechanisms\n",
    "   • Selective information flow\n",
    "   • Constant error carousel\n",
    "\n",
    "2. Gradient Clipping:\n",
    "   • Prevent exploding gradients\n",
    "   • Clip norm to threshold\n",
    "   • Stable training\n",
    "\n",
    "3. Better Initialization:\n",
    "   • Xavier/He initialization\n",
    "   • Identity matrix for W_hh\n",
    "   • Orthogonal initialization\n",
    "\n",
    "4. Residual Connections:\n",
    "   • Skip connections through time\n",
    "   • Highway networks\n",
    "   • Direct gradient paths\n",
    "\n",
    "5. Alternative Architectures:\n",
    "   • Transformers (attention)\n",
    "   • Temporal convolutional networks\n",
    "   • State space models\n",
    "\"\"\"\n",
    "\n",
    "axes[1, 2].text(0.05, 0.95, solutions_text, transform=axes[1, 2].transAxes,\n",
    "               verticalalignment='top', fontsize=10, fontfamily='monospace')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Quantitative analysis\n",
    "print(f\"\\n📊 QUANTITATIVE ANALYSIS:\")\n",
    "\n",
    "print(f\"\\nGradient decay rates:\")\n",
    "for scale in [0.5, 0.9, 1.0, 1.1]:\n",
    "    # Effective learning rate after T steps\n",
    "    effective_rate_5 = (scale * 0.5) ** 4   # After 5 steps\n",
    "    effective_rate_20 = (scale * 0.5) ** 19 # After 20 steps\n",
    "    \n",
    "    print(f\"  Weight scale {scale}:\")\n",
    "    print(f\"    After 5 steps:  {effective_rate_5:.6f} (decay factor)\")\n",
    "    print(f\"    After 20 steps: {effective_rate_20:.6f} (decay factor)\")\n",
    "\n",
    "print(f\"\\nMemory capacity estimates:\")\n",
    "threshold = 1e-3  # Minimum gradient for effective learning\n",
    "for scale in [0.5, 0.9, 1.0]:\n",
    "    # Calculate maximum effective sequence length\n",
    "    if scale * 0.5 < 1.0:\n",
    "        max_length = int(np.log(threshold) / np.log(scale * 0.5))\n",
    "        print(f\"  Weight scale {scale}: ~{max_length} time steps\")\n",
    "    else:\n",
    "        print(f\"  Weight scale {scale}: Unstable (exploding gradients)\")\n",
    "\n",
    "print(f\"\\n💡 VANISHING GRADIENT KEY INSIGHTS:\")\n",
    "print(f\"\\n1. MATHEMATICAL ROOT CAUSE:\")\n",
    "print(f\"   • Gradient = product of derivatives through time\")\n",
    "print(f\"   • Each derivative ≤ 1 (tanh) → product → 0\")\n",
    "print(f\"   • Exponential decay: (factor)^T\")\n",
    "\n",
    "print(f\"\\n2. PRACTICAL IMPLICATIONS:\")\n",
    "print(f\"   • Vanilla RNNs: ~5-10 time step memory\")\n",
    "print(f\"   • Long sequences: Early information lost\")\n",
    "print(f\"   • Training: Slow convergence on long dependencies\")\n",
    "\n",
    "print(f\"\\n3. WHY IT MATTERS:\")\n",
    "print(f\"   • Language: Long-range grammatical dependencies\")\n",
    "print(f\"   • Time series: Seasonal patterns (yearly cycles)\")\n",
    "print(f\"   • Speech: Sentence-level context\")\n",
    "\n",
    "print(f\"\\n4. SOLUTION PREVIEW:\")\n",
    "print(f\"   • LSTM: Gated memory cells\")\n",
    "print(f\"   • GRU: Simplified gating\")\n",
    "print(f\"   • Attention: Direct connections to all time steps\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}