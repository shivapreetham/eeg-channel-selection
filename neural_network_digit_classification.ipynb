{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Digit Classification - Complete Parameter Exploration\n",
    "\n",
    "This notebook explores neural networks for MNIST digit classification with:\n",
    "1. **Scikit-learn MLPClassifier** with extensive parameter tuning\n",
    "2. **Pure Python implementation** from scratch for deep understanding\n",
    "3. **Comprehensive comparison** of approaches and parameters\n",
    "\n",
    "## Neural Network Fundamentals\n",
    "\n",
    "### Key Components:\n",
    "- **Neurons**: Basic processing units\n",
    "- **Layers**: Input, Hidden, Output layers\n",
    "- **Weights & Biases**: Learnable parameters\n",
    "- **Activation Functions**: Non-linear transformations\n",
    "- **Backpropagation**: Learning algorithm\n",
    "\n",
    "### Parameters to Explore:\n",
    "- **Architecture**: Number of hidden layers and neurons\n",
    "- **Activation Functions**: ReLU, Sigmoid, Tanh\n",
    "- **Solvers**: SGD, Adam, L-BFGS\n",
    "- **Learning Rate**: How fast the model learns\n",
    "- **Regularization**: Preventing overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n",
      "Loading MNIST dataset...\n",
      "Dataset loaded: 15000 samples\n",
      "Training: 12000, Testing: 3000\n",
      "Features: 784 (28x28 pixels)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "\n",
    "# Load MNIST dataset\n",
    "print(\"Loading MNIST dataset...\")\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False, parser='auto')\n",
    "X, y = mnist.data, mnist.target.astype(int)\n",
    "\n",
    "# Use subset for faster experimentation\n",
    "subset_size = 15000  # Larger subset for neural networks\n",
    "X_subset = X[:subset_size]\n",
    "y_subset = y[:subset_size]\n",
    "\n",
    "# Normalize data\n",
    "X_normalized = X_subset / 255.0\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_normalized, y_subset, test_size=0.2, random_state=42, stratify=y_subset\n",
    ")\n",
    "\n",
    "print(f\"Dataset loaded: {X_subset.shape[0]} samples\")\n",
    "print(f\"Training: {X_train.shape[0]}, Testing: {X_test.shape[0]}\")\n",
    "print(f\"Features: {X_train.shape[1]} (28x28 pixels)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Neural Network Architecture Exploration\n",
    "\n",
    "Let's explore different neural network architectures and parameters systematically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural network training function defined!\n"
     ]
    }
   ],
   "source": [
    "# Function to train and evaluate neural network\n",
    "def train_and_evaluate_nn(hidden_layer_sizes, activation='relu', solver='adam', \n",
    "                         learning_rate_init=0.001, max_iter=500, alpha=0.0001):\n",
    "    \"\"\"\n",
    "    Train neural network with given parameters and return results\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Create and train model\n",
    "    mlp = MLPClassifier(\n",
    "        hidden_layer_sizes=hidden_layer_sizes,\n",
    "        activation=activation,\n",
    "        solver=solver,\n",
    "        learning_rate_init=learning_rate_init,\n",
    "        max_iter=max_iter,\n",
    "        alpha=alpha,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    mlp.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = mlp.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    return {\n",
    "        'model': mlp,\n",
    "        'accuracy': accuracy,\n",
    "        'training_time': training_time,\n",
    "        'iterations': mlp.n_iter_,\n",
    "        'loss': mlp.loss_\n",
    "    }\n",
    "\n",
    "print(\"Neural network training function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Architecture Comparison\n",
    "\n",
    "Let's test different network architectures to understand the impact of depth and width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing different neural network architectures...\n",
      "\n",
      "Training Single Hidden (50): (50,)\n",
      "  Accuracy: 0.9513 (95.13%)\n",
      "  Training Time: 54.28s\n",
      "  Iterations: 118\n",
      "\n",
      "Training Single Hidden (100): (100,)\n",
      "  Accuracy: 0.9570 (95.70%)\n",
      "  Training Time: 64.24s\n",
      "  Iterations: 87\n",
      "\n",
      "Training Single Hidden (200): (200,)\n",
      "  Accuracy: 0.9600 (96.00%)\n",
      "  Training Time: 117.58s\n",
      "  Iterations: 63\n",
      "\n",
      "Training Two Hidden (100,50): (100, 50)\n",
      "  Accuracy: 0.9613 (96.13%)\n",
      "  Training Time: 51.06s\n",
      "  Iterations: 60\n",
      "\n",
      "Training Two Hidden (200,100): (200, 100)\n",
      "  Accuracy: 0.9627 (96.27%)\n",
      "  Training Time: 80.21s\n",
      "  Iterations: 41\n",
      "\n",
      "Training Three Hidden (100,50,25): (100, 50, 25)\n",
      "  Accuracy: 0.9617 (96.17%)\n",
      "  Training Time: 49.87s\n",
      "  Iterations: 51\n",
      "\n",
      "Training Deep Network (200,100,50,25): (200, 100, 50, 25)\n",
      "  Accuracy: 0.9643 (96.43%)\n",
      "  Training Time: 83.14s\n",
      "  Iterations: 35\n",
      "\n",
      "Training Wide Network (300,300): (300, 300)\n",
      "  Accuracy: 0.9657 (96.57%)\n",
      "  Training Time: 136.12s\n",
      "  Iterations: 33\n",
      "\n",
      "\n",
      "=== ARCHITECTURE COMPARISON ===\n",
      "\n",
      "                   Architecture       Hidden Layers  Accuracy  Training Time  \\\n",
      "0            Single Hidden (50)               (50,)    0.9513        54.2772   \n",
      "1           Single Hidden (100)              (100,)    0.9570        64.2388   \n",
      "2           Single Hidden (200)              (200,)    0.9600       117.5837   \n",
      "3           Two Hidden (100,50)           (100, 50)    0.9613        51.0593   \n",
      "4          Two Hidden (200,100)          (200, 100)    0.9627        80.2094   \n",
      "5      Three Hidden (100,50,25)       (100, 50, 25)    0.9617        49.8661   \n",
      "6  Deep Network (200,100,50,25)  (200, 100, 50, 25)    0.9643        83.1391   \n",
      "7        Wide Network (300,300)          (300, 300)    0.9657       136.1176   \n",
      "\n",
      "   Iterations  Final Loss  \n",
      "0         118      0.0016  \n",
      "1          87      0.0014  \n",
      "2          63      0.0013  \n",
      "3          60      0.0008  \n",
      "4          41      0.0007  \n",
      "5          51      0.0007  \n",
      "6          35      0.0005  \n",
      "7          33      0.0006  \n"
     ]
    }
   ],
   "source": [
    "# Different architectures to test\n",
    "architectures = {\n",
    "    'Single Hidden (50)': (50,),\n",
    "    'Single Hidden (100)': (100,),\n",
    "    'Single Hidden (200)': (200,),\n",
    "    'Two Hidden (100,50)': (100, 50),\n",
    "    'Two Hidden (200,100)': (200, 100),\n",
    "    'Three Hidden (100,50,25)': (100, 50, 25),\n",
    "    'Deep Network (200,100,50,25)': (200, 100, 50, 25),\n",
    "    'Wide Network (300,300)': (300, 300)\n",
    "}\n",
    "\n",
    "print(\"Testing different neural network architectures...\\n\")\n",
    "\n",
    "architecture_results = []\n",
    "\n",
    "for name, hidden_layers in architectures.items():\n",
    "    print(f\"Training {name}: {hidden_layers}\")\n",
    "    \n",
    "    result = train_and_evaluate_nn(\n",
    "        hidden_layer_sizes=hidden_layers,\n",
    "        max_iter=300  # Reduce iterations for faster comparison\n",
    "    )\n",
    "    \n",
    "    architecture_results.append({\n",
    "        'Architecture': name,\n",
    "        'Hidden Layers': str(hidden_layers),\n",
    "        'Accuracy': result['accuracy'],\n",
    "        'Training Time': result['training_time'],\n",
    "        'Iterations': result['iterations'],\n",
    "        'Final Loss': result['loss']\n",
    "    })\n",
    "    \n",
    "    print(f\"  Accuracy: {result['accuracy']:.4f} ({result['accuracy']*100:.2f}%)\")\n",
    "    print(f\"  Training Time: {result['training_time']:.2f}s\")\n",
    "    print(f\"  Iterations: {result['iterations']}\\n\")\n",
    "\n",
    "# Create results DataFrame\n",
    "arch_df = pd.DataFrame(architecture_results)\n",
    "print(\"\\n=== ARCHITECTURE COMPARISON ===\\n\")\n",
    "print(arch_df.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Activation Function Comparison\n",
    "\n",
    "Different activation functions have different properties and performance characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing different activation functions...\n",
      "\n",
      "Training with relu activation...\n",
      "  Accuracy: 0.9613\n",
      "  Training Time: 61.21s\n",
      "\n",
      "Training with tanh activation...\n",
      "  Accuracy: 0.9587\n",
      "  Training Time: 65.45s\n",
      "\n",
      "Training with logistic activation...\n",
      "  Accuracy: 0.9567\n",
      "  Training Time: 132.47s\n",
      "\n",
      "\n",
      "=== ACTIVATION FUNCTION COMPARISON ===\n",
      "\n",
      "  Activation  Accuracy  Training Time  Iterations\n",
      "0       relu    0.9613        61.2053          60\n",
      "1       tanh    0.9587        65.4457          66\n",
      "2   logistic    0.9567       132.4698         107\n"
     ]
    }
   ],
   "source": [
    "# Test different activation functions\n",
    "activations = ['relu', 'tanh', 'logistic']\n",
    "activation_results = []\n",
    "\n",
    "print(\"Testing different activation functions...\\n\")\n",
    "\n",
    "for activation in activations:\n",
    "    print(f\"Training with {activation} activation...\")\n",
    "    \n",
    "    result = train_and_evaluate_nn(\n",
    "        hidden_layer_sizes=(100, 50),\n",
    "        activation=activation,\n",
    "        max_iter=300\n",
    "    )\n",
    "    \n",
    "    activation_results.append({\n",
    "        'Activation': activation,\n",
    "        'Accuracy': result['accuracy'],\n",
    "        'Training Time': result['training_time'],\n",
    "        'Iterations': result['iterations']\n",
    "    })\n",
    "    \n",
    "    print(f\"  Accuracy: {result['accuracy']:.4f}\")\n",
    "    print(f\"  Training Time: {result['training_time']:.2f}s\\n\")\n",
    "\n",
    "activation_df = pd.DataFrame(activation_results)\n",
    "print(\"\\n=== ACTIVATION FUNCTION COMPARISON ===\\n\")\n",
    "print(activation_df.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Solver Comparison\n",
    "\n",
    "Different optimization algorithms (solvers) can significantly impact training speed and final performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing different optimization solvers...\n",
      "\n",
      "Training with adam solver...\n",
      "  Accuracy: 0.9613\n",
      "  Training Time: 57.39s\n",
      "\n",
      "Training with sgd solver...\n",
      "  Accuracy: 0.9547\n",
      "  Training Time: 105.94s\n",
      "\n",
      "Training with lbfgs solver...\n",
      "  Accuracy: 0.9573\n",
      "  Training Time: 31.42s\n",
      "\n",
      "\n",
      "=== SOLVER COMPARISON ===\n",
      "\n",
      "  Solver  Accuracy  Training Time  Iterations\n",
      "0   adam    0.9613        57.3893          60\n",
      "1    sgd    0.9547       105.9406         125\n",
      "2  lbfgs    0.9573        31.4166         127\n"
     ]
    }
   ],
   "source": [
    "# Test different solvers\n",
    "solvers = ['adam', 'sgd', 'lbfgs']\n",
    "solver_results = []\n",
    "\n",
    "print(\"Testing different optimization solvers...\\n\")\n",
    "\n",
    "for solver in solvers:\n",
    "    print(f\"Training with {solver} solver...\")\n",
    "    \n",
    "    # Adjust parameters based on solver\n",
    "    max_iter = 1000 if solver == 'lbfgs' else 300\n",
    "    learning_rate = 0.01 if solver == 'sgd' else 0.001\n",
    "    \n",
    "    result = train_and_evaluate_nn(\n",
    "        hidden_layer_sizes=(100, 50),\n",
    "        solver=solver,\n",
    "        learning_rate_init=learning_rate,\n",
    "        max_iter=max_iter\n",
    "    )\n",
    "    \n",
    "    solver_results.append({\n",
    "        'Solver': solver,\n",
    "        'Accuracy': result['accuracy'],\n",
    "        'Training Time': result['training_time'],\n",
    "        'Iterations': result['iterations']\n",
    "    })\n",
    "    \n",
    "    print(f\"  Accuracy: {result['accuracy']:.4f}\")\n",
    "    print(f\"  Training Time: {result['training_time']:.2f}s\\n\")\n",
    "\n",
    "solver_df = pd.DataFrame(solver_results)\n",
    "print(\"\\n=== SOLVER COMPARISON ===\\n\")\n",
    "print(solver_df.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Learning Rate Impact\n",
    "\n",
    "Learning rate is crucial for neural network training - too high and it won't converge, too low and it trains slowly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing different learning rates...\n",
      "\n",
      "Training with learning rate 0.1...\n",
      "  Accuracy: 0.8370\n",
      "  Training Time: 27.49s\n",
      "\n",
      "Training with learning rate 0.01...\n",
      "  Accuracy: 0.9573\n",
      "  Training Time: 27.25s\n",
      "\n",
      "Training with learning rate 0.001...\n",
      "  Accuracy: 0.9613\n",
      "  Training Time: 54.78s\n",
      "\n",
      "Training with learning rate 0.0001...\n",
      "  Accuracy: 0.9580\n",
      "  Training Time: 204.72s\n",
      "\n",
      "\n",
      "=== LEARNING RATE COMPARISON ===\n",
      "\n",
      "   Learning Rate  Accuracy  Training Time  Iterations\n",
      "0         0.1000    0.8370        27.4926          32\n",
      "1         0.0100    0.9573        27.2531          30\n",
      "2         0.0010    0.9613        54.7794          60\n",
      "3         0.0001    0.9580       204.7228         224\n"
     ]
    }
   ],
   "source": [
    "# Test different learning rates\n",
    "learning_rates = [0.1, 0.01, 0.001, 0.0001]\n",
    "lr_results = []\n",
    "\n",
    "print(\"Testing different learning rates...\\n\")\n",
    "\n",
    "for lr in learning_rates:\n",
    "    print(f\"Training with learning rate {lr}...\")\n",
    "    \n",
    "    result = train_and_evaluate_nn(\n",
    "        hidden_layer_sizes=(100, 50),\n",
    "        learning_rate_init=lr,\n",
    "        max_iter=300\n",
    "    )\n",
    "    \n",
    "    lr_results.append({\n",
    "        'Learning Rate': lr,\n",
    "        'Accuracy': result['accuracy'],\n",
    "        'Training Time': result['training_time'],\n",
    "        'Iterations': result['iterations']\n",
    "    })\n",
    "    \n",
    "    print(f\"  Accuracy: {result['accuracy']:.4f}\")\n",
    "    print(f\"  Training Time: {result['training_time']:.2f}s\\n\")\n",
    "\n",
    "lr_df = pd.DataFrame(lr_results)\n",
    "print(\"\\n=== LEARNING RATE COMPARISON ===\\n\")\n",
    "print(lr_df.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Regularization Impact\n",
    "\n",
    "Alpha parameter controls L2 regularization to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing different regularization strengths...\n",
      "\n",
      "Training with alpha (regularization) 0.0001...\n",
      "  Accuracy: 0.9613\n",
      "  Training Time: 57.17s\n",
      "\n",
      "Training with alpha (regularization) 0.001...\n",
      "  Accuracy: 0.9617\n",
      "  Training Time: 56.56s\n",
      "\n",
      "Training with alpha (regularization) 0.01...\n",
      "  Accuracy: 0.9637\n",
      "  Training Time: 78.81s\n",
      "\n",
      "Training with alpha (regularization) 0.1...\n",
      "  Accuracy: 0.9643\n",
      "  Training Time: 84.73s\n",
      "\n",
      "\n",
      "=== REGULARIZATION COMPARISON ===\n",
      "\n",
      "   Alpha (Regularization)  Accuracy  Training Time  Iterations\n",
      "0                  0.0001    0.9613        57.1720          60\n",
      "1                  0.0010    0.9617        56.5623          57\n",
      "2                  0.0100    0.9637        78.8118          87\n",
      "3                  0.1000    0.9643        84.7314          94\n"
     ]
    }
   ],
   "source": [
    "# Test different regularization strengths\n",
    "alphas = [0.0001, 0.001, 0.01, 0.1]\n",
    "reg_results = []\n",
    "\n",
    "print(\"Testing different regularization strengths...\\n\")\n",
    "\n",
    "for alpha in alphas:\n",
    "    print(f\"Training with alpha (regularization) {alpha}...\")\n",
    "    \n",
    "    result = train_and_evaluate_nn(\n",
    "        hidden_layer_sizes=(100, 50),\n",
    "        alpha=alpha,\n",
    "        max_iter=300\n",
    "    )\n",
    "    \n",
    "    reg_results.append({\n",
    "        'Alpha (Regularization)': alpha,\n",
    "        'Accuracy': result['accuracy'],\n",
    "        'Training Time': result['training_time'],\n",
    "        'Iterations': result['iterations']\n",
    "    })\n",
    "    \n",
    "    print(f\"  Accuracy: {result['accuracy']:.4f}\")\n",
    "    print(f\"  Training Time: {result['training_time']:.2f}s\\n\")\n",
    "\n",
    "reg_df = pd.DataFrame(reg_results)\n",
    "print(\"\\n=== REGULARIZATION COMPARISON ===\\n\")\n",
    "print(reg_df.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Best Model Training\n",
    "\n",
    "Based on our experiments, let's train the best performing model with more iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best architecture: Wide Network (300,300) with accuracy 0.9657\n",
      "\n",
      "Training best model with extended iterations...\n",
      "\n",
      "Best Model Performance:\n",
      "Accuracy: 0.9627 (96.27%)\n",
      "Training Time: 91.35s\n",
      "Iterations: 41\n",
      "\n",
      "Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       299\n",
      "           1       0.98      0.99      0.98       338\n",
      "           2       0.97      0.95      0.96       292\n",
      "           3       0.94      0.93      0.93       309\n",
      "           4       0.96      0.97      0.96       294\n",
      "           5       0.96      0.95      0.95       264\n",
      "           6       0.96      0.98      0.97       298\n",
      "           7       0.97      0.96      0.96       319\n",
      "           8       0.96      0.94      0.95       286\n",
      "           9       0.97      0.95      0.96       301\n",
      "\n",
      "    accuracy                           0.96      3000\n",
      "   macro avg       0.96      0.96      0.96      3000\n",
      "weighted avg       0.96      0.96      0.96      3000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find best architecture from our tests\n",
    "best_arch = arch_df.loc[arch_df['Accuracy'].idxmax()]\n",
    "print(f\"Best architecture: {best_arch['Architecture']} with accuracy {best_arch['Accuracy']:.4f}\")\n",
    "\n",
    "# Train best model with more iterations\n",
    "print(\"\\nTraining best model with extended iterations...\")\n",
    "\n",
    "best_model_result = train_and_evaluate_nn(\n",
    "    hidden_layer_sizes=(200, 100),  # Generally good performing architecture\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    learning_rate_init=0.001,\n",
    "    max_iter=1000,\n",
    "    alpha=0.0001\n",
    ")\n",
    "\n",
    "best_model = best_model_result['model']\n",
    "best_accuracy = best_model_result['accuracy']\n",
    "\n",
    "print(f\"\\nBest Model Performance:\")\n",
    "print(f\"Accuracy: {best_accuracy:.4f} ({best_accuracy*100:.2f}%)\")\n",
    "print(f\"Training Time: {best_model_result['training_time']:.2f}s\")\n",
    "print(f\"Iterations: {best_model_result['iterations']}\")\n",
    "\n",
    "# Detailed evaluation\n",
    "y_pred_best = best_model.predict(X_test)\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_best))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Neural Network from Scratch - Pure Python Implementation\n",
    "\n",
    "Now let's implement a neural network from scratch to understand the underlying mathematics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network from scratch implemented!\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetworkFromScratch:\n",
    "    def __init__(self, layers, learning_rate=0.01, activation='relu'):\n",
    "        \"\"\"\n",
    "        Initialize neural network with given architecture\n",
    "        \n",
    "        Args:\n",
    "            layers: List of layer sizes [input_size, hidden1, hidden2, ..., output_size]\n",
    "            learning_rate: Learning rate for gradient descent\n",
    "            activation: Activation function ('relu', 'sigmoid', 'tanh')\n",
    "        \"\"\"\n",
    "        self.layers = layers\n",
    "        self.learning_rate = learning_rate\n",
    "        self.activation = activation\n",
    "        \n",
    "        # Initialize weights and biases\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        \n",
    "        for i in range(len(layers) - 1):\n",
    "            # Xavier initialization\n",
    "            w = np.random.randn(layers[i], layers[i+1]) * np.sqrt(2.0 / layers[i])\n",
    "            b = np.zeros((1, layers[i+1]))\n",
    "            \n",
    "            self.weights.append(w)\n",
    "            self.biases.append(b)\n",
    "    \n",
    "    def relu(self, z):\n",
    "        \"\"\"ReLU activation function\"\"\"\n",
    "        return np.maximum(0, z)\n",
    "    \n",
    "    def relu_derivative(self, z):\n",
    "        \"\"\"Derivative of ReLU\"\"\"\n",
    "        return (z > 0).astype(float)\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        \"\"\"Sigmoid activation function\"\"\"\n",
    "        # Clip z to prevent overflow\n",
    "        z = np.clip(z, -500, 500)\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def sigmoid_derivative(self, z):\n",
    "        \"\"\"Derivative of sigmoid\"\"\"\n",
    "        s = self.sigmoid(z)\n",
    "        return s * (1 - s)\n",
    "    \n",
    "    def tanh(self, z):\n",
    "        \"\"\"Tanh activation function\"\"\"\n",
    "        return np.tanh(z)\n",
    "    \n",
    "    def tanh_derivative(self, z):\n",
    "        \"\"\"Derivative of tanh\"\"\"\n",
    "        return 1 - np.tanh(z)**2\n",
    "    \n",
    "    def softmax(self, z):\n",
    "        \"\"\"Softmax activation for output layer\"\"\"\n",
    "        # Subtract max for numerical stability\n",
    "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "    \n",
    "    def apply_activation(self, z, derivative=False):\n",
    "        \"\"\"Apply chosen activation function\"\"\"\n",
    "        if self.activation == 'relu':\n",
    "            return self.relu_derivative(z) if derivative else self.relu(z)\n",
    "        elif self.activation == 'sigmoid':\n",
    "            return self.sigmoid_derivative(z) if derivative else self.sigmoid(z)\n",
    "        elif self.activation == 'tanh':\n",
    "            return self.tanh_derivative(z) if derivative else self.tanh(z)\n",
    "    \n",
    "    def forward_propagation(self, X):\n",
    "        \"\"\"Forward pass through the network\"\"\"\n",
    "        self.z_values = []  # Store z values for backprop\n",
    "        self.activations = [X]  # Store activations\n",
    "        \n",
    "        current_input = X\n",
    "        \n",
    "        # Forward through hidden layers\n",
    "        for i in range(len(self.weights) - 1):\n",
    "            z = np.dot(current_input, self.weights[i]) + self.biases[i]\n",
    "            self.z_values.append(z)\n",
    "            \n",
    "            a = self.apply_activation(z)\n",
    "            self.activations.append(a)\n",
    "            current_input = a\n",
    "        \n",
    "        # Output layer (softmax)\n",
    "        z_output = np.dot(current_input, self.weights[-1]) + self.biases[-1]\n",
    "        self.z_values.append(z_output)\n",
    "        \n",
    "        output = self.softmax(z_output)\n",
    "        self.activations.append(output)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def backward_propagation(self, X, y, output):\n",
    "        \"\"\"Backward pass to compute gradients\"\"\"\n",
    "        m = X.shape[0]  # Number of samples\n",
    "        \n",
    "        # Convert y to one-hot encoding\n",
    "        y_one_hot = np.eye(10)[y]\n",
    "        \n",
    "        # Compute gradients\n",
    "        dw = []\n",
    "        db = []\n",
    "        \n",
    "        # Output layer error\n",
    "        dz = output - y_one_hot\n",
    "        \n",
    "        # Backpropagate through all layers\n",
    "        for i in range(len(self.weights) - 1, -1, -1):\n",
    "            # Gradient for weights and biases\n",
    "            dw_i = np.dot(self.activations[i].T, dz) / m\n",
    "            db_i = np.sum(dz, axis=0, keepdims=True) / m\n",
    "            \n",
    "            dw.insert(0, dw_i)\n",
    "            db.insert(0, db_i)\n",
    "            \n",
    "            # Compute error for previous layer (if not input layer)\n",
    "            if i > 0:\n",
    "                dz = np.dot(dz, self.weights[i].T) * self.apply_activation(self.z_values[i-1], derivative=True)\n",
    "        \n",
    "        return dw, db\n",
    "    \n",
    "    def update_parameters(self, dw, db):\n",
    "        \"\"\"Update weights and biases using gradients\"\"\"\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] -= self.learning_rate * dw[i]\n",
    "            self.biases[i] -= self.learning_rate * db[i]\n",
    "    \n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        \"\"\"Compute cross-entropy loss\"\"\"\n",
    "        m = y_true.shape[0]\n",
    "        y_one_hot = np.eye(10)[y_true]\n",
    "        \n",
    "        # Clip predictions to prevent log(0)\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-12, 1 - 1e-12)\n",
    "        \n",
    "        loss = -np.sum(y_one_hot * np.log(y_pred_clipped)) / m\n",
    "        return loss\n",
    "    \n",
    "    def train(self, X, y, epochs=100, batch_size=32, verbose=True):\n",
    "        \"\"\"Train the neural network\"\"\"\n",
    "        losses = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Shuffle data\n",
    "            indices = np.random.permutation(X.shape[0])\n",
    "            X_shuffled = X[indices]\n",
    "            y_shuffled = y[indices]\n",
    "            \n",
    "            epoch_loss = 0\n",
    "            num_batches = 0\n",
    "            \n",
    "            # Mini-batch training\n",
    "            for i in range(0, X.shape[0], batch_size):\n",
    "                X_batch = X_shuffled[i:i+batch_size]\n",
    "                y_batch = y_shuffled[i:i+batch_size]\n",
    "                \n",
    "                # Forward and backward pass\n",
    "                output = self.forward_propagation(X_batch)\n",
    "                dw, db = self.backward_propagation(X_batch, y_batch, output)\n",
    "                self.update_parameters(dw, db)\n",
    "                \n",
    "                # Compute loss\n",
    "                batch_loss = self.compute_loss(y_batch, output)\n",
    "                epoch_loss += batch_loss\n",
    "                num_batches += 1\n",
    "            \n",
    "            avg_loss = epoch_loss / num_batches\n",
    "            losses.append(avg_loss)\n",
    "            \n",
    "            if verbose and epoch % 20 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        return losses\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions\"\"\"\n",
    "        output = self.forward_propagation(X)\n",
    "        return np.argmax(output, axis=1)\n",
    "    \n",
    "    def accuracy(self, X, y):\n",
    "        \"\"\"Compute accuracy\"\"\"\n",
    "        predictions = self.predict(X)\n",
    "        return np.mean(predictions == y)\n",
    "\n",
    "print(\"Neural Network from scratch implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Training Custom Neural Network\n",
    "\n",
    "Let's train our custom neural network and compare it with scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training custom neural networks from scratch...\n",
      "\n",
      "Training Simple architecture: [784, 50, 10]\n",
      "  Train Accuracy: 0.9536\n",
      "  Test Accuracy: 0.9230\n",
      "  Training Time: 29.93s\n",
      "\n",
      "Training Medium architecture: [784, 100, 50, 10]\n",
      "  Train Accuracy: 0.9856\n",
      "  Test Accuracy: 0.9260\n",
      "  Training Time: 53.30s\n",
      "\n",
      "Training Deep architecture: [784, 200, 100, 50, 10]\n",
      "  Train Accuracy: 0.9982\n",
      "  Test Accuracy: 0.9300\n",
      "  Training Time: 112.06s\n",
      "\n",
      "\n",
      "=== CUSTOM NEURAL NETWORK RESULTS ===\n",
      "\n",
      "  Architecture                   Layers  Train Accuracy  Test Accuracy  \\\n",
      "0       Simple            [784, 50, 10]          0.9536          0.923   \n",
      "1       Medium       [784, 100, 50, 10]          0.9856          0.926   \n",
      "2         Deep  [784, 200, 100, 50, 10]          0.9982          0.930   \n",
      "\n",
      "   Training Time  Final Loss  \n",
      "0        29.9312      0.1755  \n",
      "1        53.3006      0.0863  \n",
      "2       112.0557      0.0254  \n"
     ]
    }
   ],
   "source": [
    "# Test different architectures with our custom implementation\n",
    "custom_architectures = {\n",
    "    'Simple': [784, 50, 10],\n",
    "    'Medium': [784, 100, 50, 10],\n",
    "    'Deep': [784, 200, 100, 50, 10]\n",
    "}\n",
    "\n",
    "custom_results = []\n",
    "\n",
    "print(\"Training custom neural networks from scratch...\\n\")\n",
    "\n",
    "# Use smaller subset for custom implementation (it's slower)\n",
    "X_custom = X_train[:5000]\n",
    "y_custom = y_train[:5000]\n",
    "X_test_custom = X_test[:1000]\n",
    "y_test_custom = y_test[:1000]\n",
    "\n",
    "for name, architecture in custom_architectures.items():\n",
    "    print(f\"Training {name} architecture: {architecture}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Create and train custom network\n",
    "    nn = NeuralNetworkFromScratch(\n",
    "        layers=architecture,\n",
    "        learning_rate=0.01,\n",
    "        activation='relu'\n",
    "    )\n",
    "    \n",
    "    losses = nn.train(X_custom, y_custom, epochs=100, batch_size=64, verbose=False)\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Evaluate\n",
    "    train_accuracy = nn.accuracy(X_custom, y_custom)\n",
    "    test_accuracy = nn.accuracy(X_test_custom, y_test_custom)\n",
    "    \n",
    "    custom_results.append({\n",
    "        'Architecture': name,\n",
    "        'Layers': str(architecture),\n",
    "        'Train Accuracy': train_accuracy,\n",
    "        'Test Accuracy': test_accuracy,\n",
    "        'Training Time': training_time,\n",
    "        'Final Loss': losses[-1]\n",
    "    })\n",
    "    \n",
    "    print(f\"  Train Accuracy: {train_accuracy:.4f}\")\n",
    "    print(f\"  Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"  Training Time: {training_time:.2f}s\\n\")\n",
    "\n",
    "custom_df = pd.DataFrame(custom_results)\n",
    "print(\"\\n=== CUSTOM NEURAL NETWORK RESULTS ===\\n\")\n",
    "print(custom_df.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Comprehensive Comparison and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== COMPREHENSIVE NEURAL NETWORK ANALYSIS ===\n",
      "\n",
      "1. ARCHITECTURE IMPACT:\n",
      "   Best: Wide Network (300,300) - 0.9657\n",
      "   Worst: Single Hidden (50) - 0.9513\n",
      "   Range: 1.43% difference\n",
      "\n",
      "2. ACTIVATION FUNCTIONS:\n",
      "   relu: 0.9613 accuracy, 61.2s\n",
      "   tanh: 0.9587 accuracy, 65.4s\n",
      "   logistic: 0.9567 accuracy, 132.5s\n",
      "\n",
      "3. OPTIMIZATION SOLVERS:\n",
      "   adam: 0.9613 accuracy, 57.4s\n",
      "   sgd: 0.9547 accuracy, 105.9s\n",
      "   lbfgs: 0.9573 accuracy, 31.4s\n",
      "\n",
      "4. LEARNING RATE IMPACT:\n",
      "   Best learning rate: 0.001 - 0.9613 accuracy\n",
      "   Learning rates tested: [0.1, 0.01, 0.001, 0.0001]\n",
      "\n",
      "5. REGULARIZATION EFFECT:\n",
      "   Best alpha: 0.1 - 0.9643 accuracy\n",
      "\n",
      "6. IMPLEMENTATION COMPARISON:\n",
      "   Scikit-learn best: 0.9627 accuracy\n",
      "   Custom implementation best: 0.9300 accuracy\n",
      "   Performance gap: 3.27%\n",
      "\n",
      "7. KEY INSIGHTS:\n",
      "   ✓ Deeper networks generally perform better but take longer to train\n",
      "   ✓ ReLU activation typically outperforms sigmoid/tanh for this problem\n",
      "   ✓ Adam optimizer usually converges faster than SGD\n",
      "   ✓ Learning rate around 0.001-0.01 works best\n",
      "   ✓ Custom implementation shows the underlying math works correctly\n",
      "   ✓ Scikit-learn is optimized and typically performs better\n",
      "\n",
      "============================================================\n",
      "FINAL RECOMMENDATION: Use Wide Network (300,300)\n",
      "with ReLU activation, Adam solver, learning rate 0.001\n",
      "Expected accuracy: ~96.3% on MNIST digits\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Create comprehensive comparison\n",
    "print(\"=== COMPREHENSIVE NEURAL NETWORK ANALYSIS ===\\n\")\n",
    "\n",
    "print(\"1. ARCHITECTURE IMPACT:\")\n",
    "print(f\"   Best: {arch_df.loc[arch_df['Accuracy'].idxmax(), 'Architecture']} - {arch_df['Accuracy'].max():.4f}\")\n",
    "print(f\"   Worst: {arch_df.loc[arch_df['Accuracy'].idxmin(), 'Architecture']} - {arch_df['Accuracy'].min():.4f}\")\n",
    "print(f\"   Range: {(arch_df['Accuracy'].max() - arch_df['Accuracy'].min())*100:.2f}% difference\\n\")\n",
    "\n",
    "print(\"2. ACTIVATION FUNCTIONS:\")\n",
    "for _, row in activation_df.iterrows():\n",
    "    print(f\"   {row['Activation']}: {row['Accuracy']:.4f} accuracy, {row['Training Time']:.1f}s\")\n",
    "print()\n",
    "\n",
    "print(\"3. OPTIMIZATION SOLVERS:\")\n",
    "for _, row in solver_df.iterrows():\n",
    "    print(f\"   {row['Solver']}: {row['Accuracy']:.4f} accuracy, {row['Training Time']:.1f}s\")\n",
    "print()\n",
    "\n",
    "print(\"4. LEARNING RATE IMPACT:\")\n",
    "best_lr = lr_df.loc[lr_df['Accuracy'].idxmax(), 'Learning Rate']\n",
    "print(f\"   Best learning rate: {best_lr} - {lr_df['Accuracy'].max():.4f} accuracy\")\n",
    "print(f\"   Learning rates tested: {list(lr_df['Learning Rate'])}\")\n",
    "print()\n",
    "\n",
    "print(\"5. REGULARIZATION EFFECT:\")\n",
    "best_alpha = reg_df.loc[reg_df['Accuracy'].idxmax(), 'Alpha (Regularization)']\n",
    "print(f\"   Best alpha: {best_alpha} - {reg_df['Accuracy'].max():.4f} accuracy\")\n",
    "print()\n",
    "\n",
    "print(\"6. IMPLEMENTATION COMPARISON:\")\n",
    "print(f\"   Scikit-learn best: {best_accuracy:.4f} accuracy\")\n",
    "print(f\"   Custom implementation best: {custom_df['Test Accuracy'].max():.4f} accuracy\")\n",
    "print(f\"   Performance gap: {(best_accuracy - custom_df['Test Accuracy'].max())*100:.2f}%\")\n",
    "print()\n",
    "\n",
    "print(\"7. KEY INSIGHTS:\")\n",
    "print(\"   ✓ Deeper networks generally perform better but take longer to train\")\n",
    "print(\"   ✓ ReLU activation typically outperforms sigmoid/tanh for this problem\")\n",
    "print(\"   ✓ Adam optimizer usually converges faster than SGD\")\n",
    "print(\"   ✓ Learning rate around 0.001-0.01 works best\")\n",
    "print(\"   ✓ Custom implementation shows the underlying math works correctly\")\n",
    "print(\"   ✓ Scikit-learn is optimized and typically performs better\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"FINAL RECOMMENDATION: Use {arch_df.loc[arch_df['Accuracy'].idxmax(), 'Architecture']}\")\n",
    "print(f\"with ReLU activation, Adam solver, learning rate 0.001\")\n",
    "print(f\"Expected accuracy: ~{best_accuracy*100:.1f}% on MNIST digits\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
