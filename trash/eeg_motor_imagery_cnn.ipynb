{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EEG Motor Imagery Classification using CNNs\n",
    "\n",
    "## üß† Complete Guide to EEG Signal Processing and Classification\n",
    "\n",
    "### üéØ What You'll Learn:\n",
    "1. **EEG Signal Processing**: Understanding temporal brain signals\n",
    "2. **PhysioNet Motor Imagery Dataset**: Loading and preprocessing\n",
    "3. **Channel Selection**: Identifying optimal EEG electrodes\n",
    "4. **CNN for Temporal Data**: 1D and 2D convolutions for EEG\n",
    "5. **Motor Imagery Classification**: Left hand vs Right hand vs Feet vs Tongue\n",
    "6. **Advanced Techniques**: Spectral features, filtering, artifact removal\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Understanding EEG and Motor Imagery\n",
    "\n",
    "### What is EEG?\n",
    "**Electroencephalography (EEG)** measures electrical activity of the brain using electrodes placed on the scalp.\n",
    "\n",
    "### Motor Imagery\n",
    "**Motor Imagery** is the mental rehearsal of motor actions without actual movement. When you imagine moving your hand, specific brain regions activate, creating detectable EEG patterns.\n",
    "\n",
    "### Key EEG Concepts:\n",
    "- **Channels**: 64 electrodes positioned according to 10-10 system\n",
    "- **Sampling Rate**: 160 Hz (160 samples per second)\n",
    "- **Frequency Bands**: \n",
    "  - Delta (0.5-4 Hz): Deep sleep\n",
    "  - Theta (4-8 Hz): Drowsiness\n",
    "  - Alpha (8-13 Hz): Relaxed awareness\n",
    "  - Beta (13-30 Hz): Active thinking\n",
    "  - Gamma (30-100 Hz): High-level cognitive functions\n",
    "\n",
    "### Motor Imagery Tasks:\n",
    "1. **Left Hand**: Imagining left hand movement\n",
    "2. **Right Hand**: Imagining right hand movement  \n",
    "3. **Feet**: Imagining foot movement\n",
    "4. **Tongue**: Imagining tongue movement\n",
    "\n",
    "---\n",
    "\n",
    "## üîß Why CNNs for EEG?\n",
    "\n",
    "### Traditional Approach vs CNN:\n",
    "- **Traditional**: Manual feature extraction ‚Üí Classical ML\n",
    "- **CNN**: Automatic feature learning from raw signals\n",
    "\n",
    "### CNN Advantages for EEG:\n",
    "1. **Temporal Patterns**: 1D convolutions capture time-series patterns\n",
    "2. **Spatial Patterns**: 2D convolutions capture electrode relationships\n",
    "3. **Automatic Features**: No manual feature engineering needed\n",
    "4. **Hierarchical Learning**: Low-level ‚Üí High-level patterns\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Let's Begin!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Setting up EEG Motor Imagery Classification Environment\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mne'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     14\u001b[39m warnings.filterwarnings(\u001b[33m'\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# MNE for EEG processing\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmne\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmne\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Epochs, pick_types\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmne\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchannels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m make_standard_montage\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'mne'"
     ]
    }
   ],
   "source": [
    "# Cell 1: Import Essential Libraries for EEG Processing\n",
    "\n",
    "print(\"üß† Setting up EEG Motor Imagery Classification Environment\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import signal\n",
    "from scipy.stats import zscore\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# MNE for EEG processing\n",
    "import mne\n",
    "from mne import Epochs, pick_types\n",
    "from mne.channels import make_standard_montage\n",
    "from mne.datasets import eegbci\n",
    "from mne.io import concatenate_raws, read_raw_edf\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Visualization\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Utilities\n",
    "import os\n",
    "import datetime\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Configure matplotlib\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# MNE configuration\n",
    "mne.set_log_level('WARNING')  # Reduce MNE verbosity\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"üìä TensorFlow version: {tf.__version__}\")\n",
    "print(f\"üß† MNE version: {mne.__version__}\")\n",
    "print(f\"üî¢ NumPy version: {np.__version__}\")\n",
    "print(\"üöÄ Ready for EEG signal processing!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load PhysioNet Motor Imagery Dataset\n",
    "\n",
    "print(\"üì• Loading PhysioNet EEG Motor Imagery Dataset\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "def load_physionet_data(subject_ids=[1, 2, 3], runs=[6, 10, 14], verbose=True):\n",
    "    \"\"\"\n",
    "    Load PhysioNet EEG Motor Imagery data using MNE.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    subject_ids : list\n",
    "        List of subject IDs to load (1-109)\n",
    "    runs : list  \n",
    "        List of runs to load:\n",
    "        - Run 6: Left hand vs right hand motor imagery\n",
    "        - Run 10: Left hand vs right hand motor imagery  \n",
    "        - Run 14: Feet vs tongue motor imagery\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    raw_data : list\n",
    "        List of MNE Raw objects\n",
    "    \"\"\"\n",
    "    \n",
    "    raw_files = []\n",
    "    \n",
    "    for subject_id in subject_ids:\n",
    "        if verbose:\n",
    "            print(f\"\\nüë§ Loading Subject {subject_id}...\")\n",
    "        \n",
    "        subject_runs = []\n",
    "        \n",
    "        for run in runs:\n",
    "            try:\n",
    "                # Download data files for this subject and run\n",
    "                files = eegbci.load_data(subject_id, runs=[run], update_path=False)\n",
    "                \n",
    "                # Load the EDF file\n",
    "                raw = read_raw_edf(files[0], preload=True, stim_channel='auto')\n",
    "                \n",
    "                # Set standard electrode montage\n",
    "                eegbci.standardize(raw)  # Convert to standard channel names\n",
    "                montage = make_standard_montage('standard_1005')\n",
    "                raw.set_montage(montage, match_case=False)\n",
    "                \n",
    "                # Add subject and run info\n",
    "                raw.info['subject_info'] = {'id': subject_id, 'run': run}\n",
    "                \n",
    "                subject_runs.append(raw)\n",
    "                \n",
    "                if verbose:\n",
    "                    print(f\"  ‚úÖ Run {run}: {len(raw.times)} samples, {len(raw.ch_names)} channels\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                if verbose:\n",
    "                    print(f\"  ‚ùå Run {run}: Failed to load - {e}\")\n",
    "                continue\n",
    "        \n",
    "        if subject_runs:\n",
    "            # Concatenate runs for this subject\n",
    "            raw_concat = concatenate_raws(subject_runs)\n",
    "            raw_files.append(raw_concat)\n",
    "    \n",
    "    return raw_files\n",
    "\n",
    "# Load data for first 3 subjects (you can increase this later)\n",
    "print(\"üîÑ Starting data download...\")\n",
    "print(\"Note: First download may take several minutes\")\n",
    "\n",
    "# Motor imagery runs:\n",
    "# Run 6: Left hand vs right hand imagery (1 vs 2)\n",
    "# Run 10: Left hand vs right hand imagery (1 vs 2) \n",
    "# Run 14: Feet vs tongue imagery (3 vs 4)\n",
    "raw_data = load_physionet_data(subject_ids=[1, 2, 3], runs=[6, 10, 14])\n",
    "\n",
    "print(f\"\\n‚úÖ Successfully loaded data for {len(raw_data)} subjects\")\n",
    "\n",
    "# Display dataset information\n",
    "if raw_data:\n",
    "    sample_raw = raw_data[0]\n",
    "    print(f\"\\nüìä Dataset Information:\")\n",
    "    print(f\"Sampling frequency: {sample_raw.info['sfreq']} Hz\")\n",
    "    print(f\"Number of channels: {len(sample_raw.ch_names)}\")\n",
    "    print(f\"Channel types: {set(sample_raw.get_channel_types())}\")\n",
    "    print(f\"Duration: {sample_raw.times[-1]:.1f} seconds\")\n",
    "    \n",
    "    # Show channel names\n",
    "    eeg_channels = mne.pick_types(sample_raw.info, eeg=True)\n",
    "    eeg_ch_names = [sample_raw.ch_names[i] for i in eeg_channels]\n",
    "    print(f\"\\nüß† EEG Channels ({len(eeg_ch_names)}): {eeg_ch_names[:10]}...\")\n",
    "    \n",
    "    print(\"\\nüéØ Motor Imagery Task Mapping:\")\n",
    "    print(\"Event ID 1: Left Hand Imagery\")\n",
    "    print(\"Event ID 2: Right Hand Imagery\")\n",
    "    print(\"Event ID 3: Feet Imagery\")\n",
    "    print(\"Event ID 4: Tongue Imagery\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå No data loaded successfully. Check your internet connection.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: EEG Signal Preprocessing and Visualization\n",
    "\n",
    "print(\"üîß EEG Signal Preprocessing Pipeline\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "def preprocess_eeg_data(raw_data, l_freq=7., h_freq=30., notch_freq=50., \n",
    "                       tmin=-1., tmax=4., baseline=(None, 0), verbose=True):\n",
    "    \"\"\"\n",
    "    Comprehensive EEG preprocessing pipeline.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    raw_data : list of mne.Raw\n",
    "        Raw EEG data from multiple subjects\n",
    "    l_freq : float\n",
    "        Low-pass filter frequency (Hz)\n",
    "    h_freq : float  \n",
    "        High-pass filter frequency (Hz)\n",
    "    notch_freq : float\n",
    "        Notch filter frequency for power line noise (Hz)\n",
    "    tmin, tmax : float\n",
    "        Time window around events (seconds)\n",
    "    baseline : tuple\n",
    "        Baseline correction period\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    epochs_list : list\n",
    "        Preprocessed epochs for each subject\n",
    "    \"\"\"\n",
    "    \n",
    "    epochs_list = []\n",
    "    \n",
    "    for i, raw in enumerate(raw_data):\n",
    "        if verbose:\n",
    "            print(f\"\\nüîÑ Processing Subject {i+1}...\")\n",
    "        \n",
    "        # Make a copy to avoid modifying original data\n",
    "        raw_copy = raw.copy()\n",
    "        \n",
    "        # 1. Filter the data\n",
    "        if verbose:\n",
    "            print(f\"  üì∂ Applying filters: {l_freq}-{h_freq} Hz bandpass + {notch_freq} Hz notch\")\n",
    "        \n",
    "        # Bandpass filter (remove low-frequency drifts and high-frequency noise)\n",
    "        raw_copy.filter(l_freq=l_freq, h_freq=h_freq, method='iir', verbose=False)\n",
    "        \n",
    "        # Notch filter (remove power line interference)\n",
    "        raw_copy.notch_filter(freqs=notch_freq, verbose=False)\n",
    "        \n",
    "        # 2. Extract events\n",
    "        try:\n",
    "            events, event_id = mne.events_from_annotations(raw_copy)\n",
    "            if verbose:\n",
    "                print(f\"  üéØ Found {len(events)} events: {event_id}\")\n",
    "        except Exception as e:\n",
    "            if verbose:\n",
    "                print(f\"  ‚ùå Could not extract events: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # 3. Select only EEG channels\n",
    "        picks = mne.pick_types(raw_copy.info, eeg=True, exclude='bads')\n",
    "        \n",
    "        # 4. Create epochs around events\n",
    "        try:\n",
    "            epochs = Epochs(raw_copy, events, event_id, tmin=tmin, tmax=tmax,\n",
    "                          picks=picks, baseline=baseline, preload=True, verbose=False)\n",
    "            \n",
    "            # 5. Remove bad epochs (artifacts)\n",
    "            epochs.drop_bad()\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"  ‚úÖ Created {len(epochs)} epochs, shape: {epochs.get_data().shape}\")\n",
    "                print(f\"  üìä Events per class: {dict(zip(epochs.event_id.keys(), \n",
    "                                                        [sum(epochs.events[:, 2] == v) for v in epochs.event_id.values()]))}\")\n",
    "            \n",
    "            epochs_list.append(epochs)\n",
    "            \n",
    "        except Exception as e:\n",
    "            if verbose:\n",
    "                print(f\"  ‚ùå Could not create epochs: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return epochs_list\n",
    "\n",
    "# Apply preprocessing\n",
    "if raw_data:\n",
    "    print(\"üîÑ Starting preprocessing pipeline...\")\n",
    "    print(\"\\nüéõÔ∏è Preprocessing Parameters:\")\n",
    "    print(\"‚Ä¢ Bandpass filter: 7-30 Hz (removes artifacts, keeps motor rhythms)\")\n",
    "    print(\"‚Ä¢ Notch filter: 50 Hz (removes power line noise)\")\n",
    "    print(\"‚Ä¢ Epoch window: -1 to +4 seconds around event\")\n",
    "    print(\"‚Ä¢ Baseline: -1 to 0 seconds (pre-stimulus period)\")\n",
    "    \n",
    "    epochs_list = preprocess_eeg_data(raw_data, verbose=True)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Preprocessing completed for {len(epochs_list)} subjects\")\n",
    "    \n",
    "    if epochs_list:\n",
    "        # Display preprocessing results\n",
    "        total_epochs = sum(len(epochs) for epochs in epochs_list)\n",
    "        sample_epochs = epochs_list[0]\n",
    "        \n",
    "        print(f\"\\nüìà Preprocessing Summary:\")\n",
    "        print(f\"Total epochs across all subjects: {total_epochs}\")\n",
    "        print(f\"Epoch shape: {sample_epochs.get_data().shape}\")\n",
    "        print(f\"Time points per epoch: {sample_epochs.get_data().shape[-1]}\")\n",
    "        print(f\"Sampling rate: {sample_epochs.info['sfreq']} Hz\")\n",
    "        print(f\"Time window: {sample_epochs.tmin} to {sample_epochs.tmax} seconds\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå No raw data available for preprocessing\")\n",
    "    epochs_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: EEG Data Visualization and Analysis\n",
    "\n",
    "print(\"üìä EEG Signal Visualization and Analysis\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "def visualize_eeg_data(epochs_list, max_subjects=2):\n",
    "    \"\"\"\n",
    "    Create comprehensive visualizations of EEG data.\n",
    "    \"\"\"\n",
    "    \n",
    "    if not epochs_list:\n",
    "        print(\"‚ùå No epochs data available for visualization\")\n",
    "        return\n",
    "    \n",
    "    # Use first subject for detailed analysis\n",
    "    epochs = epochs_list[0]\n",
    "    data = epochs.get_data()  # Shape: (n_epochs, n_channels, n_times)\n",
    "    \n",
    "    print(f\"\\nüìä Analyzing Subject 1 Data:\")\n",
    "    print(f\"Data shape: {data.shape}\")\n",
    "    print(f\"Event types: {list(epochs.event_id.keys())}\")\n",
    "    \n",
    "    # 1. Plot average ERPs for each class\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('Event-Related Potentials (ERPs) by Motor Imagery Task', fontsize=16)\n",
    "    \n",
    "    # Get some key channels for visualization\n",
    "    key_channels = ['C3', 'C4', 'Cz', 'FC1', 'FC2']  # Motor cortex channels\n",
    "    available_channels = [ch for ch in key_channels if ch in epochs.ch_names]\n",
    "    \n",
    "    if not available_channels:\n",
    "        available_channels = epochs.ch_names[:5]  # Use first 5 channels if standard names not found\n",
    "    \n",
    "    print(f\"\\nüß† Analyzing key motor cortex channels: {available_channels}\")\n",
    "    \n",
    "    colors = ['blue', 'red', 'green', 'orange']\n",
    "    event_names = list(epochs.event_id.keys())\n",
    "    \n",
    "    for i, ch_name in enumerate(available_channels[:4]):\n",
    "        ax = axes[i//2, i%2]\n",
    "        ch_idx = epochs.ch_names.index(ch_name)\n",
    "        \n",
    "        for j, (event_name, event_code) in enumerate(epochs.event_id.items()):\n",
    "            # Get epochs for this event type\n",
    "            event_epochs = epochs[event_name]\n",
    "            if len(event_epochs) > 0:\n",
    "                # Average across epochs\n",
    "                avg_signal = event_epochs.get_data()[:, ch_idx, :].mean(axis=0)\n",
    "                times = epochs.times\n",
    "                \n",
    "                ax.plot(times, avg_signal, color=colors[j % len(colors)], \n",
    "                       label=event_name, linewidth=2)\n",
    "        \n",
    "        ax.set_title(f'Channel {ch_name}')\n",
    "        ax.set_xlabel('Time (s)')\n",
    "        ax.set_ylabel('Amplitude (¬µV)')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.axvline(x=0, color='black', linestyle='--', alpha=0.5, label='Event onset')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 2. Topographic maps showing spatial distribution\n",
    "    if len(epochs.ch_names) > 10:  # Only if we have enough channels\n",
    "        print(\"\\nüó∫Ô∏è  Creating topographic maps...\")\n",
    "        \n",
    "        fig, axes = plt.subplots(1, len(event_names), figsize=(4*len(event_names), 4))\n",
    "        if len(event_names) == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for i, (event_name, event_code) in enumerate(epochs.event_id.items()):\n",
    "            event_epochs = epochs[event_name]\n",
    "            if len(event_epochs) > 0:\n",
    "                # Average over time window 0.5-2.0 seconds (motor imagery period)\n",
    "                time_mask = (epochs.times >= 0.5) & (epochs.times <= 2.0)\n",
    "                avg_topo = event_epochs.get_data()[:, :, time_mask].mean(axis=(0, 2))\n",
    "                \n",
    "                # Create topographic plot\n",
    "                im, _ = mne.viz.plot_topomap(avg_topo, epochs.info, axes=axes[i], \n",
    "                                            show=False, cmap='RdBu_r')\n",
    "                axes[i].set_title(f'{event_name}\\n(0.5-2.0s avg)')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # 3. Power Spectral Density Analysis\n",
    "    print(\"\\n‚ö° Computing Power Spectral Density...\")\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    for i, (event_name, event_code) in enumerate(epochs.event_id.items()):\n",
    "        event_epochs = epochs[event_name]\n",
    "        if len(event_epochs) > 0:\n",
    "            # Compute PSD using Welch's method\n",
    "            psds, freqs = mne.time_frequency.psd_welch(event_epochs, fmin=1, fmax=40, \n",
    "                                                      n_fft=256, verbose=False)\n",
    "            # Average across channels and epochs\n",
    "            avg_psd = psds.mean(axis=(0, 1))\n",
    "            \n",
    "            plt.semilogy(freqs, avg_psd, color=colors[i % len(colors)], \n",
    "                        label=event_name, linewidth=2)\n",
    "    \n",
    "    plt.xlabel('Frequency (Hz)')\n",
    "    plt.ylabel('Power Spectral Density (¬µV¬≤/Hz)')\n",
    "    plt.title('Average Power Spectral Density by Motor Imagery Task')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Highlight important frequency bands\n",
    "    plt.axvspan(8, 13, alpha=0.2, color='yellow', label='Alpha (8-13 Hz)')\n",
    "    plt.axvspan(13, 30, alpha=0.2, color='cyan', label='Beta (13-30 Hz)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Create visualizations\n",
    "if epochs_list:\n",
    "    print(\"üé® Creating EEG visualizations...\")\n",
    "    print(\"\\nüìà This will show:\")\n",
    "    print(\"1. Event-Related Potentials (ERPs) for motor cortex channels\")\n",
    "    print(\"2. Topographic maps showing spatial activation patterns\")\n",
    "    print(\"3. Power spectral density analysis\")\n",
    "    \n",
    "    epoch_data = visualize_eeg_data(epochs_list)\n",
    "    \n",
    "    print(\"\\n‚úÖ Visualization completed!\")\n",
    "    print(\"\\nüß† Key Observations to Look For:\")\n",
    "    print(\"‚Ä¢ Different ERP patterns between left/right hand imagery\")\n",
    "    print(\"‚Ä¢ Lateralized activation (left motor cortex for right hand, vice versa)\")\n",
    "    print(\"‚Ä¢ Alpha/beta band power differences between tasks\")\n",
    "    print(\"‚Ä¢ Event-related desynchronization (ERD) in motor frequencies\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No epoch data available for visualization\")\n",
    "    epoch_data = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: EEG Channel Selection Techniques\n",
    "\n",
    "print(\"üéØ EEG Channel Selection for Motor Imagery\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "def analyze_channel_importance(epochs_list, method='variance', top_k=16):\n",
    "    \"\"\"\n",
    "    Analyze and select the most important EEG channels for motor imagery classification.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    epochs_list : list\n",
    "        List of MNE Epochs objects\n",
    "    method : str\n",
    "        Channel selection method ('variance', 'motor_cortex', 'statistical')\n",
    "    top_k : int\n",
    "        Number of top channels to select\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    selected_channels : list\n",
    "        Names of selected channels\n",
    "    channel_scores : dict\n",
    "        Importance scores for each channel\n",
    "    \"\"\"\n",
    "    \n",
    "    if not epochs_list:\n",
    "        return [], {}\n",
    "    \n",
    "    # Combine data from all subjects\n",
    "    all_data = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for epochs in epochs_list:\n",
    "        data = epochs.get_data()  # (n_epochs, n_channels, n_times)\n",
    "        labels = epochs.events[:, 2]  # Event codes\n",
    "        \n",
    "        all_data.append(data)\n",
    "        all_labels.append(labels)\n",
    "    \n",
    "    # Concatenate all subjects\n",
    "    X = np.concatenate(all_data, axis=0)  # (total_epochs, n_channels, n_times)\n",
    "    y = np.concatenate(all_labels, axis=0)  # (total_epochs,)\n",
    "    \n",
    "    print(f\"\\nüìä Combined dataset shape: {X.shape}\")\n",
    "    print(f\"üìä Labels shape: {y.shape}\")\n",
    "    print(f\"üìä Unique classes: {np.unique(y)}\")\n",
    "    \n",
    "    channel_names = epochs_list[0].ch_names\n",
    "    channel_scores = {}\n",
    "    \n",
    "    if method == 'motor_cortex':\n",
    "        print(\"\\nüß† Method: Motor Cortex Channel Selection\")\n",
    "        print(\"Selecting channels over motor and sensorimotor areas...\")\n",
    "        \n",
    "        # Define motor cortex channels (based on 10-20 system)\n",
    "        motor_channels = [\n",
    "            'FC5', 'FC3', 'FC1', 'FCz', 'FC2', 'FC4', 'FC6',  # Frontal motor\n",
    "            'C5', 'C3', 'C1', 'Cz', 'C2', 'C4', 'C6',        # Central motor\n",
    "            'CP5', 'CP3', 'CP1', 'CPz', 'CP2', 'CP4', 'CP6'   # Parietal motor\n",
    "        ]\n",
    "        \n",
    "        # Find available motor channels\n",
    "        available_motor = [ch for ch in motor_channels if ch in channel_names]\n",
    "        \n",
    "        # If not enough motor channels, add nearby channels\n",
    "        if len(available_motor) < top_k:\n",
    "            additional_channels = ['F3', 'F4', 'P3', 'P4', 'T7', 'T8', 'Fz', 'Pz']\n",
    "            for ch in additional_channels:\n",
    "                if ch in channel_names and ch not in available_motor:\n",
    "                    available_motor.append(ch)\n",
    "                    if len(available_motor) >= top_k:\n",
    "                        break\n",
    "        \n",
    "        selected_channels = available_motor[:top_k]\n",
    "        \n",
    "        # Assign scores based on motor relevance\n",
    "        for i, ch in enumerate(channel_names):\n",
    "            if ch in selected_channels:\n",
    "                channel_scores[ch] = 1.0 - (selected_channels.index(ch) / len(selected_channels))\n",
    "            else:\n",
    "                channel_scores[ch] = 0.0\n",
    "                \n",
    "    elif method == 'variance':\n",
    "        print(\"\\nüìà Method: Variance-Based Channel Selection\")\n",
    "        print(\"Selecting channels with highest signal variance...\")\n",
    "        \n",
    "        # Calculate variance for each channel across all epochs and time\n",
    "        channel_variances = np.var(X, axis=(0, 2))  # Variance across epochs and time\n",
    "        \n",
    "        # Rank channels by variance\n",
    "        channel_ranking = np.argsort(channel_variances)[::-1]  # Descending order\n",
    "        selected_indices = channel_ranking[:top_k]\n",
    "        selected_channels = [channel_names[i] for i in selected_indices]\n",
    "        \n",
    "        # Store scores\n",
    "        for i, ch in enumerate(channel_names):\n",
    "            channel_scores[ch] = channel_variances[i]\n",
    "            \n",
    "    elif method == 'statistical':\n",
    "        print(\"\\nüìä Method: Statistical Channel Selection (F-score)\")\n",
    "        print(\"Selecting channels that best discriminate between classes...\")\n",
    "        \n",
    "        # Flatten temporal dimension for statistical analysis\n",
    "        X_flat = X.reshape(X.shape[0], -1)  # (n_epochs, n_channels * n_times)\n",
    "        \n",
    "        # Calculate F-score for each feature (channel x time)\n",
    "        f_scores, _ = f_classif(X_flat, y)\n",
    "        \n",
    "        # Reshape back to (n_channels, n_times) and average over time\n",
    "        f_scores_reshaped = f_scores.reshape(X.shape[1], X.shape[2])\n",
    "        channel_f_scores = np.mean(f_scores_reshaped, axis=1)\n",
    "        \n",
    "        # Rank channels by F-score\n",
    "        channel_ranking = np.argsort(channel_f_scores)[::-1]\n",
    "        selected_indices = channel_ranking[:top_k]\n",
    "        selected_channels = [channel_names[i] for i in selected_indices]\n",
    "        \n",
    "        # Store scores\n",
    "        for i, ch in enumerate(channel_names):\n",
    "            channel_scores[ch] = channel_f_scores[i]\n",
    "    \n",
    "    return selected_channels, channel_scores\n",
    "\n",
    "def visualize_channel_selection(epochs_list, selected_channels, channel_scores, method):\n",
    "    \"\"\"\n",
    "    Visualize selected channels and their importance scores.\n",
    "    \"\"\"\n",
    "    \n",
    "    if not epochs_list or not selected_channels:\n",
    "        return\n",
    "    \n",
    "    epochs = epochs_list[0]\n",
    "    \n",
    "    # 1. Plot channel importance scores\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Bar plot of top channels\n",
    "    top_channels = selected_channels[:16]\n",
    "    top_scores = [channel_scores[ch] for ch in top_channels]\n",
    "    \n",
    "    ax1.bar(range(len(top_channels)), top_scores, color='steelblue', alpha=0.7)\n",
    "    ax1.set_xlabel('Channel Index')\n",
    "    ax1.set_ylabel('Importance Score')\n",
    "    ax1.set_title(f'Top {len(top_channels)} Channels - {method.title()} Method')\n",
    "    ax1.set_xticks(range(len(top_channels)))\n",
    "    ax1.set_xticklabels(top_channels, rotation=45)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Topographic map of channel importance\n",
    "    if len(epochs.ch_names) > 10:\n",
    "        try:\n",
    "            # Create importance vector for all channels\n",
    "            importance_vector = np.array([channel_scores.get(ch, 0) for ch in epochs.ch_names])\n",
    "            \n",
    "            # Normalize for better visualization\n",
    "            if np.max(importance_vector) > 0:\n",
    "                importance_vector = importance_vector / np.max(importance_vector)\n",
    "            \n",
    "            # Plot topographic map\n",
    "            im, _ = mne.viz.plot_topomap(importance_vector, epochs.info, axes=ax2, \n",
    "                                        show=False, cmap='Reds', vmin=0, vmax=1)\n",
    "            ax2.set_title(f'Channel Importance Map\\n({method.title()} Method)')\n",
    "            \n",
    "            # Add colorbar\n",
    "            cbar = plt.colorbar(im, ax=ax2, fraction=0.046, pad=0.04)\n",
    "            cbar.set_label('Importance Score')\n",
    "            \n",
    "        except Exception as e:\n",
    "            ax2.text(0.5, 0.5, f'Topographic plot not available\\n{str(e)}', \n",
    "                    transform=ax2.transAxes, ha='center', va='center')\n",
    "            ax2.set_title('Topographic Map (Not Available)')\n",
    "    else:\n",
    "        ax2.text(0.5, 0.5, 'Insufficient channels\\nfor topographic map', \n",
    "                transform=ax2.transAxes, ha='center', va='center')\n",
    "        ax2.set_title('Topographic Map (Not Available)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Apply different channel selection methods\n",
    "if epochs_list:\n",
    "    print(\"üîç Applying multiple channel selection methods...\")\n",
    "    \n",
    "    methods = ['motor_cortex', 'variance', 'statistical']\n",
    "    selection_results = {}\n",
    "    \n",
    "    for method in methods:\n",
    "        print(f\"\\n{'-'*50}\")\n",
    "        selected_channels, channel_scores = analyze_channel_importance(epochs_list, method=method, top_k=16)\n",
    "        \n",
    "        if selected_channels:\n",
    "            selection_results[method] = {\n",
    "                'channels': selected_channels,\n",
    "                'scores': channel_scores\n",
    "            }\n",
    "            \n",
    "            print(f\"\\n‚úÖ Selected {len(selected_channels)} channels using {method} method:\")\n",
    "            print(f\"Top 10: {selected_channels[:10]}\")\n",
    "            \n",
    "            # Visualize results\n",
    "            visualize_channel_selection(epochs_list, selected_channels, channel_scores, method)\n",
    "    \n",
    "    # Compare methods\n",
    "    if len(selection_results) > 1:\n",
    "        print(f\"\\nüîç Comparing Channel Selection Methods:\")\n",
    "        print(f\"{'Method':<15} {'Top 5 Channels':<50}\")\n",
    "        print(f\"{'-'*65}\")\n",
    "        \n",
    "        for method, results in selection_results.items():\n",
    "            top5 = ', '.join(results['channels'][:5])\n",
    "            print(f\"{method:<15} {top5:<50}\")\n",
    "        \n",
    "        # Find common channels across methods\n",
    "        all_channels = [set(results['channels'][:10]) for results in selection_results.values()]\n",
    "        common_channels = set.intersection(*all_channels)\n",
    "        \n",
    "        print(f\"\\nüéØ Channels selected by ALL methods: {sorted(common_channels)}\")\n",
    "        print(f\"\\nüí° Recommendation: Use motor_cortex method for interpretability\")\n",
    "        print(f\"   or statistical method for best discrimination performance.\")\n",
    "    \n",
    "    # Store best selection for later use\n",
    "    best_method = 'statistical' if 'statistical' in selection_results else list(selection_results.keys())[0]\n",
    "    best_channels = selection_results[best_method]['channels'][:16]\n",
    "    \n",
    "    print(f\"\\nüèÜ Using {best_method} method with {len(best_channels)} channels for CNN training\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No epoch data available for channel selection\")\n",
    "    selection_results = {}\n",
    "    best_channels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Prepare Data for CNN Training\n",
    "\n",
    "print(\"üîß Preparing EEG Data for CNN Training\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "def prepare_cnn_data(epochs_list, selected_channels=None, test_size=0.2, val_size=0.2):\n",
    "    \"\"\"\n",
    "    Prepare EEG data for CNN training.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    epochs_list : list\n",
    "        List of MNE Epochs objects\n",
    "    selected_channels : list\n",
    "        Names of selected channels to use\n",
    "    test_size : float\n",
    "        Fraction of data to use for testing\n",
    "    val_size : float\n",
    "        Fraction of training data to use for validation\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    X_train, X_val, X_test : numpy arrays\n",
    "        Training, validation, and test data\n",
    "    y_train, y_val, y_test : numpy arrays\n",
    "        Training, validation, and test labels\n",
    "    class_names : list\n",
    "        Names of the classes\n",
    "    \"\"\"\n",
    "    \n",
    "    if not epochs_list:\n",
    "        return None, None, None, None, None, None, None\n",
    "    \n",
    "    # Combine data from all subjects\n",
    "    all_data = []\n",
    "    all_labels = []\n",
    "    all_subjects = []\n",
    "    \n",
    "    for subject_idx, epochs in enumerate(epochs_list):\n",
    "        data = epochs.get_data()  # (n_epochs, n_channels, n_times)\n",
    "        labels = epochs.events[:, 2]  # Event codes\n",
    "        \n",
    "        # Select specific channels if provided\n",
    "        if selected_channels:\n",
    "            channel_indices = [epochs.ch_names.index(ch) for ch in selected_channels \n",
    "                             if ch in epochs.ch_names]\n",
    "            if channel_indices:\n",
    "                data = data[:, channel_indices, :]\n",
    "                print(f\"\\nüì° Subject {subject_idx+1}: Using {len(channel_indices)} selected channels\")\n",
    "            else:\n",
    "                print(f\"\\n‚ö†Ô∏è  Subject {subject_idx+1}: No selected channels found, using all channels\")\n",
    "        \n",
    "        all_data.append(data)\n",
    "        all_labels.append(labels)\n",
    "        all_subjects.extend([subject_idx] * len(data))\n",
    "    \n",
    "    # Concatenate all subjects\n",
    "    X = np.concatenate(all_data, axis=0)  # (total_epochs, n_channels, n_times)\n",
    "    y = np.concatenate(all_labels, axis=0)  # (total_epochs,)\n",
    "    subjects = np.array(all_subjects)  # (total_epochs,)\n",
    "    \n",
    "    print(f\"\\nüìä Combined Dataset Information:\")\n",
    "    print(f\"Total epochs: {X.shape[0]}\")\n",
    "    print(f\"Channels: {X.shape[1]}\")\n",
    "    print(f\"Time points: {X.shape[2]}\")\n",
    "    print(f\"Data shape: {X.shape}\")\n",
    "    print(f\"Unique classes: {np.unique(y)}\")\n",
    "    \n",
    "    # Create class names mapping\n",
    "    event_id = epochs_list[0].event_id\n",
    "    class_mapping = {v: k for k, v in event_id.items()}\n",
    "    class_names = [class_mapping[label] for label in sorted(np.unique(y))]\n",
    "    \n",
    "    print(f\"\\nüéØ Class Mapping:\")\n",
    "    for i, (label, name) in enumerate(zip(sorted(np.unique(y)), class_names)):\n",
    "        count = np.sum(y == label)\n",
    "        print(f\"  {label} ‚Üí {name}: {count} epochs ({count/len(y)*100:.1f}%)\")\n",
    "    \n",
    "    # Normalize labels to start from 0\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "    \n",
    "    # Stratified split to maintain class balance\n",
    "    print(f\"\\nüîÑ Splitting data: {1-test_size:.0%} train, {test_size:.0%} test\")\n",
    "    X_train_temp, X_test, y_train_temp, y_test, subj_train, subj_test = train_test_split(\n",
    "        X, y_encoded, subjects, test_size=test_size, stratify=y_encoded, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Further split training data into train and validation\n",
    "    print(f\"üîÑ Splitting training data: {1-val_size:.0%} train, {val_size:.0%} validation\")\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_temp, y_train_temp, test_size=val_size, stratify=y_train_temp, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Data normalization (z-score normalization)\n",
    "    print(f\"\\nüìè Applying z-score normalization...\")\n",
    "    \n",
    "    # Calculate statistics from training data only\n",
    "    train_mean = np.mean(X_train, axis=(0, 2), keepdims=True)  # Mean across epochs and time\n",
    "    train_std = np.std(X_train, axis=(0, 2), keepdims=True)    # Std across epochs and time\n",
    "    \n",
    "    # Apply normalization\n",
    "    X_train_norm = (X_train - train_mean) / (train_std + 1e-8)\n",
    "    X_val_norm = (X_val - train_mean) / (train_std + 1e-8)\n",
    "    X_test_norm = (X_test - train_mean) / (train_std + 1e-8)\n",
    "    \n",
    "    # Convert labels to categorical for CNN\n",
    "    num_classes = len(np.unique(y_encoded))\n",
    "    y_train_cat = to_categorical(y_train, num_classes)\n",
    "    y_val_cat = to_categorical(y_val, num_classes)\n",
    "    y_test_cat = to_categorical(y_test, num_classes)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Data preparation completed!\")\n",
    "    print(f\"üìä Final shapes:\")\n",
    "    print(f\"  Training: X={X_train_norm.shape}, y={y_train_cat.shape}\")\n",
    "    print(f\"  Validation: X={X_val_norm.shape}, y={y_val_cat.shape}\")\n",
    "    print(f\"  Test: X={X_test_norm.shape}, y={y_test_cat.shape}\")\n",
    "    print(f\"  Number of classes: {num_classes}\")\n",
    "    \n",
    "    # Display data statistics\n",
    "    print(f\"\\nüìà Data Statistics (after normalization):\")\n",
    "    print(f\"  Training data mean: {np.mean(X_train_norm):.6f}\")\n",
    "    print(f\"  Training data std: {np.std(X_train_norm):.6f}\")\n",
    "    print(f\"  Data range: [{np.min(X_train_norm):.3f}, {np.max(X_train_norm):.3f}]\")\n",
    "    \n",
    "    return X_train_norm, X_val_norm, X_test_norm, y_train_cat, y_val_cat, y_test_cat, class_names\n",
    "\n",
    "# Prepare data for CNN training\n",
    "if epochs_list:\n",
    "    print(\"üîÑ Preparing data for CNN training...\")\n",
    "    \n",
    "    # Use selected channels if available\n",
    "    channels_to_use = best_channels if best_channels else None\n",
    "    \n",
    "    if channels_to_use:\n",
    "        print(f\"\\nüì° Using {len(channels_to_use)} selected channels: {channels_to_use[:5]}...\")\n",
    "    else:\n",
    "        print(f\"\\nüì° Using all available channels\")\n",
    "    \n",
    "    # Prepare data\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test, class_names = prepare_cnn_data(\n",
    "        epochs_list, selected_channels=channels_to_use, test_size=0.2, val_size=0.2\n",
    "    )\n",
    "    \n",
    "    if X_train is not None:\n",
    "        print(f\"\\nüéØ Ready for CNN training!\")\n",
    "        print(f\"Motor imagery classes: {class_names}\")\n",
    "        \n",
    "        # Store data info for later use\n",
    "        data_info = {\n",
    "            'n_channels': X_train.shape[1],\n",
    "            'n_timepoints': X_train.shape[2],\n",
    "            'n_classes': len(class_names),\n",
    "            'class_names': class_names,\n",
    "            'sampling_rate': epochs_list[0].info['sfreq'],\n",
    "            'selected_channels': channels_to_use\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nüìã Dataset Summary:\")\n",
    "        for key, value in data_info.items():\n",
    "            if key != 'selected_channels':\n",
    "                print(f\"  {key}: {value}\")\n",
    "    \n",
    "    else:\n",
    "        print(\"‚ùå Failed to prepare data for CNN training\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå No epoch data available for CNN preparation\")\n",
    "    X_train = X_val = X_test = y_train = y_val = y_test = class_names = None\n",
    "    data_info = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Design CNN Architectures for EEG\n",
    "\n",
    "print(\"üèóÔ∏è Designing CNN Architectures for EEG Motor Imagery\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "def create_eeg_cnn_1d(input_shape, num_classes, name=\"EEG_CNN_1D\"):\n",
    "    \"\"\"\n",
    "    Create 1D CNN for temporal EEG signal processing.\n",
    "    \n",
    "    This architecture focuses on temporal patterns in EEG signals.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\nüîß Building {name}...\")\n",
    "    print(\"Architecture: 1D Convolutions for temporal pattern extraction\")\n",
    "    \n",
    "    model = models.Sequential([\n",
    "        # Input layer\n",
    "        layers.Input(shape=input_shape, name='input'),\n",
    "        \n",
    "        # Reshape for 1D convolution (channels, time) -> (time, channels)\n",
    "        layers.Permute((2, 1), name='permute_for_1d'),\n",
    "        \n",
    "        # First temporal convolution block\n",
    "        layers.Conv1D(filters=32, kernel_size=64, padding='same', name='temp_conv1'),\n",
    "        layers.BatchNormalization(name='bn1'),\n",
    "        layers.Activation('relu', name='relu1'),\n",
    "        layers.MaxPooling1D(pool_size=4, name='pool1'),\n",
    "        layers.Dropout(0.2, name='dropout1'),\n",
    "        \n",
    "        # Second temporal convolution block\n",
    "        layers.Conv1D(filters=64, kernel_size=32, padding='same', name='temp_conv2'),\n",
    "        layers.BatchNormalization(name='bn2'),\n",
    "        layers.Activation('relu', name='relu2'),\n",
    "        layers.MaxPooling1D(pool_size=4, name='pool2'),\n",
    "        layers.Dropout(0.3, name='dropout2'),\n",
    "        \n",
    "        # Third temporal convolution block\n",
    "        layers.Conv1D(filters=128, kernel_size=16, padding='same', name='temp_conv3'),\n",
    "        layers.BatchNormalization(name='bn3'),\n",
    "        layers.Activation('relu', name='relu3'),\n",
    "        layers.MaxPooling1D(pool_size=2, name='pool3'),\n",
    "        layers.Dropout(0.4, name='dropout3'),\n",
    "        \n",
    "        # Global average pooling\n",
    "        layers.GlobalAveragePooling1D(name='global_avg_pool'),\n",
    "        \n",
    "        # Classification head\n",
    "        layers.Dense(256, name='dense1'),\n",
    "        layers.BatchNormalization(name='bn_dense1'),\n",
    "        layers.Activation('relu', name='relu_dense1'),\n",
    "        layers.Dropout(0.5, name='dropout_dense1'),\n",
    "        \n",
    "        layers.Dense(128, name='dense2'),\n",
    "        layers.BatchNormalization(name='bn_dense2'),\n",
    "        layers.Activation('relu', name='relu_dense2'),\n",
    "        layers.Dropout(0.5, name='dropout_dense2'),\n",
    "        \n",
    "        # Output layer\n",
    "        layers.Dense(num_classes, activation='softmax', name='output')\n",
    "    ], name=name)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def create_eeg_cnn_2d(input_shape, num_classes, name=\"EEG_CNN_2D\"):\n",
    "    \"\"\"\n",
    "    Create 2D CNN for spatial-temporal EEG signal processing.\n",
    "    \n",
    "    This architecture treats EEG as 2D image (channels √ó time).\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\nüîß Building {name}...\")\n",
    "    print(\"Architecture: 2D Convolutions for spatial-temporal pattern extraction\")\n",
    "    \n",
    "    model = models.Sequential([\n",
    "        # Input layer\n",
    "        layers.Input(shape=input_shape, name='input'),\n",
    "        \n",
    "        # Reshape to add channel dimension for 2D convolution\n",
    "        layers.Reshape((*input_shape, 1), name='reshape_2d'),\n",
    "        \n",
    "        # First spatial-temporal convolution block\n",
    "        layers.Conv2D(filters=32, kernel_size=(8, 32), padding='same', name='spattemp_conv1'),\n",
    "        layers.BatchNormalization(name='bn1'),\n",
    "        layers.Activation('relu', name='relu1'),\n",
    "        layers.MaxPooling2D(pool_size=(2, 4), name='pool1'),\n",
    "        layers.Dropout(0.2, name='dropout1'),\n",
    "        \n",
    "        # Second spatial-temporal convolution block\n",
    "        layers.Conv2D(filters=64, kernel_size=(4, 16), padding='same', name='spattemp_conv2'),\n",
    "        layers.BatchNormalization(name='bn2'),\n",
    "        layers.Activation('relu', name='relu2'),\n",
    "        layers.MaxPooling2D(pool_size=(2, 4), name='pool2'),\n",
    "        layers.Dropout(0.3, name='dropout2'),\n",
    "        \n",
    "        # Third spatial-temporal convolution block\n",
    "        layers.Conv2D(filters=128, kernel_size=(2, 8), padding='same', name='spattemp_conv3'),\n",
    "        layers.BatchNormalization(name='bn3'),\n",
    "        layers.Activation('relu', name='relu3'),\n",
    "        layers.MaxPooling2D(pool_size=(1, 2), name='pool3'),\n",
    "        layers.Dropout(0.4, name='dropout3'),\n",
    "        \n",
    "        # Global average pooling\n",
    "        layers.GlobalAveragePooling2D(name='global_avg_pool'),\n",
    "        \n",
    "        # Classification head\n",
    "        layers.Dense(256, name='dense1'),\n",
    "        layers.BatchNormalization(name='bn_dense1'),\n",
    "        layers.Activation('relu', name='relu_dense1'),\n",
    "        layers.Dropout(0.5, name='dropout_dense1'),\n",
    "        \n",
    "        layers.Dense(128, name='dense2'),\n",
    "        layers.BatchNormalization(name='bn_dense2'),\n",
    "        layers.Activation('relu', name='relu_dense2'),\n",
    "        layers.Dropout(0.5, name='dropout_dense2'),\n",
    "        \n",
    "        # Output layer\n",
    "        layers.Dense(num_classes, activation='softmax', name='output')\n",
    "    ], name=name)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def create_eeg_cnn_hybrid(input_shape, num_classes, name=\"EEG_CNN_Hybrid\"):\n",
    "    \"\"\"\n",
    "    Create hybrid CNN combining spatial and temporal processing.\n",
    "    \n",
    "    This architecture first extracts spatial patterns, then temporal patterns.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\nüîß Building {name}...\")\n",
    "    print(\"Architecture: Hybrid spatial-first then temporal convolutions\")\n",
    "    \n",
    "    # Input\n",
    "    input_layer = layers.Input(shape=input_shape, name='input')\n",
    "    \n",
    "    # Reshape for 2D convolution\n",
    "    x = layers.Reshape((*input_shape, 1), name='reshape_2d')(input_layer)\n",
    "    \n",
    "    # Spatial convolution (across channels)\n",
    "    x = layers.Conv2D(filters=32, kernel_size=(input_shape[0], 1), \n",
    "                     padding='valid', name='spatial_conv')(x)\n",
    "    x = layers.BatchNormalization(name='bn_spatial')(x)\n",
    "    x = layers.Activation('relu', name='relu_spatial')(x)\n",
    "    x = layers.Dropout(0.2, name='dropout_spatial')(x)\n",
    "    \n",
    "    # Reshape for temporal processing\n",
    "    x = layers.Reshape((input_shape[1], 32), name='reshape_temporal')(x)\n",
    "    \n",
    "    # Temporal convolutions\n",
    "    x = layers.Conv1D(filters=64, kernel_size=32, padding='same', name='temp_conv1')(x)\n",
    "    x = layers.BatchNormalization(name='bn_temp1')(x)\n",
    "    x = layers.Activation('relu', name='relu_temp1')(x)\n",
    "    x = layers.MaxPooling1D(pool_size=4, name='pool_temp1')(x)\n",
    "    x = layers.Dropout(0.3, name='dropout_temp1')(x)\n",
    "    \n",
    "    x = layers.Conv1D(filters=128, kernel_size=16, padding='same', name='temp_conv2')(x)\n",
    "    x = layers.BatchNormalization(name='bn_temp2')(x)\n",
    "    x = layers.Activation('relu', name='relu_temp2')(x)\n",
    "    x = layers.MaxPooling1D(pool_size=4, name='pool_temp2')(x)\n",
    "    x = layers.Dropout(0.4, name='dropout_temp2')(x)\n",
    "    \n",
    "    # Global pooling\n",
    "    x = layers.GlobalAveragePooling1D(name='global_avg_pool')(x)\n",
    "    \n",
    "    # Classification head\n",
    "    x = layers.Dense(256, name='dense1')(x)\n",
    "    x = layers.BatchNormalization(name='bn_dense1')(x)\n",
    "    x = layers.Activation('relu', name='relu_dense1')(x)\n",
    "    x = layers.Dropout(0.5, name='dropout_dense1')(x)\n",
    "    \n",
    "    x = layers.Dense(128, name='dense2')(x)\n",
    "    x = layers.BatchNormalization(name='bn_dense2')(x)\n",
    "    x = layers.Activation('relu', name='relu_dense2')(x)\n",
    "    x = layers.Dropout(0.5, name='dropout_dense2')(x)\n",
    "    \n",
    "    # Output\n",
    "    output = layers.Dense(num_classes, activation='softmax', name='output')(x)\n",
    "    \n",
    "    model = models.Model(inputs=input_layer, outputs=output, name=name)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create CNN models\n",
    "if X_train is not None and data_info is not None:\n",
    "    print(\"üöÄ Creating CNN architectures for EEG classification...\")\n",
    "    \n",
    "    input_shape = (data_info['n_channels'], data_info['n_timepoints'])\n",
    "    num_classes = data_info['n_classes']\n",
    "    \n",
    "    print(f\"\\nüìä Model Configuration:\")\n",
    "    print(f\"Input shape: {input_shape} (channels, time_points)\")\n",
    "    print(f\"Number of classes: {num_classes}\")\n",
    "    print(f\"Classes: {data_info['class_names']}\")\n",
    "    \n",
    "    # Create different CNN architectures\n",
    "    models_dict = {}\n",
    "    \n",
    "    # 1D CNN\n",
    "    cnn_1d = create_eeg_cnn_1d(input_shape, num_classes)\n",
    "    cnn_1d.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    models_dict['1D_CNN'] = cnn_1d\n",
    "    \n",
    "    # 2D CNN\n",
    "    cnn_2d = create_eeg_cnn_2d(input_shape, num_classes)\n",
    "    cnn_2d.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    models_dict['2D_CNN'] = cnn_2d\n",
    "    \n",
    "    # Hybrid CNN\n",
    "    cnn_hybrid = create_eeg_cnn_hybrid(input_shape, num_classes)\n",
    "    cnn_hybrid.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    models_dict['Hybrid_CNN'] = cnn_hybrid\n",
    "    \n",
    "    print(f\"\\n‚úÖ Created {len(models_dict)} CNN architectures!\")\n",
    "    \n",
    "    # Display model summaries\n",
    "    for name, model in models_dict.items():\n",
    "        print(f\"\\n{'-'*50}\")\n",
    "        print(f\"üìã {name} ARCHITECTURE\")\n",
    "        print(f\"{'-'*50}\")\n",
    "        model.summary()\n",
    "        \n",
    "        # Count parameters\n",
    "        total_params = model.count_params()\n",
    "        print(f\"\\nüìä {name} Parameters: {total_params:,}\")\n",
    "    \n",
    "    print(f\"\\nüéØ CNN Architecture Comparison:\")\n",
    "    print(f\"{'Model':<15} {'Parameters':<12} {'Focus':<30}\")\n",
    "    print(f\"{'-'*60}\")\n",
    "    print(f\"{'1D_CNN':<15} {models_dict['1D_CNN'].count_params():<12,} {'Temporal patterns':<30}\")\n",
    "    print(f\"{'2D_CNN':<15} {models_dict['2D_CNN'].count_params():<12,} {'Spatial-temporal patterns':<30}\")\n",
    "    print(f\"{'Hybrid_CNN':<15} {models_dict['Hybrid_CNN'].count_params():<12,} {'Spatial then temporal':<30}\")\n",
    "    \n",
    "    print(f\"\\nüí° Architecture Insights:\")\n",
    "    print(f\"‚Ä¢ 1D CNN: Best for capturing temporal dynamics in EEG\")\n",
    "    print(f\"‚Ä¢ 2D CNN: Captures both spatial and temporal relationships\")\n",
    "    print(f\"‚Ä¢ Hybrid CNN: Explicit spatial-temporal separation\")\n",
    "    print(f\"‚Ä¢ All use BatchNorm + Dropout for regularization\")\n",
    "    print(f\"‚Ä¢ Global pooling reduces overfitting\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Cannot create CNN models - no training data available\")\n",
    "    models_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Train and Evaluate CNN Models\n",
    "\n",
    "print(\"üöÄ Training CNN Models for EEG Motor Imagery Classification\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def train_eeg_model(model, X_train, y_train, X_val, y_val, model_name, epochs=50):\n",
    "    \"\"\"\n",
    "    Train an EEG CNN model with comprehensive monitoring.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\nüî• Training {model_name}...\")\n",
    "    \n",
    "    # Create callbacks\n",
    "    callbacks_list = [\n",
    "        keras.callbacks.EarlyStopping(\n",
    "            monitor='val_accuracy',\n",
    "            patience=10,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-7,\n",
    "            verbose=1\n",
    "        ),\n",
    "        keras.callbacks.ModelCheckpoint(\n",
    "            f'best_{model_name.lower()}_model.h5',\n",
    "            monitor='val_accuracy',\n",
    "            save_best_only=True,\n",
    "            save_weights_only=True,\n",
    "            verbose=0\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        batch_size=32,\n",
    "        epochs=epochs,\n",
    "        validation_data=(X_val, y_val),\n",
    "        callbacks=callbacks_list,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    return history\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, class_names, model_name):\n",
    "    \"\"\"\n",
    "    Comprehensive model evaluation.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\nüìä Evaluating {model_name}...\")\n",
    "    \n",
    "    # Get predictions\n",
    "    y_pred_proba = model.predict(X_test, verbose=0)\n",
    "    y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "    y_true = np.argmax(y_test, axis=1)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "    \n",
    "    print(f\"\\nüìà {model_name} Results:\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "    \n",
    "    # Classification report\n",
    "    print(f\"\\nüìã Detailed Classification Report:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=class_names, digits=4))\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title(f'{model_name} - Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'test_loss': test_loss,\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'y_pred': y_pred,\n",
    "        'y_true': y_true,\n",
    "        'y_pred_proba': y_pred_proba,\n",
    "        'confusion_matrix': cm\n",
    "    }\n",
    "\n",
    "def plot_training_history(histories, model_names):\n",
    "    \"\"\"\n",
    "    Plot training histories for all models.\n",
    "    \"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('EEG CNN Training Comparison', fontsize=16)\n",
    "    \n",
    "    colors = ['blue', 'red', 'green', 'orange']\n",
    "    \n",
    "    # Training accuracy\n",
    "    axes[0, 0].set_title('Training Accuracy')\n",
    "    for i, (name, history) in enumerate(zip(model_names, histories)):\n",
    "        if history:\n",
    "            axes[0, 0].plot(history.history['accuracy'], color=colors[i % len(colors)], \n",
    "                           label=f'{name}', linewidth=2)\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Accuracy')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Validation accuracy\n",
    "    axes[0, 1].set_title('Validation Accuracy')\n",
    "    for i, (name, history) in enumerate(zip(model_names, histories)):\n",
    "        if history:\n",
    "            axes[0, 1].plot(history.history['val_accuracy'], color=colors[i % len(colors)], \n",
    "                           label=f'{name}', linewidth=2)\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Validation Accuracy')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Training loss\n",
    "    axes[1, 0].set_title('Training Loss')\n",
    "    for i, (name, history) in enumerate(zip(model_names, histories)):\n",
    "        if history:\n",
    "            axes[1, 0].plot(history.history['loss'], color=colors[i % len(colors)], \n",
    "                           label=f'{name}', linewidth=2)\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Loss')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Validation loss\n",
    "    axes[1, 1].set_title('Validation Loss')\n",
    "    for i, (name, history) in enumerate(zip(model_names, histories)):\n",
    "        if history:\n",
    "            axes[1, 1].plot(history.history['val_loss'], color=colors[i % len(colors)], \n",
    "                           label=f'{name}', linewidth=2)\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('Validation Loss')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Train and evaluate models\n",
    "if X_train is not None and models_dict:\n",
    "    print(\"üéØ Starting comprehensive CNN training and evaluation...\")\n",
    "    print(f\"\\nüìä Training Configuration:\")\n",
    "    print(f\"Training samples: {X_train.shape[0]}\")\n",
    "    print(f\"Validation samples: {X_val.shape[0]}\")\n",
    "    print(f\"Test samples: {X_test.shape[0]}\")\n",
    "    print(f\"Maximum epochs: 50 (with early stopping)\")\n",
    "    print(f\"Batch size: 32\")\n",
    "    \n",
    "    # Train all models\n",
    "    training_histories = []\n",
    "    evaluation_results = {}\n",
    "    \n",
    "    for model_name, model in models_dict.items():\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"üî• TRAINING {model_name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Train model\n",
    "        history = train_eeg_model(model, X_train, y_train, X_val, y_val, model_name, epochs=50)\n",
    "        training_histories.append(history)\n",
    "        \n",
    "        # Evaluate model\n",
    "        results = evaluate_model(model, X_test, y_test, class_names, model_name)\n",
    "        evaluation_results[model_name] = results\n",
    "        \n",
    "        print(f\"\\n‚úÖ {model_name} training and evaluation completed!\")\n",
    "    \n",
    "    # Plot training comparison\n",
    "    print(f\"\\nüìà Creating training comparison plots...\")\n",
    "    plot_training_history(training_histories, list(models_dict.keys()))\n",
    "    \n",
    "    # Model comparison summary\n",
    "    print(f\"\\nüèÜ MODEL COMPARISON SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"{'Model':<15} {'Test Accuracy':<15} {'Test Loss':<12}\")\n",
    "    print(f\"{'-'*45}\")\n",
    "    \n",
    "    best_accuracy = 0\n",
    "    best_model = None\n",
    "    \n",
    "    for model_name, results in evaluation_results.items():\n",
    "        accuracy = results['test_accuracy']\n",
    "        loss = results['test_loss']\n",
    "        print(f\"{model_name:<15} {accuracy:<15.4f} {loss:<12.4f}\")\n",
    "        \n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_model = model_name\n",
    "    \n",
    "    print(f\"\\nü•á Best Model: {best_model} with {best_accuracy:.4f} ({best_accuracy*100:.2f}%) accuracy\")\n",
    "    \n",
    "    print(f\"\\nüß† EEG CNN Classification Insights:\")\n",
    "    print(f\"‚Ä¢ Motor imagery classification achieved with CNN\")\n",
    "    print(f\"‚Ä¢ Different CNN architectures capture different aspects\")\n",
    "    print(f\"‚Ä¢ Channel selection improved performance\")\n",
    "    print(f\"‚Ä¢ Temporal patterns are crucial for EEG classification\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ CNN training and evaluation completed successfully!\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Cannot train models - no data or models available\")\n",
    "    evaluation_results = {}\n",
    "    best_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Final Summary and Insights\n",
    "\n",
    "print(\"üéì EEG Motor Imagery CNN Classification - Complete Summary\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "if X_train is not None and evaluation_results:\n",
    "    print(f\"\\nüß† WHAT WE ACCOMPLISHED:\")\n",
    "    print(f\"‚úÖ Loaded PhysioNet EEG Motor Imagery dataset\")\n",
    "    print(f\"‚úÖ Preprocessed EEG signals (filtering, epoching, normalization)\")\n",
    "    print(f\"‚úÖ Analyzed and selected optimal EEG channels\")\n",
    "    print(f\"‚úÖ Designed and trained 3 different CNN architectures\")\n",
    "    print(f\"‚úÖ Achieved motor imagery classification\")\n",
    "    \n",
    "    print(f\"\\nüìä DATASET SUMMARY:\")\n",
    "    print(f\"‚Ä¢ Subjects processed: {len(epochs_list)}\")\n",
    "    print(f\"‚Ä¢ Total epochs: {X_train.shape[0] + X_val.shape[0] + X_test.shape[0]}\")\n",
    "    print(f\"‚Ä¢ EEG channels used: {data_info['n_channels']}\")\n",
    "    print(f\"‚Ä¢ Time points per epoch: {data_info['n_timepoints']}\")\n",
    "    print(f\"‚Ä¢ Motor imagery classes: {len(data_info['class_names'])}\")\n",
    "    print(f\"‚Ä¢ Class names: {', '.join(data_info['class_names'])}\")\n",
    "    \n",
    "    print(f\"\\nüèÜ FINAL RESULTS:\")\n",
    "    for model_name, results in evaluation_results.items():\n",
    "        accuracy = results['test_accuracy']\n",
    "        print(f\"‚Ä¢ {model_name}: {accuracy:.4f} ({accuracy*100:.2f}%) accuracy\")\n",
    "    \n",
    "    if best_model:\n",
    "        best_acc = evaluation_results[best_model]['test_accuracy']\n",
    "        print(f\"\\nü•á Best performing model: {best_model} ({best_acc*100:.2f}% accuracy)\")\n",
    "    \n",
    "    print(f\"\\nüîç KEY INSIGHTS LEARNED:\")\n",
    "    print(f\"\\n1. üß† EEG Signal Processing:\")\n",
    "    print(f\"   ‚Ä¢ EEG signals contain rich temporal patterns\")\n",
    "    print(f\"   ‚Ä¢ Proper filtering (7-30 Hz) removes artifacts\")\n",
    "    print(f\"   ‚Ä¢ Motor cortex channels are most informative\")\n",
    "    \n",
    "    print(f\"\\n2. üéØ Channel Selection:\")\n",
    "    print(f\"   ‚Ä¢ Not all EEG channels are equally important\")\n",
    "    print(f\"   ‚Ä¢ Motor cortex channels (C3, C4, Cz) are crucial\")\n",
    "    print(f\"   ‚Ä¢ Statistical selection can improve performance\")\n",
    "    \n",
    "    print(f\"\\n3. üèóÔ∏è CNN Architecture Design:\")\n",
    "    print(f\"   ‚Ä¢ 1D CNNs excel at temporal pattern extraction\")\n",
    "    print(f\"   ‚Ä¢ 2D CNNs capture spatial-temporal relationships\")\n",
    "    print(f\"   ‚Ä¢ Hybrid approaches combine best of both worlds\")\n",
    "    \n",
    "    print(f\"\\n4. üéÆ Motor Imagery Classification:\")\n",
    "    print(f\"   ‚Ä¢ Different motor imagery tasks create distinct patterns\")\n",
    "    print(f\"   ‚Ä¢ CNNs can automatically learn these patterns\")\n",
    "    print(f\"   ‚Ä¢ Real-time BCI applications are possible\")\n",
    "    \n",
    "    print(f\"\\nüöÄ NEXT STEPS - GRAPH CNN COMPARISON:\")\n",
    "    print(f\"üìã Coming next: Graph CNN implementation\")\n",
    "    print(f\"‚Ä¢ Traditional CNN treats channels independently\")\n",
    "    print(f\"‚Ä¢ Graph CNN models spatial relationships between channels\")\n",
    "    print(f\"‚Ä¢ Expected benefits: Better spatial feature extraction\")\n",
    "    print(f\"‚Ä¢ Channel connectivity based on brain anatomy\")\n",
    "    \n",
    "    print(f\"\\nüí° REAL-WORLD APPLICATIONS:\")\n",
    "    print(f\"üîπ Brain-Computer Interfaces (BCI)\")\n",
    "    print(f\"üîπ Assistive technology for paralyzed patients\")\n",
    "    print(f\"üîπ Neurofeedback training\")\n",
    "    print(f\"üîπ Cognitive load monitoring\")\n",
    "    print(f\"üîπ Mental state detection\")\n",
    "    \n",
    "    print(f\"\\nüìö TECHNICAL KNOWLEDGE GAINED:\")\n",
    "    print(f\"‚úÖ EEG signal processing pipeline\")\n",
    "    print(f\"‚úÖ Motor imagery neuroscience concepts\")\n",
    "    print(f\"‚úÖ CNN architecture design for time series\")\n",
    "    print(f\"‚úÖ Channel selection techniques\")\n",
    "    print(f\"‚úÖ Model evaluation and comparison\")\n",
    "    \n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  Training was not completed successfully.\")\n",
    "    print(f\"This could be due to:\")\n",
    "    print(f\"‚Ä¢ Network connectivity issues (dataset download)\")\n",
    "    print(f\"‚Ä¢ Insufficient memory\")\n",
    "    print(f\"‚Ä¢ Missing dependencies\")\n",
    "    print(f\"\\nPlease check the error messages above and ensure:\")\n",
    "    print(f\"‚Ä¢ Stable internet connection\")\n",
    "    print(f\"‚Ä¢ All required packages are installed\")\n",
    "    print(f\"‚Ä¢ Sufficient system memory\")\n",
    "\n",
    "print(f\"\\nüéØ PREPARATION FOR GRAPH CNN:\")\n",
    "print(f\"The traditional CNN approach provides our baseline.\")\n",
    "print(f\"Next, we'll implement Graph CNN to demonstrate:\")\n",
    "print(f\"‚Ä¢ How spatial relationships between EEG channels matter\")\n",
    "print(f\"‚Ä¢ Why brain anatomy should inform model architecture\")\n",
    "print(f\"‚Ä¢ How Graph Neural Networks can outperform traditional CNNs\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*65)\n",
    "print(f\"üß† EEG CNN CLASSIFICATION COMPLETE - READY FOR GRAPH CNN! üß†\")\n",
    "print(f\"=\"*65)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
