{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PhysioNet Architecture Variants Training\n",
    "\n",
    "Test the **top 3 most promising** architectural improvements to EEG-ARNN:\n",
    "\n",
    "1. **Baseline (Current)**: 3x (TFEM + 1-layer CARM) - Reference point\n",
    "2. **Multi-Layer GCN (2-layer)**: 3x (TFEM + 2-layer CARM) - Can see friends of friends (2 hops in graph)\n",
    "3. **Multi-Layer GCN (3-layer)**: 3x (TFEM + 3-layer CARM) - Can see 3 degrees of separation (3 hops in graph)\n",
    "\n",
    "**Why these 3?**\n",
    "- Multi-layer GCN is the KEY improvement from research (Kipf & Welling 2017, GCN paper)\n",
    "- 2-layer vs 3-layer tests optimal depth for EEG channel graphs\n",
    "- Same computation time as baseline (~30 min per subject)\n",
    "\n",
    "All variants trained with:\n",
    "- Same 5 subjects\n",
    "- 20 epochs (up from 10)\n",
    "- 2-fold CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import json\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import mne\n",
    "\n",
    "from train_utils import (\n",
    "    load_preprocessed_data, filter_classes, normalize_data,\n",
    "    EEGDataset\n",
    ")\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "mne.set_log_level('ERROR')\n",
    "sns.set_context('notebook', font_scale=1.1)\n",
    "plt.style.use('seaborn-v0_8')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"data\": {\n",
      "    \"preprocessed_dir\": \"data\\\\physionet\\\\derived\\\\preprocessed\",\n",
      "    \"index_file\": \"data\\\\physionet\\\\derived\\\\physionet_preprocessed_index.csv\",\n",
      "    \"selected_classes\": [\n",
      "      1,\n",
      "      2\n",
      "    ],\n",
      "    \"tmin\": -1.0,\n",
      "    \"tmax\": 5.0,\n",
      "    \"baseline\": [\n",
      "      -0.5,\n",
      "      0\n",
      "    ]\n",
      "  },\n",
      "  \"training\": {\n",
      "    \"epochs\": 20,\n",
      "    \"learning_rate\": 0.001,\n",
      "    \"batch_size\": 32,\n",
      "    \"n_folds\": 2\n",
      "  },\n",
      "  \"subjects\": [\n",
      "    \"S001\",\n",
      "    \"S002\",\n",
      "    \"S005\",\n",
      "    \"S006\",\n",
      "    \"S007\"\n",
      "  ],\n",
      "  \"output_dir\": \"results\\\\architecture_variants\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "CONFIG = {\n",
    "    'data': {\n",
    "        'preprocessed_dir': Path('data/physionet/derived/preprocessed'),\n",
    "        'index_file': Path('data/physionet/derived/physionet_preprocessed_index.csv'),\n",
    "        'selected_classes': [1, 2],\n",
    "        'tmin': -1.0,\n",
    "        'tmax': 5.0,\n",
    "        'baseline': (-0.5, 0)\n",
    "    },\n",
    "    'training': {\n",
    "        'epochs': 20,  # Increased from 10\n",
    "        'learning_rate': 0.001,\n",
    "        'batch_size': 32,\n",
    "        'n_folds': 2\n",
    "    },\n",
    "    'subjects': ['S001', 'S002', 'S005', 'S006', 'S007'],\n",
    "    'output_dir': Path('results/architecture_variants')\n",
    "}\n",
    "\n",
    "CONFIG['output_dir'].mkdir(parents=True, exist_ok=True)\n",
    "print(json.dumps(CONFIG, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improved Architecture Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Layer CARM (Can see friends of friends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerCARM(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-layer Graph Convolution for CARM\n",
    "    Each layer can see further connections in the graph\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    num_channels : int\n",
    "        Number of EEG channels\n",
    "    hidden_dim : int\n",
    "        Hidden dimension\n",
    "    num_layers : int\n",
    "        Number of GCN layers (1, 2, or 3)\n",
    "    share_adjacency : bool\n",
    "        If True, all layers share the same adjacency matrix\n",
    "        If False, each layer learns its own adjacency\n",
    "    \"\"\"\n",
    "    def __init__(self, num_channels, hidden_dim=40, num_layers=2, share_adjacency=True):\n",
    "        super(MultiLayerCARM, self).__init__()\n",
    "        \n",
    "        self.num_channels = num_channels\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.share_adjacency = share_adjacency\n",
    "        \n",
    "        # Learnable adjacency matrix(ces)\n",
    "        if share_adjacency:\n",
    "            self.W = nn.Parameter(torch.FloatTensor(num_channels, num_channels))\n",
    "            nn.init.xavier_uniform_(self.W)\n",
    "        else:\n",
    "            self.W_list = nn.ParameterList([\n",
    "                nn.Parameter(torch.FloatTensor(num_channels, num_channels))\n",
    "                for _ in range(num_layers)\n",
    "            ])\n",
    "            for W in self.W_list:\n",
    "                nn.init.xavier_uniform_(W)\n",
    "        \n",
    "        # GCN layers\n",
    "        self.gcn_layers = nn.ModuleList([\n",
    "            nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Batch norm for each layer\n",
    "        self.bn_layers = nn.ModuleList([\n",
    "            nn.BatchNorm2d(hidden_dim)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.activation = nn.ELU()\n",
    "    \n",
    "    def get_normalized_adjacency(self, W_param, device):\n",
    "        \"\"\"\n",
    "        Compute normalized adjacency matrix: D^(-1/2) * A_tilde * D^(-1/2)\n",
    "        \"\"\"\n",
    "        # Symmetric adjacency\n",
    "        A = torch.sigmoid(W_param)\n",
    "        A_sym = (A + A.t()) / 2\n",
    "        \n",
    "        # Add self-loops\n",
    "        I = torch.eye(self.num_channels, device=device)\n",
    "        A_tilde = A_sym + I\n",
    "        \n",
    "        # Degree normalization\n",
    "        D_tilde = torch.diag(A_tilde.sum(dim=1))\n",
    "        D_inv_sqrt = torch.pow(D_tilde, -0.5)\n",
    "        D_inv_sqrt[torch.isinf(D_inv_sqrt)] = 0.0\n",
    "        \n",
    "        A_norm = D_inv_sqrt @ A_tilde @ D_inv_sqrt\n",
    "        \n",
    "        return A_norm, A_sym\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch, hidden_dim, num_channels, num_timepoints)\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        hidden_dim = x.size(1)\n",
    "        num_channels = x.size(2)\n",
    "        time_steps = x.size(3)\n",
    "        \n",
    "        # Get adjacency matrices\n",
    "        if self.share_adjacency:\n",
    "            A_norm, A_sym = self.get_normalized_adjacency(self.W, x.device)\n",
    "            adj_matrices = [A_norm] * self.num_layers\n",
    "        else:\n",
    "            adj_matrices = []\n",
    "            A_sym = None\n",
    "            for W in self.W_list:\n",
    "                A_norm, A_sym_i = self.get_normalized_adjacency(W, x.device)\n",
    "                adj_matrices.append(A_norm)\n",
    "                if A_sym is None:\n",
    "                    A_sym = A_sym_i  # Return first adjacency\n",
    "        \n",
    "        # Reshape for graph convolution: (batch * time, channels, hidden_dim)\n",
    "        x_reshaped = x.permute(0, 3, 2, 1)  # (batch, time, channels, hidden)\n",
    "        x_flat = x_reshaped.contiguous().view(batch_size * time_steps, num_channels, hidden_dim)\n",
    "        \n",
    "        # Apply multi-layer GCN\n",
    "        h = x_flat\n",
    "        for layer_idx in range(self.num_layers):\n",
    "            # Graph convolution: A_norm @ H @ Theta\n",
    "            h = torch.matmul(adj_matrices[layer_idx], h)\n",
    "            h = self.gcn_layers[layer_idx](h)\n",
    "            \n",
    "            # Reshape back for batch norm\n",
    "            h_reshaped = h.view(batch_size, time_steps, num_channels, hidden_dim)\n",
    "            h_reshaped = h_reshaped.permute(0, 3, 2, 1)  # (batch, hidden, channels, time)\n",
    "            h_reshaped = self.bn_layers[layer_idx](h_reshaped)\n",
    "            h_reshaped = self.activation(h_reshaped)\n",
    "            \n",
    "            # Reshape back to flat for next layer\n",
    "            if layer_idx < self.num_layers - 1:\n",
    "                h_reshaped = h_reshaped.permute(0, 3, 2, 1)\n",
    "                h = h_reshaped.contiguous().view(batch_size * time_steps, num_channels, hidden_dim)\n",
    "        \n",
    "        return h_reshaped, A_sym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Multi-Layer CARM (With skip connections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualMultiLayerCARM(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-layer GCN with residual connections\n",
    "    Helps gradient flow and prevents degradation\n",
    "    \"\"\"\n",
    "    def __init__(self, num_channels, hidden_dim=40, num_layers=2):\n",
    "        super(ResidualMultiLayerCARM, self).__init__()\n",
    "        \n",
    "        self.num_channels = num_channels\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Shared adjacency\n",
    "        self.W = nn.Parameter(torch.FloatTensor(num_channels, num_channels))\n",
    "        nn.init.xavier_uniform_(self.W)\n",
    "        \n",
    "        # GCN layers\n",
    "        self.gcn_layers = nn.ModuleList([\n",
    "            nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.bn_layers = nn.ModuleList([\n",
    "            nn.BatchNorm2d(hidden_dim)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.activation = nn.ELU()\n",
    "    \n",
    "    def get_normalized_adjacency(self, device):\n",
    "        A = torch.sigmoid(self.W)\n",
    "        A_sym = (A + A.t()) / 2\n",
    "        I = torch.eye(self.num_channels, device=device)\n",
    "        A_tilde = A_sym + I\n",
    "        D_tilde = torch.diag(A_tilde.sum(dim=1))\n",
    "        D_inv_sqrt = torch.pow(D_tilde, -0.5)\n",
    "        D_inv_sqrt[torch.isinf(D_inv_sqrt)] = 0.0\n",
    "        A_norm = D_inv_sqrt @ A_tilde @ D_inv_sqrt\n",
    "        return A_norm, A_sym\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        hidden_dim = x.size(1)\n",
    "        num_channels = x.size(2)\n",
    "        time_steps = x.size(3)\n",
    "        \n",
    "        A_norm, A_sym = self.get_normalized_adjacency(x.device)\n",
    "        \n",
    "        # Reshape\n",
    "        x_reshaped = x.permute(0, 3, 2, 1)\n",
    "        h = x_reshaped.contiguous().view(batch_size * time_steps, num_channels, hidden_dim)\n",
    "        \n",
    "        # Store input for residual\n",
    "        h_input = h\n",
    "        \n",
    "        for layer_idx in range(self.num_layers):\n",
    "            # Graph convolution\n",
    "            h = torch.matmul(A_norm, h)\n",
    "            h = self.gcn_layers[layer_idx](h)\n",
    "            \n",
    "            # Add residual connection every 2 layers or at the end\n",
    "            if layer_idx > 0 and layer_idx % 2 == 1:\n",
    "                h = h + h_input\n",
    "            \n",
    "            # Batch norm and activation\n",
    "            h_reshaped = h.view(batch_size, time_steps, num_channels, hidden_dim)\n",
    "            h_reshaped = h_reshaped.permute(0, 3, 2, 1)\n",
    "            h_reshaped = self.bn_layers[layer_idx](h_reshaped)\n",
    "            h_reshaped = self.activation(h_reshaped)\n",
    "            \n",
    "            if layer_idx < self.num_layers - 1:\n",
    "                h_reshaped = h_reshaped.permute(0, 3, 2, 1)\n",
    "                h = h_reshaped.contiguous().view(batch_size * time_steps, num_channels, hidden_dim)\n",
    "        \n",
    "        return h_reshaped, A_sym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFEM (Same as before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFEM(nn.Module):\n",
    "    \"\"\"\n",
    "    Temporal Feature Extraction Module\n",
    "    1D CNN across time dimension\n",
    "    \"\"\"\n",
    "    def __init__(self, num_channels, hidden_dim=40, in_channels=None):\n",
    "        super(TFEM, self).__init__()\n",
    "        \n",
    "        self.num_channels = num_channels\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # If in_channels not specified, assume first block (1 channel input)\n",
    "        if in_channels is None:\n",
    "            in_channels = 1\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.conv = nn.Conv2d(in_channels, hidden_dim, (1, 5), padding=(0, 2))\n",
    "        self.bn = nn.BatchNorm2d(hidden_dim)\n",
    "        self.activation = nn.ELU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch, channels, timepoints) or (batch, hidden_dim, channels, timepoints)\n",
    "        \"\"\"\n",
    "        if x.dim() == 3:\n",
    "            # First block: add channel dimension\n",
    "            x = x.unsqueeze(1)  # (batch, 1, channels, time)\n",
    "        \n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.activation(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture Variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGARNNVariant(nn.Module):\n",
    "    \"\"\"\n",
    "    Flexible EEG-ARNN that supports different configurations\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    variant : str\n",
    "        - 'baseline': 3x (TFEM + 1-layer CARM)\n",
    "        - 'multi2': 3x (TFEM + 2-layer CARM)\n",
    "        - 'multi3': 3x (TFEM + 3-layer CARM)\n",
    "        - 'wider': 3x (TFEM + 1-layer CARM) with hidden_dim=60\n",
    "        - 'deeper': 5x (TFEM + 1-layer CARM)\n",
    "        - 'residual': 3x (TFEM + 2-layer Residual CARM)\n",
    "    \"\"\"\n",
    "    def __init__(self, num_channels, num_timepoints, num_classes, variant='baseline'):\n",
    "        super(EEGARNNVariant, self).__init__()\n",
    "        \n",
    "        self.variant = variant\n",
    "        self.num_channels = num_channels\n",
    "        \n",
    "        # Set architecture parameters\n",
    "        if variant == 'baseline':\n",
    "            hidden_dim = 40\n",
    "            num_blocks = 3\n",
    "            gcn_layers = 1\n",
    "            use_residual = False\n",
    "        elif variant == 'multi2':\n",
    "            hidden_dim = 40\n",
    "            num_blocks = 3\n",
    "            gcn_layers = 2\n",
    "            use_residual = False\n",
    "        elif variant == 'multi3':\n",
    "            hidden_dim = 40\n",
    "            num_blocks = 3\n",
    "            gcn_layers = 3\n",
    "            use_residual = False\n",
    "        elif variant == 'wider':\n",
    "            hidden_dim = 60\n",
    "            num_blocks = 3\n",
    "            gcn_layers = 1\n",
    "            use_residual = False\n",
    "        elif variant == 'deeper':\n",
    "            hidden_dim = 40\n",
    "            num_blocks = 5\n",
    "            gcn_layers = 1\n",
    "            use_residual = False\n",
    "        elif variant == 'residual':\n",
    "            hidden_dim = 40\n",
    "            num_blocks = 3\n",
    "            gcn_layers = 2\n",
    "            use_residual = True\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown variant: {variant}\")\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_blocks = num_blocks\n",
    "        \n",
    "        # Build TFEM-CARM blocks\n",
    "        self.tfem_blocks = nn.ModuleList()\n",
    "        self.carm_blocks = nn.ModuleList()\n",
    "        \n",
    "        for i in range(num_blocks):\n",
    "            # First TFEM has 1 input channel, rest have hidden_dim input channels\n",
    "            in_channels = 1 if i == 0 else hidden_dim\n",
    "            self.tfem_blocks.append(TFEM(num_channels, hidden_dim, in_channels=in_channels))\n",
    "            \n",
    "            if use_residual:\n",
    "                self.carm_blocks.append(\n",
    "                    ResidualMultiLayerCARM(num_channels, hidden_dim, num_layers=gcn_layers)\n",
    "                )\n",
    "            else:\n",
    "                # Use MultiLayerCARM for all variants (including single-layer)\n",
    "                self.carm_blocks.append(\n",
    "                    MultiLayerCARM(num_channels, hidden_dim, num_layers=gcn_layers)\n",
    "                )\n",
    "        \n",
    "        # Global pooling and classifier\n",
    "        self.pool = nn.AdaptiveAvgPool2d((num_channels, 1))\n",
    "        self.fc = nn.Linear(hidden_dim * num_channels, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch, channels, timepoints)\n",
    "        \"\"\"\n",
    "        adjacency_matrices = []\n",
    "        \n",
    "        for i in range(self.num_blocks):\n",
    "            # TFEM\n",
    "            x = self.tfem_blocks[i](x)\n",
    "            \n",
    "            # CARM\n",
    "            x, adj = self.carm_blocks[i](x)\n",
    "            adjacency_matrices.append(adj)\n",
    "        \n",
    "        # Pool and classify\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        # Return last adjacency matrix for channel selection\n",
    "        return x, adjacency_matrices[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_subject_data(subject_id, config):\n",
    "    \"\"\"\n",
    "    Load all motor runs for a subject\n",
    "    \"\"\"\n",
    "    index_df = pd.read_csv(config['data']['index_file'])\n",
    "    success_df = index_df[index_df['status'] == 'success']\n",
    "    motor_runs = success_df[success_df['category'].isin(['motor_execution', 'motor_imagery'])]\n",
    "    \n",
    "    subject_runs = motor_runs[motor_runs['subject'] == subject_id]\n",
    "    \n",
    "    all_data = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for _, run_info in subject_runs.iterrows():\n",
    "        fif_path = Path(run_info['path'])\n",
    "        \n",
    "        if not fif_path.exists():\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            data, labels = load_preprocessed_data(\n",
    "                fif_path,\n",
    "                tmin=config['data']['tmin'],\n",
    "                tmax=config['data']['tmax'],\n",
    "                baseline=config['data']['baseline']\n",
    "            )\n",
    "            \n",
    "            if data is not None and len(data) > 0:\n",
    "                all_data.append(data)\n",
    "                all_labels.append(labels)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {fif_path.name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if len(all_data) == 0:\n",
    "        return None, None\n",
    "    \n",
    "    all_data = np.concatenate(all_data, axis=0)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "    \n",
    "    # Filter to selected classes\n",
    "    all_data, all_labels = filter_classes(\n",
    "        all_data, all_labels, config['data']['selected_classes']\n",
    "    )\n",
    "    \n",
    "    return all_data, all_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_variant(variant_name, data, labels, config, device):\n",
    "    \"\"\"\n",
    "    Train a specific architecture variant with cross-validation\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    results : dict\n",
    "        Mean accuracy, std, training time, adjacency matrix\n",
    "    \"\"\"\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    \n",
    "    num_channels = data.shape[1]\n",
    "    num_timepoints = data.shape[2]\n",
    "    num_classes = len(np.unique(labels))\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=config['training']['n_folds'], shuffle=True, random_state=42)\n",
    "    \n",
    "    fold_accuracies = []\n",
    "    fold_times = []\n",
    "    final_adjacency = None\n",
    "    \n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(skf.split(data, labels), 1):\n",
    "        print(f\"    Fold {fold_idx}/{config['training']['n_folds']}...\", end=' ')\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        X_train, X_val = data[train_idx], data[val_idx]\n",
    "        y_train, y_val = labels[train_idx], labels[val_idx]\n",
    "        \n",
    "        # Create model\n",
    "        model = EEGARNNVariant(\n",
    "            num_channels=num_channels,\n",
    "            num_timepoints=num_timepoints,\n",
    "            num_classes=num_classes,\n",
    "            variant=variant_name\n",
    "        ).to(device)\n",
    "        \n",
    "        # Data loaders\n",
    "        train_dataset = EEGDataset(X_train, y_train)\n",
    "        val_dataset = EEGDataset(X_val, y_val)\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=config['training']['batch_size'], shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=config['training']['batch_size'], shuffle=False)\n",
    "        \n",
    "        # Training\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=config['training']['learning_rate'])\n",
    "        \n",
    "        best_val_acc = 0.0\n",
    "        best_epoch = 0\n",
    "        \n",
    "        for epoch in range(config['training']['epochs']):\n",
    "            # Train\n",
    "            model.train()\n",
    "            for batch_data, batch_labels in train_loader:\n",
    "                batch_data = batch_data.to(device).float()\n",
    "                batch_labels = batch_labels.to(device).long()\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs, _ = model(batch_data)\n",
    "                loss = criterion(outputs, batch_labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            # Validate\n",
    "            model.eval()\n",
    "            val_correct = 0\n",
    "            val_total = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for batch_data, batch_labels in val_loader:\n",
    "                    batch_data = batch_data.to(device).float()\n",
    "                    batch_labels = batch_labels.to(device).long()\n",
    "                    \n",
    "                    outputs, adj = model(batch_data)\n",
    "                    _, predicted = torch.max(outputs, 1)\n",
    "                    \n",
    "                    val_total += batch_labels.size(0)\n",
    "                    val_correct += (predicted == batch_labels).sum().item()\n",
    "            \n",
    "            val_acc = val_correct / val_total\n",
    "            \n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                best_epoch = epoch + 1\n",
    "                final_adjacency = adj.detach().cpu().numpy()\n",
    "        \n",
    "        fold_time = time.time() - start_time\n",
    "        fold_accuracies.append(best_val_acc)\n",
    "        fold_times.append(fold_time)\n",
    "        \n",
    "        print(f\"Acc: {best_val_acc:.4f} (epoch {best_epoch}), Time: {fold_time:.1f}s\")\n",
    "    \n",
    "    return {\n",
    "        'mean_accuracy': np.mean(fold_accuracies),\n",
    "        'std_accuracy': np.std(fold_accuracies),\n",
    "        'mean_time': np.mean(fold_times),\n",
    "        'fold_accuracies': fold_accuracies,\n",
    "        'adjacency_matrix': final_adjacency\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run All Variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing subjects:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Subject: S001\n",
      "================================================================================\n",
      "Data shape: (231, 64, 769)\n",
      "Labels: (array([0, 1]), array([154,  77], dtype=int64))\n",
      "\n",
      "  Variant: baseline - 3x (TFEM + 1-layer CARM)\n",
      "    Fold 1/2... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing subjects:   0%|          | 0/5 [01:06<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m variant_name, variant_desc \u001b[38;5;129;01min\u001b[39;00m variants.items():\n\u001b[32m     27\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m  Variant: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvariant_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvariant_desc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     results = \u001b[43mtrain_variant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvariant_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Final: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults[\u001b[33m'\u001b[39m\u001b[33mmean_accuracy\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ± \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults[\u001b[33m'\u001b[39m\u001b[33mstd_accuracy\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     32\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Avg time per fold: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults[\u001b[33m'\u001b[39m\u001b[33mmean_time\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33ms\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 62\u001b[39m, in \u001b[36mtrain_variant\u001b[39m\u001b[34m(variant_name, data, labels, config, device)\u001b[39m\n\u001b[32m     60\u001b[39m     outputs, _ = model(batch_data)\n\u001b[32m     61\u001b[39m     loss = criterion(outputs, batch_labels)\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m     \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     63\u001b[39m     optimizer.step()\n\u001b[32m     65\u001b[39m \u001b[38;5;66;03m# Validate\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\_tensor.py:521\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    511\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    512\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    513\u001b[39m         Tensor.backward,\n\u001b[32m    514\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    519\u001b[39m         inputs=inputs,\n\u001b[32m    520\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m521\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    522\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    523\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\autograd\\__init__.py:289\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    284\u001b[39m     retain_graph = create_graph\n\u001b[32m    286\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    287\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    288\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m289\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    291\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    292\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    293\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    294\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\autograd\\graph.py:769\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    767\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    768\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m769\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    770\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    771\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    772\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    773\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Define TOP 3 variants to test (reduced from 6)\n",
    "variants = {\n",
    "    'baseline': '3x (TFEM + 1-layer CARM)',\n",
    "    'multi2': '3x (TFEM + 2-layer CARM) - 2 hops',\n",
    "    'multi3': '3x (TFEM + 3-layer CARM) - 3 hops'\n",
    "}\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for subject_id in tqdm(CONFIG['subjects'], desc='Processing subjects'):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Subject: {subject_id}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Load data\n",
    "    data, labels = load_subject_data(subject_id, CONFIG)\n",
    "    \n",
    "    if data is None or len(data) < 30:\n",
    "        print(f\"Skipping {subject_id}: insufficient data\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"Data shape: {data.shape}\")\n",
    "    print(f\"Labels: {np.unique(labels, return_counts=True)}\")\n",
    "    \n",
    "    # Test each variant\n",
    "    for variant_name, variant_desc in variants.items():\n",
    "        print(f\"\\n  Variant: {variant_name} - {variant_desc}\")\n",
    "        \n",
    "        results = train_variant(variant_name, data, labels, CONFIG, device)\n",
    "        \n",
    "        print(f\"  Final: {results['mean_accuracy']:.4f} ± {results['std_accuracy']:.4f}\")\n",
    "        print(f\"  Avg time per fold: {results['mean_time']:.1f}s\")\n",
    "        \n",
    "        all_results.append({\n",
    "            'subject': subject_id,\n",
    "            'variant': variant_name,\n",
    "            'description': variant_desc,\n",
    "            'accuracy': results['mean_accuracy'],\n",
    "            'std': results['std_accuracy'],\n",
    "            'time': results['mean_time']\n",
    "        })\n",
    "\n",
    "results_df = pd.DataFrame(all_results)\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"All variants tested!\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(results_df) > 0:\n",
    "    # Aggregate by variant\n",
    "    summary = results_df.groupby(['variant', 'description']).agg({\n",
    "        'accuracy': ['mean', 'std'],\n",
    "        'time': 'mean'\n",
    "    }).round(4)\n",
    "    \n",
    "    summary.columns = ['Mean Accuracy', 'Std Accuracy', 'Mean Time (s)']\n",
    "    summary = summary.sort_values('Mean Accuracy', ascending=False)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ARCHITECTURE VARIANT COMPARISON\")\n",
    "    print(\"=\"*80)\n",
    "    display(summary)\n",
    "    \n",
    "    # Find best variant\n",
    "    best_variant = summary['Mean Accuracy'].idxmax()[0]\n",
    "    best_acc = summary['Mean Accuracy'].max()\n",
    "    baseline_acc = summary.loc[('baseline', variants['baseline']), 'Mean Accuracy']\n",
    "    improvement = (best_acc - baseline_acc) / baseline_acc * 100\n",
    "    \n",
    "    print(f\"\\nBest variant: {best_variant} ({best_acc:.4f})\")\n",
    "    print(f\"Baseline: {baseline_acc:.4f}\")\n",
    "    print(f\"Improvement: {improvement:+.2f}%\")\n",
    "    \n",
    "    # Save results\n",
    "    results_path = CONFIG['output_dir'] / 'variant_results.csv'\n",
    "    results_df.to_csv(results_path, index=False)\n",
    "    print(f\"\\nResults saved to: {results_path}\")\n",
    "    \n",
    "    summary_path = CONFIG['output_dir'] / 'variant_summary.csv'\n",
    "    summary.to_csv(summary_path)\n",
    "    print(f\"Summary saved to: {summary_path}\")\n",
    "else:\n",
    "    print(\"No results to summarize.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(results_df) > 0:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "    \n",
    "    # 1. Mean accuracy comparison\n",
    "    variant_summary = results_df.groupby('variant').agg({\n",
    "        'accuracy': ['mean', 'std']\n",
    "    }).reset_index()\n",
    "    variant_summary.columns = ['variant', 'mean', 'std']\n",
    "    variant_summary = variant_summary.sort_values('mean', ascending=False)\n",
    "    \n",
    "    colors = ['#2ecc71' if v == 'baseline' else '#3498db' for v in variant_summary['variant']]\n",
    "    \n",
    "    axes[0, 0].barh(variant_summary['variant'], variant_summary['mean'],\n",
    "                    xerr=variant_summary['std'], color=colors, alpha=0.8, capsize=5)\n",
    "    axes[0, 0].set_xlabel('Accuracy', fontsize=12)\n",
    "    axes[0, 0].set_title('Variant Comparison (Mean ± Std)', fontsize=14, fontweight='bold')\n",
    "    axes[0, 0].grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    # 2. Per-subject comparison\n",
    "    pivot = results_df.pivot(index='subject', columns='variant', values='accuracy')\n",
    "    pivot.plot(kind='bar', ax=axes[0, 1], rot=0, alpha=0.8, width=0.8)\n",
    "    axes[0, 1].set_ylabel('Accuracy', fontsize=12)\n",
    "    axes[0, 1].set_title('Per-Subject Comparison', fontsize=14, fontweight='bold')\n",
    "    axes[0, 1].legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)\n",
    "    axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # 3. Training time comparison\n",
    "    time_summary = results_df.groupby('variant')['time'].mean().sort_values(ascending=False)\n",
    "    axes[1, 0].barh(time_summary.index, time_summary.values, color='coral', alpha=0.8)\n",
    "    axes[1, 0].set_xlabel('Training Time (s) per fold', fontsize=12)\n",
    "    axes[1, 0].set_title('Computational Cost', fontsize=14, fontweight='bold')\n",
    "    axes[1, 0].grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    # 4. Accuracy vs Time trade-off\n",
    "    tradeoff = results_df.groupby('variant').agg({\n",
    "        'accuracy': 'mean',\n",
    "        'time': 'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "    axes[1, 1].scatter(tradeoff['time'], tradeoff['accuracy'], s=200, alpha=0.6)\n",
    "    for _, row in tradeoff.iterrows():\n",
    "        axes[1, 1].annotate(row['variant'], (row['time'], row['accuracy']),\n",
    "                           fontsize=9, ha='right')\n",
    "    axes[1, 1].set_xlabel('Training Time (s) per fold', fontsize=12)\n",
    "    axes[1, 1].set_ylabel('Mean Accuracy', fontsize=12)\n",
    "    axes[1, 1].set_title('Accuracy vs Computational Cost', fontsize=14, fontweight='bold')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    fig_path = CONFIG['output_dir'] / 'variant_comparison.png'\n",
    "    plt.savefig(fig_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nVisualization saved to: {fig_path}\")\n",
    "else:\n",
    "    print(\"No results to visualize.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "if len(results_df) > 0:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STATISTICAL SIGNIFICANCE (Paired t-test vs Baseline)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    baseline_accs = results_df[results_df['variant'] == 'baseline'].sort_values('subject')['accuracy'].values\n",
    "    \n",
    "    stat_results = []\n",
    "    \n",
    "    for variant in results_df['variant'].unique():\n",
    "        if variant != 'baseline':\n",
    "            variant_accs = results_df[results_df['variant'] == variant].sort_values('subject')['accuracy'].values\n",
    "            \n",
    "            if len(variant_accs) == len(baseline_accs):\n",
    "                t_stat, p_value = stats.ttest_rel(variant_accs, baseline_accs)\n",
    "                \n",
    "                mean_diff = np.mean(variant_accs - baseline_accs)\n",
    "                pct_improvement = (mean_diff / np.mean(baseline_accs)) * 100\n",
    "                \n",
    "                significance = '***' if p_value < 0.001 else '**' if p_value < 0.01 else '*' if p_value < 0.05 else 'ns'\n",
    "                \n",
    "                stat_results.append({\n",
    "                    'Variant': variant,\n",
    "                    'Mean Diff': mean_diff,\n",
    "                    'Improvement (%)': pct_improvement,\n",
    "                    't-statistic': t_stat,\n",
    "                    'p-value': p_value,\n",
    "                    'Significance': significance\n",
    "                })\n",
    "    \n",
    "    stat_df = pd.DataFrame(stat_results)\n",
    "    stat_df = stat_df.sort_values('Improvement (%)', ascending=False)\n",
    "    \n",
    "    # Format for display\n",
    "    stat_df_display = stat_df.copy()\n",
    "    stat_df_display['Mean Diff'] = stat_df_display['Mean Diff'].apply(lambda x: f\"{x:.4f}\")\n",
    "    stat_df_display['Improvement (%)'] = stat_df_display['Improvement (%)'].apply(lambda x: f\"{x:+.2f}%\")\n",
    "    stat_df_display['t-statistic'] = stat_df_display['t-statistic'].apply(lambda x: f\"{x:.3f}\")\n",
    "    stat_df_display['p-value'] = stat_df_display['p-value'].apply(lambda x: f\"{x:.4f}\")\n",
    "    \n",
    "    display(stat_df_display)\n",
    "    \n",
    "    print(\"\\nSignificance codes: *** p<0.001, ** p<0.01, * p<0.05, ns: not significant\")\n",
    "    \n",
    "    stat_path = CONFIG['output_dir'] / 'statistical_significance.csv'\n",
    "    stat_df.to_csv(stat_path, index=False)\n",
    "    print(f\"\\nStatistical results saved to: {stat_path}\")\n",
    "else:\n",
    "    print(\"No results for statistical testing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(results_df) > 0:\n",
    "    # Recompute summary statistics for the report\n",
    "    summary = results_df.groupby(['variant', 'description']).agg({\n",
    "        'accuracy': ['mean', 'std'],\n",
    "        'time': 'mean'\n",
    "    }).round(4)\n",
    "    \n",
    "    summary.columns = ['Mean Accuracy', 'Std Accuracy', 'Mean Time (s)']\n",
    "    summary = summary.sort_values('Mean Accuracy', ascending=False)\n",
    "    \n",
    "    # Find best variant\n",
    "    best_variant = summary['Mean Accuracy'].idxmax()[0]\n",
    "    best_acc = summary['Mean Accuracy'].max()\n",
    "    baseline_acc = summary.loc[('baseline', variants['baseline']), 'Mean Accuracy']\n",
    "    improvement = (best_acc - baseline_acc) / baseline_acc * 100\n",
    "    \n",
    "    # Generate report\n",
    "    report = []\n",
    "    report.append(\"# Architecture Variant Analysis Report\\n\")\n",
    "    report.append(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "    \n",
    "    report.append(\"## Objective\\n\")\n",
    "    report.append(\"Evaluate architectural improvements to EEG-ARNN to find optimal configuration.\\n\\n\")\n",
    "    \n",
    "    report.append(\"## Variants Tested\\n\\n\")\n",
    "    for variant_name, variant_desc in variants.items():\n",
    "        report.append(f\"- **{variant_name}**: {variant_desc}\\n\")\n",
    "    report.append(\"\\n\")\n",
    "    \n",
    "    report.append(\"## Results\\n\\n\")\n",
    "    for idx, row in summary.iterrows():\n",
    "        variant_name = idx[0]\n",
    "        report.append(f\"**{variant_name}**\\n\")\n",
    "        report.append(f\"- Accuracy: {row['Mean Accuracy']:.4f} +/- {row['Std Accuracy']:.4f}\\n\")\n",
    "        report.append(f\"- Time: {row['Mean Time (s)']:.1f}s per fold\\n\\n\")\n",
    "    \n",
    "    report.append(\"## Key Findings\\n\\n\")\n",
    "    report.append(f\"1. **Best variant**: {best_variant} ({best_acc:.4f})\\n\")\n",
    "    report.append(f\"2. **Baseline**: {baseline_acc:.4f}\\n\")\n",
    "    report.append(f\"3. **Improvement**: {improvement:+.2f}%\\n\\n\")\n",
    "    \n",
    "    report.append(\"## Recommendations\\n\\n\")\n",
    "    if improvement > 2:\n",
    "        report.append(f\"The **{best_variant}** architecture shows significant improvement over baseline. \")\n",
    "        report.append(\"Recommend using this configuration for final experiments.\\n\")\n",
    "    elif improvement > 0:\n",
    "        report.append(f\"The **{best_variant}** architecture shows modest improvement. \")\n",
    "        report.append(\"Consider trade-off with computational cost.\\n\")\n",
    "    else:\n",
    "        report.append(\"No variant significantly outperforms baseline. \")\n",
    "        report.append(\"Baseline architecture is optimal for this dataset.\\n\")\n",
    "    \n",
    "    report_text = ''.join(report)\n",
    "    print(report_text)\n",
    "    \n",
    "    report_path = CONFIG['output_dir'] / 'VARIANT_ANALYSIS_REPORT.md'\n",
    "    with open(report_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(report_text)\n",
    "    \n",
    "    print(f\"\\nReport saved to: {report_path}\")\n",
    "else:\n",
    "    print(\"No results to generate report.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
