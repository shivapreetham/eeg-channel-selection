{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Foundations: From Scratch Understanding\n",
    "\n",
    "This notebook teaches **every fundamental concept** in deep learning from the ground up.\n",
    "\n",
    "## What You'll Learn:\n",
    "1. **Tensors**: The building blocks of neural networks\n",
    "2. **Gradients**: How networks learn through calculus\n",
    "3. **Backpropagation**: The core learning algorithm\n",
    "4. **Loss Functions**: How we measure network performance\n",
    "5. **Optimizers**: How we update network weights\n",
    "6. **Activation Functions**: How neurons make decisions\n",
    "\n",
    "**Teaching Philosophy**: Every line of code will be explained in detail with mathematical intuition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Essential Imports with Explanations\n",
    "\"\"\"\n",
    "LIBRARY EXPLANATIONS:\n",
    "\n",
    "numpy: Mathematical operations on arrays (tensors)\n",
    "- Why needed: Neural networks are just matrix multiplications\n",
    "- What it does: Efficient numerical computations\n",
    "\n",
    "matplotlib: Data visualization\n",
    "- Why needed: Understanding data and model behavior visually\n",
    "- What we'll plot: Loss curves, gradients, decision boundaries\n",
    "\n",
    "tensorflow: Deep learning framework\n",
    "- Why needed: Automatic differentiation and GPU acceleration\n",
    "- Key feature: Computes gradients automatically\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducible results\n",
    "np.random.seed(42)  # NumPy random seed\n",
    "tf.random.set_seed(42)  # TensorFlow random seed\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(\"All libraries imported successfully!\")\n",
    "\n",
    "# Configure matplotlib for better plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding Tensors: The Foundation of Neural Networks\n",
    "\n",
    "**What is a Tensor?**\n",
    "- A tensor is a generalization of scalars, vectors, and matrices\n",
    "- **Scalar** (0D tensor): Just a number ‚Üí `5`\n",
    "- **Vector** (1D tensor): Array of numbers ‚Üí `[1, 2, 3]`\n",
    "- **Matrix** (2D tensor): Table of numbers ‚Üí `[[1,2], [3,4]]`\n",
    "- **3D+ tensors**: Higher dimensional arrays\n",
    "\n",
    "**Why Tensors Matter in Deep Learning:**\n",
    "- Images are 3D tensors: `(height, width, channels)`\n",
    "- Text is 2D tensors: `(sequence_length, vocabulary_size)`\n",
    "- Neural network weights are matrices (2D tensors)\n",
    "- All operations in neural networks are tensor operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Tensor Fundamentals with Detailed Explanations\n",
    "\n",
    "print(\"=== TENSOR DIMENSIONS EXPLAINED ===\")\n",
    "\n",
    "# 0D Tensor (Scalar)\n",
    "scalar = tf.constant(42.0)\n",
    "print(f\"\\n0D Tensor (Scalar):\")\n",
    "print(f\"Value: {scalar}\")\n",
    "print(f\"Shape: {scalar.shape} ‚Üê Empty shape means 0 dimensions\")\n",
    "print(f\"Number of dimensions (rank): {len(scalar.shape)}\")\n",
    "\n",
    "# 1D Tensor (Vector)\n",
    "vector = tf.constant([1.0, 2.0, 3.0, 4.0])\n",
    "print(f\"\\n1D Tensor (Vector):\")\n",
    "print(f\"Value: {vector}\")\n",
    "print(f\"Shape: {vector.shape} ‚Üê (4,) means 4 elements in 1 dimension\")\n",
    "print(f\"Number of dimensions (rank): {len(vector.shape)}\")\n",
    "\n",
    "# 2D Tensor (Matrix)\n",
    "matrix = tf.constant([[1.0, 2.0, 3.0],\n",
    "                      [4.0, 5.0, 6.0]])\n",
    "print(f\"\\n2D Tensor (Matrix):\")\n",
    "print(f\"Value:\\n{matrix}\")\n",
    "print(f\"Shape: {matrix.shape} ‚Üê (2, 3) means 2 rows, 3 columns\")\n",
    "print(f\"Number of dimensions (rank): {len(matrix.shape)}\")\n",
    "\n",
    "# 3D Tensor (like a small grayscale image)\n",
    "tensor_3d = tf.constant([[[1, 2], [3, 4]],\n",
    "                         [[5, 6], [7, 8]]])\n",
    "print(f\"\\n3D Tensor:\")\n",
    "print(f\"Value:\\n{tensor_3d}\")\n",
    "print(f\"Shape: {tensor_3d.shape} ‚Üê (2, 2, 2) means 2x2x2 cube\")\n",
    "print(f\"Number of dimensions (rank): {len(tensor_3d.shape)}\")\n",
    "\n",
    "# Real-world example: RGB image\n",
    "rgb_image = tf.random.uniform((224, 224, 3), minval=0, maxval=255, dtype=tf.int32)\n",
    "print(f\"\\nReal Example - RGB Image Tensor:\")\n",
    "print(f\"Shape: {rgb_image.shape} ‚Üê (height=224, width=224, channels=3)\")\n",
    "print(f\"Total elements: {tf.size(rgb_image)} ‚Üê 224√ó224√ó3 = {224*224*3:,}\")\n",
    "print(f\"Data type: {rgb_image.dtype} ‚Üê Integer values 0-255 for pixel intensities\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tensor Operations: The Math Behind Neural Networks\n",
    "\n",
    "**Key Operations in Neural Networks:**\n",
    "\n",
    "1. **Matrix Multiplication (`@` or `tf.matmul`)**:\n",
    "   - Core operation in neural networks\n",
    "   - `input @ weights = output`\n",
    "   - Requirement: Inner dimensions must match\n",
    "\n",
    "2. **Element-wise Operations (`+`, `-`, `*`, `/`)**:\n",
    "   - Adding bias: `output + bias`\n",
    "   - Activation functions: `sigmoid(x)`, `relu(x)`\n",
    "\n",
    "3. **Broadcasting**:\n",
    "   - Automatic shape adjustment for operations\n",
    "   - Example: Adding bias vector to matrix\n",
    "\n",
    "**Mathematical Foundation:**\n",
    "- Neural network layer: `output = activation(input @ weights + bias)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Tensor Operations with Step-by-Step Explanations\n",
    "\n",
    "print(\"=== NEURAL NETWORK OPERATIONS EXPLAINED ===\")\n",
    "\n",
    "# Simulate a simple neural network layer\n",
    "print(\"\\nüß† SIMULATING A NEURAL NETWORK LAYER\")\n",
    "print(\"Formula: output = input @ weights + bias\")\n",
    "\n",
    "# Step 1: Create input data (batch of 3 samples, 4 features each)\n",
    "input_data = tf.constant([[1.0, 2.0, 3.0, 4.0],    # Sample 1\n",
    "                          [0.5, 1.5, 2.5, 3.5],    # Sample 2  \n",
    "                          [2.0, 1.0, 4.0, 3.0]])   # Sample 3\n",
    "\n",
    "print(f\"\\nüìä INPUT DATA:\")\n",
    "print(f\"Shape: {input_data.shape} ‚Üê (batch_size=3, input_features=4)\")\n",
    "print(f\"Values:\\n{input_data}\")\n",
    "print(f\"Meaning: 3 samples, each with 4 features (like height, weight, age, income)\")\n",
    "\n",
    "# Step 2: Create weights (4 input features ‚Üí 2 output neurons)\n",
    "weights = tf.constant([[0.1, 0.2],    # Weights for feature 1 ‚Üí [neuron1, neuron2]\n",
    "                       [0.3, 0.4],    # Weights for feature 2 ‚Üí [neuron1, neuron2]\n",
    "                       [0.5, 0.6],    # Weights for feature 3 ‚Üí [neuron1, neuron2]\n",
    "                       [0.7, 0.8]])   # Weights for feature 4 ‚Üí [neuron1, neuron2]\n",
    "\n",
    "print(f\"\\n‚öñÔ∏è WEIGHTS:\")\n",
    "print(f\"Shape: {weights.shape} ‚Üê (input_features=4, output_neurons=2)\")\n",
    "print(f\"Values:\\n{weights}\")\n",
    "print(f\"Meaning: How much each input feature contributes to each output neuron\")\n",
    "\n",
    "# Step 3: Matrix multiplication (the core operation)\n",
    "linear_output = tf.matmul(input_data, weights)\n",
    "# Alternative syntax: linear_output = input_data @ weights\n",
    "\n",
    "print(f\"\\nüî¢ MATRIX MULTIPLICATION:\")\n",
    "print(f\"Operation: input_data @ weights\")\n",
    "print(f\"Shape calculation: {input_data.shape} @ {weights.shape} = {linear_output.shape}\")\n",
    "print(f\"Result:\\n{linear_output}\")\n",
    "print(f\"\\nWhat happened: Each input sample was transformed into 2 values (one per neuron)\")\n",
    "\n",
    "# Step 4: Add bias (one bias value per output neuron)\n",
    "bias = tf.constant([0.1, -0.1])  # Bias for [neuron1, neuron2]\n",
    "\n",
    "print(f\"\\nüìç BIAS:\")\n",
    "print(f\"Shape: {bias.shape} ‚Üê (output_neurons=2,)\")\n",
    "print(f\"Values: {bias}\")\n",
    "print(f\"Purpose: Shifts the output, allows learning patterns that don't pass through origin\")\n",
    "\n",
    "# Broadcasting: bias is added to each row of linear_output\n",
    "final_output = linear_output + bias\n",
    "\n",
    "print(f\"\\n‚ûï ADDING BIAS (Broadcasting):\")\n",
    "print(f\"Operation: linear_output + bias\")\n",
    "print(f\"Shape: {linear_output.shape} + {bias.shape} = {final_output.shape}\")\n",
    "print(f\"Final output:\\n{final_output}\")\n",
    "print(f\"\\n‚úÖ Complete neural network layer computation!\")\n",
    "\n",
    "# Visualize the operation\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(input_data, cmap='viridis', aspect='auto')\n",
    "plt.title(f'Input Data\\nShape: {input_data.shape}')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Samples')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(weights, cmap='plasma', aspect='auto')\n",
    "plt.title(f'Weights\\nShape: {weights.shape}')\n",
    "plt.xlabel('Output Neurons')\n",
    "plt.ylabel('Input Features')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(final_output, cmap='coolwarm', aspect='auto')\n",
    "plt.title(f'Output\\nShape: {final_output.shape}')\n",
    "plt.xlabel('Neurons')\n",
    "plt.ylabel('Samples')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.suptitle('Neural Network Layer: Input ‚Üí Weights ‚Üí Output', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Activation Functions: How Neurons Make Decisions\n",
    "\n",
    "**Why Activation Functions?**\n",
    "- Without activation functions, neural networks are just linear transformations\n",
    "- Activation functions introduce **non-linearity**\n",
    "- Non-linearity allows networks to learn complex patterns\n",
    "\n",
    "**Common Activation Functions:**\n",
    "\n",
    "1. **ReLU (Rectified Linear Unit)**: `max(0, x)`\n",
    "   - Most popular in hidden layers\n",
    "   - Fast to compute, helps with gradient flow\n",
    "   - Problem: \"Dead neurons\" when x < 0\n",
    "\n",
    "2. **Sigmoid**: `1 / (1 + e^(-x))`\n",
    "   - Outputs between 0 and 1\n",
    "   - Used for binary classification\n",
    "   - Problem: Vanishing gradients\n",
    "\n",
    "3. **Tanh**: `(e^x - e^(-x)) / (e^x + e^(-x))`\n",
    "   - Outputs between -1 and 1\n",
    "   - Zero-centered (better than sigmoid)\n",
    "\n",
    "4. **Softmax**: Used for multi-class classification\n",
    "   - Converts logits to probabilities\n",
    "   - All outputs sum to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Activation Functions Deep Dive\n",
    "\n",
    "print(\"=== ACTIVATION FUNCTIONS EXPLAINED ===\")\n",
    "\n",
    "# Create a range of input values to visualize functions\n",
    "x = tf.linspace(-5.0, 5.0, 100)\n",
    "\n",
    "# 1. ReLU: max(0, x)\n",
    "relu_output = tf.nn.relu(x)\n",
    "print(f\"\\nüî• ReLU (Rectified Linear Unit):\")\n",
    "print(f\"Formula: max(0, x)\")\n",
    "print(f\"Purpose: Introduces non-linearity while being computationally simple\")\n",
    "print(f\"Range: [0, +‚àû)\")\n",
    "print(f\"Sample values: x=-2 ‚Üí {tf.nn.relu(-2.0)}, x=3 ‚Üí {tf.nn.relu(3.0)}\")\n",
    "\n",
    "# 2. Sigmoid: 1 / (1 + e^(-x))\n",
    "sigmoid_output = tf.nn.sigmoid(x)\n",
    "print(f\"\\nüìà Sigmoid:\")\n",
    "print(f\"Formula: 1 / (1 + e^(-x))\")\n",
    "print(f\"Purpose: Squashes values to (0,1), used for binary classification\")\n",
    "print(f\"Range: (0, 1)\")\n",
    "print(f\"Sample values: x=-2 ‚Üí {tf.nn.sigmoid(-2.0):.4f}, x=0 ‚Üí {tf.nn.sigmoid(0.0):.4f}, x=2 ‚Üí {tf.nn.sigmoid(2.0):.4f}\")\n",
    "\n",
    "# 3. Tanh: (e^x - e^(-x)) / (e^x + e^(-x))\n",
    "tanh_output = tf.nn.tanh(x)\n",
    "print(f\"\\nüåä Tanh (Hyperbolic Tangent):\")\n",
    "print(f\"Formula: (e^x - e^(-x)) / (e^x + e^(-x))\")\n",
    "print(f\"Purpose: Similar to sigmoid but zero-centered\")\n",
    "print(f\"Range: (-1, 1)\")\n",
    "print(f\"Sample values: x=-2 ‚Üí {tf.nn.tanh(-2.0):.4f}, x=0 ‚Üí {tf.nn.tanh(0.0):.4f}, x=2 ‚Üí {tf.nn.tanh(2.0):.4f}\")\n",
    "\n",
    "# 4. Leaky ReLU: max(0.01*x, x)\n",
    "leaky_relu_output = tf.nn.leaky_relu(x, alpha=0.01)\n",
    "print(f\"\\nüíß Leaky ReLU:\")\n",
    "print(f\"Formula: max(Œ±*x, x) where Œ±=0.01\")\n",
    "print(f\"Purpose: Fixes 'dead neuron' problem of ReLU\")\n",
    "print(f\"Range: (-‚àû, +‚àû)\")\n",
    "print(f\"Sample values: x=-2 ‚Üí {tf.nn.leaky_relu(-2.0, alpha=0.01):.4f}, x=2 ‚Üí {tf.nn.leaky_relu(2.0, alpha=0.01):.4f}\")\n",
    "\n",
    "# Visualize all activation functions\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Plot each activation function\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(x, relu_output, 'b-', linewidth=2, label='ReLU')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.title('ReLU: max(0, x)')\n",
    "plt.xlabel('Input (x)')\n",
    "plt.ylabel('Output')\n",
    "plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "plt.axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(x, sigmoid_output, 'r-', linewidth=2, label='Sigmoid')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.title('Sigmoid: 1/(1+e^(-x))')\n",
    "plt.xlabel('Input (x)')\n",
    "plt.ylabel('Output')\n",
    "plt.axhline(y=0.5, color='k', linestyle='--', alpha=0.5, label='y=0.5')\n",
    "plt.axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(x, tanh_output, 'g-', linewidth=2, label='Tanh')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.title('Tanh: (e^x - e^(-x))/(e^x + e^(-x))')\n",
    "plt.xlabel('Input (x)')\n",
    "plt.ylabel('Output')\n",
    "plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "plt.axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(x, leaky_relu_output, 'purple', linewidth=2, label='Leaky ReLU')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.title('Leaky ReLU: max(0.01*x, x)')\n",
    "plt.xlabel('Input (x)')\n",
    "plt.ylabel('Output')\n",
    "plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "plt.axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
    "\n",
    "plt.suptitle('Activation Functions Comparison', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Demonstrate softmax for classification\n",
    "print(f\"\\nüéØ SOFTMAX (for multi-class classification):\")\n",
    "logits = tf.constant([2.0, 1.0, 0.1])  # Raw network outputs\n",
    "probabilities = tf.nn.softmax(logits)\n",
    "\n",
    "print(f\"Raw logits: {logits}\")\n",
    "print(f\"After softmax: {probabilities}\")\n",
    "print(f\"Sum of probabilities: {tf.reduce_sum(probabilities):.6f} ‚Üê Should be 1.0\")\n",
    "print(f\"Purpose: Converts any real numbers to valid probabilities\")\n",
    "print(f\"Interpretation: [Class A: {probabilities[0]:.3f}, Class B: {probabilities[1]:.3f}, Class C: {probabilities[2]:.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Loss Functions: Measuring How Wrong We Are\n",
    "\n",
    "**What is a Loss Function?**\n",
    "- Measures the difference between predicted and actual values\n",
    "- Gives the neural network a \"score\" to optimize\n",
    "- Lower loss = better performance\n",
    "\n",
    "**Common Loss Functions:**\n",
    "\n",
    "1. **Mean Squared Error (MSE)**: For regression\n",
    "   - Formula: `(y_true - y_pred)¬≤`\n",
    "   - Penalizes large errors heavily\n",
    "\n",
    "2. **Binary Crossentropy**: For binary classification\n",
    "   - Formula: `-[y*log(p) + (1-y)*log(1-p)]`\n",
    "   - Measures probability distribution distance\n",
    "\n",
    "3. **Categorical Crossentropy**: For multi-class classification\n",
    "   - Used with softmax activation\n",
    "   - Measures how far predicted probabilities are from true labels\n",
    "\n",
    "**Key Insight**: The choice of loss function depends on your problem type!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Loss Functions Explained with Examples\n",
    "\n",
    "print(\"=== LOSS FUNCTIONS: MEASURING PREDICTION QUALITY ===\")\n",
    "\n",
    "# 1. REGRESSION EXAMPLE: Predicting house prices\n",
    "print(\"\\nüè† REGRESSION EXAMPLE: House Price Prediction\")\n",
    "true_prices = tf.constant([300000., 450000., 200000., 600000.])  # Actual house prices\n",
    "pred_prices = tf.constant([320000., 430000., 180000., 580000.])  # Model predictions\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "mse_loss = tf.keras.losses.mean_squared_error(true_prices, pred_prices)\n",
    "print(f\"\\nTrue prices: {true_prices}\")\n",
    "print(f\"Predicted:   {pred_prices}\")\n",
    "print(f\"Differences: {true_prices - pred_prices}\")\n",
    "print(f\"Squared errors: {(true_prices - pred_prices)**2}\")\n",
    "print(f\"MSE Loss: {mse_loss:.2f}\")\n",
    "print(f\"Interpretation: Average squared error of ${tf.sqrt(mse_loss):.0f} per prediction\")\n",
    "\n",
    "# 2. BINARY CLASSIFICATION EXAMPLE: Email spam detection\n",
    "print(\"\\nüìß BINARY CLASSIFICATION: Email Spam Detection\")\n",
    "true_labels = tf.constant([1., 0., 1., 0.])  # 1=spam, 0=not spam\n",
    "pred_probs = tf.constant([0.9, 0.1, 0.8, 0.3])  # Predicted probabilities\n",
    "\n",
    "# Binary Crossentropy\n",
    "bce_loss = tf.keras.losses.binary_crossentropy(true_labels, pred_probs)\n",
    "print(f\"\\nTrue labels: {true_labels} (1=spam, 0=not spam)\")\n",
    "print(f\"Predictions: {pred_probs} (probability of being spam)\")\n",
    "print(f\"Individual losses: {bce_loss}\")\n",
    "print(f\"Average BCE Loss: {tf.reduce_mean(bce_loss):.4f}\")\n",
    "print(f\"Interpretation: Lower loss means better probability estimates\")\n",
    "\n",
    "# Let's see what happens with perfect vs terrible predictions\n",
    "perfect_probs = tf.constant([1.0, 0.0, 1.0, 0.0])  # Perfect predictions\n",
    "terrible_probs = tf.constant([0.1, 0.9, 0.1, 0.9])  # Opposite predictions\n",
    "\n",
    "perfect_loss = tf.reduce_mean(tf.keras.losses.binary_crossentropy(true_labels, perfect_probs))\n",
    "terrible_loss = tf.reduce_mean(tf.keras.losses.binary_crossentropy(true_labels, terrible_probs))\n",
    "\n",
    "print(f\"\\nPerfect predictions loss: {perfect_loss:.6f} ‚Üê Nearly zero!\")\n",
    "print(f\"Terrible predictions loss: {terrible_loss:.4f} ‚Üê Very high!\")\n",
    "\n",
    "# 3. MULTI-CLASS CLASSIFICATION: Animal classification\n",
    "print(\"\\nüê± MULTI-CLASS CLASSIFICATION: Animal Classification\")\n",
    "# One-hot encoded true labels: [cat, dog, bird]\n",
    "true_onehot = tf.constant([[1., 0., 0.],  # cat\n",
    "                           [0., 1., 0.],  # dog  \n",
    "                           [0., 0., 1.]])  # bird\n",
    "\n",
    "# Predicted probabilities (after softmax)\n",
    "pred_probs_multi = tf.constant([[0.8, 0.15, 0.05],  # Confident cat prediction\n",
    "                                [0.3, 0.6, 0.1],     # Moderate dog prediction\n",
    "                                [0.1, 0.2, 0.7]])    # Good bird prediction\n",
    "\n",
    "# Categorical Crossentropy\n",
    "cce_loss = tf.keras.losses.categorical_crossentropy(true_onehot, pred_probs_multi)\n",
    "print(f\"\\nTrue labels (one-hot):\")\n",
    "print(f\"  Sample 1: {true_onehot[0]} ‚Üê Cat\")\n",
    "print(f\"  Sample 2: {true_onehot[1]} ‚Üê Dog\") \n",
    "print(f\"  Sample 3: {true_onehot[2]} ‚Üê Bird\")\n",
    "print(f\"\\nPredicted probabilities:\")\n",
    "print(f\"  Sample 1: {pred_probs_multi[0]} ‚Üê [cat: 80%, dog: 15%, bird: 5%]\")\n",
    "print(f\"  Sample 2: {pred_probs_multi[1]} ‚Üê [cat: 30%, dog: 60%, bird: 10%]\")\n",
    "print(f\"  Sample 3: {pred_probs_multi[2]} ‚Üê [cat: 10%, dog: 20%, bird: 70%]\")\n",
    "print(f\"\\nIndividual losses: {cce_loss}\")\n",
    "print(f\"Average CCE Loss: {tf.reduce_mean(cce_loss):.4f}\")\n",
    "\n",
    "# Visualize how loss changes with prediction quality\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# MSE Loss visualization\n",
    "plt.subplot(1, 3, 1)\n",
    "errors = np.linspace(-50000, 50000, 100)\n",
    "mse_values = errors**2\n",
    "plt.plot(errors, mse_values, 'b-', linewidth=2)\n",
    "plt.title('MSE Loss: (y_true - y_pred)¬≤')\n",
    "plt.xlabel('Prediction Error')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axvline(x=0, color='r', linestyle='--', alpha=0.7, label='Perfect prediction')\n",
    "plt.legend()\n",
    "\n",
    "# Binary Crossentropy visualization\n",
    "plt.subplot(1, 3, 2)\n",
    "probs = np.linspace(0.001, 0.999, 100)  # Avoid log(0)\n",
    "# For true label = 1, loss = -log(p)\n",
    "bce_values_true1 = -np.log(probs)\n",
    "# For true label = 0, loss = -log(1-p)\n",
    "bce_values_true0 = -np.log(1 - probs)\n",
    "\n",
    "plt.plot(probs, bce_values_true1, 'r-', linewidth=2, label='True label = 1')\n",
    "plt.plot(probs, bce_values_true0, 'b-', linewidth=2, label='True label = 0')\n",
    "plt.title('Binary Crossentropy Loss')\n",
    "plt.xlabel('Predicted Probability')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.ylim(0, 5)\n",
    "\n",
    "# Loss comparison\n",
    "plt.subplot(1, 3, 3)\n",
    "loss_types = ['MSE\\n(Regression)', 'BCE\\n(Binary)', 'CCE\\n(Multi-class)']\n",
    "example_losses = [float(mse_loss)/1e8, float(tf.reduce_mean(bce_loss)), float(tf.reduce_mean(cce_loss))]\n",
    "colors = ['skyblue', 'lightcoral', 'lightgreen']\n",
    "\n",
    "bars = plt.bar(loss_types, example_losses, color=colors)\n",
    "plt.title('Loss Function Comparison\\n(Example Values)')\n",
    "plt.ylabel('Loss Value')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, loss in zip(bars, example_losses):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "             f'{loss:.3f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚ú® KEY INSIGHTS:\")\n",
    "print(\"‚Ä¢ MSE: Penalizes large errors heavily (quadratic growth)\")\n",
    "print(\"‚Ä¢ BCE: Loss approaches infinity as predictions approach opposite of truth\")\n",
    "print(\"‚Ä¢ CCE: Measures how far predicted distribution is from true distribution\")\n",
    "print(\"‚Ä¢ Choice depends on problem: regression‚ÜíMSE, classification‚Üícrossentropy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Gradients: The Mathematics of Learning\n",
    "\n",
    "**What are Gradients?**\n",
    "- Gradients tell us **which direction** to change weights to reduce loss\n",
    "- Mathematically: the partial derivative of loss with respect to each weight\n",
    "- **Positive gradient**: Increase weight increases loss ‚Üí **decrease** weight\n",
    "- **Negative gradient**: Increase weight decreases loss ‚Üí **increase** weight\n",
    "\n",
    "**Why Gradients Matter:**\n",
    "- Without gradients, neural networks can't learn\n",
    "- Gradients guide the weight updates during training\n",
    "- TensorFlow automatically computes gradients (automatic differentiation)\n",
    "\n",
    "**Gradient Descent Algorithm:**\n",
    "1. Forward pass: Compute predictions and loss\n",
    "2. Backward pass: Compute gradients\n",
    "3. Update weights: `new_weight = old_weight - learning_rate * gradient`\n",
    "4. Repeat until convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Understanding Gradients with Automatic Differentiation\n",
    "\n",
    "print(\"=== GRADIENTS: THE MATHEMATICS OF LEARNING ===\")\n",
    "\n",
    "# Simple example: y = x¬≤ + 3x + 2, find gradient at different points\n",
    "print(\"\\nüìê MATHEMATICAL EXAMPLE: f(x) = x¬≤ + 3x + 2\")\n",
    "print(\"Analytical derivative: f'(x) = 2x + 3\")\n",
    "\n",
    "def simple_function(x):\n",
    "    \"\"\"Simple quadratic function for demonstration\"\"\"\n",
    "    return x**2 + 3*x + 2\n",
    "\n",
    "# Compute gradients at different points using TensorFlow\n",
    "test_points = [tf.constant(x, dtype=tf.float32) for x in [-2.0, -1.0, 0.0, 1.0, 2.0]]\n",
    "\n",
    "print(\"\\nPoint | Function Value | TF Gradient | Analytical | Match?\")\n",
    "print(\"-\" * 60)\n",
    "for x in test_points:\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(x)  # Tell TensorFlow to track this variable\n",
    "        y = simple_function(x)\n",
    "    \n",
    "    tf_gradient = tape.gradient(y, x)  # Automatic differentiation\n",
    "    analytical_gradient = 2*x + 3     # Hand-calculated derivative\n",
    "    match = \"‚úÖ\" if abs(tf_gradient - analytical_gradient) < 1e-6 else \"‚ùå\"\n",
    "    \n",
    "    print(f\"{x.numpy():5.1f} | {y.numpy():13.1f} | {tf_gradient.numpy():10.1f} | {analytical_gradient.numpy():10.1f} | {match}\")\n",
    "\n",
    "print(\"\\nüéØ Key Insight: TensorFlow's automatic differentiation matches analytical calculus!\")\n",
    "\n",
    "# Neural Network Gradient Example\n",
    "print(\"\\n\\nüß† NEURAL NETWORK GRADIENT EXAMPLE\")\n",
    "print(\"Simple network: y = relu(x * w + b)\")\n",
    "print(\"Loss: MSE between prediction and target\")\n",
    "\n",
    "# Create simple data\n",
    "x = tf.constant([[1.0], [2.0], [3.0], [4.0]])  # Input features\n",
    "y_true = tf.constant([[2.0], [4.0], [6.0], [8.0]])  # Target: y = 2x\n",
    "\n",
    "# Initialize trainable variables (parameters)\n",
    "w = tf.Variable([[1.5]], dtype=tf.float32, name='weight')  # Initial weight\n",
    "b = tf.Variable([0.5], dtype=tf.float32, name='bias')      # Initial bias\n",
    "\n",
    "print(f\"\\nInitial parameters:\")\n",
    "print(f\"Weight: {w.numpy()[0][0]:.2f}\")\n",
    "print(f\"Bias: {b.numpy()[0]:.2f}\")\n",
    "print(f\"Target relationship: y = 2x (so ideal weight=2, bias=0)\")\n",
    "\n",
    "# Forward pass with gradient tracking\n",
    "with tf.GradientTape() as tape:\n",
    "    # Forward pass\n",
    "    linear_output = tf.matmul(x, w) + b\n",
    "    y_pred = tf.nn.relu(linear_output)  # Apply ReLU activation\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = tf.reduce_mean(tf.square(y_true - y_pred))  # MSE loss\n",
    "\n",
    "# Compute gradients\n",
    "gradients = tape.gradient(loss, [w, b])\n",
    "grad_w, grad_b = gradients\n",
    "\n",
    "print(f\"\\nForward pass results:\")\n",
    "print(f\"Predictions: {tf.squeeze(y_pred).numpy()}\")\n",
    "print(f\"True values: {tf.squeeze(y_true).numpy()}\")\n",
    "print(f\"Loss (MSE): {loss.numpy():.4f}\")\n",
    "\n",
    "print(f\"\\nGradients (direction to reduce loss):\")\n",
    "print(f\"‚àÇLoss/‚àÇWeight = {grad_w.numpy()[0][0]:.4f}\")\n",
    "print(f\"‚àÇLoss/‚àÇBias   = {grad_b.numpy()[0]:.4f}\")\n",
    "\n",
    "# Gradient interpretation\n",
    "print(f\"\\nüìä GRADIENT INTERPRETATION:\")\n",
    "if grad_w.numpy()[0][0] > 0:\n",
    "    print(f\"‚Ä¢ Weight gradient > 0: Increasing weight increases loss ‚Üí DECREASE weight\")\n",
    "else:\n",
    "    print(f\"‚Ä¢ Weight gradient < 0: Increasing weight decreases loss ‚Üí INCREASE weight\")\n",
    "    \n",
    "if grad_b.numpy()[0] > 0:\n",
    "    print(f\"‚Ä¢ Bias gradient > 0: Increasing bias increases loss ‚Üí DECREASE bias\")\n",
    "else:\n",
    "    print(f\"‚Ä¢ Bias gradient < 0: Increasing bias decreases loss ‚Üí INCREASE bias\")\n",
    "\n",
    "# Demonstrate one gradient descent step\n",
    "learning_rate = 0.1\n",
    "print(f\"\\nüìà GRADIENT DESCENT STEP (learning_rate = {learning_rate}):\")\n",
    "print(f\"Update rule: new_param = old_param - learning_rate * gradient\")\n",
    "\n",
    "old_w = w.numpy()[0][0]\n",
    "old_b = b.numpy()[0]\n",
    "\n",
    "# Apply gradient descent update\n",
    "w.assign_sub(learning_rate * grad_w)  # w = w - lr * grad_w\n",
    "b.assign_sub(learning_rate * grad_b)  # b = b - lr * grad_b\n",
    "\n",
    "new_w = w.numpy()[0][0]\n",
    "new_b = b.numpy()[0]\n",
    "\n",
    "print(f\"Weight: {old_w:.4f} ‚Üí {new_w:.4f} (change: {new_w - old_w:+.4f})\")\n",
    "print(f\"Bias:   {old_b:.4f} ‚Üí {new_b:.4f} (change: {new_b - old_b:+.4f})\")\n",
    "\n",
    "# Verify loss decreased\n",
    "with tf.GradientTape() as tape:\n",
    "    linear_output = tf.matmul(x, w) + b\n",
    "    y_pred_new = tf.nn.relu(linear_output)\n",
    "    loss_new = tf.reduce_mean(tf.square(y_true - y_pred_new))\n",
    "\n",
    "print(f\"\\nLoss: {loss.numpy():.6f} ‚Üí {loss_new.numpy():.6f} (change: {loss_new.numpy() - loss.numpy():+.6f})\")\n",
    "if loss_new < loss:\n",
    "    print(\"‚úÖ Loss decreased! Gradient descent is working!\")\n",
    "else:\n",
    "    print(\"‚ùå Loss increased. Something might be wrong.\")\n",
    "\n",
    "# Visualize the loss surface and gradient\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Function and its derivative\n",
    "plt.subplot(1, 3, 1)\n",
    "x_vals = np.linspace(-3, 2, 100)\n",
    "y_vals = x_vals**2 + 3*x_vals + 2\n",
    "dy_vals = 2*x_vals + 3\n",
    "\n",
    "plt.plot(x_vals, y_vals, 'b-', linewidth=2, label='f(x) = x¬≤ + 3x + 2')\n",
    "plt.plot(x_vals, dy_vals, 'r--', linewidth=2, label=\"f'(x) = 2x + 3\")\n",
    "plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "plt.axvline(x=-1.5, color='g', linestyle=':', alpha=0.7, label='Minimum at x=-1.5')\n",
    "plt.title('Function and its Derivative')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Gradient descent visualization\n",
    "plt.subplot(1, 3, 2)\n",
    "# Simulate gradient descent on the simple function\n",
    "x_start = 1.5\n",
    "lr = 0.1\n",
    "steps = 10\n",
    "\n",
    "x_path = [x_start]\n",
    "for _ in range(steps):\n",
    "    grad = 2*x_path[-1] + 3  # Gradient\n",
    "    x_new = x_path[-1] - lr * grad  # Gradient descent step\n",
    "    x_path.append(x_new)\n",
    "\n",
    "y_path = [x**2 + 3*x + 2 for x in x_path]\n",
    "\n",
    "plt.plot(x_vals, y_vals, 'b-', linewidth=2, alpha=0.7)\n",
    "plt.plot(x_path, y_path, 'ro-', linewidth=2, markersize=8, label='Gradient descent path')\n",
    "plt.plot(x_path[0], y_path[0], 'go', markersize=12, label='Start')\n",
    "plt.plot(x_path[-1], y_path[-1], 'rs', markersize=12, label='End')\n",
    "plt.title('Gradient Descent Optimization')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Neural network prediction improvement\n",
    "plt.subplot(1, 3, 3)\n",
    "x_plot = tf.squeeze(x).numpy()\n",
    "y_true_plot = tf.squeeze(y_true).numpy()\n",
    "y_pred_old = tf.squeeze(y_pred).numpy()\n",
    "y_pred_new_plot = tf.squeeze(y_pred_new).numpy()\n",
    "\n",
    "plt.plot(x_plot, y_true_plot, 'go-', linewidth=2, markersize=8, label='True values')\n",
    "plt.plot(x_plot, y_pred_old, 'r^-', linewidth=2, markersize=8, label='Before gradient step')\n",
    "plt.plot(x_plot, y_pred_new_plot, 'bs-', linewidth=2, markersize=8, label='After gradient step')\n",
    "plt.title('Neural Network Improvement')\n",
    "plt.xlabel('Input (x)')\n",
    "plt.ylabel('Output (y)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚ú® GRADIENT INSIGHTS:\")\n",
    "print(\"‚Ä¢ Gradients point in the direction of steepest loss INCREASE\")\n",
    "print(\"‚Ä¢ We move OPPOSITE to gradients to minimize loss\")\n",
    "print(\"‚Ä¢ Learning rate controls how big steps we take\")\n",
    "print(\"‚Ä¢ TensorFlow automatically computes gradients for any function!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Optimizers: Smart Ways to Update Weights\n",
    "\n",
    "**Beyond Basic Gradient Descent:**\n",
    "While basic gradient descent works, we can do better! Modern optimizers use clever tricks to:\n",
    "- Adapt learning rates automatically\n",
    "- Build momentum to escape local minima\n",
    "- Handle different scales of parameters\n",
    "\n",
    "**Popular Optimizers:**\n",
    "\n",
    "1. **SGD (Stochastic Gradient Descent)**:\n",
    "   - Basic: `w = w - lr * gradient`\n",
    "   - With momentum: Accelerates in consistent directions\n",
    "\n",
    "2. **Adam (Adaptive Moment Estimation)**:\n",
    "   - Combines momentum + adaptive learning rates\n",
    "   - Most popular choice for beginners\n",
    "   - Works well across many problems\n",
    "\n",
    "3. **RMSprop**: \n",
    "   - Adapts learning rate per parameter\n",
    "   - Good for recurrent neural networks\n",
    "\n",
    "**Key Parameters:**\n",
    "- **Learning Rate**: How big steps to take (typically 0.001-0.1)\n",
    "- **Momentum**: How much to consider previous gradients (0-1)\n",
    "- **Beta values**: Control momentum and learning rate adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Optimizer Comparison with Detailed Explanations\n",
    "\n",
    "print(\"=== OPTIMIZERS: SMART WEIGHT UPDATES ===\")\n",
    "\n",
    "# Create a more complex optimization problem\n",
    "# We'll optimize a 2D function to visualize different optimizer paths\n",
    "\n",
    "def complex_loss_function(x, y):\n",
    "    \"\"\"A complex 2D function with multiple local minima\"\"\"\n",
    "    return (x**2 + y**2) + 0.3 * tf.sin(3*x) * tf.cos(3*y) + 0.1 * (x + y)**2\n",
    "\n",
    "# Starting point for optimization\n",
    "start_x, start_y = 2.0, 1.5\n",
    "\n",
    "print(f\"üéØ OPTIMIZATION PROBLEM:\")\n",
    "print(f\"Function: f(x,y) = (x¬≤ + y¬≤) + 0.3*sin(3x)*cos(3y) + 0.1*(x+y)¬≤\")\n",
    "print(f\"Goal: Find the minimum starting from ({start_x}, {start_y})\")\n",
    "print(f\"True minimum is near (0, 0)\")\n",
    "\n",
    "# Function to run optimization with different optimizers\n",
    "def optimize_with_optimizer(optimizer_class, optimizer_name, **kwargs):\n",
    "    \"\"\"Run optimization and return the path taken\"\"\"\n",
    "    \n",
    "    # Initialize variables\n",
    "    x = tf.Variable(start_x, dtype=tf.float32)\n",
    "    y = tf.Variable(start_y, dtype=tf.float32)\n",
    "    \n",
    "    # Create optimizer\n",
    "    optimizer = optimizer_class(**kwargs)\n",
    "    \n",
    "    # Track the optimization path\n",
    "    path_x, path_y, losses = [x.numpy()], [y.numpy()], []\n",
    "    \n",
    "    # Optimization loop\n",
    "    for step in range(50):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = complex_loss_function(x, y)\n",
    "        \n",
    "        # Compute gradients\n",
    "        gradients = tape.gradient(loss, [x, y])\n",
    "        \n",
    "        # Apply optimizer\n",
    "        optimizer.apply_gradients(zip(gradients, [x, y]))\n",
    "        \n",
    "        # Record path\n",
    "        path_x.append(x.numpy())\n",
    "        path_y.append(y.numpy())\n",
    "        losses.append(loss.numpy())\n",
    "    \n",
    "    return path_x, path_y, losses\n",
    "\n",
    "# Test different optimizers\n",
    "optimizers_to_test = [\n",
    "    (tf.keras.optimizers.SGD, \"SGD\", {\"learning_rate\": 0.01}),\n",
    "    (tf.keras.optimizers.SGD, \"SGD + Momentum\", {\"learning_rate\": 0.01, \"momentum\": 0.9}),\n",
    "    (tf.keras.optimizers.Adam, \"Adam\", {\"learning_rate\": 0.1}),\n",
    "    (tf.keras.optimizers.RMSprop, \"RMSprop\", {\"learning_rate\": 0.1})\n",
    "]\n",
    "\n",
    "print(f\"\\nüî¨ TESTING OPTIMIZERS:\")\n",
    "optimizer_results = {}\n",
    "\n",
    "for optimizer_class, name, kwargs in optimizers_to_test:\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Parameters: {kwargs}\")\n",
    "    \n",
    "    path_x, path_y, losses = optimize_with_optimizer(optimizer_class, name, **kwargs)\n",
    "    \n",
    "    final_loss = losses[-1]\n",
    "    final_x, final_y = path_x[-1], path_y[-1]\n",
    "    \n",
    "    print(f\"  Final position: ({final_x:.4f}, {final_y:.4f})\")\n",
    "    print(f\"  Final loss: {final_loss:.6f}\")\n",
    "    print(f\"  Distance from origin: {np.sqrt(final_x**2 + final_y**2):.4f}\")\n",
    "    \n",
    "    optimizer_results[name] = {\n",
    "        'path_x': path_x,\n",
    "        'path_y': path_y, \n",
    "        'losses': losses,\n",
    "        'final_loss': final_loss\n",
    "    }\n",
    "\n",
    "# Create visualization\n",
    "plt.figure(figsize=(20, 12))\n",
    "\n",
    "# Create meshgrid for contour plot\n",
    "x_range = np.linspace(-2.5, 2.5, 100)\n",
    "y_range = np.linspace(-2.5, 2.5, 100)\n",
    "X, Y = np.meshgrid(x_range, y_range)\n",
    "Z = (X**2 + Y**2) + 0.3 * np.sin(3*X) * np.cos(3*Y) + 0.1 * (X + Y)**2\n",
    "\n",
    "# Plot 1: Optimization paths on contour plot\n",
    "plt.subplot(2, 3, 1)\n",
    "contour = plt.contour(X, Y, Z, levels=20, alpha=0.6)\n",
    "plt.colorbar(contour)\n",
    "\n",
    "colors = ['red', 'blue', 'green', 'orange']\n",
    "for i, (name, results) in enumerate(optimizer_results.items()):\n",
    "    plt.plot(results['path_x'], results['path_y'], \n",
    "             color=colors[i], linewidth=2, marker='o', markersize=4, label=name)\n",
    "\n",
    "plt.plot(start_x, start_y, 'ko', markersize=12, label='Start')\n",
    "plt.plot(0, 0, 'r*', markersize=15, label='True minimum')\n",
    "plt.title('Optimizer Paths on Loss Surface')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Loss curves\n",
    "plt.subplot(2, 3, 2)\n",
    "for i, (name, results) in enumerate(optimizer_results.items()):\n",
    "    plt.plot(results['losses'], color=colors[i], linewidth=2, label=name)\n",
    "\n",
    "plt.title('Loss During Optimization')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')\n",
    "\n",
    "# Plot 3: Final performance comparison\n",
    "plt.subplot(2, 3, 3)\n",
    "names = list(optimizer_results.keys())\n",
    "final_losses = [results['final_loss'] for results in optimizer_results.values()]\n",
    "\n",
    "bars = plt.bar(names, final_losses, color=colors)\n",
    "plt.title('Final Loss Comparison')\n",
    "plt.ylabel('Final Loss')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, loss in zip(bars, final_losses):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height(),\n",
    "             f'{loss:.4f}', ha='center', va='bottom')\n",
    "\n",
    "# Plots 4-6: Individual optimizer details\n",
    "detailed_optimizers = ['SGD', 'Adam', 'SGD + Momentum']\n",
    "for i, opt_name in enumerate(detailed_optimizers):\n",
    "    plt.subplot(2, 3, 4 + i)\n",
    "    \n",
    "    if opt_name in optimizer_results:\n",
    "        results = optimizer_results[opt_name]\n",
    "        \n",
    "        # Plot path with gradient vectors\n",
    "        plt.contour(X, Y, Z, levels=15, alpha=0.3)\n",
    "        path_x, path_y = results['path_x'], results['path_y']\n",
    "        \n",
    "        # Plot path\n",
    "        plt.plot(path_x, path_y, 'ro-', linewidth=2, markersize=6)\n",
    "        \n",
    "        # Show first few steps with arrows\n",
    "        for j in range(min(5, len(path_x)-1)):\n",
    "            dx = path_x[j+1] - path_x[j]\n",
    "            dy = path_y[j+1] - path_y[j]\n",
    "            plt.arrow(path_x[j], path_y[j], dx, dy,\n",
    "                     head_width=0.05, head_length=0.05, fc='blue', ec='blue')\n",
    "        \n",
    "        plt.plot(path_x[0], path_y[0], 'go', markersize=10, label='Start')\n",
    "        plt.plot(path_x[-1], path_y[-1], 'rs', markersize=10, label='End')\n",
    "        plt.title(f'{opt_name} Detailed Path')\n",
    "        plt.xlabel('x')\n",
    "        plt.ylabel('y')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Explain optimizer characteristics\n",
    "print(f\"\\nüìä OPTIMIZER ANALYSIS:\")\n",
    "print(f\"\\n1. SGD (Stochastic Gradient Descent):\")\n",
    "print(f\"   ‚Ä¢ Formula: w = w - learning_rate * gradient\")\n",
    "print(f\"   ‚Ä¢ Pros: Simple, reliable, good for convex problems\")\n",
    "print(f\"   ‚Ä¢ Cons: Can be slow, may oscillate in valleys\")\n",
    "\n",
    "print(f\"\\n2. SGD + Momentum:\")\n",
    "print(f\"   ‚Ä¢ Formula: v = momentum * v + gradient; w = w - learning_rate * v\")\n",
    "print(f\"   ‚Ä¢ Pros: Accelerates in consistent directions, dampens oscillations\")\n",
    "print(f\"   ‚Ä¢ Cons: May overshoot minima\")\n",
    "\n",
    "print(f\"\\n3. Adam (Adaptive Moment Estimation):\")\n",
    "print(f\"   ‚Ä¢ Combines momentum + adaptive learning rates\")\n",
    "print(f\"   ‚Ä¢ Pros: Works well out-of-the-box, adapts to parameter scales\")\n",
    "print(f\"   ‚Ä¢ Cons: Can converge to suboptimal solutions in some cases\")\n",
    "\n",
    "print(f\"\\n4. RMSprop:\")\n",
    "print(f\"   ‚Ä¢ Adapts learning rate based on recent gradient magnitudes\")\n",
    "print(f\"   ‚Ä¢ Pros: Good for non-stationary objectives, handles sparse gradients\")\n",
    "print(f\"   ‚Ä¢ Cons: Less momentum than Adam\")\n",
    "\n",
    "print(f\"\\n‚ú® PRACTICAL RECOMMENDATIONS:\")\n",
    "print(f\"‚Ä¢ Start with Adam (learning_rate=0.001) for most problems\")\n",
    "print(f\"‚Ä¢ Use SGD + momentum for fine-tuning or when you need reproducibility\")\n",
    "print(f\"‚Ä¢ Learning rate is the most important hyperparameter to tune\")\n",
    "print(f\"‚Ä¢ Monitor loss curves to detect convergence issues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Putting It All Together: Complete Learning Example\n",
    "\n",
    "**Now we'll combine everything we've learned:**\n",
    "1. ‚úÖ **Tensors**: Data representation\n",
    "2. ‚úÖ **Operations**: Matrix multiplication, broadcasting\n",
    "3. ‚úÖ **Activations**: Non-linear transformations\n",
    "4. ‚úÖ **Loss Functions**: Measuring prediction quality\n",
    "5. ‚úÖ **Gradients**: Direction of improvement\n",
    "6. ‚úÖ **Optimizers**: Smart weight updates\n",
    "\n",
    "**Complete Training Loop:**\n",
    "```python\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataset:\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = model(batch_x)     # Forward pass\n",
    "            loss = loss_function(batch_y, predictions)\n",
    "        \n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Complete Deep Learning Example from Scratch\n",
    "\n",
    "print(\"=== COMPLETE DEEP LEARNING EXAMPLE ===\")\n",
    "print(\"Building a neural network from scratch with detailed explanations!\")\n",
    "\n",
    "# Generate synthetic dataset for binary classification\n",
    "print(\"\\nüìä CREATING DATASET:\")\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Create two classes of data with some overlap\n",
    "n_samples = 1000\n",
    "n_features = 2\n",
    "\n",
    "# Class 0: centered around (-1, -1)\n",
    "class_0 = np.random.multivariate_normal([-1, -1], [[0.5, 0.1], [0.1, 0.5]], n_samples//2)\n",
    "labels_0 = np.zeros((n_samples//2, 1))\n",
    "\n",
    "# Class 1: centered around (1, 1)\n",
    "class_1 = np.random.multivariate_normal([1, 1], [[0.5, -0.1], [-0.1, 0.5]], n_samples//2)\n",
    "labels_1 = np.ones((n_samples//2, 1))\n",
    "\n",
    "# Combine data\n",
    "X_data = np.vstack([class_0, class_1]).astype(np.float32)\n",
    "y_data = np.vstack([labels_0, labels_1]).astype(np.float32)\n",
    "\n",
    "# Shuffle the data\n",
    "indices = np.random.permutation(n_samples)\n",
    "X_data = X_data[indices]\n",
    "y_data = y_data[indices]\n",
    "\n",
    "# Convert to TensorFlow tensors\n",
    "X = tf.constant(X_data)\n",
    "y = tf.constant(y_data)\n",
    "\n",
    "print(f\"Dataset shape: {X.shape} (samples, features)\")\n",
    "print(f\"Labels shape: {y.shape}\")\n",
    "print(f\"Feature range: X1=[{X[:, 0].numpy().min():.2f}, {X[:, 0].numpy().max():.2f}], X2=[{X[:, 1].numpy().min():.2f}, {X[:, 1].numpy().max():.2f}]\")\n",
    "print(f\"Class distribution: {tf.reduce_sum(y).numpy():.0f} class 1, {len(y) - tf.reduce_sum(y).numpy():.0f} class 0\")\n",
    "\n",
    "# Visualize the dataset\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "class_0_mask = y.numpy().flatten() == 0\n",
    "class_1_mask = y.numpy().flatten() == 1\n",
    "\n",
    "plt.scatter(X[class_0_mask, 0], X[class_0_mask, 1], c='red', alpha=0.6, label='Class 0', s=20)\n",
    "plt.scatter(X[class_1_mask, 0], X[class_1_mask, 1], c='blue', alpha=0.6, label='Class 1', s=20)\n",
    "plt.title('Dataset Visualization')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Define the neural network architecture\n",
    "print(f\"\\nüèóÔ∏è BUILDING NEURAL NETWORK:\")\n",
    "print(f\"Architecture: Input(2) ‚Üí Hidden(4, ReLU) ‚Üí Output(1, Sigmoid)\")\n",
    "\n",
    "# Initialize network parameters\n",
    "input_size = 2\n",
    "hidden_size = 4\n",
    "output_size = 1\n",
    "\n",
    "# Xavier/Glorot initialization for better convergence\n",
    "# Formula: uniform(-sqrt(6/(fan_in + fan_out)), sqrt(6/(fan_in + fan_out)))\n",
    "def xavier_init(fan_in, fan_out):\n",
    "    limit = tf.sqrt(6.0 / (fan_in + fan_out))\n",
    "    return tf.random.uniform((fan_in, fan_out), -limit, limit)\n",
    "\n",
    "# Layer 1: Input ‚Üí Hidden\n",
    "W1 = tf.Variable(xavier_init(input_size, hidden_size), name='W1')\n",
    "b1 = tf.Variable(tf.zeros((hidden_size,)), name='b1')\n",
    "\n",
    "# Layer 2: Hidden ‚Üí Output\n",
    "W2 = tf.Variable(xavier_init(hidden_size, output_size), name='W2')\n",
    "b2 = tf.Variable(tf.zeros((output_size,)), name='b2')\n",
    "\n",
    "print(f\"Weight matrices initialized:\")\n",
    "print(f\"  W1 shape: {W1.shape} (input_to_hidden weights)\")\n",
    "print(f\"  b1 shape: {b1.shape} (hidden layer biases)\")\n",
    "print(f\"  W2 shape: {W2.shape} (hidden_to_output weights)\")\n",
    "print(f\"  b2 shape: {b2.shape} (output layer bias)\")\n",
    "print(f\"Total parameters: {tf.size(W1) + tf.size(b1) + tf.size(W2) + tf.size(b2)} = {input_size*hidden_size} + {hidden_size} + {hidden_size*output_size} + {output_size}\")\n",
    "\n",
    "# Define the forward pass\n",
    "def forward_pass(X, training=True):\n",
    "    \"\"\"Complete forward pass through the network\"\"\"\n",
    "    \n",
    "    # Layer 1: Input ‚Üí Hidden\n",
    "    z1 = tf.matmul(X, W1) + b1  # Linear transformation\n",
    "    a1 = tf.nn.relu(z1)         # ReLU activation\n",
    "    \n",
    "    # Layer 2: Hidden ‚Üí Output  \n",
    "    z2 = tf.matmul(a1, W2) + b2 # Linear transformation\n",
    "    a2 = tf.nn.sigmoid(z2)      # Sigmoid activation (for binary classification)\n",
    "    \n",
    "    if training:\n",
    "        return a2, {'z1': z1, 'a1': a1, 'z2': z2}  # Return intermediate values for analysis\n",
    "    else:\n",
    "        return a2\n",
    "\n",
    "# Test forward pass\n",
    "print(f\"\\nüß† TESTING FORWARD PASS:\")\n",
    "sample_predictions, intermediates = forward_pass(X[:5])  # Test with first 5 samples\n",
    "\n",
    "print(f\"Input (first 5 samples):\")\n",
    "print(X[:5].numpy())\n",
    "print(f\"\\nLayer 1 pre-activation (z1):\")\n",
    "print(intermediates['z1'].numpy())\n",
    "print(f\"\\nLayer 1 post-activation (a1, after ReLU):\")\n",
    "print(intermediates['a1'].numpy())\n",
    "print(f\"\\nLayer 2 pre-activation (z2):\")\n",
    "print(intermediates['z2'].numpy())\n",
    "print(f\"\\nFinal output (probabilities):\")\n",
    "print(sample_predictions.numpy())\n",
    "print(f\"\\nTrue labels (first 5):\")\n",
    "print(y[:5].numpy().flatten())\n",
    "\n",
    "# Setup training\n",
    "print(f\"\\nüéØ TRAINING SETUP:\")\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "loss_function = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "print(f\"Optimizer: Adam with learning_rate=0.01\")\n",
    "print(f\"Loss function: Binary Crossentropy\")\n",
    "print(f\"Metrics: Loss, Accuracy\")\n",
    "\n",
    "# Training loop with detailed logging\n",
    "epochs = 100\n",
    "loss_history = []\n",
    "accuracy_history = []\n",
    "weight_norms = {'W1': [], 'W2': []}\n",
    "\n",
    "print(f\"\\nüöÄ STARTING TRAINING:\")\n",
    "print(f\"Epochs: {epochs}\")\n",
    "print(f\"Progress: [Loss | Accuracy | W1_norm | W2_norm]\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Forward pass\n",
    "        predictions = forward_pass(X, training=False)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = loss_function(y, predictions)\n",
    "        \n",
    "        # Add L2 regularization (optional)\n",
    "        l2_reg = 0.001\n",
    "        l2_loss = l2_reg * (tf.nn.l2_loss(W1) + tf.nn.l2_loss(W2))\n",
    "        total_loss = loss + l2_loss\n",
    "    \n",
    "    # Compute gradients\n",
    "    variables = [W1, b1, W2, b2]\n",
    "    gradients = tape.gradient(total_loss, variables)\n",
    "    \n",
    "    # Apply gradients\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    \n",
    "    # Compute metrics\n",
    "    binary_predictions = tf.round(predictions)\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(binary_predictions, y), tf.float32))\n",
    "    \n",
    "    # Record metrics\n",
    "    loss_history.append(loss.numpy())\n",
    "    accuracy_history.append(accuracy.numpy())\n",
    "    weight_norms['W1'].append(tf.norm(W1).numpy())\n",
    "    weight_norms['W2'].append(tf.norm(W2).numpy())\n",
    "    \n",
    "    # Print progress\n",
    "    if epoch % 20 == 0 or epoch == epochs - 1:\n",
    "        print(f\"Epoch {epoch:3d}: {loss.numpy():.4f} | {accuracy.numpy():.4f} | {tf.norm(W1).numpy():.3f} | {tf.norm(W2).numpy():.3f}\")\n",
    "\n",
    "# Final evaluation\n",
    "final_predictions = forward_pass(X, training=False)\n",
    "final_binary_predictions = tf.round(final_predictions)\n",
    "final_accuracy = tf.reduce_mean(tf.cast(tf.equal(final_binary_predictions, y), tf.float32))\n",
    "final_loss = loss_function(y, final_predictions)\n",
    "\n",
    "print(f\"\\nüéâ TRAINING COMPLETE!\")\n",
    "print(f\"Final Loss: {final_loss:.6f}\")\n",
    "print(f\"Final Accuracy: {final_accuracy:.4f} ({final_accuracy*100:.1f}%)\")\n",
    "\n",
    "# Visualize results\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(loss_history, 'b-', linewidth=2, label='Training Loss')\n",
    "plt.title('Training Loss Over Time')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(accuracy_history, 'g-', linewidth=2, label='Training Accuracy')\n",
    "plt.title('Training Accuracy Over Time')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize decision boundary\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Create a mesh for decision boundary\n",
    "h = 0.02\n",
    "x_min, x_max = X[:, 0].numpy().min() - 1, X[:, 0].numpy().max() + 1\n",
    "y_min, y_max = X[:, 1].numpy().min() - 1, X[:, 1].numpy().max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "# Make predictions on the mesh\n",
    "mesh_points = tf.constant(np.c_[xx.ravel(), yy.ravel()], dtype=tf.float32)\n",
    "mesh_predictions = forward_pass(mesh_points, training=False)\n",
    "mesh_predictions = mesh_predictions.numpy().reshape(xx.shape)\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.contourf(xx, yy, mesh_predictions, levels=20, alpha=0.8, cmap='RdYlBu')\n",
    "plt.colorbar(label='Predicted Probability')\n",
    "plt.scatter(X[class_0_mask, 0], X[class_0_mask, 1], c='red', alpha=0.8, label='Class 0', s=20, edgecolors='black')\n",
    "plt.scatter(X[class_1_mask, 0], X[class_1_mask, 1], c='blue', alpha=0.8, label='Class 1', s=20, edgecolors='black')\n",
    "plt.title('Learned Decision Boundary')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "\n",
    "# Weight evolution\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(weight_norms['W1'], 'r-', linewidth=2, label='W1 norm')\n",
    "plt.plot(weight_norms['W2'], 'b-', linewidth=2, label='W2 norm')\n",
    "plt.title('Weight Norms During Training')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('L2 Norm')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Final network weights visualization\n",
    "plt.subplot(1, 3, 3)\n",
    "# Combine all weights for visualization\n",
    "all_weights = np.concatenate([W1.numpy().flatten(), W2.numpy().flatten()])\n",
    "plt.hist(all_weights, bins=20, alpha=0.7, color='purple', edgecolor='black')\n",
    "plt.title('Final Weight Distribution')\n",
    "plt.xlabel('Weight Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä FINAL ANALYSIS:\")\n",
    "print(f\"\\nüîç What the network learned:\")\n",
    "print(f\"‚Ä¢ W1 (input‚Üíhidden): {W1.shape} matrix maps 2D input to 4D hidden representation\")\n",
    "print(f\"‚Ä¢ b1 (hidden bias): {b1.shape} vector shifts hidden activations\")\n",
    "print(f\"‚Ä¢ W2 (hidden‚Üíoutput): {W2.shape} matrix combines hidden features for final decision\")\n",
    "print(f\"‚Ä¢ b2 (output bias): {b2.shape} scalar adjusts decision threshold\")\n",
    "\n",
    "print(f\"\\nüìà Training insights:\")\n",
    "print(f\"‚Ä¢ Loss decreased from {loss_history[0]:.4f} to {loss_history[-1]:.4f}\")\n",
    "print(f\"‚Ä¢ Accuracy improved from {accuracy_history[0]:.4f} to {accuracy_history[-1]:.4f}\")\n",
    "print(f\"‚Ä¢ Decision boundary successfully separates the two classes\")\n",
    "print(f\"‚Ä¢ Weight norms stabilized, indicating convergence\")\n",
    "\n",
    "print(f\"\\n‚úÖ CONGRATULATIONS!\")\n",
    "print(f\"You've successfully implemented and trained a neural network from scratch!\")\n",
    "print(f\"You now understand: tensors, operations, activations, loss, gradients, and optimizers!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}