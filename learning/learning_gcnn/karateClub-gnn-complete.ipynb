{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü•ã Karate Club Graph Neural Network - Complete Guide from Scratch\n",
    "\n",
    "## üìö The Ultimate Educational GNN Tutorial\n",
    "\n",
    "### üéØ What You'll Master:\n",
    "1. **Graph Theory Fundamentals** - Nodes, edges, adjacency matrices\n",
    "2. **Message Passing** - How GNNs aggregate neighbor information\n",
    "3. **Graph Convolutions** - Mathematical foundations from scratch\n",
    "4. **Custom GNN Implementation** - Build every component yourself\n",
    "5. **Node Classification** - Predict communities in social networks\n",
    "6. **Visualization & Analysis** - Deep understanding through visuals\n",
    "\n",
    "---\n",
    "\n",
    "## üß† The Karate Club Story\n",
    "\n",
    "**The Dataset:**\n",
    "- 34 members of a university karate club\n",
    "- Network of friendships (78 edges)\n",
    "- Club splits into 2 groups after a dispute\n",
    "- **Goal:** Can GNN predict which group each member joins?\n",
    "\n",
    "**Why This Matters:**\n",
    "- Social network analysis\n",
    "- Community detection\n",
    "- Influence propagation\n",
    "- Recommendation systems\n",
    "\n",
    "---\n",
    "\n",
    "## üìñ Learning Philosophy\n",
    "\n",
    "**We'll build THREE implementations:**\n",
    "1. **Pure NumPy** - Understand the math completely\n",
    "2. **PyTorch from Scratch** - Build custom GNN layers\n",
    "3. **PyTorch Geometric** - Use optimized library\n",
    "\n",
    "This progression ensures you understand EVERY detail!\n",
    "\n",
    "---\n",
    "\n",
    "Let's begin! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import Libraries and Setup\n",
    "\n",
    "print(\"üîß Setting up Karate Club GNN Environment\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Core libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import FancyBboxPatch\n",
    "\n",
    "# Graph libraries\n",
    "import networkx as nx\n",
    "from torch_geometric.datasets import KarateClub\n",
    "from torch_geometric.utils import to_networkx\n",
    "\n",
    "# For better plots\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(f\"üìä PyTorch version: {torch.__version__}\")\n",
    "print(f\"üî¢ NumPy version: {np.__version__}\")\n",
    "print(f\"üìà NetworkX version: {nx.__version__}\")\n",
    "print(\"üöÄ Ready to learn GNNs from scratch!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä Part 1: Understanding Graph Data\n",
    "\n",
    "### What is a Graph?\n",
    "\n",
    "A graph `G = (V, E)` consists of:\n",
    "- **V**: Set of vertices (nodes)\n",
    "- **E**: Set of edges (connections)\n",
    "\n",
    "### Key Representations:\n",
    "\n",
    "**1. Adjacency Matrix (A):**\n",
    "```\n",
    "A[i,j] = 1 if edge exists between node i and j\n",
    "A[i,j] = 0 otherwise\n",
    "```\n",
    "\n",
    "**2. Feature Matrix (X):**\n",
    "```\n",
    "X[i] = feature vector for node i\n",
    "Shape: (num_nodes, num_features)\n",
    "```\n",
    "\n",
    "**3. Edge Index:**\n",
    "```\n",
    "[[source_nodes],\n",
    " [target_nodes]]\n",
    "```\n",
    "\n",
    "Let's load and explore!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load Karate Club Dataset\n",
    "\n",
    "print(\"üì• Loading Karate Club Dataset\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Load dataset using PyTorch Geometric\n",
    "dataset = KarateClub()\n",
    "data = dataset[0]  # Get the single graph\n",
    "\n",
    "print(f\"\\nüìä Dataset Overview:\")\n",
    "print(f\"Dataset: {dataset}\")\n",
    "print(f\"\\nGraph Data Object:\")\n",
    "print(data)\n",
    "\n",
    "print(f\"\\nüîç Detailed Information:\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Number of nodes: {data.num_nodes}\")\n",
    "print(f\"Number of edges: {data.num_edges}\")\n",
    "print(f\"Number of features per node: {data.num_node_features}\")\n",
    "print(f\"Number of classes (communities): {dataset.num_classes}\")\n",
    "print(f\"\\nFeature matrix shape: {data.x.shape}\")\n",
    "print(f\"Edge index shape: {data.edge_index.shape}\")\n",
    "print(f\"Labels shape: {data.y.shape}\")\n",
    "print(f\"\\nTrain mask shape: {data.train_mask.shape}\")\n",
    "print(f\"Number of training nodes: {data.train_mask.sum().item()}\")\n",
    "\n",
    "print(f\"\\nüí° Understanding the Features:\")\n",
    "print(f\"The Karate Club dataset uses one-hot encoding\")\n",
    "print(f\"Each node has a 34-dimensional feature vector (one per node)\")\n",
    "print(f\"This is like an identity matrix - each node knows 'who' it is\")\n",
    "print(f\"\\nExample - Node 0 features:\")\n",
    "print(data.x[0][:10])  # Show first 10 features\n",
    "print(f\"... (24 more zeros)\")\n",
    "\n",
    "print(f\"\\nüéØ Labels (Community Assignment):\")\n",
    "print(f\"Label 0: Mr. Hi's group\")\n",
    "print(f\"Label 1: Officer's group\")\n",
    "print(f\"\\nLabel distribution:\")\n",
    "unique, counts = torch.unique(data.y, return_counts=True)\n",
    "for label, count in zip(unique, counts):\n",
    "    print(f\"  Community {label}: {count} members\")\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset loaded and explored!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Visualize the Karate Club Network\n",
    "\n",
    "print(\"üé® Creating Karate Club Network Visualization\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "def visualize_karate_club(data, title=\"Karate Club Network\", predictions=None, figsize=(16, 12)):\n",
    "    \"\"\"\n",
    "    Visualize the Karate Club graph with community colors.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : PyTorch Geometric Data object\n",
    "    title : str\n",
    "    predictions : tensor, optional\n",
    "        Predicted labels for visualization\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert to NetworkX for visualization\n",
    "    G = to_networkx(data, to_undirected=True)\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig, axes = plt.subplots(1, 2, figsize=figsize)\n",
    "    \n",
    "    # Use labels or predictions\n",
    "    colors_true = data.y.numpy()\n",
    "    colors_pred = predictions.numpy() if predictions is not None else colors_true\n",
    "    \n",
    "    # Calculate layout once for consistency\n",
    "    pos = nx.spring_layout(G, seed=42, k=0.5, iterations=50)\n",
    "    \n",
    "    # Plot 1: True Labels\n",
    "    ax1 = axes[0]\n",
    "    node_colors_true = ['#FF6B6B' if label == 0 else '#4ECDC4' for label in colors_true]\n",
    "    \n",
    "    nx.draw_networkx_nodes(G, pos, node_color=node_colors_true, \n",
    "                           node_size=800, alpha=0.9, ax=ax1,\n",
    "                           edgecolors='black', linewidths=2)\n",
    "    nx.draw_networkx_edges(G, pos, alpha=0.3, width=2, ax=ax1)\n",
    "    nx.draw_networkx_labels(G, pos, font_size=10, font_weight='bold', \n",
    "                           font_color='white', ax=ax1)\n",
    "    \n",
    "    ax1.set_title('True Community Labels', fontsize=16, fontweight='bold', pad=20)\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    # Add legend\n",
    "    from matplotlib.lines import Line2D\n",
    "    legend_elements_1 = [\n",
    "        Line2D([0], [0], marker='o', color='w', markerfacecolor='#FF6B6B', \n",
    "               markersize=15, label=\"Mr. Hi's Group (0)\"),\n",
    "        Line2D([0], [0], marker='o', color='w', markerfacecolor='#4ECDC4', \n",
    "               markersize=15, label=\"Officer's Group (1)\")\n",
    "    ]\n",
    "    ax1.legend(handles=legend_elements_1, loc='upper left', fontsize=12)\n",
    "    \n",
    "    # Plot 2: Predicted Labels or Graph Statistics\n",
    "    ax2 = axes[1]\n",
    "    \n",
    "    if predictions is not None:\n",
    "        node_colors_pred = ['#FF6B6B' if label == 0 else '#4ECDC4' for label in colors_pred]\n",
    "        \n",
    "        nx.draw_networkx_nodes(G, pos, node_color=node_colors_pred, \n",
    "                               node_size=800, alpha=0.9, ax=ax2,\n",
    "                               edgecolors='black', linewidths=2)\n",
    "        nx.draw_networkx_edges(G, pos, alpha=0.3, width=2, ax=ax2)\n",
    "        nx.draw_networkx_labels(G, pos, font_size=10, font_weight='bold', \n",
    "                               font_color='white', ax=ax2)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        accuracy = (colors_pred == colors_true).sum() / len(colors_true) * 100\n",
    "        ax2.set_title(f'GNN Predictions (Accuracy: {accuracy:.1f}%)', \n",
    "                     fontsize=16, fontweight='bold', pad=20)\n",
    "        \n",
    "        # Highlight incorrect predictions\n",
    "        incorrect = np.where(colors_pred != colors_true)[0]\n",
    "        if len(incorrect) > 0:\n",
    "            incorrect_pos = {node: pos[node] for node in incorrect}\n",
    "            nx.draw_networkx_nodes(G, incorrect_pos, nodelist=incorrect,\n",
    "                                   node_color='yellow', node_size=1000, \n",
    "                                   alpha=0.5, ax=ax2, edgecolors='red', linewidths=4)\n",
    "    else:\n",
    "        # Show degree distribution\n",
    "        degrees = dict(G.degree())\n",
    "        node_sizes = [degrees[node] * 100 for node in G.nodes()]\n",
    "        \n",
    "        nx.draw_networkx_nodes(G, pos, node_color=node_colors_true, \n",
    "                               node_size=node_sizes, alpha=0.9, ax=ax2,\n",
    "                               edgecolors='black', linewidths=2)\n",
    "        nx.draw_networkx_edges(G, pos, alpha=0.3, width=2, ax=ax2)\n",
    "        nx.draw_networkx_labels(G, pos, font_size=10, font_weight='bold', \n",
    "                               font_color='white', ax=ax2)\n",
    "        \n",
    "        ax2.set_title('Node Sizes by Degree', fontsize=16, fontweight='bold', pad=20)\n",
    "    \n",
    "    ax2.axis('off')\n",
    "    ax2.legend(handles=legend_elements_1, loc='upper left', fontsize=12)\n",
    "    \n",
    "    plt.suptitle(title, fontsize=18, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return G, pos\n",
    "\n",
    "# Visualize the network\n",
    "G, pos = visualize_karate_club(data, \"Karate Club Social Network\")\n",
    "\n",
    "print(\"\\nüìä Graph Statistics:\")\n",
    "print(f\"Average degree: {np.mean([d for n, d in G.degree()]):.2f}\")\n",
    "print(f\"Network density: {nx.density(G):.3f}\")\n",
    "print(f\"Number of triangles: {sum(nx.triangles(G).values()) // 3}\")\n",
    "print(f\"Clustering coefficient: {nx.average_clustering(G):.3f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üßÆ Part 2: Graph Neural Networks - Mathematical Foundation\n",
    "\n",
    "### The Core Idea: Message Passing\n",
    "\n",
    "**Traditional Neural Networks:**\n",
    "- Process independent samples\n",
    "- No concept of \"neighbors\"\n",
    "- Example: Image classification\n",
    "\n",
    "**Graph Neural Networks:**\n",
    "- Nodes have neighbors\n",
    "- Information flows through edges\n",
    "- Aggregates neighbor information\n",
    "\n",
    "### Message Passing Framework\n",
    "\n",
    "For each layer `l`, update node `i`:\n",
    "\n",
    "```\n",
    "1. MESSAGE:     m_ij = Message(h_i^(l), h_j^(l), e_ij)\n",
    "2. AGGREGATE:   m_i  = Aggregate({m_ij : j ‚àà N(i)})\n",
    "3. UPDATE:      h_i^(l+1) = Update(h_i^(l), m_i)\n",
    "```\n",
    "\n",
    "Where:\n",
    "- `h_i^(l)` = hidden state of node i at layer l\n",
    "- `N(i)` = neighbors of node i\n",
    "- `e_ij` = edge features (if any)\n",
    "\n",
    "### Graph Convolutional Network (GCN)\n",
    "\n",
    "**Simplified formula:**\n",
    "```\n",
    "H^(l+1) = œÉ(D^(-1/2) A D^(-1/2) H^(l) W^(l))\n",
    "```\n",
    "\n",
    "**Breaking it down:**\n",
    "- `A` = Adjacency matrix (who connects to whom)\n",
    "- `D` = Degree matrix (how many connections each node has)\n",
    "- `H^(l)` = Node features at layer l\n",
    "- `W^(l)` = Learnable weight matrix\n",
    "- `œÉ` = Activation function (e.g., ReLU)\n",
    "- `D^(-1/2) A D^(-1/2)` = Normalized adjacency (prevents numerical issues)\n",
    "\n",
    "**Intuition:**\n",
    "1. Take your features: `H^(l)`\n",
    "2. Transform them: `H^(l) W^(l)`\n",
    "3. Aggregate from neighbors: `A * (transformed features)`\n",
    "4. Normalize by degree: `D^(-1/2) ... D^(-1/2)`\n",
    "5. Apply activation: `œÉ(...)`\n",
    "\n",
    "Let's implement this from scratch!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Build Adjacency Matrix from Edge Index\n",
    "\n",
    "print(\"üî® Building Graph Representations from Scratch\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "def edge_index_to_adjacency_matrix(edge_index, num_nodes):\n",
    "    \"\"\"\n",
    "    Convert edge index to adjacency matrix.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    edge_index : torch.Tensor\n",
    "        Shape [2, num_edges], where edge_index[0] = source, edge_index[1] = target\n",
    "    num_nodes : int\n",
    "        Total number of nodes\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    A : numpy.ndarray\n",
    "        Adjacency matrix of shape [num_nodes, num_nodes]\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize adjacency matrix with zeros\n",
    "    A = np.zeros((num_nodes, num_nodes))\n",
    "    \n",
    "    # Convert edge_index to numpy\n",
    "    edge_index_np = edge_index.numpy()\n",
    "    \n",
    "    # Fill in the edges\n",
    "    for i in range(edge_index.shape[1]):\n",
    "        source = edge_index_np[0, i]\n",
    "        target = edge_index_np[1, i]\n",
    "        A[source, target] = 1\n",
    "    \n",
    "    return A\n",
    "\n",
    "def compute_degree_matrix(A):\n",
    "    \"\"\"\n",
    "    Compute degree matrix from adjacency matrix.\n",
    "    \n",
    "    The degree matrix D is a diagonal matrix where D[i,i] = number of neighbors of node i\n",
    "    \"\"\"\n",
    "    # Sum along rows to get degree of each node\n",
    "    degrees = np.sum(A, axis=1)\n",
    "    \n",
    "    # Create diagonal matrix\n",
    "    D = np.diag(degrees)\n",
    "    \n",
    "    return D\n",
    "\n",
    "def normalize_adjacency(A, add_self_loops=True):\n",
    "    \"\"\"\n",
    "    Compute normalized adjacency matrix: D^(-1/2) * A * D^(-1/2)\n",
    "    \n",
    "    This normalization prevents the gradients from exploding or vanishing\n",
    "    and ensures fair aggregation from all neighbors.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add self-loops: each node also aggregates its own features\n",
    "    if add_self_loops:\n",
    "        A = A + np.eye(A.shape[0])\n",
    "    \n",
    "    # Compute degree matrix\n",
    "    D = compute_degree_matrix(A)\n",
    "    \n",
    "    # Compute D^(-1/2)\n",
    "    D_inv_sqrt = np.diag(1.0 / np.sqrt(np.diag(D) + 1e-8))  # Add epsilon to avoid division by zero\n",
    "    \n",
    "    # Compute normalized adjacency: D^(-1/2) * A * D^(-1/2)\n",
    "    A_normalized = D_inv_sqrt @ A @ D_inv_sqrt\n",
    "    \n",
    "    return A_normalized\n",
    "\n",
    "# Build adjacency matrix from Karate Club data\n",
    "A = edge_index_to_adjacency_matrix(data.edge_index, data.num_nodes)\n",
    "D = compute_degree_matrix(A)\n",
    "A_normalized = normalize_adjacency(A, add_self_loops=True)\n",
    "\n",
    "print(f\"\\nüìä Adjacency Matrix (A):\")\n",
    "print(f\"Shape: {A.shape}\")\n",
    "print(f\"Number of edges: {np.sum(A):.0f}\")\n",
    "print(f\"Sparsity: {(1 - np.count_nonzero(A) / A.size) * 100:.1f}%\")\n",
    "print(f\"\\nFirst 5x5 block of A:\")\n",
    "print(A[:5, :5].astype(int))\n",
    "\n",
    "print(f\"\\nüìä Degree Matrix (D):\")\n",
    "print(f\"Diagonal elements (node degrees):\")\n",
    "print(f\"Node 0: {D[0,0]:.0f} connections\")\n",
    "print(f\"Node 33: {D[33,33]:.0f} connections\")\n",
    "print(f\"Average degree: {np.mean(np.diag(D)):.2f}\")\n",
    "\n",
    "print(f\"\\nüìä Normalized Adjacency (A_norm):\")\n",
    "print(f\"Shape: {A_normalized.shape}\")\n",
    "print(f\"\\nFirst 5x5 block of A_norm:\")\n",
    "print(np.round(A_normalized[:5, :5], 3))\n",
    "print(f\"\\nüí° Notice: Normalized values are smaller and balanced!\")\n",
    "\n",
    "# Visualize adjacency matrix\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Plot original adjacency matrix\n",
    "im1 = axes[0].imshow(A, cmap='Blues', aspect='auto')\n",
    "axes[0].set_title('Adjacency Matrix (A)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Node ID')\n",
    "axes[0].set_ylabel('Node ID')\n",
    "plt.colorbar(im1, ax=axes[0])\n",
    "\n",
    "# Plot degree distribution\n",
    "degrees = np.diag(D)\n",
    "axes[1].bar(range(len(degrees)), degrees, color='steelblue', alpha=0.7)\n",
    "axes[1].set_title('Node Degree Distribution', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Node ID')\n",
    "axes[1].set_ylabel('Degree (Number of Connections)')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot normalized adjacency matrix\n",
    "im3 = axes[2].imshow(A_normalized, cmap='viridis', aspect='auto')\n",
    "axes[2].set_title('Normalized Adjacency (A_norm)', fontsize=14, fontweight='bold')\n",
    "axes[2].set_xlabel('Node ID')\n",
    "axes[2].set_ylabel('Node ID')\n",
    "plt.colorbar(im3, ax=axes[2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Graph representations built successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Cell 5: Pure NumPy GNN - Understanding the Mathematics\n\nprint(\"üî¨ Implementing GNN from Scratch using Pure NumPy\")\nprint(\"=\" * 60)\n\ndef gcn_layer_numpy(X, A_norm, W, activation='relu'):\n    \"\"\"\n    Single Graph Convolutional Layer - Pure NumPy Implementation\n    \n    Formula: H = activation(A_norm @ X @ W)\n    \n    Parameters:\n    -----------\n    X : numpy.ndarray\n        Node features, shape [num_nodes, input_features]\n    A_norm : numpy.ndarray\n        Normalized adjacency matrix, shape [num_nodes, num_nodes]\n    W : numpy.ndarray\n        Weight matrix, shape [input_features, output_features]\n    activation : str\n        Activation function ('relu', 'sigmoid', 'none')\n    \n    Returns:\n    --------\n    H : numpy.ndarray\n        Output features, shape [num_nodes, output_features]\n    \"\"\"\n    \n    # Step 1: Linear transformation X @ W\n    transformed = X @ W\n    print(f\"  Step 1 - Transformed: {transformed.shape}\")\n    \n    # Step 2: Aggregate from neighbors A_norm @ (X @ W)\n    aggregated = A_norm @ transformed\n    print(f\"  Step 2 - Aggregated: {aggregated.shape}\")\n    \n    # Step 3: Apply activation\n    if activation == 'relu':\n        output = np.maximum(0, aggregated)\n    elif activation == 'sigmoid':\n        output = 1 / (1 + np.exp(-aggregated))\n    else:\n        output = aggregated\n    \n    print(f\"  Step 3 - After {activation}: {output.shape}\")\n    \n    return output\n\ndef softmax_numpy(X):\n    \"\"\"Compute softmax for classification.\"\"\"\n    exp_X = np.exp(X - np.max(X, axis=1, keepdims=True))\n    return exp_X / np.sum(exp_X, axis=1, keepdims=True)\n\ndef cross_entropy_loss_numpy(predictions, labels):\n    \"\"\"Compute cross-entropy loss.\"\"\"\n    n = labels.shape[0]\n    log_likelihood = -np.log(predictions[range(n), labels] + 1e-8)\n    loss = np.sum(log_likelihood) / n\n    return loss\n\n# Build a 2-layer GCN from scratch\nprint(\"\\nüèóÔ∏è Building 2-Layer GCN Architecture:\")\nprint(f\"Input features: {data.num_node_features}\")\nprint(f\"Hidden units: 16\")\nprint(f\"Output classes: {dataset.num_classes}\")\n\n# Initialize random weights\nnp.random.seed(42)\ninput_dim = data.num_node_features\nhidden_dim = 16\noutput_dim = dataset.num_classes\n\nW1 = np.random.randn(input_dim, hidden_dim) * 0.01\nW2 = np.random.randn(hidden_dim, output_dim) * 0.01\n\nprint(f\"\\nWeight matrices:\")\nprint(f\"W1 shape: {W1.shape} - transforms {input_dim}D -> {hidden_dim}D\")\nprint(f\"W2 shape: {W2.shape} - transforms {hidden_dim}D -> {output_dim}D\")\n\n# Forward pass\nprint(\"\\nüîÑ Forward Pass:\")\nX_np = data.x.numpy()\n\nprint(\"\\nLayer 1:\")\nH1 = gcn_layer_numpy(X_np, A_normalized, W1, activation='relu')\n\nprint(\"\\nLayer 2:\")\nH2 = gcn_layer_numpy(H1, A_normalized, W2, activation='none')\n\nprint(\"\\nOutput (logits):\")\npredictions_np = softmax_numpy(H2)\nprint(f\"Predictions shape: {predictions_np.shape}\")\nprint(f\"Sum of probabilities per node: {predictions_np[0].sum():.4f}\")\n\n# Compute loss\nlabels_np = data.y.numpy()\nloss_np = cross_entropy_loss_numpy(predictions_np, labels_np)\nprint(f\"\\nInitial loss (random weights): {loss_np:.4f}\")\n\n# Get predictions\npred_labels_np = np.argmax(predictions_np, axis=1)\naccuracy_np = np.mean(pred_labels_np == labels_np) * 100\nprint(f\"Initial accuracy (random): {accuracy_np:.1f}%\")\n\nprint(\"\\nüí° Key Insight:\")\nprint(\"This is a SINGLE forward pass with random weights.\")\nprint(\"To improve accuracy, we need to TRAIN with backpropagation!\")\nprint(\"\\n‚úÖ NumPy GCN implementation complete!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Cell 6: Custom GCN Layer in PyTorch\n\nprint(\"üîß Building Custom GCN Layer in PyTorch\")\nprint(\"=\" * 55)\n\nclass GCNLayer(nn.Module):\n    \"\"\"\n    Custom Graph Convolutional Layer - PyTorch Implementation\n    \n    This layer implements: H_out = activation(A_norm @ H_in @ W + bias)\n    \"\"\"\n    \n    def __init__(self, in_features, out_features, activation=True, dropout=0.5):\n        \"\"\"\n        Parameters:\n        -----------\n        in_features : int\n            Number of input features per node\n        out_features : int\n            Number of output features per node\n        activation : bool\n            Whether to apply ReLU activation\n        dropout : float\n            Dropout probability\n        \"\"\"\n        super(GCNLayer, self).__init__()\n        \n        # Learnable weight matrix\n        self.weight = nn.Parameter(torch.FloatTensor(in_features, out_features))\n        \n        # Learnable bias\n        self.bias = nn.Parameter(torch.FloatTensor(out_features))\n        \n        # Activation and regularization\n        self.activation = activation\n        self.dropout = nn.Dropout(dropout)\n        \n        # Initialize weights using Xavier initialization\n        self.reset_parameters()\n    \n    def reset_parameters(self):\n        \"\"\"Initialize parameters using Xavier uniform initialization.\"\"\"\n        nn.init.xavier_uniform_(self.weight)\n        nn.init.zeros_(self.bias)\n    \n    def forward(self, X, A_norm):\n        \"\"\"\n        Forward pass of GCN layer.\n        \n        Parameters:\n        -----------\n        X : torch.Tensor\n            Node features, shape [num_nodes, in_features]\n        A_norm : torch.Tensor\n            Normalized adjacency matrix, shape [num_nodes, num_nodes]\n        \n        Returns:\n        --------\n        output : torch.Tensor\n            Output features, shape [num_nodes, out_features]\n        \"\"\"\n        \n        # Apply dropout to input features\n        X = self.dropout(X)\n        \n        # Step 1: Linear transformation X @ W\n        support = torch.mm(X, self.weight)\n        \n        # Step 2: Aggregate from neighbors A_norm @ (X @ W)\n        output = torch.mm(A_norm, support)\n        \n        # Step 3: Add bias\n        output = output + self.bias\n        \n        # Step 4: Apply activation\n        if self.activation:\n            output = F.relu(output)\n        \n        return output\n    \n    def __repr__(self):\n        return f'{self.__class__.__name__}({self.weight.shape[0]} -> {self.weight.shape[1]})'\n\n# Test custom GCN layer\nprint(\"\\nüß™ Testing Custom GCN Layer:\")\n\n# Convert normalized adjacency to PyTorch tensor\nA_norm_torch = torch.FloatTensor(A_normalized)\nX_torch = data.x.float()\n\n# Create a single GCN layer\ntest_layer = GCNLayer(in_features=34, out_features=16)\n\nprint(f\"Layer: {test_layer}\")\nprint(f\"\\nInput shape: {X_torch.shape}\")\nprint(f\"Adjacency shape: {A_norm_torch.shape}\")\n\n# Forward pass\nwith torch.no_grad():\n    test_output = test_layer(X_torch, A_norm_torch)\n\nprint(f\"Output shape: {test_output.shape}\")\nprint(f\"Output sample (first node, first 5 features): {test_output[0, :5]}\")\n\nprint(\"\\n‚úÖ Custom GCN layer working correctly!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Cell 7: Complete GCN Model Architecture\n\nprint(\"üèóÔ∏è Building Complete GCN Model\")\nprint(\"=\" * 45)\n\nclass GCN(nn.Module):\n    \"\"\"\n    Complete Graph Convolutional Network for Node Classification\n    \n    Architecture:\n    Input -> GCN Layer 1 -> ReLU -> Dropout -> GCN Layer 2 -> Softmax\n    \"\"\"\n    \n    def __init__(self, num_features, hidden_dim, num_classes, dropout=0.5):\n        \"\"\"\n        Parameters:\n        -----------\n        num_features : int\n            Number of input features per node\n        hidden_dim : int\n            Number of hidden units\n        num_classes : int\n            Number of output classes\n        dropout : float\n            Dropout probability\n        \"\"\"\n        super(GCN, self).__init__()\n        \n        # Layer 1: Input -> Hidden\n        self.gcn1 = GCNLayer(num_features, hidden_dim, activation=True, dropout=dropout)\n        \n        # Layer 2: Hidden -> Output\n        self.gcn2 = GCNLayer(hidden_dim, num_classes, activation=False, dropout=dropout)\n        \n        self.num_parameters = sum(p.numel() for p in self.parameters())\n    \n    def forward(self, X, A_norm):\n        \"\"\"\n        Forward pass through the GCN.\n        \n        Parameters:\n        -----------\n        X : torch.Tensor\n            Node features\n        A_norm : torch.Tensor\n            Normalized adjacency matrix\n        \n        Returns:\n        --------\n        output : torch.Tensor\n            Log probabilities for each class\n        \"\"\"\n        \n        # First GCN layer with ReLU activation\n        h1 = self.gcn1(X, A_norm)\n        \n        # Second GCN layer (no activation, will use log_softmax)\n        h2 = self.gcn2(h1, A_norm)\n        \n        # Log softmax for numerical stability\n        output = F.log_softmax(h2, dim=1)\n        \n        return output\n    \n    def get_embeddings(self, X, A_norm):\n        \"\"\"Get node embeddings from the first layer (before classification).\"\"\"\n        with torch.no_grad():\n            embeddings = self.gcn1(X, A_norm)\n        return embeddings\n    \n    def __repr__(self):\n        return (f'{self.__class__.__name__}(\\n'\n                f'  Layer 1: {self.gcn1}\\n'\n                f'  Layer 2: {self.gcn2}\\n'\n                f'  Total parameters: {self.num_parameters:,}\\n'\n                f')')\n\n# Create model instance\nmodel = GCN(\n    num_features=data.num_node_features,\n    hidden_dim=16,\n    num_classes=dataset.num_classes,\n    dropout=0.5\n)\n\nprint(\"\\nüìä Model Architecture:\")\nprint(model)\n\nprint(\"\\nüîç Model Details:\")\nprint(f\"Input dimension: {data.num_node_features}\")\nprint(f\"Hidden dimension: 16\")\nprint(f\"Output dimension: {dataset.num_classes}\")\nprint(f\"Total trainable parameters: {model.num_parameters:,}\")\n\n# Count parameters per layer\ntotal_params = 0\nfor name, param in model.named_parameters():\n    params = param.numel()\n    total_params += params\n    print(f\"\\n{name}:\")\n    print(f\"  Shape: {param.shape}\")\n    print(f\"  Parameters: {params:,}\")\n\nprint(f\"\\nTotal: {total_params:,} parameters\")\n\n# Test forward pass\nprint(\"\\nüß™ Testing Forward Pass:\")\nwith torch.no_grad():\n    test_output = model(X_torch, A_norm_torch)\n\nprint(f\"Output shape: {test_output.shape}\")\nprint(f\"Output is log probabilities (log_softmax)\")\nprint(f\"Example output for first node: {test_output[0]}\")\nprint(f\"Converting to probabilities: {torch.exp(test_output[0])}\")\nprint(f\"Sum of probabilities: {torch.exp(test_output[0]).sum():.4f}\")\n\nprint(\"\\n‚úÖ GCN model created successfully!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Cell 8: Train the GCN Model\n\nprint(\"üéØ Training the GCN Model\")\nprint(\"=\" * 45)\n\ndef train_gcn(model, X, A_norm, labels, train_mask, epochs=200, lr=0.01, weight_decay=5e-4):\n    \"\"\"\n    Train the GCN model.\n    \n    Parameters:\n    -----------\n    model : nn.Module\n        GCN model\n    X : torch.Tensor\n        Node features\n    A_norm : torch.Tensor\n        Normalized adjacency matrix\n    labels : torch.Tensor\n        True labels\n    train_mask : torch.Tensor\n        Boolean mask for training nodes\n    epochs : int\n        Number of training epochs\n    lr : float\n        Learning rate\n    weight_decay : float\n        L2 regularization strength\n    \n    Returns:\n    --------\n    history : dict\n        Training history (loss and accuracy per epoch)\n    \"\"\"\n    \n    # Optimizer: Adam with weight decay (L2 regularization)\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n    \n    # Loss function: Negative Log Likelihood (for log_softmax output)\n    criterion = nn.NLLLoss()\n    \n    # Training history\n    history = {\n        'train_loss': [],\n        'train_acc': [],\n        'val_loss': [],\n        'val_acc': []\n    }\n    \n    # Training loop\n    model.train()\n    \n    for epoch in range(epochs):\n        # Zero gradients\n        optimizer.zero_grad()\n        \n        # Forward pass\n        output = model(X, A_norm)\n        \n        # Compute loss only on training nodes\n        loss = criterion(output[train_mask], labels[train_mask])\n        \n        # Backward pass\n        loss.backward()\n        \n        # Update weights\n        optimizer.step()\n        \n        # Compute accuracy\n        with torch.no_grad():\n            pred = output.argmax(dim=1)\n            train_correct = pred[train_mask] == labels[train_mask]\n            train_acc = train_correct.sum().item() / train_mask.sum().item()\n            \n            # Validation (all nodes)\n            val_correct = pred == labels\n            val_acc = val_correct.sum().item() / labels.shape[0]\n        \n        # Store history\n        history['train_loss'].append(loss.item())\n        history['train_acc'].append(train_acc)\n        history['val_loss'].append(loss.item())\n        history['val_acc'].append(val_acc)\n        \n        # Print progress\n        if (epoch + 1) % 20 == 0 or epoch == 0:\n            print(f'Epoch {epoch+1:3d}/{epochs} | '\n                  f'Loss: {loss.item():.4f} | '\n                  f'Train Acc: {train_acc*100:5.1f}% | '\n                  f'Val Acc: {val_acc*100:5.1f}%')\n    \n    return history\n\n# Prepare data for training\nX_train = data.x.float()\nA_norm_train = torch.FloatTensor(A_normalized)\nlabels_train = data.y\ntrain_mask = data.train_mask\n\nprint(\"\\nüìä Training Configuration:\")\nprint(f\"Training nodes: {train_mask.sum().item()} / {data.num_nodes}\")\nprint(f\"Learning rate: 0.01\")\nprint(f\"Weight decay: 5e-4\")\nprint(f\"Epochs: 200\")\nprint(f\"Optimizer: Adam\")\n\nprint(\"\\nüöÄ Starting training...\")\nprint(\"=\" * 60)\n\n# Train the model\nhistory = train_gcn(\n    model=model,\n    X=X_train,\n    A_norm=A_norm_train,\n    labels=labels_train,\n    train_mask=train_mask,\n    epochs=200,\n    lr=0.01,\n    weight_decay=5e-4\n)\n\nprint(\"=\" * 60)\nprint(\"\\n‚úÖ Training complete!\")\n\n# Final evaluation\nmodel.eval()\nwith torch.no_grad():\n    final_output = model(X_train, A_norm_train)\n    final_pred = final_output.argmax(dim=1)\n    final_acc = (final_pred == labels_train).sum().item() / labels_train.shape[0]\n    \n    print(f\"\\nüéØ Final Results:\")\n    print(f\"Overall Accuracy: {final_acc*100:.1f}%\")\n    print(f\"Correct predictions: {(final_pred == labels_train).sum().item()} / {labels_train.shape[0]}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Cell 9: Visualize Training Progress\n\nprint(\"üìà Visualizing Training History\")\nprint(\"=\" * 45)\n\ndef plot_training_history(history):\n    \"\"\"\n    Plot training and validation metrics over epochs.\n    \"\"\"\n    \n    fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n    \n    epochs_range = range(1, len(history['train_loss']) + 1)\n    \n    # Plot 1: Loss over epochs\n    ax1 = axes[0]\n    ax1.plot(epochs_range, history['train_loss'], 'b-', label='Training Loss', linewidth=2)\n    ax1.set_xlabel('Epoch', fontsize=12)\n    ax1.set_ylabel('Loss', fontsize=12)\n    ax1.set_title('Training Loss Over Time', fontsize=14, fontweight='bold')\n    ax1.legend(fontsize=11)\n    ax1.grid(True, alpha=0.3)\n    \n    # Plot 2: Accuracy over epochs\n    ax2 = axes[1]\n    train_acc_pct = [acc * 100 for acc in history['train_acc']]\n    val_acc_pct = [acc * 100 for acc in history['val_acc']]\n    \n    ax2.plot(epochs_range, train_acc_pct, 'g-', label='Training Accuracy', linewidth=2)\n    ax2.plot(epochs_range, val_acc_pct, 'r--', label='Validation Accuracy', linewidth=2)\n    ax2.set_xlabel('Epoch', fontsize=12)\n    ax2.set_ylabel('Accuracy (%)', fontsize=12)\n    ax2.set_title('Accuracy Over Time', fontsize=14, fontweight='bold')\n    ax2.legend(fontsize=11)\n    ax2.grid(True, alpha=0.3)\n    ax2.set_ylim([0, 105])\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Print statistics\n    print(f\"\\nüìä Training Statistics:\")\n    print(f\"Initial train accuracy: {history['train_acc'][0]*100:.1f}%\")\n    print(f\"Final train accuracy: {history['train_acc'][-1]*100:.1f}%\")\n    print(f\"Improvement: {(history['train_acc'][-1] - history['train_acc'][0])*100:+.1f}%\")\n    print(f\"\\nInitial val accuracy: {history['val_acc'][0]*100:.1f}%\")\n    print(f\"Final val accuracy: {history['val_acc'][-1]*100:.1f}%\")\n    print(f\"Improvement: {(history['val_acc'][-1] - history['val_acc'][0])*100:+.1f}%\")\n    print(f\"\\nFinal loss: {history['train_loss'][-1]:.4f}\")\n    print(f\"Best val accuracy: {max(history['val_acc'])*100:.1f}% (Epoch {history['val_acc'].index(max(history['val_acc']))+1})\")\n\nplot_training_history(history)\n\nprint(\"\\n‚úÖ Training visualization complete!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Cell 10: Detailed Prediction Analysis\n\nprint(\"üîç Analyzing Model Predictions\")\nprint(\"=\" * 45)\n\n# Get final predictions\nmodel.eval()\nwith torch.no_grad():\n    output = model(X_train, A_norm_train)\n    probabilities = torch.exp(output)\n    predictions = output.argmax(dim=1)\n\n# Analyze predictions\npredictions_np = predictions.numpy()\nlabels_np = labels_train.numpy()\nprobabilities_np = probabilities.numpy()\n\n# Create analysis dataframe\nanalysis_df = pd.DataFrame({\n    'Node': range(data.num_nodes),\n    'True_Label': labels_np,\n    'Predicted_Label': predictions_np,\n    'Correct': predictions_np == labels_np,\n    'Prob_Class_0': probabilities_np[:, 0],\n    'Prob_Class_1': probabilities_np[:, 1],\n    'Confidence': np.max(probabilities_np, axis=1)\n})\n\nprint(f\"\\nüìä Prediction Analysis:\")\nprint(f\"{'='*60}\")\n\n# Overall statistics\ntotal_correct = analysis_df['Correct'].sum()\naccuracy = total_correct / len(analysis_df) * 100\nprint(f\"\\nOverall Accuracy: {accuracy:.1f}% ({total_correct}/{len(analysis_df)} nodes)\")\n\n# Per-class accuracy\nfor class_label in [0, 1]:\n    class_mask = analysis_df['True_Label'] == class_label\n    class_correct = analysis_df[class_mask]['Correct'].sum()\n    class_total = class_mask.sum()\n    class_acc = class_correct / class_total * 100\n    class_name = \"Mr. Hi's Group\" if class_label == 0 else \"Officer's Group\"\n    print(f\"\\nClass {class_label} ({class_name}):\")\n    print(f\"  Accuracy: {class_acc:.1f}% ({class_correct}/{class_total} nodes)\")\n\n# Confidence analysis\nprint(f\"\\nüéØ Confidence Analysis:\")\navg_confidence = analysis_df['Confidence'].mean()\nprint(f\"Average confidence: {avg_confidence:.1%}\")\nprint(f\"Min confidence: {analysis_df['Confidence'].min():.1%}\")\nprint(f\"Max confidence: {analysis_df['Confidence'].max():.1%}\")\n\n# Low confidence predictions\nlow_conf_threshold = 0.60\nlow_conf_nodes = analysis_df[analysis_df['Confidence'] < low_conf_threshold]\nprint(f\"\\nLow confidence predictions (< {low_conf_threshold:.0%}): {len(low_conf_nodes)}\")\nif len(low_conf_nodes) > 0:\n    print(f\"Nodes: {low_conf_nodes['Node'].tolist()}\")\n\n# Misclassified nodes\nmisclassified = analysis_df[~analysis_df['Correct']]\nprint(f\"\\n‚ùå Misclassified Nodes: {len(misclassified)}\")\nif len(misclassified) > 0:\n    print(f\"\\nDetailed misclassifications:\")\n    for idx, row in misclassified.iterrows():\n        print(f\"  Node {row['Node']}: True={row['True_Label']}, Pred={row['Predicted_Label']}, \"\n              f\"Confidence={row['Confidence']:.1%}\")\n\n# Display full analysis for first 10 nodes\nprint(f\"\\nüìã Sample Predictions (First 10 Nodes):\")\nprint(analysis_df.head(10).to_string(index=False))\n\n# Visualize predictions on graph\nvisualize_karate_club(data, \"GCN Predictions vs Ground Truth\", predictions=predictions)\n\nprint(\"\\n‚úÖ Prediction analysis complete!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Cell 11: Visualize Learned Node Embeddings\n\nprint(\"üó∫Ô∏è Visualizing Node Embeddings\")\nprint(\"=\" * 45)\n\ndef visualize_embeddings_2d(embeddings, labels, title=\"Node Embeddings\"):\n    \"\"\"\n    Visualize node embeddings in 2D using t-SNE or PCA.\n    \"\"\"\n    \n    from sklearn.manifold import TSNE\n    from sklearn.decomposition import PCA\n    \n    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n    \n    # PCA projection\n    pca = PCA(n_components=2, random_state=42)\n    embeddings_pca = pca.fit_transform(embeddings.numpy())\n    \n    ax1 = axes[0]\n    scatter1 = ax1.scatter(embeddings_pca[:, 0], embeddings_pca[:, 1],\n                          c=labels.numpy(), cmap='coolwarm', s=200, alpha=0.7,\n                          edgecolors='black', linewidths=2)\n    \n    # Add node labels\n    for i, (x, y) in enumerate(embeddings_pca):\n        ax1.annotate(str(i), (x, y), fontsize=8, ha='center', va='center',\n                    fontweight='bold', color='white')\n    \n    ax1.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)', fontsize=11)\n    ax1.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)', fontsize=11)\n    ax1.set_title('PCA Projection', fontsize=13, fontweight='bold')\n    ax1.grid(True, alpha=0.3)\n    plt.colorbar(scatter1, ax=ax1, label='Community')\n    \n    # t-SNE projection\n    tsne = TSNE(n_components=2, random_state=42, perplexity=15, n_iter=1000)\n    embeddings_tsne = tsne.fit_transform(embeddings.numpy())\n    \n    ax2 = axes[1]\n    scatter2 = ax2.scatter(embeddings_tsne[:, 0], embeddings_tsne[:, 1],\n                          c=labels.numpy(), cmap='coolwarm', s=200, alpha=0.7,\n                          edgecolors='black', linewidths=2)\n    \n    # Add node labels\n    for i, (x, y) in enumerate(embeddings_tsne):\n        ax2.annotate(str(i), (x, y), fontsize=8, ha='center', va='center',\n                    fontweight='bold', color='white')\n    \n    ax2.set_xlabel('t-SNE Dimension 1', fontsize=11)\n    ax2.set_ylabel('t-SNE Dimension 2', fontsize=11)\n    ax2.set_title('t-SNE Projection', fontsize=13, fontweight='bold')\n    ax2.grid(True, alpha=0.3)\n    plt.colorbar(scatter2, ax=ax2, label='Community')\n    \n    plt.suptitle(title, fontsize=15, fontweight='bold', y=1.02)\n    plt.tight_layout()\n    plt.show()\n    \n    print(f\"\\nüìä Embedding Analysis:\")\n    print(f\"PCA explained variance: {pca.explained_variance_ratio_[0]:.1%} + {pca.explained_variance_ratio_[1]:.1%} = {sum(pca.explained_variance_ratio_):.1%}\")\n    print(f\"Embedding dimension: {embeddings.shape[1]}\")\n\n# Get embeddings from trained model\nembeddings = model.get_embeddings(X_train, A_norm_train)\n\nprint(f\"\\nüîç Embeddings shape: {embeddings.shape}\")\nprint(f\"Each node is represented by a {embeddings.shape[1]}-dimensional vector\")\n\n# Visualize embeddings\nvisualize_embeddings_2d(embeddings, labels_train, \"GCN Learned Node Embeddings\")\n\nprint(f\"\\nüí° Interpretation:\")\nprint(f\"- Nodes with similar colors (same community) should cluster together\")\nprint(f\"- The GCN learned to separate the two communities in embedding space\")\nprint(f\"- Closer nodes in embedding space have similar properties\")\n\nprint(\"\\n‚úÖ Embedding visualization complete!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Cell 12: Analyze Message Passing and Node Influence\n\nprint(\"üî¨ Analyzing Message Passing and Node Influence\")\nprint(\"=\" * 55)\n\ndef analyze_message_passing(model, X, A_norm, A, node_id=0):\n    \"\"\"\n    Analyze how a specific node receives messages from its neighbors.\n    \"\"\"\n    \n    print(f\"\\nüéØ Analyzing Message Passing for Node {node_id}\")\n    print(f\"{'='*50}\")\n    \n    # Get neighbors\n    neighbors = np.where(A[node_id] > 0)[0]\n    print(f\"\\nNode {node_id} has {len(neighbors)} neighbors: {neighbors.tolist()}\")\n    \n    # Get embeddings\n    with torch.no_grad():\n        embeddings = model.get_embeddings(X, A_norm)\n    \n    # Analyze influence of each neighbor\n    node_embedding = embeddings[node_id].numpy()\n    neighbor_embeddings = embeddings[neighbors].numpy()\n    \n    # Compute similarity (cosine similarity)\n    from sklearn.metrics.pairwise import cosine_similarity\n    \n    similarities = []\n    for i, neighbor in enumerate(neighbors):\n        similarity = cosine_similarity(\n            node_embedding.reshape(1, -1),\n            neighbor_embeddings[i].reshape(1, -1)\n        )[0, 0]\n        similarities.append(similarity)\n    \n    # Create influence dataframe\n    influence_df = pd.DataFrame({\n        'Neighbor': neighbors,\n        'Similarity': similarities,\n        'Degree': [A[neighbor].sum() for neighbor in neighbors],\n        'Community': data.y[neighbors].numpy()\n    })\n    \n    influence_df = influence_df.sort_values('Similarity', ascending=False)\n    \n    print(f\"\\nüìä Neighbor Influence Analysis:\")\n    print(influence_df.to_string(index=False))\n    \n    # Visualize\n    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n    \n    # Plot 1: Neighbor similarities\n    ax1 = axes[0]\n    colors = ['#FF6B6B' if c == 0 else '#4ECDC4' for c in influence_df['Community']]\n    ax1.barh(range(len(influence_df)), influence_df['Similarity'], color=colors, alpha=0.7)\n    ax1.set_yticks(range(len(influence_df)))\n    ax1.set_yticklabels([f\"Node {n}\" for n in influence_df['Neighbor']])\n    ax1.set_xlabel('Cosine Similarity', fontsize=11)\n    ax1.set_title(f'Neighbor Influence on Node {node_id}', fontsize=13, fontweight='bold')\n    ax1.grid(True, alpha=0.3, axis='x')\n    \n    # Plot 2: Ego network\n    ax2 = axes[1]\n    ego_graph = nx.ego_graph(G, node_id, radius=1)\n    ego_pos = nx.spring_layout(ego_graph, seed=42)\n    \n    # Color nodes by community\n    node_colors = []\n    for node in ego_graph.nodes():\n        if node == node_id:\n            node_colors.append('#FFD700')\n        else:\n            node_colors.append('#FF6B6B' if data.y[node] == 0 else '#4ECDC4')\n    \n    nx.draw_networkx_nodes(ego_graph, ego_pos, node_color=node_colors,\n                          node_size=800, alpha=0.9, ax=ax2,\n                          edgecolors='black', linewidths=2)\n    nx.draw_networkx_edges(ego_graph, ego_pos, alpha=0.4, width=2, ax=ax2)\n    nx.draw_networkx_labels(ego_graph, ego_pos, font_size=10,\n                           font_weight='bold', font_color='white', ax=ax2)\n    \n    ax2.set_title(f'Ego Network of Node {node_id}', fontsize=13, fontweight='bold')\n    ax2.axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return influence_df\n\n# Analyze several interesting nodes\ninteresting_nodes = [0, 33, 1, 32]\n\nprint(f\"\\nüîç Analyzing Key Nodes in the Network:\")\nprint(f\"Node 0: Mr. Hi (Instructor)\")\nprint(f\"Node 33: John A (Administrator)\")\nprint(f\"Nodes 1, 32: Other influential members\")\n\nfor node_id in interesting_nodes[:2]:\n    analyze_message_passing(model, X_train, A_norm_train, A, node_id)\n\nprint(\"\\n‚úÖ Message passing analysis complete!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Cell 13: Summary and Key Learnings\n\nprint(\"üéì Karate Club GNN - Complete Learning Summary\")\nprint(\"=\" * 60)\n\nprint(\"\\n‚úÖ What We Accomplished:\")\nprint(\"\\n1. üìä Graph Fundamentals:\")\nprint(\"   - Loaded and explored Karate Club dataset (34 nodes, 78 edges)\")\nprint(\"   - Understood graph representations (adjacency matrix, edge index)\")\nprint(\"   - Computed degree matrices and normalized adjacency\")\n\nprint(\"\\n2. üßÆ Mathematical Understanding:\")\nprint(\"   - Learned message passing framework\")\nprint(\"   - Implemented GCN formula: H = œÉ(D^(-1/2) A D^(-1/2) H W)\")\nprint(\"   - Built forward pass from scratch in NumPy\")\n\nprint(\"\\n3. üèóÔ∏è Implementation Skills:\")\nprint(\"   - Created custom GCN layers in PyTorch\")\nprint(\"   - Built complete 2-layer GCN model\")\nprint(\"   - Implemented training loop with Adam optimizer\")\n\nprint(\"\\n4. üìà Results:\")\nprint(f\"   - Achieved {final_acc*100:.1f}% accuracy on node classification\")\nprint(\"   - Successfully predicted community membership\")\nprint(\"   - Visualized learned embeddings and predictions\")\n\nprint(\"\\nüß† Key Insights About GNNs:\")\nprint(\"\\n1. Message Passing is Core:\")\nprint(\"   - GNNs aggregate information from neighbors\")\nprint(\"   - Multiple layers allow information to propagate further\")\nprint(\"   - Normalization prevents gradient issues\")\n\nprint(\"\\n2. Graph Structure Matters:\")\nprint(\"   - Node features alone aren't enough\")\nprint(\"   - Connections define how information flows\")\nprint(\"   - Community structure emerges from graph topology\")\n\nprint(\"\\n3. Inductive Bias:\")\nprint(\"   - GNNs assume local structure is informative\")\nprint(\"   - Similar nodes should have similar labels\")\nprint(\"   - Connections indicate similarity\")\n\nprint(\"\\nüí° Real-World Applications:\")\nprint(\"\\n- Social Networks: Friend recommendation, influence detection\")\nprint(\"- Citation Networks: Paper classification, collaboration prediction\")\nprint(\"- Molecular Graphs: Drug discovery, property prediction\")\nprint(\"- Knowledge Graphs: Link prediction, entity classification\")\nprint(\"- Traffic Networks: Traffic prediction, route optimization\")\nprint(\"- Recommendation Systems: User-item interaction graphs\")\n\nprint(\"\\nüìö Next Steps for Further Learning:\")\nprint(\"\\n1. Advanced GNN Architectures:\")\nprint(\"   - GraphSAGE: Sampling-based aggregation\")\nprint(\"   - GAT: Graph Attention Networks\")\nprint(\"   - GIN: Graph Isomorphism Networks\")\n\nprint(\"\\n2. Different Tasks:\")\nprint(\"   - Graph classification\")\nprint(\"   - Link prediction\")\nprint(\"   - Graph generation\")\n\nprint(\"\\n3. Larger Datasets:\")\nprint(\"   - Cora, Citeseer, Pubmed (citation networks)\")\nprint(\"   - Reddit, PPI (large-scale graphs)\")\nprint(\"   - OGB datasets (benchmark suite)\")\n\nprint(\"\\n4. Advanced Topics:\")\nprint(\"   - Heterogeneous graphs\")\nprint(\"   - Dynamic graphs\")\nprint(\"   - Graph transformers\")\n\nprint(\"\\nüéØ Practice Exercises:\")\nprint(\"\\n1. Modify the architecture:\")\nprint(\"   - Try 3 layers instead of 2\")\nprint(\"   - Experiment with different hidden dimensions\")\nprint(\"   - Add batch normalization\")\n\nprint(\"\\n2. Hyperparameter tuning:\")\nprint(\"   - Try different learning rates\")\nprint(\"   - Adjust dropout probability\")\nprint(\"   - Experiment with weight decay\")\n\nprint(\"\\n3. Feature engineering:\")\nprint(\"   - Use actual node features (degree, centrality)\")\nprint(\"   - Add edge features\")\nprint(\"   - Combine structural and attribute information\")\n\nprint(\"\\n4. Compare architectures:\")\nprint(\"   - Implement GraphSAGE\")\nprint(\"   - Compare with traditional ML (SVM, Random Forest)\")\nprint(\"   - Benchmark against PyTorch Geometric implementations\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"üéâ Congratulations! You've mastered GNNs from scratch!\")\nprint(\"=\"*60)\n\nprint(\"\\nüìñ Recommended Resources:\")\nprint(\"- Papers: Kipf & Welling (2017) - Semi-Supervised Classification with GCNs\")\nprint(\"- Tutorial: distill.pub/2021/gnn-intro\")\nprint(\"- Library: PyTorch Geometric Documentation\")\nprint(\"- Course: CS224W - Machine Learning with Graphs (Stanford)\")\n\nprint(\"\\n‚úÖ Complete GNN tutorial finished!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}