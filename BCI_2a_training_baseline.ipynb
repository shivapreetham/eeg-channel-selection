{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BCI 2a EEG-ARNN Training Pipeline\n",
    "\n",
    "This notebook trains subject-specific EEG-ARNN models on the BCI Competition IV 2a dataset with:\n",
    "- Subject-specific 3-fold cross-validation (20 epochs per fold)\n",
    "- Channel selection experiments reused from the PhysioNet pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import mne\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "mne.set_log_level('ERROR')\n",
    "sns.set_context('notebook', font_scale=1.1)\n",
    "plt.style.use('seaborn-v0_8')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45672cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Model architecture and training utilities (self-contained)\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class TFEM(nn.Module):\n",
    "    \"\"\"Temporal feature extraction module.\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=16, pool_size=2, use_pool=True):\n",
    "        super().__init__()\n",
    "        self.use_pool = use_pool\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=(1, kernel_size),\n",
    "            padding=(0, kernel_size // 2),\n",
    "            bias=False\n",
    "        )\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.activation = nn.ELU()\n",
    "        if use_pool:\n",
    "            self.pool = nn.AvgPool2d(kernel_size=(1, pool_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.activation(x)\n",
    "        if self.use_pool:\n",
    "            x = self.pool(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CARM(nn.Module):\n",
    "    \"\"\"Channel Active Reasoning Module implementing the graph convolution block.\"\"\"\n",
    "    def __init__(self, num_channels, hidden_dim=40):\n",
    "        super().__init__()\n",
    "        self.num_channels = num_channels\n",
    "        self.W = nn.Parameter(torch.FloatTensor(num_channels, num_channels))\n",
    "        nn.init.xavier_uniform_(self.W)\n",
    "        self.theta = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(hidden_dim)\n",
    "        self.activation = nn.ELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, hidden_dim, num_channels, time_steps = x.size()\n",
    "        A = torch.sigmoid(self.W)\n",
    "        A_sym = (A + A.t()) / 2\n",
    "        I = torch.eye(num_channels, device=x.device)\n",
    "        A_tilde = A_sym + I\n",
    "        D_tilde = torch.diag(A_tilde.sum(dim=1))\n",
    "        D_inv_sqrt = torch.pow(D_tilde, -0.5)\n",
    "        D_inv_sqrt[torch.isinf(D_inv_sqrt)] = 0.0\n",
    "        A_norm = D_inv_sqrt @ A_tilde @ D_inv_sqrt\n",
    "\n",
    "        x_reshaped = x.permute(0, 3, 2, 1)\n",
    "        x_flat = x_reshaped.contiguous().view(batch_size * time_steps, num_channels, hidden_dim)\n",
    "        x_graph = torch.matmul(A_norm, x_flat)\n",
    "        x_graph = self.theta(x_graph)\n",
    "        x_graph = x_graph.view(batch_size, time_steps, num_channels, hidden_dim)\n",
    "        out = x_graph.permute(0, 3, 2, 1)\n",
    "        out = self.bn(out)\n",
    "        out = self.activation(out)\n",
    "        return out, A_sym\n",
    "\n",
    "    def get_adjacency_matrix(self):\n",
    "        return torch.sigmoid(self.W).detach().cpu().numpy()\n",
    "\n",
    "\n",
    "class EEGARNN(nn.Module):\n",
    "    \"\"\"EEG-ARNN model implementing stacked TFEM+CARM blocks.\"\"\"\n",
    "    def __init__(self, num_channels=64, num_timepoints=512, num_classes=4, hidden_dim=40):\n",
    "        super().__init__()\n",
    "        self.num_channels = num_channels\n",
    "\n",
    "        self.tfem1 = TFEM(1, hidden_dim, kernel_size=32, pool_size=2)\n",
    "        self.tfem2 = TFEM(hidden_dim, hidden_dim, kernel_size=16, pool_size=2)\n",
    "        self.tfem3 = TFEM(hidden_dim, hidden_dim, kernel_size=8, pool_size=2)\n",
    "\n",
    "        self.carm1 = CARM(num_channels, hidden_dim)\n",
    "        self.carm2 = CARM(num_channels, hidden_dim)\n",
    "        self.carm3 = CARM(num_channels, hidden_dim)\n",
    "\n",
    "        reduced_time = num_timepoints // 8\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((num_channels, 1))\n",
    "        self.fc1 = nn.Linear(hidden_dim * num_channels, hidden_dim)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.tfem1(x)\n",
    "        x = self.tfem2(x)\n",
    "        x = self.tfem3(x)\n",
    "\n",
    "        x, _ = self.carm1(x)\n",
    "        x, _ = self.carm2(x)\n",
    "        x, _ = self.carm3(x)\n",
    "\n",
    "        x = self.global_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    def get_final_adjacency_matrix(self):\n",
    "        return self.carm3.get_adjacency_matrix()\n",
    "\n",
    "\n",
    "class ChannelSelector:\n",
    "    \"\"\"Channel selection helpers using the learned adjacency matrix.\"\"\"\n",
    "    def __init__(self, adjacency_matrix, channel_names):\n",
    "        self.adj_matrix = adjacency_matrix\n",
    "        self.channel_names = np.array(channel_names)\n",
    "        self.num_channels = len(channel_names)\n",
    "\n",
    "    def edge_selection(self, k):\n",
    "        edges = []\n",
    "        for i in range(self.num_channels):\n",
    "            for j in range(i + 1, self.num_channels):\n",
    "                edge_importance = abs(self.adj_matrix[i, j]) + abs(self.adj_matrix[j, i])\n",
    "                edges.append((i, j, edge_importance))\n",
    "        sorted_edges = sorted(edges, key=lambda x: x[2], reverse=True)\n",
    "        top_k_edges = sorted_edges[:k]\n",
    "        selected_indices = set()\n",
    "        for i, j, _ in top_k_edges:\n",
    "            selected_indices.add(i)\n",
    "            selected_indices.add(j)\n",
    "        selected_indices = np.array(sorted(selected_indices))\n",
    "        selected_channels = self.channel_names[selected_indices].tolist()\n",
    "        return selected_channels, selected_indices\n",
    "\n",
    "    def aggregation_selection(self, k):\n",
    "        channel_scores = np.sum(np.abs(self.adj_matrix), axis=1)\n",
    "        selected_indices = np.argsort(channel_scores)[-k:]\n",
    "        selected_indices = np.sort(selected_indices)\n",
    "        selected_channels = self.channel_names[selected_indices].tolist()\n",
    "        return selected_channels, selected_indices\n",
    "\n",
    "\n",
    "class EEGDataset(Dataset):\n",
    "    \"\"\"PyTorch dataset wrapper for EEG data.\"\"\"\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = torch.FloatTensor(data).unsqueeze(1)\n",
    "        self.labels = torch.LongTensor(labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "def normalize_data(data):\n",
    "    mean = data.mean(axis=(0, 2), keepdims=True)\n",
    "    std = data.std(axis=(0, 2), keepdims=True) + 1e-8\n",
    "    return (data - mean) / std\n",
    "\n",
    "\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_preds, all_labels = [], []\n",
    "    for data, labels in dataloader:\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for data, labels in dataloader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    return avg_loss, accuracy, all_preds, all_labels\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, device, epochs=100, lr=0.001, patience=15):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    try:\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='min', factor=0.5, patience=3, verbose=False\n",
    "        )\n",
    "    except TypeError:\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='min', factor=0.5, patience=3\n",
    "        )\n",
    "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "    best_val_acc = 0.0\n",
    "    best_state = None\n",
    "    epochs_no_improve = 0\n",
    "    for epoch in range(epochs):\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss, val_acc, _, _ = evaluate(model, val_loader, criterion, device)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        try:\n",
    "            scheduler.step(val_loss)\n",
    "        except TypeError:\n",
    "            try:\n",
    "                scheduler.step()\n",
    "            except Exception:\n",
    "                pass\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_state = {k: v.cpu() if hasattr(v, 'cpu') else v for k, v in model.state_dict().items()}\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "        if epochs_no_improve >= patience:\n",
    "            break\n",
    "    if best_state is None:\n",
    "        best_state = {k: v.cpu() if hasattr(v, 'cpu') else v for k, v in model.state_dict().items()}\n",
    "    model.load_state_dict(best_state)\n",
    "    return history, best_state\n",
    "\n",
    "\n",
    "def cross_validate_subject(data, labels, num_channels, num_timepoints, num_classes,\n",
    "                           device, n_splits=3, epochs=30, lr=0.001, batch_size=64, patience=10):\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    fold_results = []\n",
    "    adjacency_matrices = []\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(data, labels)):\n",
    "        print(f\"  Fold {fold + 1}/{n_splits}\", end=\" \")\n",
    "        X_train, X_val = data[train_idx], data[val_idx]\n",
    "        y_train, y_val = labels[train_idx], labels[val_idx]\n",
    "        X_train = normalize_data(X_train)\n",
    "        X_val = normalize_data(X_val)\n",
    "        train_dataset = EEGDataset(X_train, y_train)\n",
    "        val_dataset = EEGDataset(X_val, y_val)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "        model = EEGARNN(\n",
    "            num_channels=num_channels,\n",
    "            num_timepoints=num_timepoints,\n",
    "            num_classes=num_classes,\n",
    "            hidden_dim=40\n",
    "        ).to(device)\n",
    "        history, best_state = train_model(\n",
    "            model, train_loader, val_loader, device,\n",
    "            epochs=epochs, lr=lr, patience=patience\n",
    "        )\n",
    "        model.load_state_dict(best_state)\n",
    "        _, val_acc, _, _ = evaluate(model, val_loader, nn.CrossEntropyLoss(), device)\n",
    "        adj_matrix = model.get_final_adjacency_matrix()\n",
    "        adjacency_matrices.append(adj_matrix)\n",
    "        print(f\"-> Acc: {val_acc:.3f}\")\n",
    "        fold_results.append({\n",
    "            'fold': fold,\n",
    "            'val_acc': val_acc,\n",
    "            'history': history\n",
    "        })\n",
    "    avg_adjacency = np.mean(adjacency_matrices, axis=0)\n",
    "    return {\n",
    "        'fold_results': fold_results,\n",
    "        'avg_accuracy': np.mean([r['val_acc'] for r in fold_results]),\n",
    "        'std_accuracy': np.std([r['val_acc'] for r in fold_results]),\n",
    "        'adjacency_matrix': avg_adjacency\n",
    "    }\n",
    "\n",
    "\n",
    "def retrain_with_selected_channels(data, labels, selected_channel_indices, num_timepoints, num_classes,\n",
    "                                   device, n_splits=3, epochs=25, lr=0.001, batch_size=64, patience=6):\n",
    "    data_subset = data[:, selected_channel_indices, :]\n",
    "    num_channels_subset = len(selected_channel_indices)\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    fold_results = []\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(data_subset, labels)):\n",
    "        X_train, X_val = data_subset[train_idx], data_subset[val_idx]\n",
    "        y_train, y_val = labels[train_idx], labels[val_idx]\n",
    "        X_train = normalize_data(X_train)\n",
    "        X_val = normalize_data(X_val)\n",
    "        train_dataset = EEGDataset(X_train, y_train)\n",
    "        val_dataset = EEGDataset(X_val, y_val)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "        model = EEGARNN(\n",
    "            num_channels=num_channels_subset,\n",
    "            num_timepoints=num_timepoints,\n",
    "            num_classes=num_classes,\n",
    "            hidden_dim=40\n",
    "        ).to(device)\n",
    "        history, best_state = train_model(\n",
    "            model, train_loader, val_loader, device,\n",
    "            epochs=epochs, lr=lr, patience=patience\n",
    "        )\n",
    "        model.load_state_dict(best_state)\n",
    "        _, val_acc, _, _ = evaluate(model, val_loader, nn.CrossEntropyLoss(), device)\n",
    "        fold_results.append({\n",
    "            'fold': fold,\n",
    "            'val_acc': val_acc,\n",
    "            'history': history\n",
    "        })\n",
    "    return {\n",
    "        'fold_results': fold_results,\n",
    "        'avg_accuracy': np.mean([r['val_acc'] for r in fold_results]),\n",
    "        'std_accuracy': np.std([r['val_acc'] for r in fold_results])\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_CONFIG = {\n",
    "    'data': {\n",
    "        'raw_dir': Path('data/BCI_2a'),\n",
    "        'subjects': [f'A0{i}' for i in range(1, 10)],\n",
    "        'selected_classes': [769, 770, 771, 772],\n",
    "        'tmin': 0.0,\n",
    "        'tmax': 4.0,\n",
    "        'baseline': None\n",
    "    },\n",
    "    'model': {\n",
    "        'hidden_dim': 40,\n",
    "        'epochs': 20,\n",
    "        'learning_rate': 0.001,\n",
    "        'batch_size': 32,\n",
    "        'n_folds': 3\n",
    "    },\n",
    "    'channel_selection': {\n",
    "        'k_values': [10, 15, 20, 25, 'all'],\n",
    "        'methods': ['ES', 'AS']\n",
    "    },\n",
    "    'output': {\n",
    "        'results_dir': Path('results/bci_2a'),\n",
    "        'models_dir': Path('saved_models/bci_2a'),\n",
    "        'subject_results_file': 'bci2a_baseline_subject_results.csv',\n",
    "        'channel_selection_results_file': 'bci2a_channel_selection_results.csv',\n",
    "        'retrain_results_file': 'bci2a_baseline_retrain_results.csv',\n",
    "        'config_file': 'bci2a_baseline_experiment_config.json',\n",
    "        'results_summary_figure': 'bci2a_baseline_results_summary.png'\n",
    "    },\n",
    "    'max_subjects': None\n",
    "}\n",
    "\n",
    "EXPERIMENT_CONFIG['output']['results_dir'].mkdir(parents=True, exist_ok=True)\n",
    "EXPERIMENT_CONFIG['output']['models_dir'].mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print('Experiment Configuration:')\n",
    "print(json.dumps(EXPERIMENT_CONFIG, indent=2, default=str))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build BCI 2a Session Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dir = EXPERIMENT_CONFIG['data']['raw_dir']\n",
    "selected_classes = EXPERIMENT_CONFIG['data']['selected_classes']\n",
    "\n",
    "records = []\n",
    "missing_subjects = []\n",
    "\n",
    "for subject_id in EXPERIMENT_CONFIG['data']['subjects']:\n",
    "    gdf_path = raw_dir / f\"{subject_id}T.gdf\"\n",
    "    if not gdf_path.exists():\n",
    "        missing_subjects.append(subject_id)\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        raw = mne.io.read_raw_gdf(gdf_path, preload=False, verbose='ERROR')\n",
    "        events, event_ids = mne.events_from_annotations(raw, verbose='ERROR')\n",
    "        selected_event_ids = [event_ids[str(cls)] for cls in selected_classes if str(cls) in event_ids]\n",
    "        trial_mask = np.isin(events[:, 2], selected_event_ids) if selected_event_ids else np.array([])\n",
    "        num_trials = int(trial_mask.sum()) if trial_mask.size else 0\n",
    "    except Exception as exc:\n",
    "        print(f\"[warn] Could not parse {gdf_path.name}: {exc}\")\n",
    "        num_trials = 0\n",
    "\n",
    "    records.append({\n",
    "        'subject': subject_id,\n",
    "        'session': 'T',\n",
    "        'path': gdf_path,\n",
    "        'num_trials': num_trials\n",
    "    })\n",
    "\n",
    "bci_sessions = pd.DataFrame(records)\n",
    "motor_runs = bci_sessions[bci_sessions['num_trials'] > 0].copy()\n",
    "\n",
    "print(f\"Total subjects configured: {len(EXPERIMENT_CONFIG['data']['subjects'])}\")\n",
    "print(f\"Subjects with labelled training data: {motor_runs['subject'].nunique()}\")\n",
    "print(f\"Total labelled trials: {int(motor_runs['num_trials'].sum())}\")\n",
    "\n",
    "if missing_subjects:\n",
    "    print('Missing training files for subjects:', missing_subjects)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subject Selection\n",
    "\n",
    "Identify BCI 2a subjects with labelled training (T) sessions and aggregate their available trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_counts = (motor_runs.groupby('subject')['num_trials']\n",
    "                  .sum()\n",
    "                  .reset_index()\n",
    "                  .sort_values('subject'))\n",
    "\n",
    "selected_subjects = subject_counts['subject'].tolist()\n",
    "\n",
    "if not selected_subjects:\n",
    "    raise RuntimeError('No BCI 2a subjects with labelled trials were found.')\n",
    "\n",
    "max_subjects = EXPERIMENT_CONFIG.get('max_subjects')\n",
    "if max_subjects:\n",
    "    selected_subjects = selected_subjects[:max_subjects]\n",
    "    subject_counts = subject_counts[subject_counts['subject'].isin(selected_subjects)]\n",
    "\n",
    "print('Subject trial counts:')\n",
    "print(subject_counts.to_string(index=False))\n",
    "print(f\"Will train on {len(selected_subjects)} subjects\")\n",
    "print(f\"Selected subjects: {selected_subjects}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_subject_data(subject_id, subject_sessions_df, config):\n",
    "    '''\n",
    "    Load all labelled motor imagery trials for a BCI 2a subject.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data : np.ndarray or None\n",
    "        (n_trials, n_channels, n_timepoints)\n",
    "    labels : np.ndarray or None\n",
    "        (n_trials,)\n",
    "    channel_names : list[str] or None\n",
    "        Channel labels preserved from the recording\n",
    "    '''\n",
    "    subject_rows = subject_sessions_df[subject_sessions_df['subject'] == subject_id]\n",
    "\n",
    "    if subject_rows.empty:\n",
    "        return None, None, None\n",
    "\n",
    "    selected_classes = config['data']['selected_classes']\n",
    "\n",
    "    all_data = []\n",
    "    all_labels = []\n",
    "    channel_names = None\n",
    "\n",
    "    for _, row in subject_rows.iterrows():\n",
    "        gdf_path = Path(row['path'])\n",
    "        if not gdf_path.exists():\n",
    "            print(f\"[warn] Missing file: {gdf_path}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            raw = mne.io.read_raw_gdf(gdf_path, preload=True, verbose='ERROR')\n",
    "            events, event_ids = mne.events_from_annotations(raw, verbose='ERROR')\n",
    "\n",
    "            selected_event_ids = {str(cls): event_ids[str(cls)] for cls in selected_classes if str(cls) in event_ids}\n",
    "            if not selected_event_ids:\n",
    "                print(f\"[warn] No target events found in {gdf_path.name}\")\n",
    "                continue\n",
    "\n",
    "            epochs = mne.Epochs(\n",
    "                raw,\n",
    "                events,\n",
    "                event_id=selected_event_ids,\n",
    "                tmin=config['data']['tmin'],\n",
    "                tmax=config['data']['tmax'],\n",
    "                baseline=config['data']['baseline'],\n",
    "                preload=True,\n",
    "                event_repeated='merge',\n",
    "                picks='eeg',\n",
    "                verbose='ERROR'\n",
    "            )\n",
    "\n",
    "            data = epochs.get_data()\n",
    "            label_lookup = {event_ids[key]: int(key) for key in selected_event_ids}\n",
    "            labels = np.array([label_lookup[event_code] for event_code in epochs.events[:, 2]])\n",
    "\n",
    "            data, labels = filter_classes(data, labels, selected_classes)\n",
    "\n",
    "            if data.size == 0:\n",
    "                continue\n",
    "\n",
    "            all_data.append(data)\n",
    "            all_labels.append(labels)\n",
    "\n",
    "            if channel_names is None:\n",
    "                channel_names = epochs.ch_names\n",
    "\n",
    "        except Exception as exc:\n",
    "            print(f\"[warn] Failed to load {gdf_path.name}: {exc}\")\n",
    "            continue\n",
    "\n",
    "    if not all_data:\n",
    "        return None, None, None\n",
    "\n",
    "    data = np.concatenate(all_data, axis=0)\n",
    "    labels = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "    return data, labels, channel_names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Training Loop\n",
    "\n",
    "Train subject-specific models with 3-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = []\n",
    "\n",
    "for subject_id in tqdm(selected_subjects, desc='Training subjects'):\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Training subject: {subject_id}\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "    data, labels, channel_names = load_subject_data(\n",
    "        subject_id,\n",
    "        motor_runs,\n",
    "        EXPERIMENT_CONFIG\n",
    "    )\n",
    "\n",
    "    if data is None or len(data) < 30:\n",
    "        print(f\"Skipping {subject_id}: insufficient data\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Data shape: {data.shape}\")\n",
    "    print(f\"Labels: {np.unique(labels, return_counts=True)}\")\n",
    "    print(f\"Channels: {len(channel_names)}\")\n",
    "\n",
    "    num_channels = data.shape[1]\n",
    "    num_timepoints = data.shape[2]\n",
    "    num_classes = len(np.unique(labels))\n",
    "\n",
    "    cv_results = cross_validate_subject(\n",
    "        data, labels,\n",
    "        num_channels=num_channels,\n",
    "        num_timepoints=num_timepoints,\n",
    "        num_classes=num_classes,\n",
    "        device=device,\n",
    "        n_splits=EXPERIMENT_CONFIG['model']['n_folds'],\n",
    "        epochs=EXPERIMENT_CONFIG['model']['epochs'],\n",
    "        lr=EXPERIMENT_CONFIG['model']['learning_rate']\n",
    "    )\n",
    "\n",
    "    print(f\"Average accuracy (all channels): {cv_results['avg_accuracy']:.4f} +/- {cv_results['std_accuracy']:.4f}\")\n",
    "\n",
    "    result = {\n",
    "        'subject': subject_id,\n",
    "        'num_trials': len(data),\n",
    "        'num_channels': num_channels,\n",
    "        'num_timepoints': num_timepoints,\n",
    "        'num_classes': num_classes,\n",
    "        'all_channels_acc': cv_results['avg_accuracy'],\n",
    "        'all_channels_std': cv_results['std_accuracy'],\n",
    "        'adjacency_matrix': cv_results['adjacency_matrix'],\n",
    "        'channel_names': channel_names\n",
    "    }\n",
    "\n",
    "    all_results.append(result)\n",
    "\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Training complete for {len(all_results)} subjects\")\n",
    "print(f\"{'='*80}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Channel Selection Experiments\n",
    "\n",
    "Test different k values with Edge Selection and Aggregation Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_selection_results = []\n",
    "\n",
    "if len(all_results) > 0:\n",
    "    for result in tqdm(all_results, desc=\"Channel selection experiments\"):\n",
    "        subject_id = result['subject']\n",
    "        adj_matrix = result['adjacency_matrix']\n",
    "        channel_names = result['channel_names']\n",
    "        \n",
    "        print(f\"\\nProcessing channel selection for {subject_id}\")\n",
    "        \n",
    "        selector = ChannelSelector(adj_matrix, channel_names)\n",
    "        \n",
    "        for method in EXPERIMENT_CONFIG['channel_selection']['methods']:\n",
    "            print(f\"  Method: {method}\")\n",
    "            \n",
    "            for k in EXPERIMENT_CONFIG['channel_selection']['k_values']:\n",
    "                if k == 'all':\n",
    "                    k_val = result['num_channels']\n",
    "                    selected_channels = channel_names\n",
    "                else:\n",
    "                    k_val = min(k, result['num_channels'])  # Don't exceed available channels\n",
    "                    \n",
    "                    if method == 'ES':\n",
    "                        selected_channels, _ = selector.edge_selection(k_val)\n",
    "                    else:  # AS\n",
    "                        selected_channels, _ = selector.aggregation_selection(k_val)\n",
    "                \n",
    "                print(f\"    k={k_val}: {len(selected_channels)} channels selected\")\n",
    "                \n",
    "                channel_selection_results.append({\n",
    "                    'subject': subject_id,\n",
    "                    'method': method,\n",
    "                    'k': k_val,\n",
    "                    'num_selected': len(selected_channels),\n",
    "                    'selected_channels': selected_channels,\n",
    "                    'accuracy_full': result['all_channels_acc']\n",
    "                })\n",
    "\n",
    "    channel_selection_df = pd.DataFrame(channel_selection_results)\n",
    "    print(f\"\\nChannel selection results: {len(channel_selection_df)} experiments\")\n",
    "    display(channel_selection_df.head(10))\n",
    "else:\n",
    "    channel_selection_df = pd.DataFrame()\n",
    "    print(\"\\nNo results available for channel selection experiments.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store all retraining results\n",
    "retrain_results = []\n",
    "\n",
    "if len(all_results) > 0:\n",
    "    # We need the original data for each subject\n",
    "    subject_data_cache = {}\n",
    "\n",
    "    for result in all_results:\n",
    "        subject_id = result['subject']\n",
    "        print(f\"Loading data for {subject_id}\")\n",
    "\n",
    "        data, labels, channel_names = load_subject_data(\n",
    "            subject_id,\n",
    "            motor_runs,\n",
    "            EXPERIMENT_CONFIG\n",
    "        )\n",
    "\n",
    "        if data is None:\n",
    "            continue\n",
    "\n",
    "        subject_data_cache[subject_id] = {\n",
    "            'data': data,\n",
    "            'labels': labels,\n",
    "            'channel_names': channel_names\n",
    "        }\n",
    "\n",
    "    print(f\"{'='*80}\")\n",
    "    print(\"RETRAINING WITH SELECTED CHANNELS\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "    for result in tqdm(all_results, desc=\"Retraining subjects\"):\n",
    "        subject_id = result['subject']\n",
    "\n",
    "        if subject_id not in subject_data_cache:\n",
    "            continue\n",
    "\n",
    "        cache = subject_data_cache[subject_id]\n",
    "        data = cache['data']\n",
    "        labels = cache['labels']\n",
    "        channel_names = cache['channel_names']\n",
    "\n",
    "        print(f\"Retraining {subject_id}\")\n",
    "\n",
    "        selector = ChannelSelector(result['adjacency_matrix'], channel_names)\n",
    "\n",
    "        for method in EXPERIMENT_CONFIG['channel_selection']['methods']:\n",
    "            for k in EXPERIMENT_CONFIG['channel_selection']['k_values']:\n",
    "                if k == 'all':\n",
    "                    continue\n",
    "\n",
    "                k_val = min(k, result['num_channels'])\n",
    "\n",
    "                if method == 'ES':\n",
    "                    selected_channels, selected_indices = selector.edge_selection(k_val)\n",
    "                else:\n",
    "                    selected_channels, selected_indices = selector.aggregation_selection(k_val)\n",
    "\n",
    "                print(f\"  {method} k={k_val}: Retraining with {len(selected_channels)} channels...\")\n",
    "\n",
    "                retrain_res = retrain_with_selected_channels(\n",
    "                    data, labels,\n",
    "                    selected_channel_indices=selected_indices,\n",
    "                    num_timepoints=result['num_timepoints'],\n",
    "                    num_classes=result['num_classes'],\n",
    "                    device=device,\n",
    "                    n_splits=EXPERIMENT_CONFIG['model']['n_folds'],\n",
    "                    epochs=EXPERIMENT_CONFIG['model']['epochs'],\n",
    "                    lr=EXPERIMENT_CONFIG['model']['learning_rate']\n",
    "                )\n",
    "\n",
    "                acc_drop = result['all_channels_acc'] - retrain_res['avg_accuracy']\n",
    "\n",
    "                print(f\"    Accuracy: {retrain_res['avg_accuracy']:.4f} +/- {retrain_res['std_accuracy']:.4f}\")\n",
    "                print(f\"    Drop from full: {acc_drop:.4f} ({acc_drop/result['all_channels_acc']*100:.1f}%)\")\n",
    "\n",
    "                retrain_results.append({\n",
    "                    'subject': subject_id,\n",
    "                    'method': method,\n",
    "                    'k': k_val,\n",
    "                    'num_channels_selected': len(selected_channels),\n",
    "                    'selected_channels': selected_channels,\n",
    "                    'accuracy': retrain_res['avg_accuracy'],\n",
    "                    'std': retrain_res['std_accuracy'],\n",
    "                    'full_channels_acc': result['all_channels_acc'],\n",
    "                    'accuracy_drop': acc_drop,\n",
    "                    'accuracy_drop_pct': acc_drop / result['all_channels_acc'] * 100\n",
    "                })\n",
    "\n",
    "    retrain_df = pd.DataFrame(retrain_results)\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Retraining complete: {len(retrain_df)} experiments\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "    retrain_path = EXPERIMENT_CONFIG['output']['results_dir'] / EXPERIMENT_CONFIG['output']['retrain_results_file']\n",
    "    retrain_df.to_csv(retrain_path, index=False)\n",
    "    print(f\"Retrain results saved to: {retrain_path}\")\n",
    "else:\n",
    "    retrain_df = pd.DataFrame()\n",
    "    print(\"No results to retrain. Please run training first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrain with Selected Channels\n",
    "\n",
    "Now retrain the model using ONLY the selected channels and compare accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"OVERALL RESULTS SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nSubjects trained: {len(results_df)}\")\n",
    "\n",
    "if len(results_df) > 0:\n",
    "    print(f\"Mean accuracy (all channels): {results_df['all_channels_acc'].mean():.4f} ± {results_df['all_channels_acc'].std():.4f}\")\n",
    "    print(f\"Best subject: {results_df.loc[results_df['all_channels_acc'].idxmax(), 'subject']} ({results_df['all_channels_acc'].max():.4f})\")\n",
    "    print(f\"Worst subject: {results_df.loc[results_df['all_channels_acc'].idxmin(), 'subject']} ({results_df['all_channels_acc'].min():.4f})\")\n",
    "\n",
    "    # Save results\n",
    "    results_path = EXPERIMENT_CONFIG['output']['results_dir'] / 'subject_results.csv'\n",
    "    results_df[['subject', 'num_trials', 'num_channels', 'all_channels_acc', 'all_channels_std']].to_csv(results_path, index=False)\n",
    "    print(f\"\\nResults saved to: {results_path}\")\n",
    "\n",
    "    display(results_df[['subject', 'num_trials', 'num_channels', 'all_channels_acc', 'all_channels_std']].head(10))\n",
    "else:\n",
    "    print(\"\\nNo subjects were successfully trained. Check the data loading and preprocessing steps.\")\n",
    "    results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(results_df) > 0:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "    # Accuracy distribution\n",
    "    axes[0, 0].hist(results_df['all_channels_acc'], bins=20, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "    axes[0, 0].axvline(results_df['all_channels_acc'].mean(), color='red', linestyle='--', linewidth=2, label='Mean')\n",
    "    axes[0, 0].set_title('Accuracy Distribution (All Channels)', fontsize=14, fontweight='bold')\n",
    "    axes[0, 0].set_xlabel('Accuracy')\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Accuracy vs num trials\n",
    "    axes[0, 1].scatter(results_df['num_trials'], results_df['all_channels_acc'], alpha=0.6, s=100)\n",
    "    axes[0, 1].set_title('Accuracy vs Number of Trials', fontsize=14, fontweight='bold')\n",
    "    axes[0, 1].set_xlabel('Number of Trials')\n",
    "    axes[0, 1].set_ylabel('Accuracy')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    # Top 10 subjects\n",
    "    top_10 = results_df.nlargest(min(10, len(results_df)), 'all_channels_acc')\n",
    "    axes[1, 0].barh(range(len(top_10)), top_10['all_channels_acc'], color='green', alpha=0.7)\n",
    "    axes[1, 0].set_yticks(range(len(top_10)))\n",
    "    axes[1, 0].set_yticklabels(top_10['subject'])\n",
    "    axes[1, 0].set_title(f'Top {len(top_10)} Subjects by Accuracy', fontsize=14, fontweight='bold')\n",
    "    axes[1, 0].set_xlabel('Accuracy')\n",
    "    axes[1, 0].invert_yaxis()\n",
    "    axes[1, 0].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "    # Subject ranking\n",
    "    sorted_results = results_df.sort_values('all_channels_acc')\n",
    "    axes[1, 1].plot(range(len(sorted_results)), sorted_results['all_channels_acc'], marker='o', markersize=4, alpha=0.6)\n",
    "    axes[1, 1].set_title('Subject Ranking', fontsize=14, fontweight='bold')\n",
    "    axes[1, 1].set_xlabel('Rank')\n",
    "    axes[1, 1].set_ylabel('Accuracy')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    summary_path = EXPERIMENT_CONFIG['output']['results_dir'] / EXPERIMENT_CONFIG['output']['results_summary_figure']\n",
    "    plt.savefig(summary_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Visualizations saved to: {summary_path}\")\n",
    "else:\n",
    "    print('No results to visualize. Please ensure subjects were successfully trained.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Learned Adjacency Matrix (Example Subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(all_results) > 0:\n",
    "    # Pick best subject\n",
    "    best_idx = results_df['all_channels_acc'].idxmax()\n",
    "    best_result = all_results[best_idx]\n",
    "    \n",
    "    print(f\"Visualizing adjacency matrix for best subject: {best_result['subject']}\")\n",
    "    print(f\"Accuracy: {best_result['all_channels_acc']:.4f}\")\n",
    "    \n",
    "    selector = ChannelSelector(best_result['adjacency_matrix'], best_result['channel_names'])\n",
    "    \n",
    "    fig = selector.visualize_adjacency(\n",
    "        save_path=EXPERIMENT_CONFIG['output']['results_dir'] / f\"adjacency_{best_result['subject']}.png\"\n",
    "    )\n",
    "    plt.show()\n",
    "    \n",
    "    # Show top edges\n",
    "    print(\"\\nTop 10 Edges (Edge Selection):\")\n",
    "    selected_channels_es, _ = selector.edge_selection(10)\n",
    "    print(f\"Selected channels: {selected_channels_es}\")\n",
    "    \n",
    "    print(\"\\nTop 10 Channels (Aggregation Selection):\")\n",
    "    selected_channels_as, _ = selector.aggregation_selection(10)\n",
    "    print(f\"Selected channels: {selected_channels_as}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(results_df) > 0:\n",
    "    results_dir = EXPERIMENT_CONFIG['output']['results_dir']\n",
    "    subject_results_path = results_dir / EXPERIMENT_CONFIG['output']['subject_results_file']\n",
    "    results_df[['subject', 'num_trials', 'num_channels', 'all_channels_acc', 'all_channels_std']].to_csv(subject_results_path, index=False)\n",
    "\n",
    "    if len(channel_selection_df) > 0:\n",
    "        channel_selection_path = results_dir / EXPERIMENT_CONFIG['output']['channel_selection_results_file']\n",
    "        channel_selection_df.to_csv(channel_selection_path, index=False)\n",
    "    else:\n",
    "        channel_selection_path = None\n",
    "\n",
    "    config_path = results_dir / EXPERIMENT_CONFIG['output']['config_file']\n",
    "    with open(config_path, 'w') as f:\n",
    "        json.dump(EXPERIMENT_CONFIG, f, indent=2, default=str)\n",
    "\n",
    "    print('All results exported successfully!')\n",
    "    print(f'  - Subject results: {subject_results_path}')\n",
    "    if channel_selection_path:\n",
    "        print(f'  - Channel selection: {channel_selection_path}')\n",
    "    print(f'  - Config: {config_path}')\n",
    "else:\n",
    "    print('No results to export.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
