{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# BCI Competition IV 2a Motor Imagery - EEG-ARNN Models\n\n## Baseline EEG-ARNN vs Adaptive Gating EEG-ARNN\n\nThis notebook trains and evaluates:\n1. **Baseline EEG-ARNN** - Pure CNN-GCN architecture\n2. **Adaptive Gating EEG-ARNN** - Input-dependent channel gating\n\n## Configuration:\n- **35 epochs**, **0.001 LR**, **NO EARLY STOPPING**\n- **9 subjects (A01-A09)**, **3-fold CV**\n- **4 classes**: 769 (left hand), 770 (right hand), 771 (feet), 772 (tongue)\n- **22 EEG channels**, **250 Hz sampling rate**\n- **Channel Selection**: ES/AS/GS at k=[5,8,10,12,15]\n\n## Metrics:\n- Accuracy, Precision, Recall, F1-Score, AUC-ROC (multi-class)\n\n## Output:\n- `bci_2a_baseline_results.csv`\n- `bci_2a_adaptive_results.csv`\n- `bci_2a_baseline_retrain_results.csv`\n- `bci_2a_adaptive_retrain_results.csv`"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix\n",
    ")\n",
    "import gc\n",
    "\n",
    "import mne\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_context('notebook', font_scale=1.0)\n",
    "mne.set_log_level('WARNING')\n",
    "\n",
    "def set_seed(s=42):\n",
    "    random.seed(s)\n",
    "    np.random.seed(s)\n",
    "    torch.manual_seed(s)\n",
    "    torch.cuda.manual_seed_all(s)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os",
    "from pathlib import Path",
    "",
    "if os.path.exists('/kaggle/input'):",
    "    print(\"Running on Kaggle\")",
    "    kaggle_input = Path('/kaggle/input')",
    "",
    "    # Try the specific BCI_2a subdirectory first",
    "    DATA_DIR = kaggle_input / 'bci-2a' / 'BCI_2a'",
    "    if DATA_DIR.exists():",
    "        print(f\"Found dataset at: {DATA_DIR}\")",
    "    else:",
    "        # Fallback to searching",
    "        datasets = [d for d in kaggle_input.iterdir() if d.is_dir()]",
    "        print(f\"Available datasets: {[d.name for d in datasets]}\")",
    "",
    "        DATA_DIR = None",
    "        possible_names = ['bci-2a', 'bci-competition-iv-2a']",
    "        for ds_name in possible_names:",
    "            test_path = kaggle_input / ds_name / 'BCI_2a'",
    "            if test_path.exists():",
    "                DATA_DIR = test_path",
    "                print(f\"Found dataset at: {DATA_DIR}\")",
    "                break",
    "            # Try without subdirectory",
    "            test_path = kaggle_input / ds_name",
    "            if test_path.exists():",
    "                DATA_DIR = test_path",
    "                print(f\"Found dataset at: {DATA_DIR}\")",
    "                break",
    "",
    "        if DATA_DIR is None and datasets:",
    "            DATA_DIR = datasets[0]",
    "            print(f\"Using first available dataset: {DATA_DIR}\")",
    "else:",
    "    print(\"Running locally\")",
    "    DATA_DIR = Path('../data/BCI_2a')",
    "",
    "CONFIG = {",
    "    'data': {",
    "        'raw_data_dir': DATA_DIR,",
    "        'selected_classes': [769, 770, 771, 772],",
    "        'tmin': 0.5,",
    "        'tmax': 4.5,",
    "        'baseline': (-0.5, 0)",
    "    },",
    "    'preprocessing': {",
    "        'l_freq': 0.5,",
    "        'h_freq': 40.0,",
    "        'notch_freq': 50.0,",
    "        'target_sfreq': 250.0,",
    "        'apply_car': True",
    "    },",
    "    'model': {",
    "        'hidden_dim': 40,",
    "        'epochs': 35,",
    "        'learning_rate': 0.001,",
    "        'batch_size': 64,",
    "        'n_folds': 3,",
    "        'patience': 999",
    "    },",
    "    'gating': {",
    "        'l1_lambda': 1e-3,",
    "        'gate_init': 0.9",
    "    },",
    "    'channel_selection': {",
    "        'k_values': [5, 8, 10, 12, 15]",
    "    },",
    "    'output': {",
    "        'results_dir': Path('results'),",
    "    },",
    "    'max_subjects': 9,",
    "    'min_runs_per_subject': 2",
    "}",
    "",
    "CONFIG['output']['results_dir'].mkdir(exist_ok=True, parents=True)",
    "",
    "print(\"",
    "Configuration loaded!\")",
    "print(f\"Training: {CONFIG['max_subjects']} subjects, {CONFIG['model']['n_folds']}-fold CV, {CONFIG['model']['epochs']} epochs\")",
    "print(f\"Learning rate: {CONFIG['model']['learning_rate']}, No early stopping (patience={CONFIG['model']['patience']})\")",
    "print(f\"Channel selection k values: {CONFIG['channel_selection']['k_values']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Data Loading and Preprocessing Functions"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"No excluded subjects for BCI 2a dataset - all 9 subjects (A01-A09) are clean\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Data Loading and Preprocessing Functions"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_raw(raw, config):",
    "    cleaned_names = {name: name.rstrip('.') for name in raw.ch_names}",
    "    raw.rename_channels(cleaned_names)",
    "    raw.pick_types(eeg=True)",
    "    raw.set_montage('standard_1020', on_missing='ignore', match_case=False)",
    "    ",
    "    nyquist = raw.info['sfreq'] / 2.0",
    "    if config['preprocessing']['notch_freq'] < nyquist:",
    "        raw.notch_filter(freqs=config['preprocessing']['notch_freq'], verbose=False)",
    "    ",
    "    raw.filter(",
    "        l_freq=config['preprocessing']['l_freq'],",
    "        h_freq=config['preprocessing']['h_freq'],",
    "        method='fir',",
    "        fir_design='firwin',",
    "        verbose=False",
    "    )",
    "    ",
    "    if config['preprocessing']['apply_car']:",
    "        raw.set_eeg_reference('average', projection=False, verbose=False)",
    "    ",
    "    raw.resample(config['preprocessing']['target_sfreq'], npad='auto', verbose=False)",
    "    return raw",
    "",
    "",
    "def load_and_preprocess_gdf(gdf_path, config):",
    "    raw = mne.io.read_raw_gdf(gdf_path, preload=True, verbose='ERROR')",
    "    raw = preprocess_raw(raw, config)",
    "    ",
    "    events, event_ids = mne.events_from_annotations(raw, verbose='ERROR')",
    "    ",
    "    # Remove duplicate event timestamps",
    "    unique_times, unique_idx = np.unique(events[:, 0], return_index=True)",
    "    events = events[unique_idx]",
    "    ",
    "    if len(events) == 0:",
    "        return None, None, raw.ch_names",
    "    ",
    "    epochs = mne.Epochs(",
    "        raw,",
    "        events,",
    "        event_id=event_ids,",
    "        tmin=config['data']['tmin'],",
    "        tmax=config['data']['tmax'],",
    "        baseline=tuple(config['data']['baseline']),",
    "        preload=True,",
    "        verbose='ERROR'",
    "    )",
    "    ",
    "    return epochs.get_data(), epochs.events[:, 2], raw.ch_names",
    "",
    "",
    "def filter_classes(x, y, selected_classes):",
    "    mask = np.isin(y, selected_classes)",
    "    y, x = y[mask], x[mask]",
    "    label_map = {old: new for new, old in enumerate(sorted(selected_classes))}",
    "    y = np.array([label_map[int(label)] for label in y], dtype=np.int64)",
    "    return x, y",
    "",
    "",
    "def normalize(x):",
    "    mu = x.mean(axis=(0, 2), keepdims=True)",
    "    sd = x.std(axis=(0, 2), keepdims=True) + 1e-8",
    "    return (x - mu) / sd",
    "",
    "",
    "def load_subject_data(data_dir, subject_id, run_ids, config):",
    "    if not data_dir.exists():",
    "        return None, None, None",
    "    ",
    "    all_x, all_y = [], []",
    "    channel_names = None",
    "    ",
    "    for run_id in run_ids:",
    "        gdf_path = data_dir / f'{subject_id}{run_id}.gdf'",
    "        if not gdf_path.exists():",
    "            continue",
    "        ",
    "        try:",
    "            x, y, ch_names = load_and_preprocess_gdf(gdf_path, config)",
    "            if x is None or len(y) == 0:",
    "                continue",
    "            ",
    "            x, y = filter_classes(x, y, config['data']['selected_classes'])",
    "            if len(y) == 0:",
    "                continue",
    "            ",
    "            channel_names = channel_names or ch_names",
    "            all_x.append(x)",
    "            all_y.append(y)",
    "        except Exception as e:",
    "            print(f\"  Warning: Failed to load {gdf_path.name}: {e}\")",
    "            continue",
    "    ",
    "    if len(all_x) == 0:",
    "        return None, None, channel_names",
    "    ",
    "    return np.concatenate(all_x, 0), np.concatenate(all_y, 0), channel_names",
    "",
    "",
    "def get_available_subjects(data_dir, min_runs=2):",
    "    if not data_dir.exists():",
    "        raise ValueError(f\"Data directory not found: {data_dir}\")",
    "    ",
    "    subjects = []",
    "    ",
    "    for subject_id in ['A01', 'A02', 'A03', 'A04', 'A05', 'A06', 'A07', 'A08', 'A09']:",
    "        gdf_files = list(data_dir.glob(f'{subject_id}*.gdf'))",
    "        if len(gdf_files) >= min_runs:",
    "            subjects.append(subject_id)",
    "    ",
    "    return subjects",
    "",
    "",
    "print(\"",
    "Scanning for subjects...\")",
    "data_dir = CONFIG['data']['raw_data_dir']",
    "print(f\"Looking for data in: {data_dir}\")",
    "",
    "all_subjects = get_available_subjects(",
    "    data_dir, ",
    "    min_runs=CONFIG['min_runs_per_subject']",
    ")",
    "subjects = all_subjects[:CONFIG['max_subjects']]",
    "",
    "print(f\"Found {len(all_subjects)} subjects with >= {CONFIG['min_runs_per_subject']} runs\")",
    "print(f\"Will process {len(subjects)} subjects: {subjects}\")",
    "",
    "ALL_TASK_RUNS = ['T', 'E']",
    "print(f\"Using runs: {ALL_TASK_RUNS} (T=training, E=evaluation)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. PyTorch Dataset"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = torch.FloatTensor(x).unsqueeze(1)\n",
    "        self.y = torch.LongTensor(y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return self.x[i], self.y[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Comprehensive Metrics Functions"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "@torch.no_grad()\ndef calculate_comprehensive_metrics(model, dataloader, device):\n    model.eval()\n    all_preds, all_labels, all_probs = [], [], []\n\n    for X_batch, y_batch in dataloader:\n        X_batch = X_batch.to(device)\n        outputs = model(X_batch)\n        probs = torch.softmax(outputs, dim=1)\n        _, predicted = torch.max(outputs, 1)\n\n        all_preds.extend(predicted.cpu().numpy())\n        all_labels.extend(y_batch.numpy())\n        all_probs.extend(probs.cpu().numpy())\n\n    all_preds = np.array(all_preds)\n    all_labels = np.array(all_labels)\n    all_probs = np.array(all_probs)\n\n    metrics = {\n        'accuracy': accuracy_score(all_labels, all_preds),\n        'precision': precision_score(all_labels, all_preds, average='macro', zero_division=0),\n        'recall': recall_score(all_labels, all_preds, average='macro', zero_division=0),\n        'f1_score': f1_score(all_labels, all_preds, average='macro', zero_division=0),\n    }\n    \n    try:\n        metrics['auc_roc'] = roc_auc_score(all_labels, all_probs, multi_class='ovr', average='macro') if len(np.unique(all_labels)) > 1 else 0.0\n    except:\n        metrics['auc_roc'] = 0.0\n\n    return metrics\n\n\nprint(\"Comprehensive metrics functions defined!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvLayer(nn.Module):\n",
    "    def __init__(self, num_channels, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.num_channels = num_channels\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.A = nn.Parameter(torch.randn(num_channels, num_channels))\n",
    "        self.theta = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(hidden_dim)\n",
    "        self.act = nn.ELU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, H, C, T = x.shape\n",
    "        \n",
    "        A = torch.sigmoid(self.A)\n",
    "        A = 0.5 * (A + A.t())\n",
    "        I = torch.eye(C, device=A.device)\n",
    "        A_hat = A + I\n",
    "        D = torch.diag(torch.pow(A_hat.sum(1).clamp_min(1e-6), -0.5))\n",
    "        A_norm = D @ A_hat @ D\n",
    "        \n",
    "        x_batch = x.permute(0, 3, 2, 1).contiguous().view(B*T, C, H)\n",
    "        x_g = A_norm @ x_batch\n",
    "        x_g = self.theta(x_g)\n",
    "        x_g = x_g.view(B, T, C, H).permute(0, 3, 2, 1)\n",
    "        \n",
    "        x_out = self.bn(x_g)\n",
    "        x_out = self.act(x_out)\n",
    "        \n",
    "        return x_out\n",
    "    \n",
    "    def get_adjacency(self):\n",
    "        with torch.no_grad():\n",
    "            A = torch.sigmoid(self.A)\n",
    "            A = 0.5 * (A + A.t())\n",
    "            return A.cpu().numpy()\n",
    "\n",
    "\n",
    "class TemporalConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=16, pool=True):\n",
    "        super().__init__()\n",
    "        self.pool = pool\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, \n",
    "                            kernel_size=(1, kernel_size), \n",
    "                            padding=(0, kernel_size//2), bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.act = nn.ELU()\n",
    "        self.pool_layer = nn.AvgPool2d(kernel_size=(1, 2)) if pool else None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.act(self.bn(self.conv(x)))\n",
    "        return self.pool_layer(x) if self.pool else x\n",
    "\n",
    "\n",
    "class BaselineEEGARNN(nn.Module):\n",
    "    def __init__(self, C, T, K, H):\n",
    "        super().__init__()\n",
    "        self.t1 = TemporalConv(1, H, 16, False)\n",
    "        self.g1 = GraphConvLayer(C, H)\n",
    "        self.t2 = TemporalConv(H, H, 16, True)\n",
    "        self.g2 = GraphConvLayer(C, H)\n",
    "        self.t3 = TemporalConv(H, H, 16, True)\n",
    "        self.g3 = GraphConvLayer(C, H)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            ft = self._forward_features(torch.zeros(1, 1, C, T))\n",
    "            fs = ft.view(1, -1).size(1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(fs, 256)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(256, K)\n",
    "    \n",
    "    def _forward_features(self, x):\n",
    "        x = self.g1(self.t1(x))\n",
    "        x = self.g2(self.t2(x))\n",
    "        x = self.g3(self.t3(x))\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self._forward_features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        return self.fc2(x)\n",
    "    \n",
    "    def get_final_adjacency(self):\n",
    "        return self.g3.get_adjacency()\n",
    "\n",
    "\n",
    "class AdaptiveGatedEEGARNN(BaselineEEGARNN):\n",
    "    def __init__(self, C, T, K, H, gate_init=0.9):\n",
    "        super().__init__(C, T, K, H)\n",
    "        \n",
    "        self.gate_net = nn.Sequential(\n",
    "            nn.Linear(C * 2, C),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(C, C),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            self.gate_net[-2].bias.fill_(2.0)\n",
    "        \n",
    "        self.latest_gates = None\n",
    "    \n",
    "    def compute_gates(self, x):\n",
    "        B, _, C, T = x.shape\n",
    "        x_squeeze = x.squeeze(1)\n",
    "        ch_mean = x_squeeze.mean(dim=2)\n",
    "        ch_std = x_squeeze.std(dim=2)\n",
    "        stats = torch.cat([ch_mean, ch_std], dim=1)\n",
    "        gates = self.gate_net(stats)\n",
    "        return gates\n",
    "    \n",
    "    def forward(self, x):\n",
    "        gates = self.compute_gates(x)\n",
    "        self.latest_gates = gates.detach().cpu()\n",
    "        x = x * gates.view(-1, 1, gates.size(1), 1)\n",
    "        return super().forward(x)\n",
    "    \n",
    "    def get_gate_values(self):\n",
    "        if self.latest_gates is not None:\n",
    "            return self.latest_gates.mean(dim=0)\n",
    "        return None\n",
    "\n",
    "\n",
    "print(\"EEG-ARNN architectures defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer, device, l1_lambda=0.0):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    all_preds, all_labels = [], []\n",
    "    \n",
    "    for x, y in dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        \n",
    "        if l1_lambda > 0 and hasattr(model, 'get_gate_values'):\n",
    "            gate_values = model.get_gate_values()\n",
    "            if gate_values is not None:\n",
    "                loss = loss + l1_lambda * gate_values.abs().mean()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        all_preds += torch.argmax(logits, 1).cpu().tolist()\n",
    "        all_labels += y.cpu().tolist()\n",
    "    \n",
    "    return total_loss / max(1, len(dataloader)), accuracy_score(all_labels, all_preds)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_preds, all_labels = [], []\n",
    "    \n",
    "    for x, y in dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        all_preds += torch.argmax(logits, 1).cpu().tolist()\n",
    "        all_labels += y.cpu().tolist()\n",
    "    \n",
    "    return total_loss / max(1, len(dataloader)), accuracy_score(all_labels, all_preds)\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, device, epochs, lr, patience, l1_lambda=0.0, verbose=True):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=3, verbose=False\n",
    "    )\n",
    "    \n",
    "    best_acc = 0.0\n",
    "    best_state = None\n",
    "    no_improve = 0\n",
    "    \n",
    "    epoch_iterator = tqdm(range(epochs), desc='    Epochs', leave=False) if verbose else range(epochs)\n",
    "    \n",
    "    for epoch in epoch_iterator:\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device, l1_lambda)\n",
    "        val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        if verbose:\n",
    "            epoch_iterator.set_postfix({\n",
    "                'train_loss': f'{train_loss:.4f}',\n",
    "                'train_acc': f'{train_acc:.4f}',\n",
    "                'val_loss': f'{val_loss:.4f}',\n",
    "                'val_acc': f'{val_acc:.4f}',\n",
    "                'best': f'{best_acc:.4f}'\n",
    "            })\n",
    "        \n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            best_state = deepcopy(model.state_dict())\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "        \n",
    "        if no_improve >= patience:\n",
    "            if verbose:\n",
    "                print(f'      Early stopping at epoch {epoch+1}/{epochs}')\n",
    "            break\n",
    "    \n",
    "    if best_state is None:\n",
    "        best_state = deepcopy(model.state_dict())\n",
    "    \n",
    "    model.load_state_dict(best_state)\n",
    "    return best_state, best_acc\n",
    "\n",
    "\n",
    "print(\"Training functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "all_results = {'baseline': [], 'adaptive': []}\n\nprint(\"\\nStarting training for EEG-ARNN models...\\n\")\n\nfor subject_id in tqdm(subjects, desc='Training subjects'):\n    print(f\"\\nProcessing {subject_id}...\")\n    \n    X, Y, channel_names = load_subject_data(\n        data_dir,\n        subject_id,\n        ALL_TASK_RUNS,\n        CONFIG\n    )\n    \n    if X is None or len(Y) == 0:\n        print(f\"  Skipped: No data available\")\n        continue\n    \n    C, T = X.shape[1], X.shape[2]\n    K = len(set(CONFIG['data']['selected_classes']))\n    H = CONFIG['model']['hidden_dim']\n    \n    print(f\"  Data shape: {X.shape}\")\n    print(f\"  Label distribution: {np.bincount(Y)}\")\n    \n    for model_type in ['baseline', 'adaptive']:\n        print(f\"\\n  Training {model_type.upper()}...\")\n        \n        skf = StratifiedKFold(n_splits=CONFIG['model']['n_folds'], shuffle=True, random_state=42)\n        fold_results = []\n        adjacencies = []\n        gate_values_list = []\n        \n        for fold, (train_idx, val_idx) in enumerate(skf.split(X, Y)):\n            X_train, X_val = normalize(X[train_idx]), normalize(X[val_idx])\n            Y_train, Y_val = Y[train_idx], Y[val_idx]\n            \n            train_loader = DataLoader(\n                EEGDataset(X_train, Y_train),\n                batch_size=CONFIG['model']['batch_size'],\n                shuffle=True,\n                num_workers=0\n            )\n            val_loader = DataLoader(\n                EEGDataset(X_val, Y_val),\n                batch_size=CONFIG['model']['batch_size'],\n                shuffle=False,\n                num_workers=0\n            )\n            \n            if model_type == 'baseline':\n                model = BaselineEEGARNN(C, T, K, H).to(device)\n                l1_lambda = 0.0\n            else:\n                model = AdaptiveGatedEEGARNN(C, T, K, H, CONFIG['gating']['gate_init']).to(device)\n                l1_lambda = CONFIG['gating']['l1_lambda']\n            \n            best_state, best_acc = train_model(\n                model, train_loader, val_loader, device,\n                CONFIG['model']['epochs'],\n                CONFIG['model']['learning_rate'],\n                CONFIG['model']['patience'],\n                l1_lambda\n            )\n            model.load_state_dict(best_state)\n            \n            metrics = calculate_comprehensive_metrics(model, val_loader, device)\n            fold_results.append({'fold': fold, **metrics})\n            \n            adjacency = model.get_final_adjacency()\n            adjacencies.append(adjacency)\n            \n            if hasattr(model, 'get_gate_values'):\n                gate_values = model.get_gate_values()\n                if gate_values is not None:\n                    if isinstance(gate_values, torch.Tensor):\n                        gate_values = gate_values.detach().cpu().numpy()\n                    gate_values_list.append(gate_values)\n            \n            del model\n            torch.cuda.empty_cache()\n            gc.collect()\n        \n        avg_metrics = {}\n        for key in ['accuracy', 'precision', 'recall', 'f1_score', 'auc_roc']:\n            values = [f[key] for f in fold_results]\n            avg_metrics[f'avg_{key}'] = float(np.mean(values))\n            avg_metrics[f'std_{key}'] = float(np.std(values))\n        \n        avg_adjacency = np.mean(np.stack(adjacencies, 0), 0)\n        \n        result = {\n            'subject': subject_id,\n            'num_trials': X.shape[0],\n            'num_channels': C,\n            **avg_metrics,\n            'adjacency_matrix': avg_adjacency,\n            'channel_names': channel_names\n        }\n        \n        if gate_values_list:\n            result['avg_gate_values'] = np.mean(np.stack(gate_values_list, 0), 0)\n        \n        all_results[model_type].append(result)\n        \n        print(f\"    Accuracy: {avg_metrics['avg_accuracy']:.4f} \u00c2\u00b1 {avg_metrics['std_accuracy']:.4f}\")\n        print(f\"    F1-Score: {avg_metrics['avg_f1_score']:.4f} \u00c2\u00b1 {avg_metrics['std_f1_score']:.4f}\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"Training Complete!\")\nprint(\"=\"*80)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Channel Selection Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class ChannelSelector:\n    def __init__(self, adjacency, channel_names, gate_values=None):\n        self.A = adjacency\n        self.names = np.array(channel_names)\n        self.C = adjacency.shape[0]\n        self.gate_values = gate_values\n\n    def edge_selection(self, k):\n        edge_importance = np.zeros(self.C)\n        for i in range(self.C):\n            for j in range(self.C):\n                if i != j:\n                    edge_importance[i] += abs(self.A[i, j])\n                    edge_importance[j] += abs(self.A[i, j])\n\n        indices = np.sort(np.argsort(edge_importance)[-int(k):])\n        return self.names[indices].tolist(), indices\n\n    def aggregation_selection(self, k):\n        agg_scores = np.sum(np.abs(self.A), 1)\n        indices = np.sort(np.argsort(agg_scores)[-int(k):])\n        return self.names[indices].tolist(), indices\n\n    def gate_selection(self, k):\n        if self.gate_values is None:\n            raise ValueError(\"Gate values not available.\")\n\n        indices = np.sort(np.argsort(self.gate_values)[-int(k):])\n        return self.names[indices].tolist(), indices\n\n\ndef retrain_with_selected_channels(x, y, selected_indices, T, K, device, config, model_type='baseline'):\n    x_selected = x[:, selected_indices, :]\n    C = len(selected_indices)\n    H = config['model']['hidden_dim']\n\n    skf = StratifiedKFold(n_splits=config['model']['n_folds'], shuffle=True, random_state=42)\n    fold_results = []\n\n    for fold, (train_idx, val_idx) in enumerate(skf.split(x_selected, y)):\n        X_train = normalize(x_selected[train_idx])\n        X_val = normalize(x_selected[val_idx])\n        Y_train, Y_val = y[train_idx], y[val_idx]\n\n        train_loader = DataLoader(\n            EEGDataset(X_train, Y_train),\n            batch_size=config['model']['batch_size'],\n            shuffle=True,\n            num_workers=0\n        )\n        val_loader = DataLoader(\n            EEGDataset(X_val, Y_val),\n            batch_size=config['model']['batch_size'],\n            shuffle=False,\n            num_workers=0\n        )\n\n        if model_type == 'baseline':\n            model = BaselineEEGARNN(C, T, K, H).to(device)\n            l1_lambda = 0.0\n        else:\n            model = AdaptiveGatedEEGARNN(C, T, K, H, config['gating']['gate_init']).to(device)\n            l1_lambda = config['gating']['l1_lambda']\n\n        best_state, best_acc = train_model(\n            model, train_loader, val_loader, device,\n            config['model']['epochs'],\n            config['model']['learning_rate'],\n            config['model']['patience'],\n            l1_lambda\n        )\n        model.load_state_dict(best_state)\n\n        metrics = calculate_comprehensive_metrics(model, val_loader, device)\n        fold_results.append(metrics)\n        \n        del model\n        torch.cuda.empty_cache()\n        gc.collect()\n\n    avg_metrics = {}\n    for key in ['accuracy', 'precision', 'recall', 'f1_score', 'auc_roc']:\n        values = [f[key] for f in fold_results]\n        avg_metrics[f'avg_{key}'] = float(np.mean(values))\n        avg_metrics[f'std_{key}'] = float(np.std(values))\n    \n    return avg_metrics\n\n\nprint(\"Channel selection functions defined!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Channel Selection and Retraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrain_results = {'baseline': [], 'adaptive': []}\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CHANNEL SELECTION AND RETRAINING\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "for subject_id in tqdm(subjects, desc='Retraining'):\n",
    "    print(f\"\\nProcessing {subject_id}...\")\n",
    "\n",
    "    X, Y, channel_names = load_subject_data(\n",
    "        data_dir,\n",
    "        subject_id,\n",
    "        ALL_TASK_RUNS,\n",
    "        CONFIG\n",
    "    )\n",
    "\n",
    "    if X is None:\n",
    "        continue\n",
    "\n",
    "    C, T = X.shape[1], X.shape[2]\n",
    "    K = len(set(CONFIG['data']['selected_classes']))\n",
    "\n",
    "    for model_type in ['baseline', 'adaptive']:\n",
    "        subj_result = None\n",
    "        for res in all_results[model_type]:\n",
    "            if res['subject'] == subject_id:\n",
    "                subj_result = res\n",
    "                break\n",
    "\n",
    "        if subj_result is None:\n",
    "            continue\n",
    "\n",
    "        adjacency = subj_result['adjacency_matrix']\n",
    "        gate_values = subj_result.get('avg_gate_values', None)\n",
    "        selector = ChannelSelector(adjacency, channel_names, gate_values)\n",
    "\n",
    "        selection_methods = ['ES', 'AS']\n",
    "        if model_type == 'adaptive':\n",
    "            selection_methods.append('GS')\n",
    "\n",
    "        for method_name in selection_methods:\n",
    "            for k in CONFIG['channel_selection']['k_values']:\n",
    "                if method_name == 'ES':\n",
    "                    selected_channels, selected_indices = selector.edge_selection(k)\n",
    "                elif method_name == 'AS':\n",
    "                    selected_channels, selected_indices = selector.aggregation_selection(k)\n",
    "                elif method_name == 'GS':\n",
    "                    selected_channels, selected_indices = selector.gate_selection(k)\n",
    "\n",
    "                retrain_metrics = retrain_with_selected_channels(\n",
    "                    X, Y, selected_indices, T, K, device, CONFIG, model_type\n",
    "                )\n",
    "\n",
    "                acc_drop = subj_result['avg_accuracy'] - retrain_metrics['avg_accuracy']\n",
    "\n",
    "                retrain_results[model_type].append({\n",
    "                    'subject': subject_id,\n",
    "                    'method': method_name,\n",
    "                    'k': k,\n",
    "                    'num_channels_selected': len(selected_channels),\n",
    "                    **retrain_metrics,\n",
    "                    'full_channels_acc': subj_result['avg_accuracy'],\n",
    "                    'accuracy_drop': acc_drop,\n",
    "                    'accuracy_drop_pct': (acc_drop / subj_result['avg_accuracy'] * 100)\n",
    "                })\n",
    "\n",
    "                print(f\"  {model_type.upper()} - {method_name}, k={k}: \"\n",
    "                      f\"{retrain_metrics['avg_accuracy']:.4f} (drop: {acc_drop:.4f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Retraining Complete!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "results_dir = CONFIG['output']['results_dir']\n\nfor model_type in ['baseline', 'adaptive']:\n    if len(all_results[model_type]) > 0:\n        df = pd.DataFrame([{\n            'subject': r['subject'],\n            'num_trials': r['num_trials'],\n            'num_channels': r['num_channels'],\n            'accuracy': r['avg_accuracy'],\n            'std_accuracy': r['std_accuracy'],\n            'precision': r['avg_precision'],\n            'std_precision': r['std_precision'],\n            'recall': r['avg_recall'],\n            'std_recall': r['std_recall'],\n            'f1_score': r['avg_f1_score'],\n            'std_f1_score': r['std_f1_score'],\n            'auc_roc': r['avg_auc_roc'],\n            'std_auc_roc': r['std_auc_roc']\n        } for r in all_results[model_type]])\n        \n        df.to_csv(results_dir / f'bci_2a_{model_type}_results.csv', index=False)\n        print(f\"Saved: bci_2a_{model_type}_results.csv\")\n\nfor model_type in ['baseline', 'adaptive']:\n    if len(retrain_results[model_type]) > 0:\n        df = pd.DataFrame(retrain_results[model_type])\n        df.to_csv(results_dir / f'bci_2a_{model_type}_retrain_results.csv', index=False)\n        print(f\"Saved: bci_2a_{model_type}_retrain_results.csv\")\n\nprint(f\"\\nAll results saved to {results_dir}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESULTS SUMMARY\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "for model_type in ['baseline', 'adaptive']:\n",
    "    if len(all_results[model_type]) > 0:\n",
    "        accs = [r['avg_accuracy'] for r in all_results[model_type]]\n",
    "        f1s = [r['avg_f1_score'] for r in all_results[model_type]]\n",
    "        aucs = [r['avg_auc_roc'] for r in all_results[model_type]]\n",
    "        \n",
    "        print(f\"{model_type.upper()} Results:\")\n",
    "        print(f\"  Subjects: {len(all_results[model_type])}\")\n",
    "        print(f\"  Mean accuracy: {np.mean(accs):.4f} \u00c2\u00b1 {np.std(accs):.4f}\")\n",
    "        print(f\"  Mean F1-Score: {np.mean(f1s):.4f} \u00c2\u00b1 {np.std(f1s):.4f}\")\n",
    "        print(f\"  Mean AUC-ROC: {np.mean(aucs):.4f} \u00c2\u00b1 {np.std(aucs):.4f}\")\n",
    "        print()\n",
    "\n",
    "if len(all_results['baseline']) > 0 and len(all_results['adaptive']) > 0:\n",
    "    baseline_acc = np.mean([r['avg_accuracy'] for r in all_results['baseline']])\n",
    "    adaptive_acc = np.mean([r['avg_accuracy'] for r in all_results['adaptive']])\n",
    "    improvement = adaptive_acc - baseline_acc\n",
    "    \n",
    "    print(f\"\\nAdaptive vs Baseline:\")\n",
    "    print(f\"  Improvement: {improvement:.4f} ({improvement/baseline_acc*100:.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DONE!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}