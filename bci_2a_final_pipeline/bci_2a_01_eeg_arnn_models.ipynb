{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# BCI Competition IV 2a Motor Imagery - EEG-ARNN Models (FIXED)\n\n## Baseline EEG-ARNN vs Adaptive Gating EEG-ARNN\n\nThis notebook trains and evaluates:\n1. **Baseline EEG-ARNN** - Pure CNN-GCN architecture\n2. **Adaptive Gating EEG-ARNN** - Input-dependent channel gating\n\n## UPDATES (Latest Version):\n- **FIXED**: Using 2-class (left hand vs right hand) - standard BCI benchmark\n- **FIXED**: Simplified GDF loading based on working gen2/gen3 implementations\n- **FIXED**: Correct parameters (250Hz, 0.5-2.5s window, 4-38Hz, CAR, 288 trials)\n- **IMPROVED**: 5-fold cross-validation (increased from 3-fold)\n- **NEW**: Per-epoch validation results for BOTH training and retraining\n- **IMPROVED**: Better error handling and logging\n\n## Configuration:\n- **30 epochs**, **0.001 LR**, **NO EARLY STOPPING**\n- **9 subjects (A01-A09)**, **5-fold CV**\n- **2 classes**: 769 (left hand), 770 (right hand) - BINARY CLASSIFICATION\n- **22 EEG channels**, **250 Hz sampling rate**, **4-38 Hz bandpass**\n- **Channel Selection**: ES/AS/GS at k=[5,8,10,12,15]\n- **Expected Accuracy**: ~90% (binary classification is easier than 4-class)\n\n## Metrics:\n- Accuracy, Precision, Recall, F1-Score, AUC-ROC (binary)\n- Per-epoch training and validation metrics (train + retrain)\n\n## Output Files (4 types):\n\n### 1. Training Results (Full Channels):\n- `results/bci_2a_baseline_results.csv` - Per-subject metrics (baseline)\n- `results/bci_2a_adaptive_results.csv` - Per-subject metrics (adaptive)\n\n### 2. Training Per-Epoch Results:\n- `results/epoch_results/all_baseline_epoch_results.csv` - All epochs (baseline)\n- `results/epoch_results/all_adaptive_epoch_results.csv` - All epochs (adaptive)\n- `results/epoch_results/{subject}_{model}_fold{n}_epochs.csv` - Per subject/fold\n\n### 3. Retraining Results (Selected Channels):\n- `results/bci_2a_baseline_retrain_results.csv` - Per method/k results (baseline)\n- `results/bci_2a_adaptive_retrain_results.csv` - Per method/k results (adaptive)\n  - Includes: method, k, selected channels, accuracy, accuracy drop\n\n### 4. Retraining Per-Epoch Results:\n- `results/epoch_results/all_baseline_retrain_epoch_results.csv` - All retrain epochs\n- `results/epoch_results/all_adaptive_retrain_epoch_results.csv` - All retrain epochs\n- `results/epoch_results/{subject}_{model}_retrain_{method}_k{n}_epochs.csv` - Per config"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix\n",
    ")\n",
    "import gc\n",
    "\n",
    "import mne\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_context('notebook', font_scale=1.0)\n",
    "mne.set_log_level('WARNING')\n",
    "\n",
    "def set_seed(s=42):\n",
    "    random.seed(s)\n",
    "    np.random.seed(s)\n",
    "    torch.manual_seed(s)\n",
    "    torch.cuda.manual_seed_all(s)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nfrom pathlib import Path\n\nif os.path.exists('/kaggle/input'):\n    print(\"Running on Kaggle\")\n    kaggle_input = Path('/kaggle/input')\n    DATA_DIR = kaggle_input / 'bci-2a' / 'BCI_2a'\n    if DATA_DIR.exists():\n        print(f\"Found dataset at: {DATA_DIR}\")\n    else:\n        datasets = [d for d in kaggle_input.iterdir() if d.is_dir()]\n        print(f\"Available datasets: {[d.name for d in datasets]}\")\n        DATA_DIR = None\n        possible_names = ['bci-2a', 'bci-competition-iv-2a']\n        for ds_name in possible_names:\n            test_path = kaggle_input / ds_name / 'BCI_2a'\n            if test_path.exists():\n                DATA_DIR = test_path\n                print(f\"Found dataset at: {DATA_DIR}\")\n                break\n            test_path = kaggle_input / ds_name\n            if test_path.exists():\n                DATA_DIR = test_path\n                print(f\"Found dataset at: {DATA_DIR}\")\n                break\n        if DATA_DIR is None and datasets:\n            DATA_DIR = datasets[0]\n            print(f\"Using first available dataset: {DATA_DIR}\")\nelse:\n    print(\"Running locally\")\n    DATA_DIR = Path('../data/BCI_2a')\n\nCONFIG = {\n    'data': {\n        'raw_data_dir': DATA_DIR,\n        'selected_classes': [769, 770],\n        'tmin': -0.5,\n        'tmax': 4.0,\n        'baseline': (-0.5, 0)\n    },\n    'preprocessing': {\n        'l_freq': 0.5,\n        'h_freq': 40.0,\n        'notch_freq': 50.0,\n        'target_sfreq': 128.0,\n        'apply_car': True\n    },\n    'model': {\n        'hidden_dim': 16,\n        'epochs': 35,\n        'learning_rate': 0.002,\n        'batch_size': 20,\n        'n_folds': 5,\n        'patience': 999,\n        'adj_lr': 0.001\n    },\n    'gating': {\n        'l1_lambda': 1e-3,\n        'gate_init': 0.9\n    },\n    'channel_selection': {\n        'k_values': [5, 8, 10, 12, 15]\n    },\n    'output': {\n        'results_dir': Path('results'),\n        'epoch_results_dir': Path('results/epoch_results')\n    },\n    'max_subjects': 9,\n    'min_runs_per_subject': 1\n}\n\nCONFIG['output']['results_dir'].mkdir(exist_ok=True, parents=True)\nCONFIG['output']['epoch_results_dir'].mkdir(exist_ok=True, parents=True)\nprint(\"=\"*80)\nprint(\"BCI COMPETITION IV 2a - BINARY CLASSIFICATION (OPTIMIZED SETTINGS)\")\nprint(\"=\"*80)\nprint(\"CRITICAL FIXES APPLIED:\")\nprint(\"  1. MIN-MAX NORMALIZATION (was z-score)\")\nprint(\"  2. CUSTOM ADJACENCY LEARNING (paper's method)\")\nprint(\"  3. BATCH SIZE 20 (was 64)\")\nprint(\"  4. HIDDEN DIM 16 (was 40)\")\nprint(\"=\"*80)\nprint(\"USER PREFERENCES:\")\nprint(\"  - 35 epochs (faster training)\")\nprint(\"  - 5-fold CV (balanced)\")\nprint(\"  - LR 0.002 (slightly higher)\")\nprint(\"=\"*80)\nprint(f\"Training: {CONFIG['max_subjects']} subjects, {CONFIG['model']['n_folds']}-fold CV, {CONFIG['model']['epochs']} epochs\")\nprint(f\"Learning rate: {CONFIG['model']['learning_rate']}, Adjacency LR: {CONFIG['model']['adj_lr']}\")\nprint(f\"CLASSES: 2-class (769=left hand, 770=right hand)\")\nprint(f\"EPOCH WINDOW: {CONFIG['data']['tmin']}-{CONFIG['data']['tmax']}s (relative to cue at t=0)\")\nprint(f\"Frequency band: {CONFIG['preprocessing']['l_freq']}-{CONFIG['preprocessing']['h_freq']} Hz\")\nprint(f\"Sampling rate: {CONFIG['preprocessing']['target_sfreq']} Hz (downsampled from 250Hz)\")\nprint(f\"CAR: {CONFIG['preprocessing']['apply_car']}\")\nprint(f\"Normalization: MIN-MAX (matching paper)\")\nprint(f\"Channel selection k values: {CONFIG['channel_selection']['k_values']}\")\nprint(f\"Expected accuracy: ~75-85% (with 35 epochs - higher LR helps)\")\nprint(\"=\"*80)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Data Loading and Preprocessing Functions"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"No excluded subjects for BCI 2a dataset - all 9 subjects (A01-A09) are clean\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Data Loading and Preprocessing Functions"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def load_and_preprocess_gdf(gdf_path, config):\n    \"\"\"Simplified GDF loading matching PhysioNet working implementation\"\"\"\n    raw = mne.io.read_raw_gdf(gdf_path, preload=True, verbose=False)\n    \n    eog_channels = [ch for ch in raw.ch_names if 'EOG' in ch.upper()]\n    if eog_channels:\n        raw.drop_channels(eog_channels)\n    \n    if config['preprocessing']['notch_freq'] is not None:\n        raw.notch_filter(config['preprocessing']['notch_freq'], verbose=False)\n    \n    if config['preprocessing']['l_freq'] is not None and config['preprocessing']['h_freq'] is not None:\n        raw.filter(config['preprocessing']['l_freq'], config['preprocessing']['h_freq'], \n                  fir_design='firwin', verbose=False)\n    \n    if config['preprocessing']['apply_car']:\n        raw.set_eeg_reference('average', projection=False, verbose=False)\n    \n    if config['preprocessing']['target_sfreq'] is not None and config['preprocessing']['target_sfreq'] != raw.info['sfreq']:\n        raw.resample(config['preprocessing']['target_sfreq'], npad='auto', verbose=False)\n    \n    events, event_id = mne.events_from_annotations(raw, verbose=False)\n    \n    mi_event_ids = {}\n    for orig_code in config['data']['selected_classes']:\n        code_str = str(orig_code)\n        if code_str in event_id:\n            mi_event_ids[code_str] = event_id[code_str]\n    \n    if len(mi_event_ids) == 0:\n        return None, None, raw.ch_names\n    \n    epochs = mne.Epochs(\n        raw, events, event_id=mi_event_ids,\n        tmin=config['data']['tmin'], tmax=config['data']['tmax'],\n        baseline=tuple(config['data']['baseline']), preload=True, verbose=False\n    )\n    \n    if len(epochs) == 0:\n        return None, None, raw.ch_names\n    \n    data = epochs.get_data()\n    labels = epochs.events[:, -1]\n    \n    label_mapping = {v: i for i, (k, v) in enumerate(sorted(mi_event_ids.items(), key=lambda x: x[1]))}\n    labels = np.array([label_mapping[l] for l in labels])\n    \n    return data, labels, raw.ch_names\n\ndef load_subject_data(data_dir, subject_id, run_ids, config):\n    \"\"\"Load all runs for a subject\"\"\"\n    if not data_dir.exists():\n        print(f\"  Data directory does not exist: {data_dir}\")\n        return None, None, None\n    \n    all_x, all_y = [], []\n    channel_names = None\n    \n    for run_id in run_ids:\n        gdf_path = data_dir / f'{subject_id}{run_id}.gdf'\n        if not gdf_path.exists():\n            continue\n        \n        try:\n            x, y, ch_names = load_and_preprocess_gdf(gdf_path, config)\n            if x is None or y is None or len(y) == 0:\n                continue\n            \n            channel_names = channel_names or ch_names\n            all_x.append(x)\n            all_y.append(y)\n            print(f\"  Loaded {gdf_path.name}: {len(y)} epochs, shape {x.shape}\")\n        except Exception as e:\n            print(f\"  Error loading {gdf_path.name}: {e}\")\n            continue\n    \n    if len(all_x) == 0:\n        return None, None, channel_names\n    \n    return np.concatenate(all_x, 0), np.concatenate(all_y, 0), channel_names\n\ndef normalize(x):\n    \"\"\"Min-max normalization matching the paper implementation - CRITICAL FIX\"\"\"\n    x_normalized = np.zeros_like(x)\n    for i in range(x.shape[0]):\n        x_min = np.min(x[i])\n        x_max = np.max(x[i])\n        x_normalized[i] = (x[i] - x_min) / (x_max - x_min + 1e-8)\n    return x_normalized\n\ndef get_available_subjects(data_dir, min_runs=1):\n    \"\"\"Get list of available subjects\"\"\"\n    if not data_dir.exists():\n        raise ValueError(f\"Data directory not found: {data_dir}\")\n    \n    subjects = []\n    for subject_id in ['A01', 'A02', 'A03', 'A04', 'A05', 'A06', 'A07', 'A08', 'A09']:\n        gdf_files = list(data_dir.glob(f'{subject_id}*.gdf'))\n        if len(gdf_files) >= min_runs:\n            subjects.append(subject_id)\n    \n    return subjects\n\nprint(\"Scanning for subjects...\")\ndata_dir = CONFIG['data']['raw_data_dir']\nprint(f\"Looking for data in: {data_dir}\")\nall_subjects = get_available_subjects(data_dir, min_runs=CONFIG['min_runs_per_subject'])\nsubjects = all_subjects[:CONFIG['max_subjects']]\nprint(f\"Found {len(all_subjects)} subjects with >= {CONFIG['min_runs_per_subject']} runs\")\nprint(f\"Will process {len(subjects)} subjects: {subjects}\")\nALL_TASK_RUNS = ['T']\nprint(f\"Using runs: {ALL_TASK_RUNS} (T=training session only)\")\nprint(f\"Note: E files don't have labels (evaluation/competition format)\")\nprint(f\"Expected: ~144 trials per subject (72 left + 72 right per session)\")\nprint(\"Data loading functions defined!\")\nprint(\"NORMALIZATION FIXED: Using min-max normalization from paper!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. PyTorch Dataset"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = torch.FloatTensor(x).unsqueeze(1)\n",
    "        self.y = torch.LongTensor(y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return self.x[i], self.y[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Comprehensive Metrics Functions"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "@torch.no_grad()\ndef calculate_comprehensive_metrics(model, dataloader, device):\n    model.eval()\n    all_preds, all_labels, all_probs = [], [], []\n\n    for X_batch, y_batch in dataloader:\n        X_batch = X_batch.to(device)\n        outputs = model(X_batch)\n        probs = torch.softmax(outputs, dim=1)\n        _, predicted = torch.max(outputs, 1)\n\n        all_preds.extend(predicted.cpu().numpy())\n        all_labels.extend(y_batch.numpy())\n        all_probs.extend(probs.cpu().numpy())\n\n    all_preds = np.array(all_preds)\n    all_labels = np.array(all_labels)\n    all_probs = np.array(all_probs)\n\n    metrics = {\n        'accuracy': accuracy_score(all_labels, all_preds),\n        'precision': precision_score(all_labels, all_preds, average='macro', zero_division=0),\n        'recall': recall_score(all_labels, all_preds, average='macro', zero_division=0),\n        'f1_score': f1_score(all_labels, all_preds, average='macro', zero_division=0),\n    }\n    \n    try:\n        metrics['auc_roc'] = roc_auc_score(all_labels, all_probs, multi_class='ovr', average='macro') if len(np.unique(all_labels)) > 1 else 0.0\n    except:\n        metrics['auc_roc'] = 0.0\n\n    return metrics\n\n\nprint(\"Comprehensive metrics functions defined!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvLayer(nn.Module):\n",
    "    def __init__(self, num_channels, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.num_channels = num_channels\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.A = nn.Parameter(torch.randn(num_channels, num_channels))\n",
    "        self.theta = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(hidden_dim)\n",
    "        self.act = nn.ELU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, H, C, T = x.shape\n",
    "        \n",
    "        A = torch.sigmoid(self.A)\n",
    "        A = 0.5 * (A + A.t())\n",
    "        I = torch.eye(C, device=A.device)\n",
    "        A_hat = A + I\n",
    "        D = torch.diag(torch.pow(A_hat.sum(1).clamp_min(1e-6), -0.5))\n",
    "        A_norm = D @ A_hat @ D\n",
    "        \n",
    "        x_batch = x.permute(0, 3, 2, 1).contiguous().view(B*T, C, H)\n",
    "        x_g = A_norm @ x_batch\n",
    "        x_g = self.theta(x_g)\n",
    "        x_g = x_g.view(B, T, C, H).permute(0, 3, 2, 1)\n",
    "        \n",
    "        x_out = self.bn(x_g)\n",
    "        x_out = self.act(x_out)\n",
    "        \n",
    "        return x_out\n",
    "    \n",
    "    def get_adjacency(self):\n",
    "        with torch.no_grad():\n",
    "            A = torch.sigmoid(self.A)\n",
    "            A = 0.5 * (A + A.t())\n",
    "            return A.cpu().numpy()\n",
    "\n",
    "\n",
    "class TemporalConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=16, pool=True):\n",
    "        super().__init__()\n",
    "        self.pool = pool\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, \n",
    "                            kernel_size=(1, kernel_size), \n",
    "                            padding=(0, kernel_size//2), bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.act = nn.ELU()\n",
    "        self.pool_layer = nn.AvgPool2d(kernel_size=(1, 2)) if pool else None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.act(self.bn(self.conv(x)))\n",
    "        return self.pool_layer(x) if self.pool else x\n",
    "\n",
    "\n",
    "class BaselineEEGARNN(nn.Module):\n",
    "    def __init__(self, C, T, K, H):\n",
    "        super().__init__()\n",
    "        self.t1 = TemporalConv(1, H, 16, False)\n",
    "        self.g1 = GraphConvLayer(C, H)\n",
    "        self.t2 = TemporalConv(H, H, 16, True)\n",
    "        self.g2 = GraphConvLayer(C, H)\n",
    "        self.t3 = TemporalConv(H, H, 16, True)\n",
    "        self.g3 = GraphConvLayer(C, H)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            ft = self._forward_features(torch.zeros(1, 1, C, T))\n",
    "            fs = ft.view(1, -1).size(1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(fs, 256)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(256, K)\n",
    "    \n",
    "    def _forward_features(self, x):\n",
    "        x = self.g1(self.t1(x))\n",
    "        x = self.g2(self.t2(x))\n",
    "        x = self.g3(self.t3(x))\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self._forward_features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        return self.fc2(x)\n",
    "    \n",
    "    def get_final_adjacency(self):\n",
    "        return self.g3.get_adjacency()\n",
    "\n",
    "\n",
    "class AdaptiveGatedEEGARNN(BaselineEEGARNN):\n",
    "    def __init__(self, C, T, K, H, gate_init=0.9):\n",
    "        super().__init__(C, T, K, H)\n",
    "        \n",
    "        self.gate_net = nn.Sequential(\n",
    "            nn.Linear(C * 2, C),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(C, C),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            self.gate_net[-2].bias.fill_(2.0)\n",
    "        \n",
    "        self.latest_gates = None\n",
    "    \n",
    "    def compute_gates(self, x):\n",
    "        B, _, C, T = x.shape\n",
    "        x_squeeze = x.squeeze(1)\n",
    "        ch_mean = x_squeeze.mean(dim=2)\n",
    "        ch_std = x_squeeze.std(dim=2)\n",
    "        stats = torch.cat([ch_mean, ch_std], dim=1)\n",
    "        gates = self.gate_net(stats)\n",
    "        return gates\n",
    "    \n",
    "    def forward(self, x):\n",
    "        gates = self.compute_gates(x)\n",
    "        self.latest_gates = gates.detach().cpu()\n",
    "        x = x * gates.view(-1, 1, gates.size(1), 1)\n",
    "        return super().forward(x)\n",
    "    \n",
    "    def get_gate_values(self):\n",
    "        if self.latest_gates is not None:\n",
    "            return self.latest_gates.mean(dim=0)\n",
    "        return None\n",
    "\n",
    "\n",
    "print(\"EEG-ARNN architectures defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def train_epoch_with_adj_learning(model, dataloader, criterion, optimizer, device, adj_lr=0.001, l1_lambda=0.0):\n    \"\"\"Training epoch with custom adjacency matrix learning - PAPER IMPLEMENTATION\"\"\"\n    model.train()\n    total_loss = 0.0\n    all_preds, all_labels = [], []\n    \n    for x, y in dataloader:\n        x, y = x.to(device), y.to(device)\n        optimizer.zero_grad()\n        logits = model(x)\n        loss = criterion(logits, y)\n        \n        if l1_lambda > 0 and hasattr(model, 'get_gate_values'):\n            gate_values = model.get_gate_values()\n            if gate_values is not None:\n                loss = loss + l1_lambda * gate_values.abs().mean()\n\n        loss.backward()\n        \n        if hasattr(model, 'g1') and hasattr(model.g1, 'A'):\n            with torch.no_grad():\n                for gcn_layer in [model.g1, model.g2, model.g3]:\n                    if gcn_layer.A.grad is not None:\n                        W_grad = gcn_layer.A.grad\n                        gcn_layer.A.data = (1 - adj_lr) * gcn_layer.A.data - adj_lr * W_grad\n                        gcn_layer.A.grad.zero_()\n        \n        optimizer.step()\n        \n        total_loss += loss.item()\n        all_preds += torch.argmax(logits, 1).cpu().tolist()\n        all_labels += y.cpu().tolist()\n    \n    return total_loss / max(1, len(dataloader)), accuracy_score(all_labels, all_preds)\n\n\n@torch.no_grad()\ndef evaluate(model, dataloader, criterion, device):\n    model.eval()\n    total_loss = 0.0\n    all_preds, all_labels = [], []\n    \n    for x, y in dataloader:\n        x, y = x.to(device), y.to(device)\n        logits = model(x)\n        loss = criterion(logits, y)\n        \n        total_loss += loss.item()\n        all_preds += torch.argmax(logits, 1).cpu().tolist()\n        all_labels += y.cpu().tolist()\n    \n    return total_loss / max(1, len(dataloader)), accuracy_score(all_labels, all_preds)\n\n\ndef train_model(model, train_loader, val_loader, device, epochs, lr, patience, adj_lr=0.001, l1_lambda=0.0, verbose=True, \n                subject_id=None, fold=None, model_type=None, config=None):\n    \"\"\"Train model with custom adjacency learning and store per-epoch results\"\"\"\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n    \n    best_acc = 0.0\n    best_state = None\n    no_improve = 0\n    epoch_history = []\n    \n    epoch_iterator = tqdm(range(epochs), desc='    Epochs', leave=False) if verbose else range(epochs)\n    \n    for epoch in epoch_iterator:\n        train_loss, train_acc = train_epoch_with_adj_learning(\n            model, train_loader, criterion, optimizer, device, adj_lr, l1_lambda\n        )\n        val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n        \n        epoch_result = {\n            'epoch': epoch + 1,\n            'train_loss': float(train_loss),\n            'train_acc': float(train_acc),\n            'val_loss': float(val_loss),\n            'val_acc': float(val_acc),\n            'learning_rate': float(lr)\n        }\n        \n        if subject_id:\n            epoch_result['subject'] = subject_id\n        if fold is not None:\n            epoch_result['fold'] = fold\n        if model_type:\n            epoch_result['model_type'] = model_type\n        \n        epoch_history.append(epoch_result)\n        \n        if verbose:\n            epoch_iterator.set_postfix({\n                'train_loss': f'{train_loss:.4f}',\n                'train_acc': f'{train_acc:.4f}',\n                'val_loss': f'{val_loss:.4f}',\n                'val_acc': f'{val_acc:.4f}',\n                'best': f'{best_acc:.4f}'\n            })\n        \n        if val_acc > best_acc:\n            best_acc = val_acc\n            best_state = deepcopy(model.state_dict())\n            no_improve = 0\n        else:\n            no_improve += 1\n        \n        if no_improve >= patience:\n            if verbose:\n                print(f'      Early stopping at epoch {epoch+1}/{epochs}')\n            break\n    \n    if best_state is None:\n        best_state = deepcopy(model.state_dict())\n    \n    model.load_state_dict(best_state)\n    \n    if config and subject_id and fold is not None and model_type:\n        epoch_df = pd.DataFrame(epoch_history)\n        epoch_results_file = config['output']['epoch_results_dir'] / f'{subject_id}_{model_type}_fold{fold}_epochs.csv'\n        epoch_df.to_csv(epoch_results_file, index=False)\n    \n    return best_state, best_acc, epoch_history\n\n\nprint(\"Training functions defined with CUSTOM ADJACENCY MATRIX LEARNING!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "all_results = {'baseline': [], 'adaptive': []}\nall_epoch_histories = {'baseline': [], 'adaptive': []}\n\nprint(\"\\nStarting training with PAPER IMPLEMENTATION...\\n\")\nprint(\"Expected Results (from paper Table II):\")\nprint(\"  Subject 1: 93.2% ± 5.4%\")\nprint(\"  Subject 2: 98.4% ± 3.1%\")\nprint(\"  Subject 3: 98.0% ± 2.3%\")\nprint(\"  Subject 8: 99.3% ± 2.7%\")\nprint(\"\\n\")\n\nfor subject_id in tqdm(subjects, desc='Training subjects'):\n    print(f\"\\nProcessing {subject_id}...\")\n    \n    X, Y, channel_names = load_subject_data(\n        data_dir,\n        subject_id,\n        ALL_TASK_RUNS,\n        CONFIG\n    )\n    \n    if X is None or len(Y) == 0:\n        print(f\"  Skipped: No data available\")\n        continue\n    \n    C, T = X.shape[1], X.shape[2]\n    K = len(set(np.unique(Y)))\n    H = CONFIG['model']['hidden_dim']\n    \n    print(f\"  Data shape: {X.shape}\")\n    print(f\"  Label distribution: {np.bincount(Y)}\")\n    print(f\"  Channels: {C}, Timepoints: {T}, Classes: {K}\")\n    \n    for model_type in ['baseline', 'adaptive']:\n        print(f\"\\n  Training {model_type.upper()}...\")\n        \n        skf = StratifiedKFold(n_splits=CONFIG['model']['n_folds'], shuffle=True, random_state=42)\n        fold_results = []\n        adjacencies = []\n        gate_values_list = []\n        \n        for fold, (train_idx, val_idx) in enumerate(skf.split(X, Y)):\n            print(f\"    Fold {fold+1}/{CONFIG['model']['n_folds']}\")\n            X_train, X_val = normalize(X[train_idx]), normalize(X[val_idx])\n            Y_train, Y_val = Y[train_idx], Y[val_idx]\n            \n            train_loader = DataLoader(\n                EEGDataset(X_train, Y_train),\n                batch_size=CONFIG['model']['batch_size'],\n                shuffle=True,\n                num_workers=0\n            )\n            val_loader = DataLoader(\n                EEGDataset(X_val, Y_val),\n                batch_size=CONFIG['model']['batch_size'],\n                shuffle=False,\n                num_workers=0\n            )\n            \n            if model_type == 'baseline':\n                model = BaselineEEGARNN(C, T, K, H).to(device)\n                l1_lambda = 0.0\n            else:\n                model = AdaptiveGatedEEGARNN(C, T, K, H, CONFIG['gating']['gate_init']).to(device)\n                l1_lambda = CONFIG['gating']['l1_lambda']\n            \n            best_state, best_acc, epoch_history = train_model(\n                model, train_loader, val_loader, device,\n                CONFIG['model']['epochs'],\n                CONFIG['model']['learning_rate'],\n                CONFIG['model']['patience'],\n                adj_lr=CONFIG['model']['adj_lr'],\n                l1_lambda=l1_lambda,\n                verbose=True,\n                subject_id=subject_id,\n                fold=fold,\n                model_type=model_type,\n                config=CONFIG\n            )\n            model.load_state_dict(best_state)\n            \n            all_epoch_histories[model_type].extend(epoch_history)\n            \n            metrics = calculate_comprehensive_metrics(model, val_loader, device)\n            fold_results.append({'fold': fold, **metrics})\n            \n            adjacency = model.get_final_adjacency()\n            adjacencies.append(adjacency)\n            \n            if hasattr(model, 'get_gate_values'):\n                gate_values = model.get_gate_values()\n                if gate_values is not None:\n                    if isinstance(gate_values, torch.Tensor):\n                        gate_values = gate_values.detach().cpu().numpy()\n                    gate_values_list.append(gate_values)\n            \n            del model\n            torch.cuda.empty_cache()\n            gc.collect()\n        \n        avg_metrics = {}\n        for key in ['accuracy', 'precision', 'recall', 'f1_score', 'auc_roc']:\n            values = [f[key] for f in fold_results]\n            avg_metrics[f'avg_{key}'] = float(np.mean(values))\n            avg_metrics[f'std_{key}'] = float(np.std(values))\n        \n        avg_adjacency = np.mean(np.stack(adjacencies, 0), 0)\n        \n        result = {\n            'subject': subject_id,\n            'num_trials': X.shape[0],\n            'num_channels': C,\n            **avg_metrics,\n            'adjacency_matrix': avg_adjacency,\n            'channel_names': channel_names\n        }\n        \n        if gate_values_list:\n            result['avg_gate_values'] = np.mean(np.stack(gate_values_list, 0), 0)\n        \n        all_results[model_type].append(result)\n        \n        print(f\"    Accuracy: {avg_metrics['avg_accuracy']:.4f} ± {avg_metrics['std_accuracy']:.4f}\")\n        print(f\"    F1-Score: {avg_metrics['avg_f1_score']:.4f} ± {avg_metrics['std_f1_score']:.4f}\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"Training Complete!\")\nprint(\"=\"*80)\n\nfor model_type in ['baseline', 'adaptive']:\n    if all_epoch_histories[model_type]:\n        epoch_df = pd.DataFrame(all_epoch_histories[model_type])\n        epoch_file = CONFIG['output']['epoch_results_dir'] / f'all_{model_type}_epoch_results.csv'\n        epoch_df.to_csv(epoch_file, index=False)\n        print(f\"Saved epoch results: {epoch_file}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Channel Selection Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class ChannelSelector:\n    def __init__(self, adjacency, channel_names, gate_values=None):\n        self.A = adjacency\n        self.names = np.array(channel_names)\n        self.C = adjacency.shape[0]\n        self.gate_values = gate_values\n\n    def edge_selection(self, k):\n        edge_importance = np.zeros(self.C)\n        for i in range(self.C):\n            for j in range(self.C):\n                if i != j:\n                    edge_importance[i] += abs(self.A[i, j])\n                    edge_importance[j] += abs(self.A[i, j])\n\n        indices = np.sort(np.argsort(edge_importance)[-int(k):])\n        return self.names[indices].tolist(), indices\n\n    def aggregation_selection(self, k):\n        agg_scores = np.sum(np.abs(self.A), 1)\n        indices = np.sort(np.argsort(agg_scores)[-int(k):])\n        return self.names[indices].tolist(), indices\n\n    def gate_selection(self, k):\n        if self.gate_values is None:\n            raise ValueError(\"Gate values not available.\")\n\n        indices = np.sort(np.argsort(self.gate_values)[-int(k):])\n        return self.names[indices].tolist(), indices\n\n\ndef retrain_with_selected_channels(x, y, selected_indices, T, K, device, config, model_type='baseline', \n                                   subject_id=None, method_name=None, k=None):\n    \"\"\"Retrain with selected channels and store per-epoch results\"\"\"\n    x_selected = x[:, selected_indices, :]\n    C = len(selected_indices)\n    H = config['model']['hidden_dim']\n\n    skf = StratifiedKFold(n_splits=config['model']['n_folds'], shuffle=True, random_state=42)\n    fold_results = []\n    all_epoch_history = []\n\n    for fold, (train_idx, val_idx) in enumerate(skf.split(x_selected, y)):\n        X_train = normalize(x_selected[train_idx])\n        X_val = normalize(x_selected[val_idx])\n        Y_train, Y_val = y[train_idx], y[val_idx]\n\n        train_loader = DataLoader(\n            EEGDataset(X_train, Y_train),\n            batch_size=config['model']['batch_size'],\n            shuffle=True,\n            num_workers=0\n        )\n        val_loader = DataLoader(\n            EEGDataset(X_val, Y_val),\n            batch_size=config['model']['batch_size'],\n            shuffle=False,\n            num_workers=0\n        )\n\n        if model_type == 'baseline':\n            model = BaselineEEGARNN(C, T, K, H).to(device)\n            l1_lambda = 0.0\n        else:\n            model = AdaptiveGatedEEGARNN(C, T, K, H, config['gating']['gate_init']).to(device)\n            l1_lambda = config['gating']['l1_lambda']\n\n        best_state, best_acc, epoch_history = train_model(\n            model, train_loader, val_loader, device,\n            config['model']['epochs'],\n            config['model']['learning_rate'],\n            config['model']['patience'],\n            adj_lr=config['model']['adj_lr'],\n            l1_lambda=l1_lambda,\n            verbose=False,\n            subject_id=subject_id,\n            fold=fold,\n            model_type=f\"{model_type}_retrain_{method_name}_k{k}\",\n            config=None\n        )\n        model.load_state_dict(best_state)\n        \n        for epoch_result in epoch_history:\n            epoch_result['method'] = method_name\n            epoch_result['k'] = k\n            epoch_result['num_channels'] = C\n        all_epoch_history.extend(epoch_history)\n\n        metrics = calculate_comprehensive_metrics(model, val_loader, device)\n        fold_results.append(metrics)\n        \n        del model\n        torch.cuda.empty_cache()\n        gc.collect()\n\n    avg_metrics = {}\n    for key in ['accuracy', 'precision', 'recall', 'f1_score', 'auc_roc']:\n        values = [f[key] for f in fold_results]\n        avg_metrics[f'avg_{key}'] = float(np.mean(values))\n        avg_metrics[f'std_{key}'] = float(np.std(values))\n    \n    if config and subject_id and method_name and k:\n        epoch_df = pd.DataFrame(all_epoch_history)\n        retrain_epoch_file = config['output']['epoch_results_dir'] / f'{subject_id}_{model_type}_retrain_{method_name}_k{k}_epochs.csv'\n        epoch_df.to_csv(retrain_epoch_file, index=False)\n    \n    return avg_metrics, all_epoch_history\n\n\nprint(\"Channel selection functions defined!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Channel Selection and Retraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "retrain_results = {'baseline': [], 'adaptive': []}\nretrain_epoch_histories = {'baseline': [], 'adaptive': []}\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"CHANNEL SELECTION AND RETRAINING\")\nprint(\"=\"*80 + \"\\n\")\n\nfor subject_id in tqdm(subjects, desc='Retraining'):\n    print(f\"\\nProcessing {subject_id}...\")\n\n    X, Y, channel_names = load_subject_data(\n        data_dir,\n        subject_id,\n        ALL_TASK_RUNS,\n        CONFIG\n    )\n\n    if X is None:\n        continue\n\n    C, T = X.shape[1], X.shape[2]\n    K = len(set(CONFIG['data']['selected_classes']))\n\n    for model_type in ['baseline', 'adaptive']:\n        subj_result = None\n        for res in all_results[model_type]:\n            if res['subject'] == subject_id:\n                subj_result = res\n                break\n\n        if subj_result is None:\n            continue\n\n        adjacency = subj_result['adjacency_matrix']\n        gate_values = subj_result.get('avg_gate_values', None)\n        selector = ChannelSelector(adjacency, channel_names, gate_values)\n\n        selection_methods = ['ES', 'AS']\n        if model_type == 'adaptive':\n            selection_methods.append('GS')\n\n        for method_name in selection_methods:\n            for k in CONFIG['channel_selection']['k_values']:\n                if method_name == 'ES':\n                    selected_channels, selected_indices = selector.edge_selection(k)\n                elif method_name == 'AS':\n                    selected_channels, selected_indices = selector.aggregation_selection(k)\n                elif method_name == 'GS':\n                    selected_channels, selected_indices = selector.gate_selection(k)\n\n                retrain_metrics, epoch_history = retrain_with_selected_channels(\n                    X, Y, selected_indices, T, K, device, CONFIG, model_type,\n                    subject_id=subject_id, method_name=method_name, k=k\n                )\n                \n                retrain_epoch_histories[model_type].extend(epoch_history)\n\n                acc_drop = subj_result['avg_accuracy'] - retrain_metrics['avg_accuracy']\n\n                retrain_results[model_type].append({\n                    'subject': subject_id,\n                    'method': method_name,\n                    'k': k,\n                    'num_channels_selected': len(selected_channels),\n                    'selected_channels': ','.join(selected_channels),\n                    **retrain_metrics,\n                    'full_channels_acc': subj_result['avg_accuracy'],\n                    'accuracy_drop': acc_drop,\n                    'accuracy_drop_pct': (acc_drop / subj_result['avg_accuracy'] * 100)\n                })\n\n                print(f\"  {model_type.upper()} - {method_name}, k={k}: \"\n                      f\"{retrain_metrics['avg_accuracy']:.4f} (drop: {acc_drop:.4f})\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"Retraining Complete!\")\nprint(\"=\"*80)\n\nfor model_type in ['baseline', 'adaptive']:\n    if retrain_epoch_histories[model_type]:\n        epoch_df = pd.DataFrame(retrain_epoch_histories[model_type])\n        epoch_file = CONFIG['output']['epoch_results_dir'] / f'all_{model_type}_retrain_epoch_results.csv'\n        epoch_df.to_csv(epoch_file, index=False)\n        print(f\"Saved retrain epoch results: {epoch_file}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "results_dir = CONFIG['output']['results_dir']\n\nfor model_type in ['baseline', 'adaptive']:\n    if len(all_results[model_type]) > 0:\n        df = pd.DataFrame([{\n            'subject': r['subject'],\n            'num_trials': r['num_trials'],\n            'num_channels': r['num_channels'],\n            'accuracy': r['avg_accuracy'],\n            'std_accuracy': r['std_accuracy'],\n            'precision': r['avg_precision'],\n            'std_precision': r['std_precision'],\n            'recall': r['avg_recall'],\n            'std_recall': r['std_recall'],\n            'f1_score': r['avg_f1_score'],\n            'std_f1_score': r['std_f1_score'],\n            'auc_roc': r['avg_auc_roc'],\n            'std_auc_roc': r['std_auc_roc']\n        } for r in all_results[model_type]])\n        \n        df.to_csv(results_dir / f'bci_2a_{model_type}_results.csv', index=False)\n        print(f\"Saved: bci_2a_{model_type}_results.csv\")\n\nfor model_type in ['baseline', 'adaptive']:\n    if len(retrain_results[model_type]) > 0:\n        df = pd.DataFrame(retrain_results[model_type])\n        df.to_csv(results_dir / f'bci_2a_{model_type}_retrain_results.csv', index=False)\n        print(f\"Saved: bci_2a_{model_type}_retrain_results.csv\")\n\nprint(f\"\\nAll results saved to {results_dir}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESULTS SUMMARY\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "for model_type in ['baseline', 'adaptive']:\n",
    "    if len(all_results[model_type]) > 0:\n",
    "        accs = [r['avg_accuracy'] for r in all_results[model_type]]\n",
    "        f1s = [r['avg_f1_score'] for r in all_results[model_type]]\n",
    "        aucs = [r['avg_auc_roc'] for r in all_results[model_type]]\n",
    "        \n",
    "        print(f\"{model_type.upper()} Results:\")\n",
    "        print(f\"  Subjects: {len(all_results[model_type])}\")\n",
    "        print(f\"  Mean accuracy: {np.mean(accs):.4f} Â± {np.std(accs):.4f}\")\n",
    "        print(f\"  Mean F1-Score: {np.mean(f1s):.4f} Â± {np.std(f1s):.4f}\")\n",
    "        print(f\"  Mean AUC-ROC: {np.mean(aucs):.4f} Â± {np.std(aucs):.4f}\")\n",
    "        print()\n",
    "\n",
    "if len(all_results['baseline']) > 0 and len(all_results['adaptive']) > 0:\n",
    "    baseline_acc = np.mean([r['avg_accuracy'] for r in all_results['baseline']])\n",
    "    adaptive_acc = np.mean([r['avg_accuracy'] for r in all_results['adaptive']])\n",
    "    improvement = adaptive_acc - baseline_acc\n",
    "    \n",
    "    print(f\"\\nAdaptive vs Baseline:\")\n",
    "    print(f\"  Improvement: {improvement:.4f} ({improvement/baseline_acc*100:.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DONE!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}