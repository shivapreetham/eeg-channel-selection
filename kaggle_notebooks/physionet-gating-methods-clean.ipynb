{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PhysioNet Motor Imagery - Gating Methods Comparison\n",
    "\n",
    "This notebook compares four different gating methods:\n",
    "1. **Baseline EEG-ARNN** - Pure CNN-GCN architecture (no gating)\n",
    "2. **Static Gating** - Fixed channel gates learned during training\n",
    "3. **Adaptive Gating** - Dynamic gates that adapt per input\n",
    "4. **Early Halting** - Progressive channel dropout with early stopping\n",
    "\n",
    "All methods include:\n",
    "- Proper data cleaning (removing faulty subjects)\n",
    "- In-notebook preprocessing with MNE\n",
    "- Cross-validation\n",
    "- Channel importance analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SHIVAPREETHAM ROHITH\\AppData\\Roaming\\Python\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import mne\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_context('notebook', font_scale=1.0)\n",
    "mne.set_log_level('WARNING')\n",
    "\n",
    "def set_seed(s=42):\n",
    "    random.seed(s)\n",
    "    np.random.seed(s)\n",
    "    torch.manual_seed(s)\n",
    "    torch.cuda.manual_seed_all(s)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running locally\n",
      "\n",
      "Configuration loaded!\n",
      "Training: 20 subjects, 3-fold CV, 30 epochs\n"
     ]
    }
   ],
   "source": [
    "# Auto-detect Kaggle environment\n",
    "import os\n",
    "if os.path.exists('/kaggle/input'):\n",
    "    print(\"Running on Kaggle\")\n",
    "    kaggle_input = Path('/kaggle/input')\n",
    "    datasets = [d for d in kaggle_input.iterdir() if d.is_dir()]\n",
    "    print(f\"Available datasets: {[d.name for d in datasets]}\")\n",
    "    \n",
    "    DATA_DIR = None\n",
    "    possible_names = ['physioneteegmi', 'eeg-motor-movementimagery-dataset']\n",
    "    for ds_name in possible_names:\n",
    "        test_path = kaggle_input / ds_name\n",
    "        if test_path.exists():\n",
    "            DATA_DIR = test_path\n",
    "            print(f\"Found dataset: {DATA_DIR}\")\n",
    "            break\n",
    "    \n",
    "    if DATA_DIR is None and datasets:\n",
    "        DATA_DIR = datasets[0]\n",
    "        print(f\"Using first available dataset: {DATA_DIR}\")\n",
    "else:\n",
    "    print(\"Running locally\")\n",
    "    DATA_DIR = Path('data/physionet/files')\n",
    "\n",
    "CONFIG = {\n",
    "    'data': {\n",
    "        'raw_data_dir': DATA_DIR,\n",
    "        'selected_classes': [1, 2],  # T1 (left fist), T2 (right fist)\n",
    "        'tmin': -1.0,\n",
    "        'tmax': 5.0,\n",
    "        'baseline': (-0.5, 0)\n",
    "    },\n",
    "    'preprocessing': {\n",
    "        'l_freq': 0.5,\n",
    "        'h_freq': 40.0,\n",
    "        'notch_freq': 50.0,\n",
    "        'target_sfreq': 128.0,\n",
    "        'apply_car': True\n",
    "    },\n",
    "    'model': {\n",
    "        'hidden_dim': 40,\n",
    "        'epochs': 30,\n",
    "        'learning_rate': 1e-3,\n",
    "        'batch_size': 32,\n",
    "        'n_folds': 3,\n",
    "        'patience': 8\n",
    "    },\n",
    "    'gating': {\n",
    "        'l1_lambda': 1e-3,\n",
    "        'gate_init': 0.9,\n",
    "        'halting_threshold': 0.5,\n",
    "        'halting_penalty': 1e-2\n",
    "    },\n",
    "    'output': {\n",
    "        'results_dir': Path('results'),\n",
    "    },\n",
    "    'max_subjects': 20,\n",
    "    'min_runs_per_subject': 8\n",
    "}\n",
    "\n",
    "CONFIG['output']['results_dir'].mkdir(exist_ok=True, parents=True)\n",
    "print(f\"\\nConfiguration loaded!\")\n",
    "print(f\"Training: {CONFIG['max_subjects']} subjects, {CONFIG['model']['n_folds']}-fold CV, {CONFIG['model']['epochs']} epochs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Cleaning - Remove Faulty Subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total excluded subjects: 27\n",
      "Excluded subjects: ['S003', 'S004', 'S009', 'S010', 'S012', 'S013', 'S017', 'S018', 'S019', 'S021', 'S022', 'S023', 'S024', 'S025', 'S026', 'S027', 'S028', 'S029', 'S088', 'S089', 'S092', 'S100', 'S104', 'S106', 'S107', 'S108', 'S109']\n"
     ]
    }
   ],
   "source": [
    "# Known faulty subjects from data cleaning analysis\n",
    "KNOWN_BAD_SUBJECTS = [\n",
    "    'S088', 'S089', 'S092', 'S100', 'S104', 'S106', 'S107', 'S108', 'S109'\n",
    "]\n",
    "\n",
    "# Additional subjects with high clipping or amplitude issues\n",
    "HIGH_ISSUE_SUBJECTS = [\n",
    "    'S003', 'S004', 'S009', 'S010', 'S012', 'S013', 'S017', 'S018', 'S019',\n",
    "    'S021', 'S022', 'S023', 'S024', 'S025', 'S026', 'S027', 'S028', 'S029'\n",
    "]\n",
    "\n",
    "EXCLUDED_SUBJECTS = set(KNOWN_BAD_SUBJECTS + HIGH_ISSUE_SUBJECTS)\n",
    "\n",
    "print(f\"Total excluded subjects: {len(EXCLUDED_SUBJECTS)}\")\n",
    "print(f\"Excluded subjects: {sorted(EXCLUDED_SUBJECTS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Loading and Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scanning for subjects...\n",
      "Looking for data in: data\\physionet\\files\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Data directory not found: data\\physionet\\files",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 132\u001b[39m\n\u001b[32m    129\u001b[39m data_dir = CONFIG[\u001b[33m'\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mraw_data_dir\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m    130\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLooking for data in: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m all_subjects = \u001b[43mget_available_subjects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmin_runs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmin_runs_per_subject\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexcluded\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEXCLUDED_SUBJECTS\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m    137\u001b[39m subjects = all_subjects[:CONFIG[\u001b[33m'\u001b[39m\u001b[33mmax_subjects\u001b[39m\u001b[33m'\u001b[39m]]\n\u001b[32m    139\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFound \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(all_subjects)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m clean subjects with >= \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCONFIG[\u001b[33m'\u001b[39m\u001b[33mmin_runs_per_subject\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m runs\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 107\u001b[39m, in \u001b[36mget_available_subjects\u001b[39m\u001b[34m(data_dir, min_runs, excluded)\u001b[39m\n\u001b[32m    105\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Get list of subjects with at least min_runs available, excluding bad subjects.\"\"\"\u001b[39;00m\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data_dir.exists():\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mData directory not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    109\u001b[39m excluded = excluded \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mset\u001b[39m()\n\u001b[32m    110\u001b[39m subjects = []\n",
      "\u001b[31mValueError\u001b[39m: Data directory not found: data\\physionet\\files"
     ]
    }
   ],
   "source": [
    "def preprocess_raw(raw, config):\n",
    "    \"\"\"Apply preprocessing to raw EEG data.\"\"\"\n",
    "    cleaned_names = {name: name.rstrip('.') for name in raw.ch_names}\n",
    "    raw.rename_channels(cleaned_names)\n",
    "    raw.pick_types(eeg=True)\n",
    "    raw.set_montage('standard_1020', on_missing='ignore', match_case=False)\n",
    "    \n",
    "    nyquist = raw.info['sfreq'] / 2.0\n",
    "    if config['preprocessing']['notch_freq'] < nyquist:\n",
    "        raw.notch_filter(freqs=config['preprocessing']['notch_freq'], verbose=False)\n",
    "    \n",
    "    raw.filter(\n",
    "        l_freq=config['preprocessing']['l_freq'],\n",
    "        h_freq=config['preprocessing']['h_freq'],\n",
    "        method='fir',\n",
    "        fir_design='firwin',\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    if config['preprocessing']['apply_car']:\n",
    "        raw.set_eeg_reference('average', projection=False, verbose=False)\n",
    "    \n",
    "    raw.resample(config['preprocessing']['target_sfreq'], npad='auto', verbose=False)\n",
    "    return raw\n",
    "\n",
    "\n",
    "def load_and_preprocess_edf(edf_path, config):\n",
    "    \"\"\"Load raw EDF file, preprocess it, and extract epochs.\"\"\"\n",
    "    raw = mne.io.read_raw_edf(edf_path, preload=True, verbose='ERROR')\n",
    "    raw = preprocess_raw(raw, config)\n",
    "    \n",
    "    # Use events_from_annotations to get correct event IDs\n",
    "    events, event_ids = mne.events_from_annotations(raw, verbose='ERROR')\n",
    "    \n",
    "    if len(events) == 0:\n",
    "        return None, None, raw.ch_names\n",
    "    \n",
    "    epochs = mne.Epochs(\n",
    "        raw,\n",
    "        events,\n",
    "        event_id=event_ids,\n",
    "        tmin=config['data']['tmin'],\n",
    "        tmax=config['data']['tmax'],\n",
    "        baseline=tuple(config['data']['baseline']),\n",
    "        preload=True,\n",
    "        verbose='ERROR'\n",
    "    )\n",
    "    \n",
    "    return epochs.get_data(), epochs.events[:, 2], raw.ch_names\n",
    "\n",
    "\n",
    "def filter_classes(x, y, selected_classes):\n",
    "    \"\"\"Filter to keep only selected classes and remap labels.\"\"\"\n",
    "    mask = np.isin(y, selected_classes)\n",
    "    y, x = y[mask], x[mask]\n",
    "    label_map = {old: new for new, old in enumerate(sorted(selected_classes))}\n",
    "    y = np.array([label_map[int(label)] for label in y], dtype=np.int64)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def normalize(x):\n",
    "    \"\"\"Z-score normalization per channel.\"\"\"\n",
    "    mu = x.mean(axis=(0, 2), keepdims=True)\n",
    "    sd = x.std(axis=(0, 2), keepdims=True) + 1e-8\n",
    "    return (x - mu) / sd\n",
    "\n",
    "\n",
    "def load_subject_data(data_dir, subject_id, run_ids, config):\n",
    "    \"\"\"Load all runs for a subject, preprocess, and concatenate.\"\"\"\n",
    "    subject_dir = data_dir / subject_id\n",
    "    if not subject_dir.exists():\n",
    "        return None, None, None\n",
    "    \n",
    "    all_x, all_y = [], []\n",
    "    channel_names = None\n",
    "    \n",
    "    for run_id in run_ids:\n",
    "        edf_path = subject_dir / f'{subject_id}{run_id}.edf'\n",
    "        if not edf_path.exists():\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            x, y, ch_names = load_and_preprocess_edf(edf_path, config)\n",
    "            if x is None or len(y) == 0:\n",
    "                continue\n",
    "            \n",
    "            x, y = filter_classes(x, y, config['data']['selected_classes'])\n",
    "            if len(y) == 0:\n",
    "                continue\n",
    "            \n",
    "            channel_names = channel_names or ch_names\n",
    "            all_x.append(x)\n",
    "            all_y.append(y)\n",
    "        except Exception as e:\n",
    "            print(f\"  Warning: Failed to load {edf_path.name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if len(all_x) == 0:\n",
    "        return None, None, channel_names\n",
    "    \n",
    "    return np.concatenate(all_x, 0), np.concatenate(all_y, 0), channel_names\n",
    "\n",
    "\n",
    "def get_available_subjects(data_dir, min_runs=8, excluded=None):\n",
    "    \"\"\"Get list of subjects with at least min_runs available, excluding bad subjects.\"\"\"\n",
    "    if not data_dir.exists():\n",
    "        raise ValueError(f\"Data directory not found: {data_dir}\")\n",
    "    \n",
    "    excluded = excluded or set()\n",
    "    subjects = []\n",
    "    \n",
    "    for subject_dir in sorted(data_dir.iterdir()):\n",
    "        if not subject_dir.is_dir() or not subject_dir.name.startswith('S'):\n",
    "            continue\n",
    "        \n",
    "        # Skip excluded subjects\n",
    "        if subject_dir.name in excluded:\n",
    "            continue\n",
    "        \n",
    "        edf_files = list(subject_dir.glob('*.edf'))\n",
    "        if len(edf_files) >= min_runs:\n",
    "            subjects.append(subject_dir.name)\n",
    "    \n",
    "    return subjects\n",
    "\n",
    "\n",
    "# Scan for available subjects\n",
    "print(\"\\nScanning for subjects...\")\n",
    "data_dir = CONFIG['data']['raw_data_dir']\n",
    "print(f\"Looking for data in: {data_dir}\")\n",
    "\n",
    "all_subjects = get_available_subjects(\n",
    "    data_dir, \n",
    "    min_runs=CONFIG['min_runs_per_subject'],\n",
    "    excluded=EXCLUDED_SUBJECTS\n",
    ")\n",
    "subjects = all_subjects[:CONFIG['max_subjects']]\n",
    "\n",
    "print(f\"Found {len(all_subjects)} clean subjects with >= {CONFIG['min_runs_per_subject']} runs\")\n",
    "print(f\"Will process {len(subjects)} subjects: {subjects}\")\n",
    "\n",
    "# Define which runs to use\n",
    "MOTOR_IMAGERY_RUNS = ['R07', 'R08', 'R09', 'R10', 'R11', 'R12', 'R13', 'R14']\n",
    "MOTOR_EXECUTION_RUNS = ['R03', 'R04', 'R05', 'R06']\n",
    "ALL_TASK_RUNS = MOTOR_IMAGERY_RUNS + MOTOR_EXECUTION_RUNS\n",
    "print(f\"Using runs: {ALL_TASK_RUNS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = torch.FloatTensor(x).unsqueeze(1)  # Add channel dim for Conv2d\n",
    "        self.y = torch.LongTensor(y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return self.x[i], self.y[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Architectures\n",
    "\n",
    "### 6.1 Baseline Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvLayer(nn.Module):\n",
    "    \"\"\"Graph Convolution Layer with learned adjacency.\"\"\"\n",
    "    def __init__(self, num_channels, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.num_channels = num_channels\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Learnable adjacency matrix\n",
    "        self.A = nn.Parameter(torch.randn(num_channels, num_channels))\n",
    "        self.theta = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(hidden_dim)\n",
    "        self.act = nn.ELU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, H, C, T = x.shape\n",
    "        \n",
    "        # Normalize adjacency matrix\n",
    "        A = torch.sigmoid(self.A)\n",
    "        A = 0.5 * (A + A.t())\n",
    "        I = torch.eye(C, device=A.device)\n",
    "        A_hat = A + I\n",
    "        D = torch.diag(torch.pow(A_hat.sum(1).clamp_min(1e-6), -0.5))\n",
    "        A_norm = D @ A_hat @ D\n",
    "        \n",
    "        # Graph convolution\n",
    "        x_batch = x.permute(0, 3, 2, 1).contiguous().view(B*T, C, H)\n",
    "        x_g = A_norm @ x_batch\n",
    "        x_g = self.theta(x_g)\n",
    "        x_g = x_g.view(B, T, C, H).permute(0, 3, 2, 1)\n",
    "        \n",
    "        x_out = self.bn(x_g)\n",
    "        x_out = self.act(x_out)\n",
    "        \n",
    "        return x_out\n",
    "    \n",
    "    def get_adjacency(self):\n",
    "        with torch.no_grad():\n",
    "            A = torch.sigmoid(self.A)\n",
    "            A = 0.5 * (A + A.t())\n",
    "            return A.cpu().numpy()\n",
    "\n",
    "\n",
    "class TemporalConv(nn.Module):\n",
    "    \"\"\"Temporal Convolution Layer.\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=16, pool=True):\n",
    "        super().__init__()\n",
    "        self.pool = pool\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, \n",
    "                            kernel_size=(1, kernel_size), \n",
    "                            padding=(0, kernel_size//2), bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.act = nn.ELU()\n",
    "        self.pool_layer = nn.AvgPool2d(kernel_size=(1, 2)) if pool else None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.act(self.bn(self.conv(x)))\n",
    "        return self.pool_layer(x) if self.pool else x\n",
    "\n",
    "\n",
    "class BaselineEEGARNN(nn.Module):\n",
    "    \"\"\"Baseline EEG-ARNN with pure CNN-GCN architecture.\"\"\"\n",
    "    def __init__(self, C, T, K, H):\n",
    "        super().__init__()\n",
    "        self.t1 = TemporalConv(1, H, 16, False)\n",
    "        self.g1 = GraphConvLayer(C, H)\n",
    "        self.t2 = TemporalConv(H, H, 16, True)\n",
    "        self.g2 = GraphConvLayer(C, H)\n",
    "        self.t3 = TemporalConv(H, H, 16, True)\n",
    "        self.g3 = GraphConvLayer(C, H)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            ft = self._forward_features(torch.zeros(1, 1, C, T))\n",
    "            fs = ft.view(1, -1).size(1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(fs, 256)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(256, K)\n",
    "    \n",
    "    def _forward_features(self, x):\n",
    "        x = self.g1(self.t1(x))\n",
    "        x = self.g2(self.t2(x))\n",
    "        x = self.g3(self.t3(x))\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self._forward_features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        return self.fc2(x)\n",
    "    \n",
    "    def get_final_adjacency(self):\n",
    "        return self.g3.get_adjacency()\n",
    "\n",
    "\n",
    "print(\"Baseline EEG-ARNN architecture defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Static Gating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StaticGatedEEGARNN(BaselineEEGARNN):\n",
    "    \"\"\"EEG-ARNN with static learnable channel gates.\"\"\"\n",
    "    def __init__(self, C, T, K, H, gate_init=0.9):\n",
    "        super().__init__(C, T, K, H)\n",
    "        init = torch.full((C,), float(gate_init), dtype=torch.float32)\n",
    "        init = torch.clamp(init, 1e-4, 1 - 1e-4)\n",
    "        gate_logits = torch.logit(init)\n",
    "        self.gate_logits = nn.Parameter(gate_logits)\n",
    "        self.latest_gates = None\n",
    "    \n",
    "    def get_gate_values(self):\n",
    "        return torch.sigmoid(self.gate_logits)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        gate_values = torch.sigmoid(self.gate_logits)\n",
    "        self.latest_gates = gate_values.detach().cpu()\n",
    "        x = x * gate_values.view(1, 1, -1, 1)\n",
    "        return super().forward(x)\n",
    "\n",
    "\n",
    "print(\"Static Gating architecture defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Adaptive Gating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveGatedEEGARNN(BaselineEEGARNN):\n",
    "    \"\"\"EEG-ARNN with adaptive input-dependent channel gates.\"\"\"\n",
    "    def __init__(self, C, T, K, H, gate_init=0.9):\n",
    "        super().__init__(C, T, K, H)\n",
    "        \n",
    "        # Gate network: computes gates from input statistics\n",
    "        self.gate_net = nn.Sequential(\n",
    "            nn.Linear(C * 2, C),  # Input: mean and std per channel\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(C, C),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Initialize to produce values around gate_init\n",
    "        with torch.no_grad():\n",
    "            self.gate_net[-2].bias.fill_(2.0)  # Bias toward high values\n",
    "        \n",
    "        self.latest_gates = None\n",
    "    \n",
    "    def compute_gates(self, x):\n",
    "        \"\"\"Compute adaptive gates from input.\"\"\"\n",
    "        B, _, C, T = x.shape\n",
    "        \n",
    "        # Compute channel statistics\n",
    "        x_squeeze = x.squeeze(1)  # (B, C, T)\n",
    "        ch_mean = x_squeeze.mean(dim=2)  # (B, C)\n",
    "        ch_std = x_squeeze.std(dim=2)    # (B, C)\n",
    "        \n",
    "        # Concatenate statistics\n",
    "        stats = torch.cat([ch_mean, ch_std], dim=1)  # (B, 2*C)\n",
    "        \n",
    "        # Compute gates\n",
    "        gates = self.gate_net(stats)  # (B, C)\n",
    "        \n",
    "        return gates\n",
    "    \n",
    "    def forward(self, x):\n",
    "        gates = self.compute_gates(x)  # (B, C)\n",
    "        self.latest_gates = gates.detach().cpu()\n",
    "        \n",
    "        # Apply gates\n",
    "        x = x * gates.view(-1, 1, gates.size(1), 1)\n",
    "        \n",
    "        return super().forward(x)\n",
    "    \n",
    "    def get_gate_values(self):\n",
    "        \"\"\"Return average gate values from last forward pass.\"\"\"\n",
    "        if self.latest_gates is not None:\n",
    "            return self.latest_gates.mean(dim=0)\n",
    "        return None\n",
    "\n",
    "\n",
    "print(\"Adaptive Gating architecture defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Early Halting (Progressive Channel Dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyHaltingEEGARNN(BaselineEEGARNN):\n",
    "    \"\"\"EEG-ARNN with progressive channel halting.\"\"\"\n",
    "    def __init__(self, C, T, K, H, halt_threshold=0.5):\n",
    "        super().__init__(C, T, K, H)\n",
    "        \n",
    "        # Halting network: decides when to stop using each channel\n",
    "        self.halt_net = nn.Sequential(\n",
    "            nn.Linear(C * 2, C),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(C, C),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.halt_threshold = halt_threshold\n",
    "        self.latest_halt_probs = None\n",
    "        self.latest_active_mask = None\n",
    "    \n",
    "    def compute_halting_probs(self, x):\n",
    "        \"\"\"Compute halting probabilities for each channel.\"\"\"\n",
    "        B, _, C, T = x.shape\n",
    "        \n",
    "        # Compute channel statistics\n",
    "        x_squeeze = x.squeeze(1)  # (B, C, T)\n",
    "        ch_mean = x_squeeze.mean(dim=2)  # (B, C)\n",
    "        ch_std = x_squeeze.std(dim=2)    # (B, C)\n",
    "        \n",
    "        # Concatenate statistics\n",
    "        stats = torch.cat([ch_mean, ch_std], dim=1)  # (B, 2*C)\n",
    "        \n",
    "        # Compute halting probabilities\n",
    "        halt_probs = self.halt_net(stats)  # (B, C)\n",
    "        \n",
    "        return halt_probs\n",
    "    \n",
    "    def forward(self, x):\n",
    "        halt_probs = self.compute_halting_probs(x)  # (B, C)\n",
    "        self.latest_halt_probs = halt_probs.detach().cpu()\n",
    "        \n",
    "        # Create active mask (1 = keep, 0 = halt)\n",
    "        if self.training:\n",
    "            # During training: soft masking\n",
    "            active_mask = 1.0 - halt_probs\n",
    "        else:\n",
    "            # During inference: hard masking\n",
    "            active_mask = (halt_probs < self.halt_threshold).float()\n",
    "        \n",
    "        self.latest_active_mask = active_mask.detach().cpu()\n",
    "        \n",
    "        # Apply mask\n",
    "        x = x * active_mask.view(-1, 1, active_mask.size(1), 1)\n",
    "        \n",
    "        return super().forward(x)\n",
    "    \n",
    "    def get_gate_values(self):\n",
    "        \"\"\"Return average active channel mask from last forward pass.\"\"\"\n",
    "        if self.latest_active_mask is not None:\n",
    "            return self.latest_active_mask.mean(dim=0)\n",
    "        return None\n",
    "\n",
    "\n",
    "print(\"Early Halting architecture defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer, device, l1_lambda=0.0, halt_penalty=0.0):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    all_preds, all_labels = [], []\n",
    "    \n",
    "    for x, y in dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        \n",
    "        # Add L1 regularization for gated models\n",
    "        if l1_lambda > 0 and hasattr(model, 'get_gate_values'):\n",
    "            gate_values = model.get_gate_values()\n",
    "            if gate_values is not None:\n",
    "                if gate_values.dim() == 0:\n",
    "                    gate_values = gate_values.unsqueeze(0)\n",
    "                loss = loss + l1_lambda * gate_values.abs().mean()\n",
    "        \n",
    "        # Add halting penalty (encourage using fewer channels)\n",
    "        if halt_penalty > 0 and hasattr(model, 'latest_active_mask'):\n",
    "            if model.latest_active_mask is not None:\n",
    "                active_count = model.latest_active_mask.mean()\n",
    "                loss = loss + halt_penalty * active_count\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        all_preds += torch.argmax(logits, 1).cpu().tolist()\n",
    "        all_labels += y.cpu().tolist()\n",
    "    \n",
    "    return total_loss / max(1, len(dataloader)), accuracy_score(all_labels, all_preds)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_preds, all_labels = [], []\n",
    "    \n",
    "    for x, y in dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        all_preds += torch.argmax(logits, 1).cpu().tolist()\n",
    "        all_labels += y.cpu().tolist()\n",
    "    \n",
    "    return total_loss / max(1, len(dataloader)), accuracy_score(all_labels, all_preds)\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, device, epochs, lr, patience, l1_lambda=0.0, halt_penalty=0.0):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=3, verbose=False\n",
    "    )\n",
    "    \n",
    "    best_acc = 0.0\n",
    "    best_state = None\n",
    "    no_improve = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device, l1_lambda, halt_penalty)\n",
    "        val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            best_state = deepcopy(model.state_dict())\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "        \n",
    "        if no_improve >= patience:\n",
    "            break\n",
    "    \n",
    "    if best_state is None:\n",
    "        best_state = deepcopy(model.state_dict())\n",
    "    \n",
    "    model.load_state_dict(best_state)\n",
    "    return best_state, best_acc\n",
    "\n",
    "\n",
    "print(\"Training functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Cross-Validation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_subject(x, y, channel_names, T, K, device, config, model_type='baseline'):\n",
    "    \"\"\"Cross-validate subject with specified model type.\"\"\"\n",
    "    C = x.shape[1]\n",
    "    skf = StratifiedKFold(n_splits=config['model']['n_folds'], shuffle=True, random_state=42)\n",
    "    \n",
    "    batch_size = config['model']['batch_size']\n",
    "    epochs = config['model']['epochs']\n",
    "    lr = config['model']['learning_rate']\n",
    "    patience = config['model']['patience']\n",
    "    \n",
    "    folds = []\n",
    "    adjacencies = []\n",
    "    gate_values_list = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(x, y)):\n",
    "        X_train, X_val = normalize(x[train_idx]), normalize(x[val_idx])\n",
    "        Y_train, Y_val = y[train_idx], y[val_idx]\n",
    "        \n",
    "        train_loader = DataLoader(\n",
    "            EEGDataset(X_train, Y_train),\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=0\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            EEGDataset(X_val, Y_val),\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=0\n",
    "        )\n",
    "        \n",
    "        # Create model based on type\n",
    "        if model_type == 'baseline':\n",
    "            model = BaselineEEGARNN(C, T, K, config['model']['hidden_dim']).to(device)\n",
    "            l1_lambda = 0.0\n",
    "            halt_penalty = 0.0\n",
    "        elif model_type == 'static':\n",
    "            model = StaticGatedEEGARNN(C, T, K, config['model']['hidden_dim'], \n",
    "                                      config['gating']['gate_init']).to(device)\n",
    "            l1_lambda = config['gating']['l1_lambda']\n",
    "            halt_penalty = 0.0\n",
    "        elif model_type == 'adaptive':\n",
    "            model = AdaptiveGatedEEGARNN(C, T, K, config['model']['hidden_dim'],\n",
    "                                        config['gating']['gate_init']).to(device)\n",
    "            l1_lambda = config['gating']['l1_lambda']\n",
    "            halt_penalty = 0.0\n",
    "        elif model_type == 'halting':\n",
    "            model = EarlyHaltingEEGARNN(C, T, K, config['model']['hidden_dim'],\n",
    "                                       config['gating']['halting_threshold']).to(device)\n",
    "            l1_lambda = 0.0\n",
    "            halt_penalty = config['gating']['halting_penalty']\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "        \n",
    "        best_state, best_acc = train_model(model, train_loader, val_loader, device, \n",
    "                                          epochs, lr, patience, l1_lambda, halt_penalty)\n",
    "        model.load_state_dict(best_state)\n",
    "        \n",
    "        _, accuracy = evaluate(model, val_loader, nn.CrossEntropyLoss(), device)\n",
    "        \n",
    "        adjacency = model.get_final_adjacency()\n",
    "        adjacencies.append(adjacency)\n",
    "        \n",
    "        # Get gate values for gated models\n",
    "        if hasattr(model, 'get_gate_values'):\n",
    "            gate_values = model.get_gate_values()\n",
    "            if gate_values is not None:\n",
    "                if isinstance(gate_values, torch.Tensor):\n",
    "                    gate_values = gate_values.detach().cpu().numpy()\n",
    "                gate_values_list.append(gate_values)\n",
    "        \n",
    "        folds.append({'fold': fold, 'val_acc': accuracy})\n",
    "    \n",
    "    avg_acc = float(np.mean([f['val_acc'] for f in folds]))\n",
    "    std_acc = float(np.std([f['val_acc'] for f in folds]))\n",
    "    avg_adjacency = np.mean(np.stack([a for a in adjacencies if a is not None], 0), 0) \\\n",
    "                    if any(a is not None for a in adjacencies) else None\n",
    "    \n",
    "    result = {\n",
    "        'fold_results': folds,\n",
    "        'avg_accuracy': avg_acc,\n",
    "        'std_accuracy': std_acc,\n",
    "        'adjacency_matrix': avg_adjacency,\n",
    "        'channel_names': channel_names\n",
    "    }\n",
    "    \n",
    "    if gate_values_list:\n",
    "        result['avg_gate_values'] = np.mean(np.stack(gate_values_list, 0), 0)\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "print(\"Cross-validation function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = {\n",
    "    'baseline': [],\n",
    "    'static': [],\n",
    "    'adaptive': [],\n",
    "    'halting': []\n",
    "}\n",
    "\n",
    "print(\"\\nStarting training for all four methods...\\n\")\n",
    "\n",
    "for subject_id in tqdm(subjects, desc='Training subjects'):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Subject {subject_id}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Load subject data\n",
    "    X, Y, channel_names = load_subject_data(\n",
    "        data_dir,\n",
    "        subject_id,\n",
    "        ALL_TASK_RUNS,\n",
    "        CONFIG\n",
    "    )\n",
    "    \n",
    "    if X is None or len(Y) == 0:\n",
    "        print(f\"Skipped: No data available\")\n",
    "        continue\n",
    "    \n",
    "    C, T = X.shape[1], X.shape[2]\n",
    "    K = len(set(CONFIG['data']['selected_classes']))\n",
    "    \n",
    "    print(f\"Data shape: {X.shape} (trials={X.shape[0]}, channels={C}, timepoints={T})\")\n",
    "    print(f\"Label distribution: {np.bincount(Y)}\")\n",
    "    \n",
    "    # Train all four methods\n",
    "    for model_type in ['baseline', 'static', 'adaptive', 'halting']:\n",
    "        print(f\"\\nTraining {model_type.upper()}...\")\n",
    "        result = cross_validate_subject(X, Y, channel_names, T, K, device, CONFIG, model_type)\n",
    "        \n",
    "        print(f\"  Accuracy: {result['avg_accuracy']:.4f} ± {result['std_accuracy']:.4f}\")\n",
    "        \n",
    "        all_results[model_type].append({\n",
    "            'subject': subject_id,\n",
    "            'num_trials': X.shape[0],\n",
    "            'num_channels': C,\n",
    "            'accuracy': result['avg_accuracy'],\n",
    "            'std': result['std_accuracy'],\n",
    "            'adjacency_matrix': result['adjacency_matrix'],\n",
    "            'channel_names': result['channel_names'],\n",
    "            'gate_values': result.get('avg_gate_values', None)\n",
    "        })\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Training Complete!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrames\n",
    "results_dfs = {}\n",
    "for model_type in ['baseline', 'static', 'adaptive', 'halting']:\n",
    "    if len(all_results[model_type]) > 0:\n",
    "        df = pd.DataFrame(all_results[model_type])\n",
    "        results_dfs[model_type] = df[['subject', 'num_trials', 'num_channels', 'accuracy', 'std']]\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESULTS SUMMARY\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "for model_type in ['baseline', 'static', 'adaptive', 'halting']:\n",
    "    if model_type in results_dfs:\n",
    "        df = results_dfs[model_type]\n",
    "        print(f\"{model_type.upper()} Results:\")\n",
    "        print(f\"  Subjects: {len(df)}\")\n",
    "        print(f\"  Mean accuracy: {df['accuracy'].mean():.4f} ± {df['accuracy'].std():.4f}\")\n",
    "        print(f\"  Best: {df['accuracy'].max():.4f} (Subject {df.loc[df['accuracy'].idxmax(), 'subject']})\")\n",
    "        print(f\"  Worst: {df['accuracy'].min():.4f} (Subject {df.loc[df['accuracy'].idxmin(), 'subject']})\")\n",
    "        print()\n",
    "\n",
    "# Save results\n",
    "results_dir = CONFIG['output']['results_dir']\n",
    "for model_type, df in results_dfs.items():\n",
    "    df.to_csv(results_dir / f'{model_type}_gating_results.csv', index=False)\n",
    "\n",
    "print(f\"Results saved to {results_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Enhanced Visualizations for Gating Methods Comparison\n",
    "\n",
    "Replace the visualization cell (cell 11) in the notebook with this code\n",
    "for much better, more insightful visualizations.\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# ENHANCED VISUALIZATION CODE - Copy this to your notebook\n",
    "# ============================================================================\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set superior plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "plt.rcParams['font.sans-serif'] = ['Arial', 'DejaVu Sans']\n",
    "plt.rcParams['axes.titlesize'] = 12\n",
    "plt.rcParams['axes.labelsize'] = 10\n",
    "\n",
    "colors = {\n",
    "    'baseline': '#3498db',  # Blue\n",
    "    'static': '#e74c3c',    # Red\n",
    "    'adaptive': '#2ecc71',  # Green\n",
    "    'halting': '#f39c12'    # Orange\n",
    "}\n",
    "\n",
    "# Create comprehensive figure\n",
    "fig = plt.figure(figsize=(20, 16))\n",
    "gs = fig.add_gridspec(4, 3, hspace=0.35, wspace=0.3)\n",
    "\n",
    "# ============================================================================\n",
    "# 1. BOX PLOT - Accuracy Distribution\n",
    "# ============================================================================\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "data_for_box = []\n",
    "labels_for_box = []\n",
    "for model_type in ['baseline', 'static', 'adaptive', 'halting']:\n",
    "    if model_type in results_dfs:\n",
    "        data_for_box.append(results_dfs[model_type]['accuracy'].values)\n",
    "        labels_for_box.append(model_type.upper())\n",
    "\n",
    "bp = ax1.boxplot(data_for_box, labels=labels_for_box, patch_artist=True,\n",
    "                showmeans=True, meanline=True,\n",
    "                boxprops=dict(linewidth=1.5),\n",
    "                whiskerprops=dict(linewidth=1.5),\n",
    "                capprops=dict(linewidth=1.5))\n",
    "\n",
    "for patch, model in zip(bp['boxes'], ['baseline', 'static', 'adaptive', 'halting']):\n",
    "    patch.set_facecolor(colors[model])\n",
    "    patch.set_alpha(0.7)\n",
    "\n",
    "ax1.set_ylabel('Accuracy', fontweight='bold')\n",
    "ax1.set_title('Accuracy Distribution Across Methods', fontweight='bold', pad=10)\n",
    "ax1.grid(True, alpha=0.3, linestyle='--')\n",
    "ax1.set_ylim([0.5, 1.0])\n",
    "\n",
    "# Add mean value annotations\n",
    "for i, model in enumerate(['baseline', 'static', 'adaptive', 'halting']):\n",
    "    if model in results_dfs:\n",
    "        mean_val = results_dfs[model]['accuracy'].mean()\n",
    "        ax1.text(i+1, mean_val + 0.02, f'{mean_val:.3f}',\n",
    "                ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "\n",
    "# ============================================================================\n",
    "# 2. VIOLIN PLOT - Detailed Distribution\n",
    "# ============================================================================\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "violin_data = []\n",
    "violin_labels = []\n",
    "for model_type in ['baseline', 'static', 'adaptive', 'halting']:\n",
    "    if model_type in results_dfs:\n",
    "        violin_data.append(results_dfs[model_type]['accuracy'].values)\n",
    "        violin_labels.append(model_type.upper())\n",
    "\n",
    "parts = ax2.violinplot(violin_data, positions=range(1, len(violin_data)+1),\n",
    "                      showmeans=True, showextrema=True)\n",
    "\n",
    "for i, (pc, model) in enumerate(zip(parts['bodies'], ['baseline', 'static', 'adaptive', 'halting'])):\n",
    "    pc.set_facecolor(colors[model])\n",
    "    pc.set_alpha(0.7)\n",
    "\n",
    "ax2.set_xticks(range(1, len(violin_labels)+1))\n",
    "ax2.set_xticklabels(violin_labels)\n",
    "ax2.set_ylabel('Accuracy', fontweight='bold')\n",
    "ax2.set_title('Accuracy Density Distribution', fontweight='bold', pad=10)\n",
    "ax2.grid(True, alpha=0.3, axis='y', linestyle='--')\n",
    "ax2.set_ylim([0.5, 1.0])\n",
    "\n",
    "# ============================================================================\n",
    "# 3. RANKING COMPARISON - Which method wins per subject?\n",
    "# ============================================================================\n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "if all(m in results_dfs for m in ['baseline', 'static', 'adaptive', 'halting']):\n",
    "    # Count wins for each method\n",
    "    win_counts = {'baseline': 0, 'static': 0, 'adaptive': 0, 'halting': 0}\n",
    "\n",
    "    for idx in range(len(results_dfs['baseline'])):\n",
    "        accs = {\n",
    "            'baseline': results_dfs['baseline'].iloc[idx]['accuracy'],\n",
    "            'static': results_dfs['static'].iloc[idx]['accuracy'],\n",
    "            'adaptive': results_dfs['adaptive'].iloc[idx]['accuracy'],\n",
    "            'halting': results_dfs['halting'].iloc[idx]['accuracy']\n",
    "        }\n",
    "        winner = max(accs, key=accs.get)\n",
    "        win_counts[winner] += 1\n",
    "\n",
    "    # Create pie chart\n",
    "    methods = list(win_counts.keys())\n",
    "    counts = list(win_counts.values())\n",
    "    method_colors = [colors[m] for m in methods]\n",
    "\n",
    "    wedges, texts, autotexts = ax3.pie(counts, labels=[m.upper() for m in methods],\n",
    "                                        colors=method_colors, autopct='%1.1f%%',\n",
    "                                        startangle=90, textprops={'fontweight': 'bold'})\n",
    "\n",
    "    for autotext in autotexts:\n",
    "        autotext.set_color('white')\n",
    "        autotext.set_fontsize(10)\n",
    "\n",
    "    ax3.set_title('Method Wins Per Subject', fontweight='bold', pad=10)\n",
    "\n",
    "# ============================================================================\n",
    "# 4. SUBJECT-WISE BAR CHART\n",
    "# ============================================================================\n",
    "ax4 = fig.add_subplot(gs[1, :])\n",
    "if all(m in results_dfs for m in ['baseline', 'static', 'adaptive', 'halting']):\n",
    "    subjects_plot = results_dfs['baseline']['subject'].values\n",
    "    x = np.arange(len(subjects_plot))\n",
    "    width = 0.2\n",
    "\n",
    "    bars1 = ax4.bar(x - 1.5*width, results_dfs['baseline']['accuracy'].values, width,\n",
    "                   label='Baseline', color=colors['baseline'], alpha=0.8, edgecolor='black', linewidth=0.5)\n",
    "    bars2 = ax4.bar(x - 0.5*width, results_dfs['static']['accuracy'].values, width,\n",
    "                   label='Static', color=colors['static'], alpha=0.8, edgecolor='black', linewidth=0.5)\n",
    "    bars3 = ax4.bar(x + 0.5*width, results_dfs['adaptive']['accuracy'].values, width,\n",
    "                   label='Adaptive', color=colors['adaptive'], alpha=0.8, edgecolor='black', linewidth=0.5)\n",
    "    bars4 = ax4.bar(x + 1.5*width, results_dfs['halting']['accuracy'].values, width,\n",
    "                   label='Halting', color=colors['halting'], alpha=0.8, edgecolor='black', linewidth=0.5)\n",
    "\n",
    "    ax4.set_xticks(x)\n",
    "    ax4.set_xticklabels(subjects_plot, rotation=45, ha='right')\n",
    "    ax4.set_ylabel('Accuracy', fontweight='bold')\n",
    "    ax4.set_xlabel('Subject ID', fontweight='bold')\n",
    "    ax4.set_title('Subject-wise Accuracy Comparison', fontweight='bold', pad=10)\n",
    "    ax4.legend(loc='upper left', ncol=4, framealpha=0.9)\n",
    "    ax4.grid(True, alpha=0.3, axis='y', linestyle='--')\n",
    "    ax4.axhline(y=0.5, color='red', linestyle=':', alpha=0.5, label='Chance')\n",
    "    ax4.set_ylim([0.5, 1.0])\n",
    "\n",
    "# ============================================================================\n",
    "# 5. IMPROVEMENT OVER BASELINE - Heatmap\n",
    "# ============================================================================\n",
    "ax5 = fig.add_subplot(gs[2, 0])\n",
    "if all(m in results_dfs for m in ['baseline', 'static', 'adaptive', 'halting']):\n",
    "    # Calculate improvements\n",
    "    improvements = []\n",
    "    for model in ['static', 'adaptive', 'halting']:\n",
    "        diff = (results_dfs[model]['accuracy'].values -\n",
    "                results_dfs['baseline']['accuracy'].values) * 100\n",
    "        improvements.append(diff)\n",
    "\n",
    "    improvements = np.array(improvements).T\n",
    "\n",
    "    im = ax5.imshow(improvements, cmap='RdYlGn', aspect='auto', vmin=-10, vmax=10)\n",
    "    ax5.set_xticks(range(3))\n",
    "    ax5.set_xticklabels(['STATIC', 'ADAPTIVE', 'HALTING'])\n",
    "    ax5.set_yticks(range(len(subjects_plot)))\n",
    "    ax5.set_yticklabels(subjects_plot, fontsize=8)\n",
    "    ax5.set_title('Improvement over Baseline (%)', fontweight='bold', pad=10)\n",
    "\n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(im, ax=ax5)\n",
    "    cbar.set_label('Accuracy Gain (%)', fontweight='bold')\n",
    "\n",
    "    # Add text annotations\n",
    "    for i in range(len(subjects_plot)):\n",
    "        for j in range(3):\n",
    "            text = ax5.text(j, i, f'{improvements[i, j]:.1f}',\n",
    "                          ha=\"center\", va=\"center\", color=\"black\", fontsize=7)\n",
    "\n",
    "# ============================================================================\n",
    "# 6. CUMULATIVE DISTRIBUTION\n",
    "# ============================================================================\n",
    "ax6 = fig.add_subplot(gs[2, 1])\n",
    "for model_type in ['baseline', 'static', 'adaptive', 'halting']:\n",
    "    if model_type in results_dfs:\n",
    "        sorted_acc = np.sort(results_dfs[model_type]['accuracy'].values)\n",
    "        cumulative = np.arange(1, len(sorted_acc) + 1) / len(sorted_acc)\n",
    "        ax6.plot(sorted_acc, cumulative, marker='o', linewidth=2,\n",
    "                label=model_type.upper(), color=colors[model_type], markersize=4)\n",
    "\n",
    "ax6.set_xlabel('Accuracy', fontweight='bold')\n",
    "ax6.set_ylabel('Cumulative Probability', fontweight='bold')\n",
    "ax6.set_title('Cumulative Distribution Function', fontweight='bold', pad=10)\n",
    "ax6.legend(loc='lower right')\n",
    "ax6.grid(True, alpha=0.3, linestyle='--')\n",
    "ax6.set_xlim([0.5, 1.0])\n",
    "\n",
    "# ============================================================================\n",
    "# 7. ACCURACY RANGE (Min-Mean-Max)\n",
    "# ============================================================================\n",
    "ax7 = fig.add_subplot(gs[2, 2])\n",
    "methods_list = []\n",
    "means_list = []\n",
    "mins_list = []\n",
    "maxs_list = []\n",
    "\n",
    "for model_type in ['baseline', 'static', 'adaptive', 'halting']:\n",
    "    if model_type in results_dfs:\n",
    "        methods_list.append(model_type.upper())\n",
    "        means_list.append(results_dfs[model_type]['accuracy'].mean())\n",
    "        mins_list.append(results_dfs[model_type]['accuracy'].min())\n",
    "        maxs_list.append(results_dfs[model_type]['accuracy'].max())\n",
    "\n",
    "x_pos = np.arange(len(methods_list))\n",
    "errors_low = np.array(means_list) - np.array(mins_list)\n",
    "errors_high = np.array(maxs_list) - np.array(means_list)\n",
    "\n",
    "for i, (method, mean_val) in enumerate(zip(methods_list, means_list)):\n",
    "    model_type_lower = method.lower()\n",
    "    ax7.errorbar(i, mean_val, yerr=[[errors_low[i]], [errors_high[i]]],\n",
    "                fmt='o', markersize=12, capsize=8, capthick=2,\n",
    "                color=colors[model_type_lower], linewidth=2, label=method)\n",
    "    ax7.text(i, mean_val + 0.02, f'{mean_val:.3f}',\n",
    "            ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
    "\n",
    "ax7.set_xticks(x_pos)\n",
    "ax7.set_xticklabels(methods_list)\n",
    "ax7.set_ylabel('Accuracy', fontweight='bold')\n",
    "ax7.set_title('Mean ± (Min-Max Range)', fontweight='bold', pad=10)\n",
    "ax7.grid(True, alpha=0.3, axis='y', linestyle='--')\n",
    "ax7.set_ylim([0.5, 1.0])\n",
    "\n",
    "# ============================================================================\n",
    "# 8. STATISTICAL SIGNIFICANCE TABLE\n",
    "# ============================================================================\n",
    "ax8 = fig.add_subplot(gs[3, 0])\n",
    "ax8.axis('tight')\n",
    "ax8.axis('off')\n",
    "\n",
    "stats_data = []\n",
    "for model_type in ['baseline', 'static', 'adaptive', 'halting']:\n",
    "    if model_type in results_dfs:\n",
    "        df = results_dfs[model_type]\n",
    "        stats_data.append([\n",
    "            model_type.upper(),\n",
    "            f\"{df['accuracy'].mean():.4f}\",\n",
    "            f\"{df['accuracy'].std():.4f}\",\n",
    "            f\"{df['accuracy'].min():.4f}\",\n",
    "            f\"{df['accuracy'].max():.4f}\",\n",
    "            f\"{df['accuracy'].median():.4f}\"\n",
    "        ])\n",
    "\n",
    "table = ax8.table(cellText=stats_data,\n",
    "                 colLabels=['Method', 'Mean', 'Std', 'Min', 'Max', 'Median'],\n",
    "                 cellLoc='center', loc='center',\n",
    "                 colWidths=[0.15, 0.15, 0.15, 0.15, 0.15, 0.15])\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1, 2.5)\n",
    "\n",
    "# Color header\n",
    "for i in range(6):\n",
    "    table[(0, i)].set_facecolor('#3498db')\n",
    "    table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "\n",
    "# Color rows\n",
    "for i, model in enumerate(['baseline', 'static', 'adaptive', 'halting']):\n",
    "    if i < len(stats_data):\n",
    "        table[(i+1, 0)].set_facecolor(colors[model])\n",
    "        table[(i+1, 0)].set_text_props(weight='bold', color='white')\n",
    "\n",
    "ax8.set_title('Statistical Summary', fontweight='bold', y=0.85, fontsize=12)\n",
    "\n",
    "# ============================================================================\n",
    "# 9. IMPROVEMENT SUMMARY BAR CHART\n",
    "# ============================================================================\n",
    "ax9 = fig.add_subplot(gs[3, 1])\n",
    "if all(m in results_dfs for m in ['baseline', 'static', 'adaptive', 'halting']):\n",
    "    baseline_mean = results_dfs['baseline']['accuracy'].mean()\n",
    "    improvements_pct = []\n",
    "    method_names = []\n",
    "\n",
    "    for model in ['static', 'adaptive', 'halting']:\n",
    "        improvement = (results_dfs[model]['accuracy'].mean() - baseline_mean) * 100\n",
    "        improvements_pct.append(improvement)\n",
    "        method_names.append(model.upper())\n",
    "\n",
    "    bar_colors = [colors[m.lower()] for m in method_names]\n",
    "    bars = ax9.bar(range(len(method_names)), improvements_pct, color=bar_colors,\n",
    "                  alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "\n",
    "    # Add value labels\n",
    "    for i, (bar, val) in enumerate(zip(bars, improvements_pct)):\n",
    "        height = bar.get_height()\n",
    "        ax9.text(bar.get_x() + bar.get_width()/2., height + 0.1 if height > 0 else height - 0.1,\n",
    "                f'{val:+.2f}%', ha='center', va='bottom' if height > 0 else 'top',\n",
    "                fontweight='bold', fontsize=10)\n",
    "\n",
    "    ax9.set_xticks(range(len(method_names)))\n",
    "    ax9.set_xticklabels(method_names)\n",
    "    ax9.set_ylabel('Improvement over Baseline (%)', fontweight='bold')\n",
    "    ax9.set_title('Average Improvement vs Baseline', fontweight='bold', pad=10)\n",
    "    ax9.axhline(y=0, color='black', linestyle='-', linewidth=1)\n",
    "    ax9.grid(True, alpha=0.3, axis='y', linestyle='--')\n",
    "\n",
    "# ============================================================================\n",
    "# 10. CONSISTENCY ANALYSIS - Coefficient of Variation\n",
    "# ============================================================================\n",
    "ax10 = fig.add_subplot(gs[3, 2])\n",
    "cv_values = []\n",
    "method_names_cv = []\n",
    "\n",
    "for model_type in ['baseline', 'static', 'adaptive', 'halting']:\n",
    "    if model_type in results_dfs:\n",
    "        df = results_dfs[model_type]\n",
    "        # Coefficient of Variation = (std / mean) * 100\n",
    "        cv = (df['accuracy'].std() / df['accuracy'].mean()) * 100\n",
    "        cv_values.append(cv)\n",
    "        method_names_cv.append(model_type.upper())\n",
    "\n",
    "bar_colors_cv = [colors[m.lower()] for m in method_names_cv]\n",
    "bars = ax10.bar(range(len(method_names_cv)), cv_values, color=bar_colors_cv,\n",
    "               alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "\n",
    "# Add value labels\n",
    "for bar, val in zip(bars, cv_values):\n",
    "    height = bar.get_height()\n",
    "    ax10.text(bar.get_x() + bar.get_width()/2., height + 0.2,\n",
    "             f'{val:.2f}%', ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
    "\n",
    "ax10.set_xticks(range(len(method_names_cv)))\n",
    "ax10.set_xticklabels(method_names_cv)\n",
    "ax10.set_ylabel('Coefficient of Variation (%)', fontweight='bold')\n",
    "ax10.set_title('Consistency Across Subjects (Lower = More Consistent)', fontweight='bold', pad=10)\n",
    "ax10.grid(True, alpha=0.3, axis='y', linestyle='--')\n",
    "\n",
    "# ============================================================================\n",
    "# Overall title\n",
    "# ============================================================================\n",
    "plt.suptitle('PhysioNet Motor Imagery - Comprehensive Gating Methods Analysis',\n",
    "            fontsize=18, fontweight='bold', y=0.995)\n",
    "\n",
    "plt.savefig(CONFIG['output']['results_dir'] / 'gating_methods_comprehensive.png',\n",
    "           dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Enhanced visualization complete!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# BONUS: Channel Importance Analysis (if gate values available)\n",
    "# ============================================================================\n",
    "print(\"\\nGenerating channel importance visualizations...\")\n",
    "\n",
    "fig2, axes2 = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "for idx, model_type in enumerate(['baseline', 'static', 'adaptive', 'halting']):\n",
    "    ax = axes2[idx // 2, idx % 2]\n",
    "\n",
    "    if model_type in ['static', 'adaptive', 'halting']:\n",
    "        # Get average gate values across subjects\n",
    "        gate_values_all = []\n",
    "        for result in all_results[model_type]:\n",
    "            if result['gate_values'] is not None:\n",
    "                gate_values_all.append(result['gate_values'])\n",
    "\n",
    "        if gate_values_all:\n",
    "            avg_gates = np.mean(gate_values_all, axis=0)\n",
    "            channel_names_list = all_results[model_type][0]['channel_names']\n",
    "\n",
    "            # Sort by importance\n",
    "            sorted_idx = np.argsort(avg_gates)[::-1]\n",
    "            top_20_idx = sorted_idx[:20]\n",
    "\n",
    "            ax.barh(range(20), avg_gates[top_20_idx], color=colors[model_type], alpha=0.7)\n",
    "            ax.set_yticks(range(20))\n",
    "            ax.set_yticklabels([channel_names_list[i] for i in top_20_idx])\n",
    "            ax.set_xlabel('Gate Value / Importance', fontweight='bold')\n",
    "            ax.set_title(f'{model_type.upper()} - Top 20 Channels', fontweight='bold')\n",
    "            ax.grid(True, alpha=0.3, axis='x')\n",
    "            ax.invert_yaxis()\n",
    "\n",
    "            # Add threshold line\n",
    "            if model_type == 'static':\n",
    "                ax.axvline(x=0.5, color='red', linestyle='--', alpha=0.5, label='Threshold')\n",
    "                ax.legend()\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, 'No gating (all channels equally important)',\n",
    "               ha='center', va='center', fontsize=14, transform=ax.transAxes)\n",
    "        ax.set_title(f'{model_type.upper()}', fontweight='bold')\n",
    "        ax.axis('off')\n",
    "\n",
    "plt.suptitle('Channel Importance Analysis', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(CONFIG['output']['results_dir'] / 'channel_importance.png',\n",
    "           dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Channel importance visualizations complete!\")\n",
    "print(f\"\\nAll visualizations saved to: {CONFIG['output']['results_dir']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "print(\"Dataset Information:\")\n",
    "print(f\"  - Total subjects processed: {len(subjects)}\")\n",
    "print(f\"  - Excluded subjects: {len(EXCLUDED_SUBJECTS)}\")\n",
    "print(f\"  - Task runs used: {len(ALL_TASK_RUNS)}\")\n",
    "print(f\"  - Selected classes: {CONFIG['data']['selected_classes']}\")\n",
    "\n",
    "print(\"\\nTraining Configuration:\")\n",
    "print(f\"  - Folds: {CONFIG['model']['n_folds']}\")\n",
    "print(f\"  - Epochs: {CONFIG['model']['epochs']}\")\n",
    "print(f\"  - Batch size: {CONFIG['model']['batch_size']}\")\n",
    "print(f\"  - Learning rate: {CONFIG['model']['learning_rate']}\")\n",
    "\n",
    "print(\"\\nMethod Performance:\")\n",
    "for model_type in ['baseline', 'static', 'adaptive', 'halting']:\n",
    "    if model_type in results_dfs:\n",
    "        df = results_dfs[model_type]\n",
    "        print(f\"  {model_type.upper()}:\")\n",
    "        print(f\"    Mean accuracy: {df['accuracy'].mean():.4f} ± {df['accuracy'].std():.4f}\")\n",
    "        print(f\"    Range: [{df['accuracy'].min():.4f}, {df['accuracy'].max():.4f}]\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"All results saved to:\", CONFIG['output']['results_dir'])\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
