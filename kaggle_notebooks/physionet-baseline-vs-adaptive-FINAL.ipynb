{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline vs Adaptive Gating: Comprehensive Comparison\n",
    "\n",
    "## Final Draft - Publication Ready\n",
    "\n",
    "**Objective:** Compare Baseline EEG-ARNN against Adaptive Gating EEG-ARNN comprehensively\n",
    "\n",
    "**Dataset:** PhysioNet Motor Movement/Imagery Dataset\n",
    "\n",
    "**Configuration:**\n",
    "- **Subjects:** 50 (clean data only)\n",
    "- **Epochs:** 30 per fold\n",
    "- **Classes:** 2 (Left Fist vs Right Fist)\n",
    "- **Cross-validation:** 3-fold stratified\n",
    "- **Channel Selection:** k=[10, 15, 20, 25, 30, 35, 40]\n",
    "\n",
    "**Methods Compared:**\n",
    "1. **Baseline EEG-ARNN** - Pure CNN + Graph Convolution (reference implementation)\n",
    "2. **Adaptive Gating EEG-ARNN** - Input-dependent channel gating\n",
    "\n",
    "**Comprehensive Metrics:**\n",
    "- Accuracy, Precision, Recall, F1-Score\n",
    "- Confusion Matrix\n",
    "- ROC-AUC, PR-AUC\n",
    "- Cohen's Kappa\n",
    "- Training time, Convergence analysis\n",
    "- Channel importance analysis\n",
    "- Statistical significance tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import warnings\n",
    "import time\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report,\n",
    "    roc_auc_score, roc_curve, precision_recall_curve,\n",
    "    average_precision_score, cohen_kappa_score\n",
    ")\n",
    "from scipy import stats\n",
    "\n",
    "import mne\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set publication-quality plotting\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_context('paper', font_scale=1.2)\n",
    "mne.set_log_level('WARNING')\n",
    "\n",
    "def set_seed(s=42):\n",
    "    random.seed(s)\n",
    "    np.random.seed(s)\n",
    "    torch.manual_seed(s)\n",
    "    torch.cuda.manual_seed_all(s)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"BASELINE VS ADAPTIVE GATING - COMPREHENSIVE COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nDevice: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Auto-detect environment\n",
    "if os.path.exists('/kaggle/input'):\n",
    "    print(\"Running on Kaggle\")\n",
    "    kaggle_input = Path('/kaggle/input')\n",
    "    datasets = [d for d in kaggle_input.iterdir() if d.is_dir()]\n",
    "    DATA_DIR = None\n",
    "    for ds_name in ['physioneteegmi', 'eeg-motor-movementimagery-dataset']:\n",
    "        test_path = kaggle_input / ds_name\n",
    "        if test_path.exists():\n",
    "            DATA_DIR = test_path\n",
    "            break\n",
    "    if DATA_DIR is None and datasets:\n",
    "        DATA_DIR = datasets[0]\n",
    "else:\n",
    "    print(\"Running locally\")\n",
    "    DATA_DIR = Path('data/physionet/files')\n",
    "\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "\n",
    "# Configuration\n",
    "CONFIG = {\n",
    "    'experiment_name': 'baseline_vs_adaptive_final',\n",
    "    'data': {\n",
    "        'raw_data_dir': DATA_DIR,\n",
    "        'selected_classes': [1, 2],  # T1 (left fist), T2 (right fist)\n",
    "        'tmin': -1.0,\n",
    "        'tmax': 5.0,\n",
    "        'baseline': (-0.5, 0)\n",
    "    },\n",
    "    'preprocessing': {\n",
    "        'l_freq': 0.5,\n",
    "        'h_freq': 40.0,\n",
    "        'notch_freq': 50.0,\n",
    "        'target_sfreq': 128.0,\n",
    "        'apply_car': True\n",
    "    },\n",
    "    'model': {\n",
    "        'hidden_dim': 40,\n",
    "        'epochs': 30,  # FINAL: 30 epochs\n",
    "        'learning_rate': 1e-3,\n",
    "        'batch_size': 64,\n",
    "        'n_folds': 3,\n",
    "        'patience': 10  # Increased for 30 epochs\n",
    "    },\n",
    "    'gating': {\n",
    "        'l1_lambda': 1e-3,\n",
    "        'gate_init': 0.9\n",
    "    },\n",
    "    'channel_selection': {\n",
    "        'k_values': [10, 15, 20, 25, 30, 35, 40]\n",
    "    },\n",
    "    'output': {\n",
    "        'results_dir': Path('results/baseline_vs_adaptive_final'),\n",
    "    },\n",
    "    'max_subjects': 50,  # FINAL: 50 subjects\n",
    "    'min_runs_per_subject': 8,\n",
    "    'save_models': True,\n",
    "    'save_predictions': True\n",
    "}\n",
    "\n",
    "# Create output directories\n",
    "CONFIG['output']['results_dir'].mkdir(exist_ok=True, parents=True)\n",
    "(CONFIG['output']['results_dir'] / 'figures').mkdir(exist_ok=True)\n",
    "(CONFIG['output']['results_dir'] / 'tables').mkdir(exist_ok=True)\n",
    "(CONFIG['output']['results_dir'] / 'models').mkdir(exist_ok=True)\n",
    "\n",
    "# Known bad subjects (from data cleaning)\n",
    "EXCLUDED_SUBJECTS = set([\n",
    "    'S003', 'S004', 'S009', 'S010', 'S012', 'S013', 'S017', 'S018', 'S019',\n",
    "    'S021', 'S022', 'S023', 'S024', 'S025', 'S026', 'S027', 'S028', 'S029',\n",
    "    'S088', 'S089', 'S092', 'S100', 'S104', 'S106', 'S107', 'S108', 'S109'\n",
    "])\n",
    "\n",
    "# Motor imagery and execution runs\n",
    "MOTOR_IMAGERY_RUNS = ['R07', 'R08', 'R09', 'R10', 'R11', 'R12', 'R13', 'R14']\n",
    "MOTOR_EXECUTION_RUNS = ['R03', 'R04', 'R05', 'R06']\n",
    "ALL_TASK_RUNS = MOTOR_IMAGERY_RUNS + MOTOR_EXECUTION_RUNS\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXPERIMENT CONFIGURATION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Experiment: {CONFIG['experiment_name']}\")\n",
    "print(f\"Target subjects: {CONFIG['max_subjects']}\")\n",
    "print(f\"Epochs per fold: {CONFIG['model']['epochs']}\")\n",
    "print(f\"Cross-validation folds: {CONFIG['model']['n_folds']}\")\n",
    "print(f\"Channel selection k values: {CONFIG['channel_selection']['k_values']}\")\n",
    "print(f\"Excluded subjects: {len(EXCLUDED_SUBJECTS)}\")\n",
    "print(f\"Task runs: {len(ALL_TASK_RUNS)} ({', '.join(ALL_TASK_RUNS)})\")\n",
    "print(f\"\\nEstimated total experiments:\")\n",
    "print(f\"  Initial training: {CONFIG['max_subjects']} subjects × 2 methods = {CONFIG['max_subjects'] * 2}\")\n",
    "print(f\"  Retraining: {CONFIG['max_subjects']} subjects × 2 methods × {len(CONFIG['channel_selection']['k_values'])} k × 2 selections = {CONFIG['max_subjects'] * 2 * len(CONFIG['channel_selection']['k_values']) * 2}\")\n",
    "print(f\"  Total: {CONFIG['max_subjects'] * 2 + CONFIG['max_subjects'] * 2 * len(CONFIG['channel_selection']['k_values']) * 2} experiments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading Functions\n",
    "\n",
    "Same preprocessing pipeline as reference implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_raw(raw, config):\n",
    "    \"\"\"Apply preprocessing to raw EEG data.\"\"\"\n",
    "    cleaned_names = {name: name.rstrip('.') for name in raw.ch_names}\n",
    "    raw.rename_channels(cleaned_names)\n",
    "    raw.pick_types(eeg=True)\n",
    "    raw.set_montage('standard_1020', on_missing='ignore', match_case=False)\n",
    "    \n",
    "    nyquist = raw.info['sfreq'] / 2.0\n",
    "    if config['preprocessing']['notch_freq'] < nyquist:\n",
    "        raw.notch_filter(freqs=config['preprocessing']['notch_freq'], verbose=False)\n",
    "    \n",
    "    raw.filter(\n",
    "        l_freq=config['preprocessing']['l_freq'],\n",
    "        h_freq=config['preprocessing']['h_freq'],\n",
    "        method='fir',\n",
    "        fir_design='firwin',\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    if config['preprocessing']['apply_car']:\n",
    "        raw.set_eeg_reference('average', projection=False, verbose=False)\n",
    "    \n",
    "    raw.resample(config['preprocessing']['target_sfreq'], npad='auto', verbose=False)\n",
    "    return raw\n",
    "\n",
    "\n",
    "def load_and_preprocess_edf(edf_path, config):\n",
    "    \"\"\"Load raw EDF file, preprocess it, and extract epochs.\"\"\"\n",
    "    raw = mne.io.read_raw_edf(edf_path, preload=True, verbose='ERROR')\n",
    "    raw = preprocess_raw(raw, config)\n",
    "    events, event_ids = mne.events_from_annotations(raw, verbose='ERROR')\n",
    "    \n",
    "    if len(events) == 0:\n",
    "        return None, None, raw.ch_names\n",
    "    \n",
    "    epochs = mne.Epochs(\n",
    "        raw, events, event_id=event_ids,\n",
    "        tmin=config['data']['tmin'], tmax=config['data']['tmax'],\n",
    "        baseline=tuple(config['data']['baseline']),\n",
    "        preload=True, verbose='ERROR'\n",
    "    )\n",
    "    \n",
    "    return epochs.get_data(), epochs.events[:, 2], raw.ch_names\n",
    "\n",
    "\n",
    "def filter_classes(x, y, selected_classes):\n",
    "    \"\"\"Filter to keep only selected classes and remap labels.\"\"\"\n",
    "    mask = np.isin(y, selected_classes)\n",
    "    y, x = y[mask], x[mask]\n",
    "    label_map = {old: new for new, old in enumerate(sorted(selected_classes))}\n",
    "    y = np.array([label_map[int(label)] for label in y], dtype=np.int64)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def normalize(x):\n",
    "    \"\"\"Z-score normalization per channel.\"\"\"\n",
    "    mu = x.mean(axis=(0, 2), keepdims=True)\n",
    "    sd = x.std(axis=(0, 2), keepdims=True) + 1e-8\n",
    "    return (x - mu) / sd\n",
    "\n",
    "\n",
    "def load_subject_data(data_dir, subject_id, run_ids, config):\n",
    "    \"\"\"Load all runs for a subject, preprocess, and concatenate.\"\"\"\n",
    "    subject_dir = data_dir / subject_id\n",
    "    if not subject_dir.exists():\n",
    "        return None, None, None\n",
    "    \n",
    "    all_x, all_y = [], []\n",
    "    channel_names = None\n",
    "    \n",
    "    for run_id in run_ids:\n",
    "        edf_path = subject_dir / f'{subject_id}{run_id}.edf'\n",
    "        if not edf_path.exists():\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            x, y, ch_names = load_and_preprocess_edf(edf_path, config)\n",
    "            if x is None or len(y) == 0:\n",
    "                continue\n",
    "            \n",
    "            x, y = filter_classes(x, y, config['data']['selected_classes'])\n",
    "            if len(y) == 0:\n",
    "                continue\n",
    "            \n",
    "            channel_names = channel_names or ch_names\n",
    "            all_x.append(x)\n",
    "            all_y.append(y)\n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    if len(all_x) == 0:\n",
    "        return None, None, channel_names\n",
    "    \n",
    "    return np.concatenate(all_x, 0), np.concatenate(all_y, 0), channel_names\n",
    "\n",
    "\n",
    "def get_available_subjects(data_dir, min_runs=8, excluded=None):\n",
    "    \"\"\"Get list of subjects with at least min_runs available, excluding bad subjects.\"\"\"\n",
    "    if not data_dir.exists():\n",
    "        raise ValueError(f\"Data directory not found: {data_dir}\")\n",
    "    \n",
    "    excluded = excluded or set()\n",
    "    subjects = []\n",
    "    \n",
    "    for subject_dir in sorted(data_dir.iterdir()):\n",
    "        if not subject_dir.is_dir() or not subject_dir.name.startswith('S'):\n",
    "            continue\n",
    "        if subject_dir.name in excluded:\n",
    "            continue\n",
    "        edf_files = list(subject_dir.glob('*.edf'))\n",
    "        if len(edf_files) >= min_runs:\n",
    "            subjects.append(subject_dir.name)\n",
    "    \n",
    "    return subjects\n",
    "\n",
    "\n",
    "# Scan for subjects\n",
    "print(\"\\nScanning for available subjects...\")\n",
    "all_subjects = get_available_subjects(\n",
    "    CONFIG['data']['raw_data_dir'],\n",
    "    min_runs=CONFIG['min_runs_per_subject'],\n",
    "    excluded=EXCLUDED_SUBJECTS\n",
    ")\n",
    "subjects = all_subjects[:CONFIG['max_subjects']]\n",
    "\n",
    "print(f\"Found {len(all_subjects)} clean subjects with >= {CONFIG['min_runs_per_subject']} runs\")\n",
    "print(f\"Will process: {len(subjects)} subjects\")\n",
    "print(f\"Subjects: {', '.join(subjects[:10])}{'...' if len(subjects) > 10 else ''}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = torch.FloatTensor(x).unsqueeze(1)\n",
    "        self.y = torch.LongTensor(y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return self.x[i], self.y[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Architectures\n",
    "\n",
    "### 5.1 Baseline EEG-ARNN (Reference Implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvLayer(nn.Module):\n",
    "    \"\"\"Graph Convolution Layer with learned adjacency.\"\"\"\n",
    "    def __init__(self, num_channels, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.num_channels = num_channels\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.A = nn.Parameter(torch.randn(num_channels, num_channels))\n",
    "        self.theta = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(hidden_dim)\n",
    "        self.act = nn.ELU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, H, C, T = x.shape\n",
    "        A = torch.sigmoid(self.A)\n",
    "        A = 0.5 * (A + A.t())\n",
    "        I = torch.eye(C, device=A.device)\n",
    "        A_hat = A + I\n",
    "        D = torch.diag(torch.pow(A_hat.sum(1).clamp_min(1e-6), -0.5))\n",
    "        A_norm = D @ A_hat @ D\n",
    "        \n",
    "        x_batch = x.permute(0, 3, 2, 1).contiguous().view(B*T, C, H)\n",
    "        x_g = A_norm @ x_batch\n",
    "        x_g = self.theta(x_g)\n",
    "        x_g = x_g.view(B, T, C, H).permute(0, 3, 2, 1)\n",
    "        \n",
    "        return self.act(self.bn(x_g))\n",
    "    \n",
    "    def get_adjacency(self):\n",
    "        with torch.no_grad():\n",
    "            A = torch.sigmoid(self.A)\n",
    "            A = 0.5 * (A + A.t())\n",
    "            return A.cpu().numpy()\n",
    "\n",
    "\n",
    "class TemporalConv(nn.Module):\n",
    "    \"\"\"Temporal Convolution Layer.\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=16, pool=True):\n",
    "        super().__init__()\n",
    "        self.pool = pool\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, \n",
    "                            kernel_size=(1, kernel_size), \n",
    "                            padding=(0, kernel_size//2), bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.act = nn.ELU()\n",
    "        self.pool_layer = nn.AvgPool2d(kernel_size=(1, 2)) if pool else None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.act(self.bn(self.conv(x)))\n",
    "        return self.pool_layer(x) if self.pool else x\n",
    "\n",
    "\n",
    "class BaselineEEGARNN(nn.Module):\n",
    "    \"\"\"Baseline EEG-ARNN - Reference Implementation.\"\"\"\n",
    "    def __init__(self, C, T, K, H):\n",
    "        super().__init__()\n",
    "        self.t1 = TemporalConv(1, H, 16, False)\n",
    "        self.g1 = GraphConvLayer(C, H)\n",
    "        self.t2 = TemporalConv(H, H, 16, True)\n",
    "        self.g2 = GraphConvLayer(C, H)\n",
    "        self.t3 = TemporalConv(H, H, 16, True)\n",
    "        self.g3 = GraphConvLayer(C, H)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            ft = self._forward_features(torch.zeros(1, 1, C, T))\n",
    "            fs = ft.view(1, -1).size(1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(fs, 256)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(256, K)\n",
    "    \n",
    "    def _forward_features(self, x):\n",
    "        x = self.g1(self.t1(x))\n",
    "        x = self.g2(self.t2(x))\n",
    "        x = self.g3(self.t3(x))\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self._forward_features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        return self.fc2(x)\n",
    "    \n",
    "    def get_final_adjacency(self):\n",
    "        return self.g3.get_adjacency()\n",
    "\n",
    "\n",
    "print(\"Baseline EEG-ARNN defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Adaptive Gating EEG-ARNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveGatedEEGARNN(BaselineEEGARNN):\n",
    "    \"\"\"EEG-ARNN with adaptive input-dependent channel gates.\"\"\"\n",
    "    def __init__(self, C, T, K, H, gate_init=0.9):\n",
    "        super().__init__(C, T, K, H)\n",
    "        \n",
    "        # Gate network: 2-layer MLP\n",
    "        self.gate_net = nn.Sequential(\n",
    "            nn.Linear(C * 2, C),  # Input: mean + std per channel\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(C, C),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Initialize to start with high gate values\n",
    "        with torch.no_grad():\n",
    "            self.gate_net[-2].bias.fill_(2.0)\n",
    "        \n",
    "        self.latest_gates = None\n",
    "    \n",
    "    def compute_gates(self, x):\n",
    "        \"\"\"Compute input-dependent gates.\"\"\"\n",
    "        B, _, C, T = x.shape\n",
    "        x_squeeze = x.squeeze(1)\n",
    "        ch_mean = x_squeeze.mean(dim=2)\n",
    "        ch_std = x_squeeze.std(dim=2)\n",
    "        stats = torch.cat([ch_mean, ch_std], dim=1)\n",
    "        gates = self.gate_net(stats)\n",
    "        return gates\n",
    "    \n",
    "    def forward(self, x):\n",
    "        gates = self.compute_gates(x)\n",
    "        self.latest_gates = gates.detach().cpu()\n",
    "        x = x * gates.view(-1, 1, gates.size(1), 1)\n",
    "        return super().forward(x)\n",
    "    \n",
    "    def get_gate_values(self):\n",
    "        if self.latest_gates is not None:\n",
    "            return self.latest_gates.mean(dim=0)\n",
    "        return None\n",
    "\n",
    "\n",
    "print(\"Adaptive Gating EEG-ARNN defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Functions with Comprehensive Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(y_true, y_pred, y_prob=None):\n",
    "    \"\"\"Compute comprehensive evaluation metrics.\"\"\"\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'precision': precision_score(y_true, y_pred, average='weighted', zero_division=0),\n",
    "        'recall': recall_score(y_true, y_pred, average='weighted', zero_division=0),\n",
    "        'f1': f1_score(y_true, y_pred, average='weighted', zero_division=0),\n",
    "        'kappa': cohen_kappa_score(y_true, y_pred)\n",
    "    }\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    metrics['confusion_matrix'] = cm\n",
    "    \n",
    "    # ROC-AUC and PR-AUC if probabilities provided\n",
    "    if y_prob is not None:\n",
    "        try:\n",
    "            if len(np.unique(y_true)) == 2:  # Binary classification\n",
    "                metrics['roc_auc'] = roc_auc_score(y_true, y_prob[:, 1])\n",
    "                metrics['pr_auc'] = average_precision_score(y_true, y_prob[:, 1])\n",
    "            else:\n",
    "                metrics['roc_auc'] = roc_auc_score(y_true, y_prob, average='weighted', multi_class='ovr')\n",
    "                metrics['pr_auc'] = average_precision_score(y_true, y_prob, average='weighted')\n",
    "        except:\n",
    "            metrics['roc_auc'] = None\n",
    "            metrics['pr_auc'] = None\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device, l1_lambda=0.0):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    all_preds, all_labels, all_probs = [], [], []\n",
    "    \n",
    "    for x, y in dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        \n",
    "        # L1 regularization for adaptive gating\n",
    "        if l1_lambda > 0 and hasattr(model, 'get_gate_values'):\n",
    "            gate_values = model.get_gate_values()\n",
    "            if gate_values is not None:\n",
    "                loss = loss + l1_lambda * gate_values.abs().mean()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        all_probs.append(probs.detach().cpu().numpy())\n",
    "        all_preds += torch.argmax(logits, 1).cpu().tolist()\n",
    "        all_labels += y.cpu().tolist()\n",
    "    \n",
    "    all_probs = np.vstack(all_probs)\n",
    "    metrics = compute_metrics(all_labels, all_preds, all_probs)\n",
    "    metrics['loss'] = total_loss / max(1, len(dataloader))\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    \"\"\"Evaluate model.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_preds, all_labels, all_probs = [], [], []\n",
    "    \n",
    "    for x, y in dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        all_probs.append(probs.cpu().numpy())\n",
    "        all_preds += torch.argmax(logits, 1).cpu().tolist()\n",
    "        all_labels += y.cpu().tolist()\n",
    "    \n",
    "    all_probs = np.vstack(all_probs)\n",
    "    metrics = compute_metrics(all_labels, all_preds, all_probs)\n",
    "    metrics['loss'] = total_loss / max(1, len(dataloader))\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, device, epochs, lr, patience, l1_lambda=0.0):\n",
    "    \"\"\"Train model with early stopping and return training history.\"\"\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=3, verbose=False\n",
    "    )\n",
    "    \n",
    "    best_acc = 0.0\n",
    "    best_state = None\n",
    "    no_improve = 0\n",
    "    history = {'train': [], 'val': []}\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train_metrics = train_epoch(model, train_loader, criterion, optimizer, device, l1_lambda)\n",
    "        val_metrics = evaluate(model, val_loader, criterion, device)\n",
    "        \n",
    "        history['train'].append(train_metrics)\n",
    "        history['val'].append(val_metrics)\n",
    "        \n",
    "        scheduler.step(val_metrics['loss'])\n",
    "        \n",
    "        if val_metrics['accuracy'] > best_acc:\n",
    "            best_acc = val_metrics['accuracy']\n",
    "            best_state = deepcopy(model.state_dict())\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "        \n",
    "        if no_improve >= patience:\n",
    "            break\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    if best_state is None:\n",
    "        best_state = deepcopy(model.state_dict())\n",
    "    \n",
    "    model.load_state_dict(best_state)\n",
    "    return best_state, best_acc, history, training_time\n",
    "\n",
    "\n",
    "print(\"Training functions with comprehensive metrics defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cross-Validation with Full Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_subject(x, y, channel_names, T, K, device, config, model_type='baseline'):\n",
    "    \"\"\"Cross-validate subject with comprehensive metrics tracking.\"\"\"\n",
    "    C = x.shape[1]\n",
    "    skf = StratifiedKFold(n_splits=config['model']['n_folds'], shuffle=True, random_state=42)\n",
    "    \n",
    "    batch_size = config['model']['batch_size']\n",
    "    epochs = config['model']['epochs']\n",
    "    lr = config['model']['learning_rate']\n",
    "    patience = config['model']['patience']\n",
    "    \n",
    "    folds = []\n",
    "    adjacencies = []\n",
    "    gate_values_list = []\n",
    "    all_histories = []\n",
    "    total_training_time = 0\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(x, y)):\n",
    "        X_train, X_val = normalize(x[train_idx]), normalize(x[val_idx])\n",
    "        Y_train, Y_val = y[train_idx], y[val_idx]\n",
    "        \n",
    "        train_loader = DataLoader(\n",
    "            EEGDataset(X_train, Y_train),\n",
    "            batch_size=batch_size, shuffle=True, num_workers=0\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            EEGDataset(X_val, Y_val),\n",
    "            batch_size=batch_size, shuffle=False, num_workers=0\n",
    "        )\n",
    "        \n",
    "        # Create model\n",
    "        if model_type == 'baseline':\n",
    "            model = BaselineEEGARNN(C, T, K, config['model']['hidden_dim']).to(device)\n",
    "            l1_lambda = 0.0\n",
    "        elif model_type == 'adaptive':\n",
    "            model = AdaptiveGatedEEGARNN(C, T, K, config['model']['hidden_dim'],\n",
    "                                        config['gating']['gate_init']).to(device)\n",
    "            l1_lambda = config['gating']['l1_lambda']\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "        \n",
    "        # Train\n",
    "        best_state, best_acc, history, train_time = train_model(\n",
    "            model, train_loader, val_loader, device, epochs, lr, patience, l1_lambda\n",
    "        )\n",
    "        model.load_state_dict(best_state)\n",
    "        \n",
    "        # Final evaluation\n",
    "        final_metrics = evaluate(model, val_loader, nn.CrossEntropyLoss(), device)\n",
    "        final_metrics['fold'] = fold\n",
    "        final_metrics['training_time'] = train_time\n",
    "        final_metrics['epochs_trained'] = len(history['val'])\n",
    "        \n",
    "        folds.append(final_metrics)\n",
    "        all_histories.append(history)\n",
    "        total_training_time += train_time\n",
    "        \n",
    "        # Store adjacency and gates\n",
    "        adjacency = model.get_final_adjacency()\n",
    "        adjacencies.append(adjacency)\n",
    "        \n",
    "        if hasattr(model, 'get_gate_values'):\n",
    "            gate_values = model.get_gate_values()\n",
    "            if gate_values is not None:\n",
    "                if isinstance(gate_values, torch.Tensor):\n",
    "                    gate_values = gate_values.detach().cpu().numpy()\n",
    "                gate_values_list.append(gate_values)\n",
    "    \n",
    "    # Aggregate metrics\n",
    "    avg_metrics = {}\n",
    "    for key in ['accuracy', 'precision', 'recall', 'f1', 'kappa', 'roc_auc', 'pr_auc']:\n",
    "        values = [f[key] for f in folds if f.get(key) is not None]\n",
    "        if values:\n",
    "            avg_metrics[f'avg_{key}'] = float(np.mean(values))\n",
    "            avg_metrics[f'std_{key}'] = float(np.std(values))\n",
    "    \n",
    "    avg_adjacency = np.mean(np.stack([a for a in adjacencies if a is not None], 0), 0) \\\n",
    "                    if any(a is not None for a in adjacencies) else None\n",
    "    \n",
    "    result = {\n",
    "        'fold_results': folds,\n",
    "        'training_histories': all_histories,\n",
    "        'total_training_time': total_training_time,\n",
    "        'avg_training_time_per_fold': total_training_time / len(folds),\n",
    "        'adjacency_matrix': avg_adjacency,\n",
    "        'channel_names': channel_names,\n",
    "        **avg_metrics\n",
    "    }\n",
    "    \n",
    "    if gate_values_list:\n",
    "        result['avg_gate_values'] = np.mean(np.stack(gate_values_list, 0), 0)\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "print(\"Cross-validation with full metrics defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Channel Selection Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelSelector:\n",
    "    \"\"\"Select channels from trained model using different strategies.\"\"\"\n",
    "    \n",
    "    def __init__(self, adjacency, channel_names, gate_values=None):\n",
    "        self.A = adjacency\n",
    "        self.names = np.array(channel_names)\n",
    "        self.C = adjacency.shape[0]\n",
    "        self.gate_values = gate_values\n",
    "    \n",
    "    def edge_selection(self, k):\n",
    "        \"\"\"Edge Selection (ES).\"\"\"\n",
    "        edge_importance = np.zeros(self.C)\n",
    "        for i in range(self.C):\n",
    "            for j in range(self.C):\n",
    "                if i != j:\n",
    "                    edge_importance[i] += abs(self.A[i, j])\n",
    "                    edge_importance[j] += abs(self.A[i, j])\n",
    "        indices = np.sort(np.argsort(edge_importance)[-int(k):])\n",
    "        return self.names[indices].tolist(), indices\n",
    "    \n",
    "    def aggregation_selection(self, k):\n",
    "        \"\"\"Aggregation Selection (AS).\"\"\"\n",
    "        agg_scores = np.sum(np.abs(self.A), 1)\n",
    "        indices = np.sort(np.argsort(agg_scores)[-int(k):])\n",
    "        return self.names[indices].tolist(), indices\n",
    "    \n",
    "    def gate_selection(self, k):\n",
    "        \"\"\"Gate Selection (GS) - for adaptive gating only.\"\"\"\n",
    "        if self.gate_values is None:\n",
    "            raise ValueError(\"Gate values not available. Use ES or AS instead.\")\n",
    "        indices = np.sort(np.argsort(self.gate_values)[-int(k):])\n",
    "        return self.names[indices].tolist(), indices\n",
    "\n",
    "\n",
    "def retrain_with_selected_channels(x, y, selected_indices, T, K, device, config, model_type='baseline'):\n",
    "    \"\"\"Retrain model with selected channels and return metrics.\"\"\"\n",
    "    x_selected = x[:, selected_indices, :]\n",
    "    C = len(selected_indices)\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=config['model']['n_folds'], shuffle=True, random_state=42)\n",
    "    \n",
    "    batch_size = config['model']['batch_size']\n",
    "    epochs = config['model']['epochs']\n",
    "    lr = config['model']['learning_rate']\n",
    "    patience = config['model']['patience']\n",
    "    \n",
    "    fold_metrics = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(x_selected, y)):\n",
    "        X_train = normalize(x_selected[train_idx])\n",
    "        X_val = normalize(x_selected[val_idx])\n",
    "        Y_train, Y_val = y[train_idx], y[val_idx]\n",
    "        \n",
    "        train_loader = DataLoader(\n",
    "            EEGDataset(X_train, Y_train),\n",
    "            batch_size=batch_size, shuffle=True, num_workers=0\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            EEGDataset(X_val, Y_val),\n",
    "            batch_size=batch_size, shuffle=False, num_workers=0\n",
    "        )\n",
    "        \n",
    "        if model_type == 'baseline':\n",
    "            model = BaselineEEGARNN(C, T, K, config['model']['hidden_dim']).to(device)\n",
    "            l1_lambda = 0.0\n",
    "        elif model_type == 'adaptive':\n",
    "            model = AdaptiveGatedEEGARNN(C, T, K, config['model']['hidden_dim'],\n",
    "                                        config['gating']['gate_init']).to(device)\n",
    "            l1_lambda = config['gating']['l1_lambda']\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "        \n",
    "        best_state, best_acc, history, train_time = train_model(\n",
    "            model, train_loader, val_loader, device, epochs, lr, patience, l1_lambda\n",
    "        )\n",
    "        model.load_state_dict(best_state)\n",
    "        \n",
    "        metrics = evaluate(model, val_loader, nn.CrossEntropyLoss(), device)\n",
    "        fold_metrics.append(metrics)\n",
    "    \n",
    "    # Aggregate\n",
    "    avg_metrics = {}\n",
    "    for key in ['accuracy', 'precision', 'recall', 'f1', 'kappa', 'roc_auc', 'pr_auc']:\n",
    "        values = [m[key] for m in fold_metrics if m.get(key) is not None]\n",
    "        if values:\n",
    "            avg_metrics[f'avg_{key}'] = float(np.mean(values))\n",
    "            avg_metrics[f'std_{key}'] = float(np.std(values))\n",
    "    \n",
    "    return avg_metrics\n",
    "\n",
    "\n",
    "print(\"Channel selection functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Main Training Loop\n",
    "\n",
    "### Training both methods on all subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storage for results\n",
    "all_results = {'baseline': [], 'adaptive': []}\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MAIN TRAINING - BASELINE VS ADAPTIVE GATING\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Training {len(subjects)} subjects with 2 methods...\\n\")\n",
    "\n",
    "for subject_idx, subject_id in enumerate(tqdm(subjects, desc='Processing subjects')):\n",
    "    print(f\"\\n[{subject_idx+1}/{len(subjects)}] Processing {subject_id}...\")\n",
    "    \n",
    "    # Load subject data\n",
    "    X, Y, channel_names = load_subject_data(\n",
    "        CONFIG['data']['raw_data_dir'],\n",
    "        subject_id,\n",
    "        ALL_TASK_RUNS,\n",
    "        CONFIG\n",
    "    )\n",
    "    \n",
    "    if X is None or len(Y) == 0:\n",
    "        print(f\"  Skipped: No data available\")\n",
    "        continue\n",
    "    \n",
    "    C, T = X.shape[1], X.shape[2]\n",
    "    K = len(set(CONFIG['data']['selected_classes']))\n",
    "    \n",
    "    print(f\"  Data: {X.shape[0]} trials, {C} channels, {T} timepoints\")\n",
    "    print(f\"  Class distribution: {np.bincount(Y)}\")\n",
    "    \n",
    "    # Train both methods\n",
    "    for method in ['baseline', 'adaptive']:\n",
    "        print(f\"\\n  Training {method.upper()}...\")\n",
    "        result = cross_validate_subject(X, Y, channel_names, T, K, device, CONFIG, method)\n",
    "        \n",
    "        print(f\"    Accuracy: {result['avg_accuracy']:.4f} ± {result['std_accuracy']:.4f}\")\n",
    "        print(f\"    F1-Score: {result['avg_f1']:.4f} ± {result['std_f1']:.4f}\")\n",
    "        if result.get('avg_roc_auc'):\n",
    "            print(f\"    ROC-AUC: {result['avg_roc_auc']:.4f} ± {result['std_roc_auc']:.4f}\")\n",
    "        print(f\"    Training time: {result['avg_training_time_per_fold']:.1f}s per fold\")\n",
    "        \n",
    "        # Store results\n",
    "        all_results[method].append({\n",
    "            'subject': subject_id,\n",
    "            'num_trials': X.shape[0],\n",
    "            'num_channels': C,\n",
    "            **{k: v for k, v in result.items() if k.startswith('avg_') or k.startswith('std_')},\n",
    "            'adjacency_matrix': result['adjacency_matrix'],\n",
    "            'channel_names': result['channel_names'],\n",
    "            'gate_values': result.get('avg_gate_values', None),\n",
    "            'training_time_per_fold': result['avg_training_time_per_fold'],\n",
    "            'total_training_time': result['total_training_time'],\n",
    "            'fold_results': result['fold_results'],\n",
    "            'training_histories': result['training_histories']\n",
    "        })\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"INITIAL TRAINING COMPLETE!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Channel Selection and Retraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storage for retraining results\n",
    "retrain_results = {'baseline': [], 'adaptive': []}\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CHANNEL SELECTION AND RETRAINING\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Testing {len(CONFIG['channel_selection']['k_values'])} different k values...\\n\")\n",
    "\n",
    "for subject_idx, subject_id in enumerate(tqdm(subjects, desc='Retraining subjects')):\n",
    "    print(f\"\\n[{subject_idx+1}/{len(subjects)}] Retraining {subject_id}...\")\n",
    "    \n",
    "    # Load subject data\n",
    "    X, Y, channel_names = load_subject_data(\n",
    "        CONFIG['data']['raw_data_dir'],\n",
    "        subject_id,\n",
    "        ALL_TASK_RUNS,\n",
    "        CONFIG\n",
    "    )\n",
    "    \n",
    "    if X is None:\n",
    "        continue\n",
    "    \n",
    "    C, T = X.shape[1], X.shape[2]\n",
    "    K = len(set(CONFIG['data']['selected_classes']))\n",
    "    \n",
    "    # Process both methods\n",
    "    for method in ['baseline', 'adaptive']:\n",
    "        # Find result for this subject\n",
    "        subj_result = None\n",
    "        for res in all_results[method]:\n",
    "            if res['subject'] == subject_id:\n",
    "                subj_result = res\n",
    "                break\n",
    "        \n",
    "        if subj_result is None:\n",
    "            continue\n",
    "        \n",
    "        adjacency = subj_result['adjacency_matrix']\n",
    "        gate_values = subj_result.get('gate_values', None)\n",
    "        selector = ChannelSelector(adjacency, channel_names, gate_values)\n",
    "        \n",
    "        # Determine selection methods\n",
    "        if method == 'baseline':\n",
    "            selection_methods = ['ES', 'AS']\n",
    "        else:\n",
    "            selection_methods = ['ES', 'AS', 'GS']\n",
    "        \n",
    "        # Test different k values\n",
    "        for sel_method in selection_methods:\n",
    "            for k in CONFIG['channel_selection']['k_values']:\n",
    "                # Select channels\n",
    "                if sel_method == 'ES':\n",
    "                    selected_channels, selected_indices = selector.edge_selection(k)\n",
    "                elif sel_method == 'AS':\n",
    "                    selected_channels, selected_indices = selector.aggregation_selection(k)\n",
    "                elif sel_method == 'GS':\n",
    "                    selected_channels, selected_indices = selector.gate_selection(k)\n",
    "                \n",
    "                # Retrain\n",
    "                retrain_metrics = retrain_with_selected_channels(\n",
    "                    X, Y, selected_indices, T, K, device, CONFIG, method\n",
    "                )\n",
    "                \n",
    "                # Compute drops\n",
    "                acc_drop = subj_result['avg_accuracy'] - retrain_metrics['avg_accuracy']\n",
    "                \n",
    "                retrain_results[method].append({\n",
    "                    'subject': subject_id,\n",
    "                    'method': sel_method,\n",
    "                    'k': k,\n",
    "                    'num_channels_selected': len(selected_channels),\n",
    "                    'selected_channels': selected_channels,\n",
    "                    **retrain_metrics,\n",
    "                    'full_channels_accuracy': subj_result['avg_accuracy'],\n",
    "                    'accuracy_drop': acc_drop,\n",
    "                    'accuracy_drop_pct': (acc_drop / subj_result['avg_accuracy'] * 100)\n",
    "                })\n",
    "                \n",
    "                print(f\"  {method.upper()}-{sel_method}, k={k}: \"\n",
    "                      f\"{retrain_metrics['avg_accuracy']:.4f} (drop: {acc_drop:.4f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RETRAINING COMPLETE!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save All Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "print(\"\\nSaving results...\")\n",
    "\n",
    "# Save main results\n",
    "for method in ['baseline', 'adaptive']:\n",
    "    # Create DataFrame\n",
    "    df_cols = ['subject', 'num_trials', 'num_channels']\n",
    "    df_cols += [k for k in all_results[method][0].keys() if k.startswith('avg_') or k.startswith('std_')]\n",
    "    df_cols += ['training_time_per_fold', 'total_training_time']\n",
    "    \n",
    "    df = pd.DataFrame([{k: res[k] for k in df_cols if k in res} for res in all_results[method]])\n",
    "    df.to_csv(CONFIG['output']['results_dir'] / 'tables' / f'{method}_results.csv', index=False)\n",
    "    \n",
    "    # Save full results with histories\n",
    "    with open(CONFIG['output']['results_dir'] / f'{method}_full_results.pkl', 'wb') as f:\n",
    "        pickle.dump(all_results[method], f)\n",
    "\n",
    "# Save retrain results\n",
    "for method in ['baseline', 'adaptive']:\n",
    "    df = pd.DataFrame(retrain_results[method])\n",
    "    df.to_csv(CONFIG['output']['results_dir'] / 'tables' / f'{method}_retrain_results.csv', index=False)\n",
    "\n",
    "print(f\"Results saved to: {CONFIG['output']['results_dir']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Comprehensive Visualizations\n",
    "\n",
    "### Figure 1: Overall Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO BE CONTINUED IN NEXT CELL..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
