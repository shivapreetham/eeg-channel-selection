{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2967738,"sourceType":"datasetVersion","datasetId":1819423}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# PhysioNet Motor Imagery - SparseGCN-CARM (FIXED)\n\n## Goal: Better Channel Selection using Learned Importance\n\n**Previous Best (CARMv2):**\n- 86.83% accuracy with all 64 channels\n- ES @ k=20: 0.71% drop (best)\n- AS @ k=20: 2.37% drop\n\n**SparseGCN-CARM Approach:**\n1. Train with ALL 64 channels + channel attention\n2. Learn channel importance explicitly during training\n3. Select top-k using importance scores\n4. Retrain with selected k channels\n5. Compare: Does learned importance beat edge/aggregation selection?\n\n**Key Innovations:**\n- Multi-head channel attention learns importance\n- Multi-scale temporal conv [8, 16, 32]\n- Feature-adaptive GCN from CARMv2\n- FOUR selection methods: IMP (importance), HYB (hybrid), ES (edges), AS (aggregation)","metadata":{}},{"cell_type":"markdown","source":"## 1. Setup and Imports","metadata":{}},{"cell_type":"code","source":"import json\nimport random\nimport warnings\nfrom pathlib import Path\nfrom copy import deepcopy\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm.auto import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\n\nimport mne\n\nwarnings.filterwarnings('ignore')\nsns.set_context('notebook', font_scale=1.0)\nmne.set_log_level('WARNING')\n\ndef set_seed(s=42):\n    random.seed(s)\n    np.random.seed(s)\n    torch.manual_seed(s)\n    torch.cuda.manual_seed_all(s)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nset_seed(42)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'Device: {device}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T18:11:29.493004Z","iopub.execute_input":"2025-11-01T18:11:29.493240Z","iopub.status.idle":"2025-11-01T18:11:36.979043Z","shell.execute_reply.started":"2025-11-01T18:11:29.493223Z","shell.execute_reply":"2025-11-01T18:11:36.978410Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## 2. Configuration","metadata":{}},{"cell_type":"code","source":"# Auto-detect Kaggle\nimport os\nif os.path.exists('/kaggle/input'):\n    print(\"Running on Kaggle\")\n    kaggle_input = Path('/kaggle/input')\n    datasets = [d for d in kaggle_input.iterdir() if d.is_dir()]\n    DATA_DIR = None\n    for ds_name in ['physioneteegmi', 'eeg-motor-movementimagery-dataset']:\n        test_path = kaggle_input / ds_name\n        if test_path.exists():\n            DATA_DIR = test_path\n            break\n    if DATA_DIR is None and datasets:\n        DATA_DIR = datasets[0]\nelse:\n    DATA_DIR = Path('data/physionet/files')\n\nCONFIG = {\n    'data': {\n        'raw_data_dir': DATA_DIR,\n        'selected_classes': [1, 2],\n        'tmin': -1.0,\n        'tmax': 5.0,\n        'baseline': (-0.5, 0)\n    },\n    'preprocessing': {\n        'l_freq': 0.5,\n        'h_freq': 40.0,\n        'notch_freq': 50.0,\n        'target_sfreq': 128.0,\n        'apply_car': True\n    },\n    'model': {\n        'hidden_dim': 40,\n        'epochs': 30,\n        'learning_rate': 1e-3,\n        'batch_size': 32,\n        'n_folds': 3,\n        'patience': 8\n    },\n    'sparsegcn': {\n        'topk_k': 8,\n        'lambda_feat': 0.3,\n        'hop_alpha': 0.5,\n        'edge_dropout': 0.1,\n        'use_pairnorm': True,\n        'use_residual': True,\n        'attention_heads': 4,\n        'temporal_scales': [8, 16, 32]\n    },\n    'channel_selection': {\n        'k_values': [10, 15, 20, 25]\n    },\n    'output': {\n        'results_dir': Path('results'),\n    },\n    'max_subjects': 20,\n    'min_runs_per_subject': 8\n}\n\nCONFIG['output']['results_dir'].mkdir(exist_ok=True, parents=True)\nprint(\"Configuration loaded!\")\nprint(f\"Training: {CONFIG['max_subjects']} subjects, {CONFIG['model']['n_folds']}-fold CV, {CONFIG['model']['epochs']} epochs\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T18:11:36.980251Z","iopub.execute_input":"2025-11-01T18:11:36.980677Z","iopub.status.idle":"2025-11-01T18:11:36.989276Z","shell.execute_reply.started":"2025-11-01T18:11:36.980651Z","shell.execute_reply":"2025-11-01T18:11:36.988455Z"}},"outputs":[{"name":"stdout","text":"Running on Kaggle\nConfiguration loaded!\nTraining: 20 subjects, 3-fold CV, 30 epochs\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## 3. Data Cleaning","metadata":{}},{"cell_type":"code","source":"KNOWN_BAD_SUBJECTS = ['S088', 'S089', 'S092', 'S100', 'S104', 'S106', 'S107', 'S108', 'S109']\nHIGH_ISSUE_SUBJECTS = ['S003', 'S004', 'S009', 'S010', 'S012', 'S013', 'S017', 'S018', 'S019',\n                       'S021', 'S022', 'S023', 'S024', 'S025', 'S026', 'S027', 'S028', 'S029']\nEXCLUDED_SUBJECTS = set(KNOWN_BAD_SUBJECTS + HIGH_ISSUE_SUBJECTS)\nprint(f\"Excluding {len(EXCLUDED_SUBJECTS)} faulty subjects\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T18:11:36.989889Z","iopub.execute_input":"2025-11-01T18:11:36.990076Z","iopub.status.idle":"2025-11-01T18:11:37.004863Z","shell.execute_reply.started":"2025-11-01T18:11:36.990060Z","shell.execute_reply":"2025-11-01T18:11:37.004188Z"}},"outputs":[{"name":"stdout","text":"Excluding 27 faulty subjects\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## 4. Data Loading","metadata":{}},{"cell_type":"code","source":"def preprocess_raw(raw, config):\n    cleaned_names = {name: name.rstrip('.') for name in raw.ch_names}\n    raw.rename_channels(cleaned_names)\n    raw.pick_types(eeg=True)\n    raw.set_montage('standard_1020', on_missing='ignore', match_case=False)\n    nyquist = raw.info['sfreq'] / 2.0\n    if config['preprocessing']['notch_freq'] < nyquist:\n        raw.notch_filter(freqs=config['preprocessing']['notch_freq'], verbose=False)\n    raw.filter(l_freq=config['preprocessing']['l_freq'], h_freq=config['preprocessing']['h_freq'],\n               method='fir', fir_design='firwin', verbose=False)\n    if config['preprocessing']['apply_car']:\n        raw.set_eeg_reference('average', projection=False, verbose=False)\n    raw.resample(config['preprocessing']['target_sfreq'], npad='auto', verbose=False)\n    return raw\n\ndef load_and_preprocess_edf(edf_path, config):\n    raw = mne.io.read_raw_edf(edf_path, preload=True, verbose='ERROR')\n    raw = preprocess_raw(raw, config)\n    try:\n        events = mne.find_events(raw, verbose='ERROR')\n        event_ids = {f'T{i}': i for i in np.unique(events[:, 2])}\n        assert len(events) > 0\n    except Exception:\n        events, event_ids = mne.events_from_annotations(raw, verbose='ERROR')\n    if len(events) == 0:\n        return None, None, raw.ch_names\n    epochs = mne.Epochs(raw, events, event_id=event_ids, tmin=config['data']['tmin'],\n                       tmax=config['data']['tmax'], baseline=tuple(config['data']['baseline']),\n                       preload=True, verbose='ERROR')\n    return epochs.get_data(), epochs.events[:, 2], raw.ch_names\n\ndef filter_classes(x, y, selected_classes):\n    mask = np.isin(y, selected_classes)\n    y, x = y[mask], x[mask]\n    label_map = {old: new for new, old in enumerate(sorted(selected_classes))}\n    y = np.array([label_map[int(label)] for label in y], dtype=np.int64)\n    return x, y\n\ndef normalize(x):\n    mu = x.mean(axis=(0, 2), keepdims=True)\n    sd = x.std(axis=(0, 2), keepdims=True) + 1e-8\n    return (x - mu) / sd\n\ndef load_subject_data(data_dir, subject_id, run_ids, config):\n    subject_dir = data_dir / subject_id\n    if not subject_dir.exists():\n        return None, None, None\n    all_x, all_y = [], []\n    channel_names = None\n    for run_id in run_ids:\n        edf_path = subject_dir / f'{subject_id}{run_id}.edf'\n        if not edf_path.exists():\n            continue\n        try:\n            x, y, ch_names = load_and_preprocess_edf(edf_path, config)\n            if x is None or len(y) == 0:\n                continue\n            x, y = filter_classes(x, y, config['data']['selected_classes'])\n            if len(y) == 0:\n                continue\n            channel_names = channel_names or ch_names\n            all_x.append(x)\n            all_y.append(y)\n        except Exception:\n            continue\n    if len(all_x) == 0:\n        return None, None, channel_names\n    return np.concatenate(all_x, 0), np.concatenate(all_y, 0), channel_names\n\ndef get_available_subjects(data_dir, min_runs=8, excluded=None):\n    if not data_dir.exists():\n        raise ValueError(f\"Data directory not found: {data_dir}\")\n    excluded = excluded or set()\n    subjects = []\n    for subject_dir in sorted(data_dir.iterdir()):\n        if not subject_dir.is_dir() or not subject_dir.name.startswith('S'):\n            continue\n        if subject_dir.name in excluded:\n            continue\n        edf_files = list(subject_dir.glob('*.edf'))\n        if len(edf_files) >= min_runs:\n            subjects.append(subject_dir.name)\n    return subjects\n\ndata_dir = CONFIG['data']['raw_data_dir']\nall_subjects = get_available_subjects(data_dir, CONFIG['min_runs_per_subject'], EXCLUDED_SUBJECTS)\nsubjects = all_subjects[:CONFIG['max_subjects']]\nprint(f\"Found {len(all_subjects)} clean subjects, will process {len(subjects)}\")\n\nMOTOR_IMAGERY_RUNS = ['R07', 'R08', 'R09', 'R10', 'R11', 'R12', 'R13', 'R14']\nMOTOR_EXECUTION_RUNS = ['R03', 'R04', 'R05', 'R06']\nALL_TASK_RUNS = MOTOR_IMAGERY_RUNS + MOTOR_EXECUTION_RUNS","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T18:11:37.006639Z","iopub.execute_input":"2025-11-01T18:11:37.007262Z","iopub.status.idle":"2025-11-01T18:11:37.578563Z","shell.execute_reply.started":"2025-11-01T18:11:37.007238Z","shell.execute_reply":"2025-11-01T18:11:37.577904Z"}},"outputs":[{"name":"stdout","text":"Found 82 clean subjects, will process 20\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"## 5. PyTorch Dataset","metadata":{}},{"cell_type":"code","source":"class EEGDataset(Dataset):\n    def __init__(self, x, y):\n        self.x = torch.FloatTensor(x).unsqueeze(1)\n        self.y = torch.LongTensor(y)\n    def __len__(self):\n        return len(self.y)\n    def __getitem__(self, i):\n        return self.x[i], self.y[i]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T18:11:37.579180Z","iopub.execute_input":"2025-11-01T18:11:37.579380Z","iopub.status.idle":"2025-11-01T18:11:37.583812Z","shell.execute_reply.started":"2025-11-01T18:11:37.579364Z","shell.execute_reply":"2025-11-01T18:11:37.583055Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## 6. SparseGCN-CARM Architecture\n\n### 6.1 Channel Attention\nLearns explicit channel importance scores","metadata":{}},{"cell_type":"code","source":"class ChannelAttention(nn.Module):\n    def __init__(self, num_channels, num_heads=4):\n        super().__init__()\n        self.num_channels = num_channels\n        self.num_heads = num_heads\n        self.head_dim = max(1, num_channels // num_heads)\n\n        self.channel_embed = nn.Parameter(torch.randn(num_channels, self.head_dim * num_heads))\n        nn.init.xavier_uniform_(self.channel_embed)\n\n        self.query = nn.Linear(self.head_dim * num_heads, self.head_dim * num_heads)\n        self.key = nn.Linear(self.head_dim * num_heads, self.head_dim * num_heads)\n        self.importance_proj = nn.Sequential(nn.Linear(self.head_dim * num_heads, 1), nn.Sigmoid())\n\n    def forward(self, x=None):\n        embed = self.channel_embed\n        Q = self.query(embed).view(self.num_channels, self.num_heads, self.head_dim)\n        K = self.key(embed).view(self.num_channels, self.num_heads, self.head_dim)\n\n        attn = torch.einsum('chd,khd->chk', Q, K) / np.sqrt(self.head_dim)\n        attn = F.softmax(attn, dim=-1)\n\n        attn_pooled = attn.mean(dim=1)\n        channel_scores = attn_pooled.sum(dim=1)\n        channel_scores = channel_scores / (channel_scores.sum() + 1e-8)\n\n        importance = self.importance_proj(embed).squeeze(-1)\n\n        final_importance = 0.5 * channel_scores + 0.5 * importance\n        final_importance = final_importance / (final_importance.sum() + 1e-8)\n\n        return final_importance, attn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T18:11:37.584498Z","iopub.execute_input":"2025-11-01T18:11:37.584730Z","iopub.status.idle":"2025-11-01T18:11:37.597813Z","shell.execute_reply.started":"2025-11-01T18:11:37.584710Z","shell.execute_reply":"2025-11-01T18:11:37.597120Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"### 6.2 Multi-Scale Temporal + GCN","metadata":{}},{"cell_type":"code","source":"class MultiScaleTemporalConv(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_sizes=[8, 16, 32], pool=True):\n        super().__init__()\n        self.pool = pool\n        k_num = len(kernel_sizes)\n        base = out_channels // k_num\n        rem = out_channels - base * k_num\n        out_list = [base + (1 if i < rem else 0) for i in range(k_num)]\n\n        branches = []\n        for i, k in enumerate(kernel_sizes):\n            oc = out_list[i]\n            branches.append(nn.Sequential(\n                nn.Conv2d(in_channels, oc, kernel_size=(1, k), padding=(0, k//2), bias=False),\n                nn.BatchNorm2d(oc),\n                nn.ELU()\n            ))\n        self.branches = nn.ModuleList(branches)\n        self.pool_layer = nn.AvgPool2d(kernel_size=(1, 2)) if pool else None\n\n    def forward(self, x):\n        branch_outputs = [branch(x) for branch in self.branches]\n        x = torch.cat(branch_outputs, dim=1)\n        return self.pool_layer(x) if self.pool else x\n\ndef pairnorm(x, node_dim=2, eps=1e-6):\n    m = x.mean(dim=node_dim, keepdim=True)\n    xc = x - m\n    v = (xc * xc).mean(dim=node_dim, keepdim=True)\n    return xc / torch.sqrt(v + eps)\n\ndef build_feat_topk_adj(x, k):\n    B, H, C, T = x.shape\n    E = x.permute(2, 1, 0, 3).contiguous().view(C, H, B*T).mean(2)\n    En = F.normalize(E, p=2, dim=1)\n    S = (En @ En.t()).clamp_min(0.0)\n    k = max(1, min(int(k), C))\n    vals, idx = torch.topk(S, k, dim=1)\n    M = torch.zeros_like(S)\n    M.scatter_(1, idx, 1.0)\n    A = S * M\n    A = torch.softmax(A, 1)\n    A = 0.5 * (A + A.t())\n    return A\n\nclass AdaptiveGCNLayer(nn.Module):\n    def __init__(self, C, H, topk_k=8, lambda_feat=0.3, hop_alpha=0.5, edge_dropout=0.1,\n                 use_pairnorm=True, use_residual=True):\n        super().__init__()\n        self.C, self.H, self.k, self.lf, self.ha, self.ed, self.pn, self.res =             C, H, topk_k, lambda_feat, hop_alpha, edge_dropout, use_pairnorm, use_residual\n        self.W = nn.Parameter(torch.empty(C, C))\n        nn.init.xavier_uniform_(self.W)\n        self.th = nn.Linear(H, H, bias=False)\n        self.bn = nn.BatchNorm2d(H)\n        self.act = nn.ELU()\n        self.last = None\n\n    def _learned(self, dev):\n        A = torch.sigmoid(self.W)\n        A = 0.5 * (A + A.t())\n        I = torch.eye(self.C, device=dev, dtype=A.dtype)\n        At = A + I\n        d = torch.pow(At.sum(1).clamp_min(1e-6), -0.5)\n        D = torch.diag(d)\n        return D @ At @ D\n\n    def forward(self, x):\n        B, H, C, T = x.shape\n        Al = self._learned(x.device)\n        A2 = Al @ Al\n        Ah = (1 - self.ha) * Al + self.ha * A2\n        Af = build_feat_topk_adj(x, self.k)\n        A = (1 - self.lf) * Ah + self.lf * Af\n\n        if self.training and self.ed > 0:\n            M = (torch.rand_like(A) > self.ed).float()\n            A = 0.5 * ((A * M) + (A * M).t())\n            A = A + torch.eye(C, device=A.device, dtype=A.dtype)\n\n        d = torch.pow(A.sum(1).clamp_min(1e-6), -0.5)\n        D = torch.diag(d)\n        A = D @ A @ D\n\n        xb = x.permute(0, 3, 2, 1).contiguous().view(B*T, C, H)\n        xg = A @ xb\n        xg = self.th(xg).view(B, T, C, H).permute(0, 3, 2, 1)\n\n        out = xg + x if self.res else xg\n        out = pairnorm(out, 2) if self.pn else out\n        out = self.bn(out)\n        out = self.act(out)\n\n        self.last = {'learned': Al.detach().cpu().numpy()}\n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T18:11:37.598627Z","iopub.execute_input":"2025-11-01T18:11:37.598930Z","iopub.status.idle":"2025-11-01T18:11:37.617125Z","shell.execute_reply.started":"2025-11-01T18:11:37.598913Z","shell.execute_reply":"2025-11-01T18:11:37.616390Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"### 6.3 Main Model (NO PRUNING!)","metadata":{}},{"cell_type":"code","source":"class SparseGCNCARMModel(nn.Module):\n    def __init__(self, C, T, K, H, config):\n        super().__init__()\n        self.C = C\n        self.channel_attention = ChannelAttention(C, config.get('attention_heads', 4))\n\n        scales = config.get('temporal_scales', [8, 16, 32])\n        self.t1 = MultiScaleTemporalConv(1, H, scales, False)\n        self.g1 = AdaptiveGCNLayer(C, H, config.get('topk_k', 8), config.get('lambda_feat', 0.3),\n                                   config.get('hop_alpha', 0.5), config.get('edge_dropout', 0.1),\n                                   config.get('use_pairnorm', True), config.get('use_residual', True))\n        self.t2 = MultiScaleTemporalConv(H, H, scales, True)\n        self.g2 = AdaptiveGCNLayer(C, H, config.get('topk_k', 8), config.get('lambda_feat', 0.3),\n                                   config.get('hop_alpha', 0.5), config.get('edge_dropout', 0.1),\n                                   config.get('use_pairnorm', True), config.get('use_residual', True))\n        self.t3 = MultiScaleTemporalConv(H, H, scales, True)\n        self.g3 = AdaptiveGCNLayer(C, H, config.get('topk_k', 8), config.get('lambda_feat', 0.3),\n                                   config.get('hop_alpha', 0.5), config.get('edge_dropout', 0.1),\n                                   config.get('use_pairnorm', True), config.get('use_residual', True))\n\n        with torch.no_grad():\n            ft = self._forward_features(torch.zeros(1, 1, C, T))\n            fs = ft.view(1, -1).size(1)\n\n        self.fc1 = nn.Linear(fs, 256)\n        self.dropout = nn.Dropout(0.5)\n        self.fc2 = nn.Linear(256, K)\n        self.channel_importance = None\n\n    def _forward_features(self, x):\n        x = self.g1(self.t1(x))\n        x = self.g2(self.t2(x))\n        x = self.g3(self.t3(x))\n        return x\n\n    def forward(self, x):\n        importance, attn = self.channel_attention(x)\n        self.channel_importance = importance\n        x = self._forward_features(x)\n        x = x.view(x.size(0), -1)\n        x = F.relu(self.fc1(x))\n        x = self.dropout(x)\n        return self.fc2(x)\n\n    def get_channel_importance(self):\n        return self.channel_importance.detach().cpu().numpy() if self.channel_importance is not None else None\n\n    def get_final_adjacency(self):\n        return self.g3.last.get('learned', None) if self.g3.last else None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T18:11:37.617799Z","iopub.execute_input":"2025-11-01T18:11:37.617984Z","iopub.status.idle":"2025-11-01T18:11:37.632332Z","shell.execute_reply.started":"2025-11-01T18:11:37.617969Z","shell.execute_reply":"2025-11-01T18:11:37.631725Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"## 7. Training Functions","metadata":{}},{"cell_type":"code","source":"@torch.no_grad()\ndef evaluate(model, dataloader, criterion, device):\n    model.eval()\n    total_loss = 0.0\n    all_preds, all_labels = [], []\n    for x, y in dataloader:\n        x, y = x.to(device), y.to(device)\n        logits = model(x)\n        loss = criterion(logits, y)\n        total_loss += loss.item()\n        all_preds += torch.argmax(logits, 1).cpu().tolist()\n        all_labels += y.cpu().tolist()\n    return total_loss / max(1, len(dataloader)), accuracy_score(all_labels, all_preds)\n\ndef train_model(model, train_loader, val_loader, device, epochs, lr, patience):\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=False)\n\n    best_acc = 0.0\n    best_state = None\n    no_improve = 0\n\n    for epoch in range(epochs):\n        model.train()\n        for x, y in train_loader:\n            x, y = x.to(device), y.to(device)\n            optimizer.zero_grad()\n            logits = model(x)\n            loss = criterion(logits, y)\n            loss.backward()\n            optimizer.step()\n\n        val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n        scheduler.step(val_loss)\n\n        if val_acc > best_acc:\n            best_acc = val_acc\n            best_state = deepcopy(model.state_dict())\n            no_improve = 0\n        else:\n            no_improve += 1\n\n        if no_improve >= patience:\n            break\n\n    if best_state is not None:\n        model.load_state_dict(best_state)\n    return best_state, best_acc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T18:11:37.633056Z","iopub.execute_input":"2025-11-01T18:11:37.633326Z","iopub.status.idle":"2025-11-01T18:11:37.647292Z","shell.execute_reply.started":"2025-11-01T18:11:37.633304Z","shell.execute_reply":"2025-11-01T18:11:37.646690Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"## 8. Channel Selection Methods","metadata":{}},{"cell_type":"code","source":"class ImprovedChannelSelector:\n    def __init__(self, adjacency, importance_scores, channel_names):\n        self.A = adjacency\n        self.importance = importance_scores\n        self.names = np.array(channel_names)\n        self.C = adjacency.shape[0]\n\n    def importance_selection(self, k):\n        \"\"\"IMP: Select channels by learned importance scores.\"\"\"\n        k = min(int(k), self.C)\n        indices = np.argsort(self.importance)[-k:]\n        indices = np.sort(indices)\n        return self.names[indices].tolist(), indices\n\n    def hybrid_selection(self, k):\n        \"\"\"HYB: Combine importance (70%) + connectivity (30%).\"\"\"\n        connectivity = np.sum(np.abs(self.A), 1)\n        combined_score = 0.7 * self.importance + 0.3 * (connectivity / connectivity.max())\n        k = min(int(k), self.C)\n        indices = np.argsort(combined_score)[-k:]\n        indices = np.sort(indices)\n        return self.names[indices].tolist(), indices\n\n    def edge_selection(self, k):\n        \"\"\"ES: Select channels based on edge importance (like comparison notebook).\"\"\"\n        edges = []\n        for i in range(self.C):\n            for j in range(i+1, self.C):\n                edges.append((i, j, abs(self.A[i, j]) + abs(self.A[j, i])))\n        edges.sort(key=lambda t: t[2], reverse=True)\n        top_edges = edges[:int(k)]\n        indices = sorted(set([i for i, _, _ in top_edges] + [j for _, j, _ in top_edges]))\n        return self.names[indices].tolist(), np.array(indices)\n\n    def aggregation_selection(self, k):\n        \"\"\"AS: Select channels based on aggregated connectivity (like comparison notebook).\"\"\"\n        scores = np.sum(np.abs(self.A), 1)\n        indices = np.sort(np.argsort(scores)[-int(k):])\n        return self.names[indices].tolist(), indices","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T18:11:37.649057Z","iopub.execute_input":"2025-11-01T18:11:37.649323Z","iopub.status.idle":"2025-11-01T18:11:37.661397Z","shell.execute_reply.started":"2025-11-01T18:11:37.649309Z","shell.execute_reply":"2025-11-01T18:11:37.660739Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"## 9. Main Training Loop","metadata":{}},{"cell_type":"code","source":"all_results = []\n\nfor subject_id in tqdm(subjects, desc='Training SparseGCN'):\n    print(f\"\\nProcessing {subject_id}...\")\n\n    X, Y, channel_names = load_subject_data(data_dir, subject_id, ALL_TASK_RUNS, CONFIG)\n    if X is None:\n        continue\n\n    C, T = X.shape[1], X.shape[2]\n    K = len(set(CONFIG['data']['selected_classes']))\n\n    skf = StratifiedKFold(n_splits=CONFIG['model']['n_folds'], shuffle=True, random_state=42)\n    fold_results = []\n    adjacencies = []\n    importance_scores = []\n\n    for fold, (train_idx, val_idx) in enumerate(skf.split(X, Y)):\n        X_train, X_val = normalize(X[train_idx]), normalize(X[val_idx])\n        Y_train, Y_val = Y[train_idx], Y[val_idx]\n\n        train_loader = DataLoader(EEGDataset(X_train, Y_train), batch_size=CONFIG['model']['batch_size'],\n                                 shuffle=True, num_workers=0)\n        val_loader = DataLoader(EEGDataset(X_val, Y_val), batch_size=CONFIG['model']['batch_size'],\n                               shuffle=False, num_workers=0)\n\n        model = SparseGCNCARMModel(C, T, K, CONFIG['model']['hidden_dim'], CONFIG['sparsegcn']).to(device)\n\n        best_state, best_acc = train_model(model, train_loader, val_loader, device,\n                                          CONFIG['model']['epochs'], CONFIG['model']['learning_rate'],\n                                          CONFIG['model']['patience'])\n\n        model.load_state_dict(best_state)\n        _, accuracy = evaluate(model, val_loader, nn.CrossEntropyLoss(), device)\n\n        adjacency = model.get_final_adjacency()\n        importance = model.get_channel_importance()\n\n        adjacencies.append(adjacency)\n        importance_scores.append(importance)\n        fold_results.append({'fold': fold, 'val_acc': accuracy})\n\n        print(f\"  Fold {fold+1}: {accuracy:.4f}\")\n\n    avg_acc = np.mean([f['val_acc'] for f in fold_results])\n\n    all_results.append({\n        'subject': subject_id,\n        'accuracy': avg_acc,\n        'adjacency_matrix': np.mean(adjacencies, 0),\n        'channel_importance': np.mean(importance_scores, 0),\n        'channel_names': channel_names\n    })\n\n    print(f\"  Final: {avg_acc:.4f}\")\n\nprint(\"\\nTraining complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T18:11:37.661933Z","iopub.execute_input":"2025-11-01T18:11:37.662121Z","iopub.status.idle":"2025-11-01T19:50:16.850198Z","shell.execute_reply.started":"2025-11-01T18:11:37.662107Z","shell.execute_reply":"2025-11-01T19:50:16.849361Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Training SparseGCN:   0%|          | 0/20 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f322e533ec743fcaae28ac70e5a948a"}},"metadata":{}},{"name":"stdout","text":"\nProcessing S001...\n  Fold 1: 0.9286\n  Fold 2: 0.8929\n  Fold 3: 0.8929\n  Final: 0.9048\n\nProcessing S002...\n  Fold 1: 0.8452\n  Fold 2: 0.7857\n  Fold 3: 0.7738\n  Final: 0.8016\n\nProcessing S005...\n  Fold 1: 0.8333\n  Fold 2: 0.8333\n  Fold 3: 0.8690\n  Final: 0.8452\n\nProcessing S006...\n  Fold 1: 0.8214\n  Fold 2: 0.8810\n  Fold 3: 0.8571\n  Final: 0.8532\n\nProcessing S007...\n  Fold 1: 0.9405\n  Fold 2: 0.8690\n  Fold 3: 0.9643\n  Final: 0.9246\n\nProcessing S008...\n  Fold 1: 0.9524\n  Fold 2: 0.9524\n  Fold 3: 0.9405\n  Final: 0.9484\n\nProcessing S011...\n  Fold 1: 0.8571\n  Fold 2: 0.8452\n  Fold 3: 0.8214\n  Final: 0.8413\n\nProcessing S014...\n  Fold 1: 0.8095\n  Fold 2: 0.8333\n  Fold 3: 0.8452\n  Final: 0.8294\n\nProcessing S015...\n  Fold 1: 0.9048\n  Fold 2: 0.9286\n  Fold 3: 0.9167\n  Final: 0.9167\n\nProcessing S016...\n  Fold 1: 0.7262\n  Fold 2: 0.8214\n  Fold 3: 0.7262\n  Final: 0.7579\n\nProcessing S020...\n  Fold 1: 0.9643\n  Fold 2: 0.9762\n  Fold 3: 0.9405\n  Final: 0.9603\n\nProcessing S030...\n  Fold 1: 0.9286\n  Fold 2: 0.8452\n  Fold 3: 0.9286\n  Final: 0.9008\n\nProcessing S031...\n  Fold 1: 0.8333\n  Fold 2: 0.8690\n  Fold 3: 0.9167\n  Final: 0.8730\n\nProcessing S032...\n  Fold 1: 0.9405\n  Fold 2: 0.9524\n  Fold 3: 0.9405\n  Final: 0.9444\n\nProcessing S033...\n  Fold 1: 0.9524\n  Fold 2: 0.9048\n  Fold 3: 0.9524\n  Final: 0.9365\n\nProcessing S034...\n  Fold 1: 0.9167\n  Fold 2: 0.8690\n  Fold 3: 0.9048\n  Final: 0.8968\n\nProcessing S035...\n  Fold 1: 0.9286\n  Fold 2: 0.9405\n  Fold 3: 0.9048\n  Final: 0.9246\n\nProcessing S036...\n  Fold 1: 0.9048\n  Fold 2: 0.8690\n  Fold 3: 0.8571\n  Final: 0.8770\n\nProcessing S037...\n  Fold 1: 0.8214\n  Fold 2: 0.8690\n  Fold 3: 0.8214\n  Final: 0.8373\n\nProcessing S038...\n  Fold 1: 0.7619\n  Fold 2: 0.8452\n  Fold 3: 0.7738\n  Final: 0.7937\n\nTraining complete!\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"## 10. Results","metadata":{}},{"cell_type":"code","source":"results_df = pd.DataFrame(all_results)\nprint(\"SparseGCN-CARM Results (full 64 channels):\")\nprint(f\"Mean accuracy: {results_df['accuracy'].mean():.4f}\")\nprint(f\"Std accuracy: {results_df['accuracy'].std():.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T19:50:16.851104Z","iopub.execute_input":"2025-11-01T19:50:16.851599Z","iopub.status.idle":"2025-11-01T19:50:16.862642Z","shell.execute_reply.started":"2025-11-01T19:50:16.851580Z","shell.execute_reply":"2025-11-01T19:50:16.862028Z"}},"outputs":[{"name":"stdout","text":"SparseGCN-CARM Results (full 64 channels):\nMean accuracy: 0.8784\nStd accuracy: 0.0569\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"## 11. Channel Selection Experiments","metadata":{}},{"cell_type":"code","source":"k_values = CONFIG['channel_selection']['k_values']\nselection_results = []\n\nfor result in tqdm(all_results, desc='Channel Selection'):\n    subject_id = result['subject']\n    full_acc = result['accuracy']\n    adjacency = result['adjacency_matrix']\n    importance = result['channel_importance']\n    channel_names = result['channel_names']\n\n    X, Y, _ = load_subject_data(data_dir, subject_id, ALL_TASK_RUNS, CONFIG)\n    if X is None:\n        continue\n\n    C, T = X.shape[1], X.shape[2]\n    K = len(set(CONFIG['data']['selected_classes']))\n\n    selector = ImprovedChannelSelector(adjacency, importance, channel_names)\n\n    for k in k_values:\n        for method_name, method_func in [('IMP', selector.importance_selection),\n                                         ('HYB', selector.hybrid_selection),\n                                         ('ES', selector.edge_selection),\n                                         ('AS', selector.aggregation_selection)]:\n            selected_names, selected_indices = method_func(k)\n            X_subset = X[:, selected_indices, :]\n\n            skf = StratifiedKFold(n_splits=CONFIG['model']['n_folds'], shuffle=True, random_state=42)\n            fold_accs = []\n\n            for fold, (train_idx, val_idx) in enumerate(skf.split(X_subset, Y)):\n                X_train = normalize(X_subset[train_idx])\n                X_val = normalize(X_subset[val_idx])\n                Y_train, Y_val = Y[train_idx], Y[val_idx]\n\n                train_loader = DataLoader(EEGDataset(X_train, Y_train),\n                                        batch_size=CONFIG['model']['batch_size'], shuffle=True, num_workers=0)\n                val_loader = DataLoader(EEGDataset(X_val, Y_val),\n                                       batch_size=CONFIG['model']['batch_size'], shuffle=False, num_workers=0)\n\n                model = SparseGCNCARMModel(len(selected_indices), T, K,\n                                          CONFIG['model']['hidden_dim'], CONFIG['sparsegcn']).to(device)\n\n                best_state, best_acc = train_model(model, train_loader, val_loader, device,\n                                                  CONFIG['model']['epochs'], CONFIG['model']['learning_rate'],\n                                                  CONFIG['model']['patience'])\n\n                model.load_state_dict(best_state)\n                _, accuracy = evaluate(model, val_loader, nn.CrossEntropyLoss(), device)\n                fold_accs.append(accuracy)\n\n            subset_acc = np.mean(fold_accs)\n            drop = full_acc - subset_acc\n\n            selection_results.append({\n                'subject': subject_id,\n                'method': method_name,\n                'k': k,\n                'full_acc': full_acc,\n                'subset_acc': subset_acc,\n                'drop': drop,\n                'channels': ','.join(selected_names)\n            })\n\n            print(f\"{subject_id} {method_name} k={k}: {subset_acc:.4f} (drop: {drop:.4f})\")\n\nselection_df = pd.DataFrame(selection_results)\nprint(\"\\nChannel selection complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T19:50:16.863216Z","iopub.execute_input":"2025-11-01T19:50:16.863376Z","execution_failed":"2025-11-01T23:09:23.253Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Channel Selection:   0%|          | 0/20 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df8add1abc7440648608c139e5fec6c2"}},"metadata":{}},{"name":"stdout","text":"S001 IMP k=10: 0.8770 (drop: 0.0278)\nS001 HYB k=10: 0.8690 (drop: 0.0357)\nS001 ES k=10: 0.8889 (drop: 0.0159)\nS001 AS k=10: 0.8929 (drop: 0.0119)\nS001 IMP k=15: 0.8968 (drop: 0.0079)\nS001 HYB k=15: 0.8889 (drop: 0.0159)\nS001 ES k=15: 0.9365 (drop: -0.0317)\nS001 AS k=15: 0.8889 (drop: 0.0159)\nS001 IMP k=20: 0.9127 (drop: -0.0079)\nS001 HYB k=20: 0.9048 (drop: 0.0000)\nS001 ES k=20: 0.8889 (drop: 0.0159)\nS001 AS k=20: 0.9087 (drop: -0.0040)\nS001 IMP k=25: 0.9127 (drop: -0.0079)\nS001 HYB k=25: 0.9127 (drop: -0.0079)\nS001 ES k=25: 0.9048 (drop: 0.0000)\nS001 AS k=25: 0.9167 (drop: -0.0119)\nS002 IMP k=10: 0.8056 (drop: -0.0040)\nS002 HYB k=10: 0.7778 (drop: 0.0238)\nS002 ES k=10: 0.7778 (drop: 0.0238)\nS002 AS k=10: 0.7778 (drop: 0.0238)\nS002 IMP k=15: 0.8214 (drop: -0.0198)\nS002 HYB k=15: 0.8056 (drop: -0.0040)\nS002 ES k=15: 0.7698 (drop: 0.0317)\nS002 AS k=15: 0.7738 (drop: 0.0278)\nS002 IMP k=20: 0.7421 (drop: 0.0595)\nS002 HYB k=20: 0.7937 (drop: 0.0079)\nS002 ES k=20: 0.8254 (drop: -0.0238)\nS002 AS k=20: 0.7778 (drop: 0.0238)\nS002 IMP k=25: 0.7698 (drop: 0.0317)\nS002 HYB k=25: 0.7976 (drop: 0.0040)\nS002 ES k=25: 0.7897 (drop: 0.0119)\nS002 AS k=25: 0.8056 (drop: -0.0040)\nS005 IMP k=10: 0.8254 (drop: 0.0198)\nS005 HYB k=10: 0.8175 (drop: 0.0278)\nS005 ES k=10: 0.8056 (drop: 0.0397)\nS005 AS k=10: 0.7659 (drop: 0.0794)\nS005 IMP k=15: 0.8214 (drop: 0.0238)\nS005 HYB k=15: 0.8254 (drop: 0.0198)\nS005 ES k=15: 0.7698 (drop: 0.0754)\nS005 AS k=15: 0.7897 (drop: 0.0556)\nS005 IMP k=20: 0.8175 (drop: 0.0278)\nS005 HYB k=20: 0.8016 (drop: 0.0437)\nS005 ES k=20: 0.7738 (drop: 0.0714)\nS005 AS k=20: 0.8532 (drop: -0.0079)\nS005 IMP k=25: 0.8532 (drop: -0.0079)\nS005 HYB k=25: 0.8413 (drop: 0.0040)\nS005 ES k=25: 0.7817 (drop: 0.0635)\nS005 AS k=25: 0.8849 (drop: -0.0397)\nS006 IMP k=10: 0.7540 (drop: 0.0992)\nS006 HYB k=10: 0.7619 (drop: 0.0913)\nS006 ES k=10: 0.7738 (drop: 0.0794)\nS006 AS k=10: 0.7183 (drop: 0.1349)\nS006 IMP k=15: 0.7738 (drop: 0.0794)\nS006 HYB k=15: 0.7460 (drop: 0.1071)\nS006 ES k=15: 0.7460 (drop: 0.1071)\nS006 AS k=15: 0.7659 (drop: 0.0873)\nS006 IMP k=20: 0.7738 (drop: 0.0794)\nS006 HYB k=20: 0.7659 (drop: 0.0873)\nS006 ES k=20: 0.7698 (drop: 0.0833)\nS006 AS k=20: 0.7500 (drop: 0.1032)\nS006 IMP k=25: 0.7817 (drop: 0.0714)\nS006 HYB k=25: 0.7857 (drop: 0.0675)\nS006 ES k=25: 0.7817 (drop: 0.0714)\nS006 AS k=25: 0.8532 (drop: 0.0000)\nS007 IMP k=10: 0.8889 (drop: 0.0357)\nS007 HYB k=10: 0.8968 (drop: 0.0278)\nS007 ES k=10: 0.9048 (drop: 0.0198)\nS007 AS k=10: 0.9048 (drop: 0.0198)\nS007 IMP k=15: 0.9206 (drop: 0.0040)\nS007 HYB k=15: 0.9167 (drop: 0.0079)\nS007 ES k=15: 0.9206 (drop: 0.0040)\nS007 AS k=15: 0.9286 (drop: -0.0040)\nS007 IMP k=20: 0.8968 (drop: 0.0278)\nS007 HYB k=20: 0.9286 (drop: -0.0040)\nS007 ES k=20: 0.9167 (drop: 0.0079)\nS007 AS k=20: 0.9206 (drop: 0.0040)\nS007 IMP k=25: 0.9206 (drop: 0.0040)\nS007 HYB k=25: 0.9206 (drop: 0.0040)\nS007 ES k=25: 0.9206 (drop: 0.0040)\nS007 AS k=25: 0.9206 (drop: 0.0040)\nS008 IMP k=10: 0.9087 (drop: 0.0397)\nS008 HYB k=10: 0.8849 (drop: 0.0635)\nS008 ES k=10: 0.9246 (drop: 0.0238)\nS008 AS k=10: 0.8889 (drop: 0.0595)\nS008 IMP k=15: 0.9167 (drop: 0.0317)\nS008 HYB k=15: 0.8929 (drop: 0.0556)\nS008 ES k=15: 0.9206 (drop: 0.0278)\nS008 AS k=15: 0.9087 (drop: 0.0397)\nS008 IMP k=20: 0.9127 (drop: 0.0357)\nS008 HYB k=20: 0.9048 (drop: 0.0437)\nS008 ES k=20: 0.9524 (drop: -0.0040)\nS008 AS k=20: 0.9048 (drop: 0.0437)\nS008 IMP k=25: 0.9008 (drop: 0.0476)\nS008 HYB k=25: 0.9444 (drop: 0.0040)\nS008 ES k=25: 0.9484 (drop: -0.0000)\nS008 AS k=25: 0.9286 (drop: 0.0198)\nS011 IMP k=10: 0.8532 (drop: -0.0119)\nS011 HYB k=10: 0.8254 (drop: 0.0159)\nS011 ES k=10: 0.8571 (drop: -0.0159)\nS011 AS k=10: 0.8373 (drop: 0.0040)\nS011 IMP k=15: 0.8770 (drop: -0.0357)\nS011 HYB k=15: 0.8492 (drop: -0.0079)\nS011 ES k=15: 0.8849 (drop: -0.0437)\nS011 AS k=15: 0.8611 (drop: -0.0198)\nS011 IMP k=20: 0.8333 (drop: 0.0079)\nS011 HYB k=20: 0.8413 (drop: 0.0000)\nS011 ES k=20: 0.8651 (drop: -0.0238)\nS011 AS k=20: 0.8532 (drop: -0.0119)\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"## 12. Summary","metadata":{}},{"cell_type":"code","source":"summary = selection_df.groupby(['method', 'k']).agg({'drop': ['mean', 'std']}).reset_index()\nprint(\"\\nChannel Selection Summary:\")\nprint(summary)\n\nfig, ax = plt.subplots(1, 1, figsize=(10, 6))\nfor method in ['IMP', 'HYB', 'ES', 'AS']:\n    method_data = selection_df[selection_df['method'] == method]\n    grouped = method_data.groupby('k')['drop'].mean()\n    ax.plot(grouped.index, grouped.values, marker='o', label=method)\n\nax.set_xlabel('Number of Selected Channels (k)')\nax.set_ylabel('Accuracy Drop')\nax.set_title('SparseGCN-CARM: Channel Selection Performance')\nax.legend()\nax.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nBest method @ k=20: {selection_df[selection_df['k']==20].groupby('method')['drop'].mean().idxmin()}\")\nprint(f\"Mean drop @ k=20:\")\nfor method in ['IMP', 'HYB', 'ES', 'AS']:\n    drop_20 = selection_df[(selection_df['k']==20) & (selection_df['method']==method)]['drop'].mean()\n    print(f\"  {method}: {drop_20:.4f}\")\nprint(f\"\\nCARMv2 Baseline @ k=20:\")\nprint(f\"  ES: 0.0071 (0.71% drop)\")\nprint(f\"  AS: 0.0237 (2.37% drop)\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-01T23:09:23.254Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 13. Save Results","metadata":{}},{"cell_type":"code","source":"results_df.to_csv('results/sparsegcn_fixed_results.csv', index=False)\nselection_df.to_csv('results/sparsegcn_fixed_selection.csv', index=False)\nprint(\"Results saved!\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-01T23:09:23.255Z"}},"outputs":[],"execution_count":null}]}