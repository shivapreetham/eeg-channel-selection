{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PhysioNet CARMv2 - Complete Standalone Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and Setup\n",
    "import json, random, warnings\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm.auto import tqdm\n",
    "import mne\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_context('notebook', font_scale=1.0)\n",
    "mne.set_log_level('WARNING')\n",
    "\n",
    "def seed(s=42):\n",
    "    random.seed(s)\n",
    "    np.random.seed(s)\n",
    "    torch.manual_seed(s)\n",
    "    torch.cuda.manual_seed_all(s)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - AUTO-DETECT KAGGLE DATASET\n",
    "import os\n",
    "\n",
    "# Auto-detect environment and find dataset\n",
    "if os.path.exists('/kaggle/input'):\n",
    "    print(\"Running on Kaggle\")\n",
    "    print(\"\\nAvailable datasets:\")\n",
    "    \n",
    "    kaggle_input = Path('/kaggle/input')\n",
    "    datasets = [d for d in kaggle_input.iterdir() if d.is_dir()]\n",
    "    \n",
    "    for ds in datasets:\n",
    "        print(f\"  - {ds.name}\")\n",
    "    \n",
    "    # Try to find PhysioNet dataset\n",
    "    DATA_DIR = None\n",
    "    possible_names = ['physioneteegmi']\n",
    "    \n",
    "    for ds_name in possible_names:\n",
    "        test_path = kaggle_input / ds_name\n",
    "        if test_path.exists():\n",
    "            DATA_DIR = test_path\n",
    "            print(f\"\\n✓ Found dataset: {DATA_DIR}\")\n",
    "            break\n",
    "    \n",
    "    # If not found by name, use first available dataset\n",
    "    if DATA_DIR is None and datasets:\n",
    "        DATA_DIR = datasets[0]\n",
    "        print(f\"\\n⚠ Using first available dataset: {DATA_DIR}\")\n",
    "    \n",
    "    if DATA_DIR is None:\n",
    "        raise ValueError(\n",
    "            \"No dataset found! Please add the PhysioNet dataset:\\n\"\n",
    "        )\n",
    "\n",
    "print(f\"\\nData directory: {DATA_DIR}\")\n",
    "print(f\"Directory exists: {DATA_DIR.exists()}\")\n",
    "\n",
    "if DATA_DIR.exists():\n",
    "    # Show structure\n",
    "    contents = [d.name for d in sorted(DATA_DIR.iterdir()) if d.is_dir()][:10]\n",
    "    print(f\"\\nContents (first 10): {contents}\")\n",
    "    \n",
    "    # Look for subject folders\n",
    "    subjects_preview = [d.name for d in sorted(DATA_DIR.iterdir()) if d.is_dir() and d.name.startswith('S')][:5]\n",
    "    if subjects_preview:\n",
    "        print(f\"Subject folders found: {subjects_preview}\")\n",
    "        \n",
    "        # Check first subject's runs\n",
    "        first_subj = DATA_DIR / subjects_preview[0]\n",
    "        runs = sorted([f.name for f in first_subj.glob('*.edf')])\n",
    "        print(f\"Runs in {subjects_preview[0]}: {len(runs)} files\")\n",
    "        if runs:\n",
    "            print(f\"Sample runs: {runs[:5]}\")\n",
    "    else:\n",
    "        print(\"\\n⚠ No subject folders starting with 'S' found!\")\n",
    "        print(\"Please check if the dataset structure is correct.\")\n",
    "\n",
    "EXPERIMENT_CONFIG = {\n",
    "    'data': {\n",
    "        'raw_data_dir': DATA_DIR,\n",
    "        'selected_classes': [1, 2],\n",
    "        'tmin': -1.0,\n",
    "        'tmax': 5.0,\n",
    "        'baseline': (-0.5, 0)\n",
    "    },\n",
    "    'preprocessing': {\n",
    "        'l_freq': 0.5,\n",
    "        'h_freq': 40.0,\n",
    "        'notch_freq': 50.0,\n",
    "        'target_sfreq': 128.0,\n",
    "        'apply_car': True\n",
    "    },\n",
    "    'model': {\n",
    "        'hidden_dim': 40,\n",
    "        'epochs': 30,\n",
    "        'learning_rate': 1e-3,\n",
    "        'batch_size': 32,\n",
    "        'n_folds': 3,\n",
    "        'patience': 8\n",
    "    },\n",
    "    'carmv2': {\n",
    "        'topk_k': 8,\n",
    "        'lambda_feat': 0.3,\n",
    "        'hop_alpha': 0.5,\n",
    "        'edge_dropout': 0.1,\n",
    "        'use_pairnorm': True,\n",
    "        'use_residual': True,\n",
    "        'low_rank_r': 0\n",
    "    },\n",
    "    'output': {\n",
    "        'results_dir': Path('results'),\n",
    "        'results_file': 'carmv2_subject_results.csv',\n",
    "        'channel_selection_file': 'carmv2_channel_selection.csv',\n",
    "        'adjacency_prefix': 'carmv2_adjacency'\n",
    "    },\n",
    "    'max_subjects': 20,\n",
    "    'min_runs_per_subject': 8\n",
    "}\n",
    "\n",
    "EXPERIMENT_CONFIG['output']['results_dir'].mkdir(exist_ok=True, parents=True)\n",
    "print(f\"\\n✓ Configuration loaded successfully!\")\n",
    "print(f\"✓ Training: {EXPERIMENT_CONFIG['max_subjects']} subjects, {EXPERIMENT_CONFIG['model']['n_folds']}-fold CV, {EXPERIMENT_CONFIG['model']['epochs']} epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading and Preprocessing Functions\n",
    "\n",
    "def preprocess_raw(raw, config):\n",
    "    \"\"\"Apply preprocessing to raw EEG data.\"\"\"\n",
    "    cleaned_names = {name: name.rstrip('.') for name in raw.ch_names}\n",
    "    raw.rename_channels(cleaned_names)\n",
    "    raw.pick_types(eeg=True)\n",
    "    raw.set_montage('standard_1020', on_missing='ignore', match_case=False)\n",
    "    \n",
    "    nyquist = raw.info['sfreq'] / 2.0\n",
    "    if config['preprocessing']['notch_freq'] < nyquist:\n",
    "        raw.notch_filter(freqs=config['preprocessing']['notch_freq'], verbose=False)\n",
    "    \n",
    "    raw.filter(\n",
    "        l_freq=config['preprocessing']['l_freq'],\n",
    "        h_freq=config['preprocessing']['h_freq'],\n",
    "        method='fir',\n",
    "        fir_design='firwin',\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    if config['preprocessing']['apply_car']:\n",
    "        raw.set_eeg_reference('average', projection=False, verbose=False)\n",
    "    \n",
    "    raw.resample(config['preprocessing']['target_sfreq'], npad='auto', verbose=False)\n",
    "    return raw\n",
    "\n",
    "\n",
    "def load_and_preprocess_edf(edf_path, config):\n",
    "    \"\"\"Load raw EDF file, preprocess it, and extract epochs.\"\"\"\n",
    "    raw = mne.io.read_raw_edf(edf_path, preload=True, verbose='ERROR')\n",
    "    raw = preprocess_raw(raw, config)\n",
    "    \n",
    "    try:\n",
    "        events = mne.find_events(raw, verbose='ERROR')\n",
    "        event_ids = {f'T{i}': i for i in np.unique(events[:, 2])}\n",
    "        assert len(events) > 0\n",
    "    except Exception:\n",
    "        events, event_ids = mne.events_from_annotations(raw, verbose='ERROR')\n",
    "    \n",
    "    if len(events) == 0:\n",
    "        return None, None, raw.ch_names\n",
    "    \n",
    "    epochs = mne.Epochs(\n",
    "        raw,\n",
    "        events,\n",
    "        event_id=event_ids,\n",
    "        tmin=config['data']['tmin'],\n",
    "        tmax=config['data']['tmax'],\n",
    "        baseline=tuple(config['data']['baseline']),\n",
    "        preload=True,\n",
    "        verbose='ERROR'\n",
    "    )\n",
    "    \n",
    "    return epochs.get_data(), epochs.events[:, 2], raw.ch_names\n",
    "\n",
    "\n",
    "def filter_classes(x, y, selected_classes):\n",
    "    \"\"\"Filter to keep only selected classes and remap labels.\"\"\"\n",
    "    mask = np.isin(y, selected_classes)\n",
    "    y, x = y[mask], x[mask]\n",
    "    label_map = {old: new for new, old in enumerate(sorted(selected_classes))}\n",
    "    y = np.array([label_map[int(label)] for label in y], dtype=np.int64)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def normalize(x):\n",
    "    \"\"\"Z-score normalization per channel.\"\"\"\n",
    "    mu = x.mean(axis=(0, 2), keepdims=True)\n",
    "    sd = x.std(axis=(0, 2), keepdims=True) + 1e-8\n",
    "    return (x - mu) / sd\n",
    "\n",
    "\n",
    "def load_subject_data(data_dir, subject_id, run_ids, config):\n",
    "    \"\"\"Load all runs for a subject, preprocess, and concatenate.\"\"\"\n",
    "    subject_dir = data_dir / subject_id\n",
    "    if not subject_dir.exists():\n",
    "        return None, None, None\n",
    "    \n",
    "    all_x, all_y = [], []\n",
    "    channel_names = None\n",
    "    \n",
    "    for run_id in run_ids:\n",
    "        edf_path = subject_dir / f'{subject_id}{run_id}.edf'\n",
    "        if not edf_path.exists():\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            x, y, ch_names = load_and_preprocess_edf(edf_path, config)\n",
    "            if x is None or len(y) == 0:\n",
    "                continue\n",
    "            \n",
    "            x, y = filter_classes(x, y, config['data']['selected_classes'])\n",
    "            if len(y) == 0:\n",
    "                continue\n",
    "            \n",
    "            channel_names = channel_names or ch_names\n",
    "            all_x.append(x)\n",
    "            all_y.append(y)\n",
    "        except Exception as e:\n",
    "            print(f\"  Warning: Failed to load {edf_path.name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if len(all_x) == 0:\n",
    "        return None, None, channel_names\n",
    "    \n",
    "    return np.concatenate(all_x, 0), np.concatenate(all_y, 0), channel_names\n",
    "\n",
    "\n",
    "def get_available_subjects(data_dir, min_runs=8):\n",
    "    \"\"\"Get list of subjects with at least min_runs available.\"\"\"\n",
    "    if not data_dir.exists():\n",
    "        raise ValueError(f\"Data directory not found: {data_dir}\")\n",
    "    \n",
    "    subjects = []\n",
    "    for subject_dir in sorted(data_dir.iterdir()):\n",
    "        if not subject_dir.is_dir() or not subject_dir.name.startswith('S'):\n",
    "            continue\n",
    "        edf_files = list(subject_dir.glob('*.edf'))\n",
    "        if len(edf_files) >= min_runs:\n",
    "            subjects.append(subject_dir.name)\n",
    "    \n",
    "    return subjects\n",
    "\n",
    "\n",
    "# Scan for available subjects\n",
    "print(\"\\nScanning for subjects...\")\n",
    "data_dir = EXPERIMENT_CONFIG['data']['raw_data_dir']\n",
    "print(f\"Looking for data in: {data_dir}\")\n",
    "\n",
    "all_subjects = get_available_subjects(data_dir, min_runs=EXPERIMENT_CONFIG['min_runs_per_subject'])\n",
    "subjects = all_subjects[:EXPERIMENT_CONFIG['max_subjects']]\n",
    "\n",
    "print(f\"Found {len(all_subjects)} subjects with >= {EXPERIMENT_CONFIG['min_runs_per_subject']} runs\")\n",
    "print(f\"Will process {len(subjects)} subjects: {subjects}\")\n",
    "\n",
    "# Define which runs to use\n",
    "MOTOR_IMAGERY_RUNS = ['R07', 'R08', 'R09', 'R10', 'R11', 'R12', 'R13', 'R14']\n",
    "MOTOR_EXECUTION_RUNS = ['R03', 'R04', 'R05', 'R06']\n",
    "ALL_TASK_RUNS = MOTOR_IMAGERY_RUNS + MOTOR_EXECUTION_RUNS\n",
    "print(f\"Using runs: {ALL_TASK_RUNS}\")\n",
    "\n",
    "print(\"\\nData loading functions ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Dataset\n",
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = torch.FloatTensor(x).unsqueeze(1)  # Add channel dim for Conv2d\n",
    "        self.y = torch.LongTensor(y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return self.x[i], self.y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CARMv2 Model Architecture\n",
    "\n",
    "def pairnorm(x, node_dim=2, eps=1e-6):\n",
    "    m = x.mean(dim=node_dim, keepdim=True)\n",
    "    xc = x - m\n",
    "    v = (xc * xc).mean(dim=node_dim, keepdim=True)\n",
    "    return xc / torch.sqrt(v + eps)\n",
    "\n",
    "\n",
    "def build_feat_topk_adj(x, k):\n",
    "    B, H, C, T = x.shape\n",
    "    E = x.permute(2, 1, 0, 3).contiguous().view(C, H, B*T).mean(2)\n",
    "    En = F.normalize(E, p=2, dim=1)\n",
    "    S = (En @ En.t()).clamp_min(0.0)\n",
    "    k = max(1, min(int(k), C))\n",
    "    vals, idx = torch.topk(S, k, dim=1)\n",
    "    M = torch.zeros_like(S)\n",
    "    M.scatter_(1, idx, 1.0)\n",
    "    A = S * M\n",
    "    A = torch.softmax(A, 1)\n",
    "    A = 0.5 * (A + A.t())\n",
    "    return A\n",
    "\n",
    "\n",
    "class CARMv2(nn.Module):\n",
    "    def __init__(self, C, H, cfg):\n",
    "        super().__init__()\n",
    "        self.C = C\n",
    "        self.H = H\n",
    "        self.k = int(cfg['topk_k'])\n",
    "        self.lf = float(cfg['lambda_feat'])\n",
    "        self.ha = float(cfg['hop_alpha'])\n",
    "        self.ed = float(cfg['edge_dropout'])\n",
    "        self.pn = bool(cfg['use_pairnorm'])\n",
    "        self.res = bool(cfg['use_residual'])\n",
    "        r = int(cfg['low_rank_r'])\n",
    "        \n",
    "        if r > 0:\n",
    "            self.B = nn.Parameter(torch.empty(C, r))\n",
    "            nn.init.xavier_uniform_(self.B)\n",
    "            self.W = None\n",
    "        else:\n",
    "            self.W = nn.Parameter(torch.empty(C, C))\n",
    "            nn.init.xavier_uniform_(self.W)\n",
    "            self.B = None\n",
    "        \n",
    "        self.th = nn.Linear(H, H, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(H)\n",
    "        self.act = nn.ELU()\n",
    "        self.last = None\n",
    "    \n",
    "    def _learned(self, dev):\n",
    "        W = self.W if self.B is None else (self.B @ self.B.t())\n",
    "        A = torch.sigmoid(W)\n",
    "        A = 0.5 * (A + A.t())\n",
    "        I = torch.eye(self.C, device=dev, dtype=A.dtype)\n",
    "        At = A + I\n",
    "        d = torch.pow(At.sum(1).clamp_min(1e-6), -0.5)\n",
    "        D = torch.diag(d)\n",
    "        return D @ At @ D\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, H, C, T = x.shape\n",
    "        Al = self._learned(x.device)\n",
    "        A2 = Al @ Al\n",
    "        Ah = (1 - self.ha) * Al + self.ha * A2\n",
    "        Af = build_feat_topk_adj(x, self.k)\n",
    "        A = (1 - self.lf) * Ah + self.lf * Af\n",
    "        \n",
    "        if self.training and self.ed > 0:\n",
    "            M = (torch.rand_like(A) > self.ed).float()\n",
    "            A = 0.5 * ((A * M) + (A * M).t())\n",
    "            A = A + torch.eye(C, device=A.device, dtype=A.dtype)\n",
    "        \n",
    "        d = torch.pow(A.sum(1).clamp_min(1e-6), -0.5)\n",
    "        D = torch.diag(d)\n",
    "        A = D @ A @ D\n",
    "        \n",
    "        xb = x.permute(0, 3, 2, 1).contiguous().view(B*T, C, H)\n",
    "        xg = A @ xb\n",
    "        xg = self.th(xg)\n",
    "        xg = xg.view(B, T, C, H).permute(0, 3, 2, 1)\n",
    "        \n",
    "        out = xg + x if self.res else xg\n",
    "        out = pairnorm(out, 2) if self.pn else out\n",
    "        out = self.bn(out)\n",
    "        out = self.act(out)\n",
    "        \n",
    "        self.last = {\n",
    "            'learned': Al.detach().cpu().numpy(),\n",
    "            'effective': A.detach().cpu().numpy()\n",
    "        }\n",
    "        return out\n",
    "    \n",
    "    def get_adjs(self):\n",
    "        return self.last or {}\n",
    "\n",
    "\n",
    "class TFEM(nn.Module):\n",
    "    \"\"\"Temporal Feature Extraction Module\"\"\"\n",
    "    def __init__(self, i, o, k=16, pool=True):\n",
    "        super().__init__()\n",
    "        self.pool = pool\n",
    "        self.cv = nn.Conv2d(i, o, kernel_size=(1, k), padding=(0, k//2), bias=False)\n",
    "        self.bn = nn.BatchNorm2d(o)\n",
    "        self.act = nn.ELU()\n",
    "        self.pl = nn.AvgPool2d(kernel_size=(1, 2)) if pool else None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.act(self.bn(self.cv(x)))\n",
    "        return self.pl(x) if self.pool else x\n",
    "\n",
    "\n",
    "class EEGARNN_CARMv2(nn.Module):\n",
    "    def __init__(self, C, T, K, H, cfg):\n",
    "        super().__init__()\n",
    "        self.t1 = TFEM(1, H, 16, False)\n",
    "        self.g1 = CARMv2(C, H, cfg)\n",
    "        self.t2 = TFEM(H, H, 16, True)\n",
    "        self.g2 = CARMv2(C, H, cfg)\n",
    "        self.t3 = TFEM(H, H, 16, True)\n",
    "        self.g3 = CARMv2(C, H, cfg)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            ft = self._f(torch.zeros(1, 1, C, T))\n",
    "            fs = ft.view(1, -1).size(1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(fs, 256)\n",
    "        self.do = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(256, K)\n",
    "    \n",
    "    def _f(self, x):\n",
    "        x = self.g1(self.t1(x))\n",
    "        x = self.g2(self.t2(x))\n",
    "        x = self.g3(self.t3(x))\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self._f(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.do(x)\n",
    "        return self.fc2(x)\n",
    "    \n",
    "    def get_final_adjs(self):\n",
    "        return self.g3.get_adjs()\n",
    "\n",
    "\n",
    "print(\"Model architecture defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Functions\n",
    "\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    all_preds, all_labels = [], []\n",
    "    \n",
    "    for x, y in dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        all_preds += torch.argmax(logits, 1).cpu().tolist()\n",
    "        all_labels += y.cpu().tolist()\n",
    "    \n",
    "    return total_loss / max(1, len(dataloader)), accuracy_score(all_labels, all_preds)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_preds, all_labels = [], []\n",
    "    \n",
    "    for x, y in dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        all_preds += torch.argmax(logits, 1).cpu().tolist()\n",
    "        all_labels += y.cpu().tolist()\n",
    "    \n",
    "    return total_loss / max(1, len(dataloader)), accuracy_score(all_labels, all_preds), all_preds, all_labels\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, device, epochs, lr, patience):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    \n",
    "    try:\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='min', factor=0.5, patience=3, verbose=False\n",
    "        )\n",
    "    except TypeError:\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='min', factor=0.5, patience=3\n",
    "        )\n",
    "    \n",
    "    best_acc = 0.0\n",
    "    best_state = None\n",
    "    no_improve = 0\n",
    "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss, val_acc, _, _ = evaluate(model, val_loader, criterion, device)\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        try:\n",
    "            scheduler.step(val_loss)\n",
    "        except Exception:\n",
    "            pass\n",
    "        \n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            best_state = {k: v.detach().cpu() if hasattr(v, 'detach') else v \n",
    "                         for k, v in model.state_dict().items()}\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "        \n",
    "        if no_improve >= patience:\n",
    "            break\n",
    "    \n",
    "    if best_state is None:\n",
    "        best_state = {k: v.detach().cpu() if hasattr(v, 'detach') else v \n",
    "                     for k, v in model.state_dict().items()}\n",
    "    \n",
    "    model.load_state_dict(best_state)\n",
    "    return history, best_state\n",
    "\n",
    "\n",
    "def cross_validate_subject(x, y, channel_names, T, K, device, config):\n",
    "    C = x.shape[1]\n",
    "    skf = StratifiedKFold(\n",
    "        n_splits=int(config['model']['n_folds']),\n",
    "        shuffle=True,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    batch_size = int(config['model']['batch_size'])\n",
    "    epochs = int(config['model']['epochs'])\n",
    "    lr = float(config['model']['learning_rate'])\n",
    "    patience = int(config['model']['patience'])\n",
    "    \n",
    "    folds = []\n",
    "    adjacencies = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(x, y)):\n",
    "        X_train, X_val = normalize(x[train_idx]), normalize(x[val_idx])\n",
    "        Y_train, Y_val = y[train_idx], y[val_idx]\n",
    "        \n",
    "        train_loader = DataLoader(\n",
    "            EEGDataset(X_train, Y_train),\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=0\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            EEGDataset(X_val, Y_val),\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=0\n",
    "        )\n",
    "        \n",
    "        model = EEGARNN_CARMv2(C, T, K, config['model']['hidden_dim'], config['carmv2']).to(device)\n",
    "        history, best_state = train_model(model, train_loader, val_loader, device, epochs, lr, patience)\n",
    "        model.load_state_dict(best_state)\n",
    "        \n",
    "        _, accuracy, _, _ = evaluate(model, val_loader, nn.CrossEntropyLoss(), device)\n",
    "        \n",
    "        adjacency = model.get_final_adjs().get('learned', None)\n",
    "        adjacencies.append(adjacency)\n",
    "        folds.append({'fold': fold, 'val_acc': accuracy, 'history': history})\n",
    "    \n",
    "    avg_acc = float(np.mean([f['val_acc'] for f in folds]))\n",
    "    std_acc = float(np.std([f['val_acc'] for f in folds]))\n",
    "    avg_adjacency = np.mean(np.stack([a for a in adjacencies if a is not None], 0), 0) \\\n",
    "                    if any(a is not None for a in adjacencies) else None\n",
    "    \n",
    "    return {\n",
    "        'fold_results': folds,\n",
    "        'avg_accuracy': avg_acc,\n",
    "        'std_accuracy': std_acc,\n",
    "        'adjacency_matrix': avg_adjacency,\n",
    "        'channel_names': channel_names\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"Training functions ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Channel Selection\n",
    "\n",
    "class ChannelSelector:\n",
    "    def __init__(self, adjacency, channel_names):\n",
    "        self.A = adjacency\n",
    "        self.names = np.array(channel_names)\n",
    "        self.C = adjacency.shape[0]\n",
    "    \n",
    "    def edge_selection(self, k):\n",
    "        \"\"\"Select channels based on edge importance.\"\"\"\n",
    "        edges = []\n",
    "        for i in range(self.C):\n",
    "            for j in range(i+1, self.C):\n",
    "                edges.append((i, j, abs(self.A[i, j]) + abs(self.A[j, i])))\n",
    "        \n",
    "        edges.sort(key=lambda t: t[2], reverse=True)\n",
    "        top_edges = edges[:int(k)]\n",
    "        indices = sorted(set([i for i, _, _ in top_edges] + [j for _, j, _ in top_edges]))\n",
    "        return self.names[indices].tolist(), np.array(indices)\n",
    "    \n",
    "    def aggregation_selection(self, k):\n",
    "        \"\"\"Select channels based on aggregated connectivity.\"\"\"\n",
    "        scores = np.sum(np.abs(self.A), 1)\n",
    "        indices = np.sort(np.argsort(scores)[-int(k):])\n",
    "        return self.names[indices].tolist(), indices\n",
    "\n",
    "\n",
    "def viz_adjacency(adjacency, channel_names, save_path=None):\n",
    "    \"\"\"Visualize adjacency matrix as heatmap.\"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(\n",
    "        adjacency,\n",
    "        xticklabels=channel_names,\n",
    "        yticklabels=channel_names,\n",
    "        cmap='RdYlGn',\n",
    "        center=0,\n",
    "        square=True,\n",
    "        linewidths=0.4\n",
    "    )\n",
    "    plt.title('CARMv2 Learned Adjacency Matrix')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=200, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        return save_path\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "print(\"Channel selection ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN TRAINING LOOP\n",
    "\n",
    "results_dir = EXPERIMENT_CONFIG['output']['results_dir']\n",
    "all_results = []\n",
    "summary_records = []\n",
    "\n",
    "print(\"\\nStarting training...\\n\")\n",
    "\n",
    "for subject_id in tqdm(subjects, desc='Training CARMv2'):\n",
    "    print(f\"\\nProcessing {subject_id}...\")\n",
    "    \n",
    "    # Load subject data with preprocessing\n",
    "    X, Y, channel_names = load_subject_data(\n",
    "        data_dir,\n",
    "        subject_id,\n",
    "        ALL_TASK_RUNS,\n",
    "        EXPERIMENT_CONFIG\n",
    "    )\n",
    "    \n",
    "    if X is None or len(Y) == 0:\n",
    "        print(f\"  Skipped: No data available\")\n",
    "        continue\n",
    "    \n",
    "    C, T = X.shape[1], X.shape[2]\n",
    "    K = len(set(EXPERIMENT_CONFIG['data']['selected_classes']))\n",
    "    \n",
    "    print(f\"  Data shape: {X.shape} (trials={X.shape[0]}, channels={C}, timepoints={T})\")\n",
    "    print(f\"  Label distribution: {np.bincount(Y)}\")\n",
    "    \n",
    "    # Cross-validate\n",
    "    result = cross_validate_subject(X, Y, channel_names, T, K, device, EXPERIMENT_CONFIG)\n",
    "    \n",
    "    print(f\"  Accuracy: {result['avg_accuracy']:.4f} ± {result['std_accuracy']:.4f}\")\n",
    "    \n",
    "    all_results.append({\n",
    "        'subject': subject_id,\n",
    "        'num_trials': X.shape[0],\n",
    "        'num_channels': C,\n",
    "        'carmv2_acc': result['avg_accuracy'],\n",
    "        'carmv2_std': result['std_accuracy'],\n",
    "        'adjacency_matrix': result['adjacency_matrix'],\n",
    "        'channel_names': result['channel_names'],\n",
    "        'fold_results': result['fold_results']\n",
    "    })\n",
    "    \n",
    "    summary_records.append({\n",
    "        'subject': subject_id,\n",
    "        'num_trials': X.shape[0],\n",
    "        'num_channels': C,\n",
    "        'carmv2_acc': result['avg_accuracy'],\n",
    "        'carmv2_std': result['std_accuracy']\n",
    "    })\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Training Complete!\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Results and Visualizations\n",
    "\n",
    "results_df = pd.DataFrame.from_records(summary_records)\n",
    "print(f\"\\nSubjects trained: {len(results_df)}\")\n",
    "\n",
    "if len(results_df) > 0:\n",
    "    print(f\"\\nMean accuracy: {results_df['carmv2_acc'].mean():.4f} ± {results_df['carmv2_acc'].std():.4f}\")\n",
    "    print(f\"Best subject: {results_df.loc[results_df['carmv2_acc'].idxmax(), 'subject']}\")\n",
    "    print(f\"Worst subject: {results_df.loc[results_df['carmv2_acc'].idxmin(), 'subject']}\")\n",
    "    \n",
    "    # Save results CSV\n",
    "    results_path = results_dir / EXPERIMENT_CONFIG['output']['results_file']\n",
    "    results_df.to_csv(results_path, index=False)\n",
    "    print(f\"\\nSaved results to {results_path}\")\n",
    "    \n",
    "    # Save config\n",
    "    config_path = results_dir / 'experiment_config.json'\n",
    "    with open(config_path, 'w') as f:\n",
    "        json.dump(EXPERIMENT_CONFIG, f, indent=2, default=str)\n",
    "    print(f\"Saved config to {config_path}\")\n",
    "    \n",
    "    # Get best subject's adjacency matrix\n",
    "    best_idx = int(np.argmax([r['carmv2_acc'] for r in all_results]))\n",
    "    best_result = all_results[best_idx]\n",
    "    \n",
    "    adjacency = best_result['adjacency_matrix']\n",
    "    channel_names = best_result['channel_names']\n",
    "    \n",
    "    if adjacency is not None and channel_names is not None:\n",
    "        # Save adjacency visualization\n",
    "        adj_path = results_dir / f\"{EXPERIMENT_CONFIG['output']['adjacency_prefix']}_{best_result['subject']}.png\"\n",
    "        viz_adjacency(adjacency, channel_names, adj_path)\n",
    "        print(f\"\\nSaved adjacency matrix to {adj_path}\")\n",
    "        \n",
    "        # Channel selection\n",
    "        selector = ChannelSelector(adjacency, channel_names)\n",
    "        channel_selections = []\n",
    "        \n",
    "        for method in ['ES', 'AS']:\n",
    "            for k in [10, 15, 20]:\n",
    "                if method == 'ES':\n",
    "                    selected, _ = selector.edge_selection(k)\n",
    "                else:\n",
    "                    selected, _ = selector.aggregation_selection(k)\n",
    "                \n",
    "                channel_selections.append({\n",
    "                    'subject': best_result['subject'],\n",
    "                    'method': method,\n",
    "                    'k': k,\n",
    "                    'channels': selected\n",
    "                })\n",
    "                print(f\"  Selected ({method}, k={k}): {selected}\")\n",
    "        \n",
    "        # Save channel selections\n",
    "        channel_df = pd.DataFrame(channel_selections)\n",
    "        channel_path = results_dir / EXPERIMENT_CONFIG['output']['channel_selection_file']\n",
    "        channel_df.to_csv(channel_path, index=False)\n",
    "        print(f\"\\nSaved channel selections to {channel_path}\")\n",
    "    \n",
    "    # Summary visualizations\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    # Accuracy distribution\n",
    "    axes[0, 0].hist(results_df['carmv2_acc'], bins=15, color='steelblue', alpha=0.8, edgecolor='black')\n",
    "    axes[0, 0].set_title('Accuracy Distribution')\n",
    "    axes[0, 0].set_xlabel('Accuracy')\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "    \n",
    "    # Trials vs Accuracy\n",
    "    axes[0, 1].scatter(results_df['num_trials'], results_df['carmv2_acc'], alpha=0.6)\n",
    "    axes[0, 1].set_title('Number of Trials vs Accuracy')\n",
    "    axes[0, 1].set_xlabel('Number of Trials')\n",
    "    axes[0, 1].set_ylabel('Accuracy')\n",
    "    \n",
    "    # Top subjects\n",
    "    top_n = min(10, len(results_df))\n",
    "    top_subjects = results_df.nlargest(top_n, 'carmv2_acc')\n",
    "    axes[1, 0].barh(range(len(top_subjects)), top_subjects['carmv2_acc'], color='forestgreen', alpha=0.7)\n",
    "    axes[1, 0].set_yticks(range(len(top_subjects)))\n",
    "    axes[1, 0].set_yticklabels(top_subjects['subject'])\n",
    "    axes[1, 0].invert_yaxis()\n",
    "    axes[1, 0].set_title(f'Top {top_n} Subjects')\n",
    "    axes[1, 0].set_xlabel('Accuracy')\n",
    "    \n",
    "    # Ranking curve\n",
    "    sorted_results = results_df.sort_values('carmv2_acc')\n",
    "    axes[1, 1].plot(range(len(sorted_results)), sorted_results['carmv2_acc'], marker='o', color='coral')\n",
    "    axes[1, 1].set_title('Subject Ranking')\n",
    "    axes[1, 1].set_xlabel('Rank')\n",
    "    axes[1, 1].set_ylabel('Accuracy')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nAll done! Check the 'results' folder for outputs.\")\n",
    "else:\n",
    "    print(\"\\nNo results to save.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
