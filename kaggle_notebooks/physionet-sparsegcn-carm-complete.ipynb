{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2967738,"sourceType":"datasetVersion","datasetId":1819423}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# PhysioNet Motor Imagery - SparseGCN-CARM\n\n## Improved Method for Channel Selection\n\n**Previous Results:**\n- Baseline: 86.49% accuracy, 0.74% drop @ k=20\n- Gated CARM: 87.20% accuracy, 3.08% drop @ k=20\n- CARMv2: 86.83% accuracy, 0.71% drop @ k=20 (best selection)\n\n**SparseGCN-CARM Target:**\n- Accuracy: > 88.5%\n- Channel selection: < 0.5% drop @ k=20\n- Auto channel pruning: 64 -> 30-40 channels\n\n**Key Innovations:**\n1. Progressive channel pruning during training\n2. Multi-head attention for explicit channel importance\n3. Multi-scale temporal convolution [8, 16, 32]\n4. Feature-adaptive GCN from CARMv2 (keeps what works)","metadata":{}},{"cell_type":"markdown","source":"## 1. Setup and Imports","metadata":{}},{"cell_type":"code","source":"import json\nimport random\nimport warnings\nfrom pathlib import Path\nfrom copy import deepcopy\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm.auto import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\n\nimport mne\n\nwarnings.filterwarnings('ignore')\nsns.set_context('notebook', font_scale=1.0)\nmne.set_log_level('WARNING')\n\ndef set_seed(s=42):\n    random.seed(s)\n    np.random.seed(s)\n    torch.manual_seed(s)\n    torch.cuda.manual_seed_all(s)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nset_seed(42)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'Device: {device}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T21:43:04.466182Z","iopub.execute_input":"2025-10-31T21:43:04.466420Z","iopub.status.idle":"2025-10-31T21:43:11.009425Z","shell.execute_reply.started":"2025-10-31T21:43:04.466401Z","shell.execute_reply":"2025-10-31T21:43:11.008654Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## 2. Configuration","metadata":{}},{"cell_type":"code","source":"# Auto-detect Kaggle environment\nimport os\nif os.path.exists('/kaggle/input'):\n    print(\"Running on Kaggle\")\n    kaggle_input = Path('/kaggle/input')\n    datasets = [d for d in kaggle_input.iterdir() if d.is_dir()]\n    print(f\"Available datasets: {[d.name for d in datasets]}\")\n\n    DATA_DIR = None\n    possible_names = ['physioneteegmi', 'eeg-motor-movementimagery-dataset']\n    for ds_name in possible_names:\n        test_path = kaggle_input / ds_name\n        if test_path.exists():\n            DATA_DIR = test_path\n            print(f\"Found dataset: {DATA_DIR}\")\n            break\n\n    if DATA_DIR is None and datasets:\n        DATA_DIR = datasets[0]\n        print(f\"Using first available dataset: {DATA_DIR}\")\nelse:\n    print(\"Running locally\")\n    DATA_DIR = Path('data/physionet/files')\n\nCONFIG = {\n    'data': {\n        'raw_data_dir': DATA_DIR,\n        'selected_classes': [1, 2],\n        'tmin': -1.0,\n        'tmax': 5.0,\n        'baseline': (-0.5, 0)\n    },\n    'preprocessing': {\n        'l_freq': 0.5,\n        'h_freq': 40.0,\n        'notch_freq': 50.0,\n        'target_sfreq': 128.0,\n        'apply_car': True\n    },\n    'model': {\n        'hidden_dim': 40,\n        'epochs': 40,\n        'learning_rate': 1e-3,\n        'batch_size': 32,\n        'n_folds': 3,\n        'patience': 10\n    },\n    'sparsegcn': {\n        'topk_k': 8,\n        'lambda_feat': 0.3,\n        'hop_alpha': 0.5,\n        'edge_dropout': 0.1,\n        'use_pairnorm': True,\n        'use_residual': True,\n        'use_channel_attention': True,\n        'attention_heads': 4,\n        'prune_enabled': True,\n        'prune_start_epoch': 10,\n        'prune_every': 2,\n        'prune_ratio': 0.05,\n        'min_channels': 20,\n        'temporal_scales': [8, 16, 32],\n        'channel_importance_loss': 1e-3\n    },\n    'channel_selection': {\n        'k_values': [10, 15, 20, 25]\n    },\n    'output': {\n        'results_dir': Path('results'),\n    },\n    'max_subjects': 20,\n    'min_runs_per_subject': 8\n}\n\nCONFIG['output']['results_dir'].mkdir(exist_ok=True, parents=True)\nprint(\"\\nConfiguration loaded!\")\nprint(f\"Training: {CONFIG['max_subjects']} subjects, {CONFIG['model']['n_folds']}-fold CV\")\nprint(f\"Progressive pruning: Start epoch {CONFIG['sparsegcn']['prune_start_epoch']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T21:43:11.010881Z","iopub.execute_input":"2025-10-31T21:43:11.011320Z","iopub.status.idle":"2025-10-31T21:43:11.022252Z","shell.execute_reply.started":"2025-10-31T21:43:11.011300Z","shell.execute_reply":"2025-10-31T21:43:11.021408Z"}},"outputs":[{"name":"stdout","text":"Running on Kaggle\nAvailable datasets: ['physioneteegmi']\nFound dataset: /kaggle/input/physioneteegmi\n\nConfiguration loaded!\nTraining: 20 subjects, 3-fold CV\nProgressive pruning: Start epoch 10\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## 3. Data Cleaning - Remove Faulty Subjects\n\nBased on quality analysis, exclude subjects with:\n- Less than 10 good runs\n- Good run ratio < 70%\n- Known problematic subjects","metadata":{}},{"cell_type":"code","source":"# Known faulty subjects from data cleaning analysis\nKNOWN_BAD_SUBJECTS = [\n    'S088', 'S089', 'S092', 'S100', 'S104', 'S106', 'S107', 'S108', 'S109'\n]\n\n# Additional subjects with high clipping or amplitude issues\nHIGH_ISSUE_SUBJECTS = [\n    'S003', 'S004', 'S009', 'S010', 'S012', 'S013', 'S017', 'S018', 'S019',\n    'S021', 'S022', 'S023', 'S024', 'S025', 'S026', 'S027', 'S028', 'S029'\n]\n\nEXCLUDED_SUBJECTS = set(KNOWN_BAD_SUBJECTS + HIGH_ISSUE_SUBJECTS)\n\nprint(f\"Total excluded subjects: {len(EXCLUDED_SUBJECTS)}\")\nprint(f\"Excluded: {sorted(EXCLUDED_SUBJECTS)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T21:43:11.022997Z","iopub.execute_input":"2025-10-31T21:43:11.023196Z","iopub.status.idle":"2025-10-31T21:43:11.039941Z","shell.execute_reply.started":"2025-10-31T21:43:11.023180Z","shell.execute_reply":"2025-10-31T21:43:11.039064Z"}},"outputs":[{"name":"stdout","text":"Total excluded subjects: 27\nExcluded: ['S003', 'S004', 'S009', 'S010', 'S012', 'S013', 'S017', 'S018', 'S019', 'S021', 'S022', 'S023', 'S024', 'S025', 'S026', 'S027', 'S028', 'S029', 'S088', 'S089', 'S092', 'S100', 'S104', 'S106', 'S107', 'S108', 'S109']\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## 4. Data Loading and Preprocessing","metadata":{}},{"cell_type":"code","source":"def preprocess_raw(raw, config):\n    cleaned_names = {name: name.rstrip('.') for name in raw.ch_names}\n    raw.rename_channels(cleaned_names)\n    raw.pick_types(eeg=True)\n    raw.set_montage('standard_1020', on_missing='ignore', match_case=False)\n\n    nyquist = raw.info['sfreq'] / 2.0\n    if config['preprocessing']['notch_freq'] < nyquist:\n        raw.notch_filter(freqs=config['preprocessing']['notch_freq'], verbose=False)\n\n    raw.filter(\n        l_freq=config['preprocessing']['l_freq'],\n        h_freq=config['preprocessing']['h_freq'],\n        method='fir',\n        fir_design='firwin',\n        verbose=False\n    )\n\n    if config['preprocessing']['apply_car']:\n        raw.set_eeg_reference('average', projection=False, verbose=False)\n\n    raw.resample(config['preprocessing']['target_sfreq'], npad='auto', verbose=False)\n    return raw\n\n\ndef load_and_preprocess_edf(edf_path, config):\n    raw = mne.io.read_raw_edf(edf_path, preload=True, verbose='ERROR')\n    raw = preprocess_raw(raw, config)\n\n    try:\n        events = mne.find_events(raw, verbose='ERROR')\n        event_ids = {f'T{i}': i for i in np.unique(events[:, 2])}\n        assert len(events) > 0\n    except Exception:\n        events, event_ids = mne.events_from_annotations(raw, verbose='ERROR')\n\n    if len(events) == 0:\n        return None, None, raw.ch_names\n\n    epochs = mne.Epochs(\n        raw,\n        events,\n        event_id=event_ids,\n        tmin=config['data']['tmin'],\n        tmax=config['data']['tmax'],\n        baseline=tuple(config['data']['baseline']),\n        preload=True,\n        verbose='ERROR'\n    )\n\n    return epochs.get_data(), epochs.events[:, 2], raw.ch_names\n\n\ndef filter_classes(x, y, selected_classes):\n    mask = np.isin(y, selected_classes)\n    y, x = y[mask], x[mask]\n    label_map = {old: new for new, old in enumerate(sorted(selected_classes))}\n    y = np.array([label_map[int(label)] for label in y], dtype=np.int64)\n    return x, y\n\n\ndef normalize(x):\n    mu = x.mean(axis=(0, 2), keepdims=True)\n    sd = x.std(axis=(0, 2), keepdims=True) + 1e-8\n    return (x - mu) / sd\n\n\ndef load_subject_data(data_dir, subject_id, run_ids, config):\n    subject_dir = data_dir / subject_id\n    if not subject_dir.exists():\n        return None, None, None\n\n    all_x, all_y = [], []\n    channel_names = None\n\n    for run_id in run_ids:\n        edf_path = subject_dir / f'{subject_id}{run_id}.edf'\n        if not edf_path.exists():\n            continue\n\n        try:\n            x, y, ch_names = load_and_preprocess_edf(edf_path, config)\n            if x is None or len(y) == 0:\n                continue\n\n            x, y = filter_classes(x, y, config['data']['selected_classes'])\n            if len(y) == 0:\n                continue\n\n            channel_names = channel_names or ch_names\n            all_x.append(x)\n            all_y.append(y)\n        except Exception as e:\n            print(f\"  Warning: Failed to load {edf_path.name}: {e}\")\n            continue\n\n    if len(all_x) == 0:\n        return None, None, channel_names\n\n    return np.concatenate(all_x, 0), np.concatenate(all_y, 0), channel_names\n\n\ndef get_available_subjects(data_dir, min_runs=8, excluded=None):\n    if not data_dir.exists():\n        raise ValueError(f\"Data directory not found: {data_dir}\")\n\n    excluded = excluded or set()\n    subjects = []\n\n    for subject_dir in sorted(data_dir.iterdir()):\n        if not subject_dir.is_dir() or not subject_dir.name.startswith('S'):\n            continue\n\n        if subject_dir.name in excluded:\n            continue\n\n        edf_files = list(subject_dir.glob('*.edf'))\n        if len(edf_files) >= min_runs:\n            subjects.append(subject_dir.name)\n\n    return subjects\n\n\n# Scan for available subjects\nprint(\"\\nScanning for subjects...\")\ndata_dir = CONFIG['data']['raw_data_dir']\nprint(f\"Looking for data in: {data_dir}\")\n\nall_subjects = get_available_subjects(\n    data_dir,\n    min_runs=CONFIG['min_runs_per_subject'],\n    excluded=EXCLUDED_SUBJECTS\n)\nsubjects = all_subjects[:CONFIG['max_subjects']]\n\nprint(f\"Found {len(all_subjects)} clean subjects\")\nprint(f\"Will process {len(subjects)} subjects: {subjects}\")\n\n# Define which runs to use\nMOTOR_IMAGERY_RUNS = ['R07', 'R08', 'R09', 'R10', 'R11', 'R12', 'R13', 'R14']\nMOTOR_EXECUTION_RUNS = ['R03', 'R04', 'R05', 'R06']\nALL_TASK_RUNS = MOTOR_IMAGERY_RUNS + MOTOR_EXECUTION_RUNS\nprint(f\"Using runs: {ALL_TASK_RUNS}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T21:43:11.042158Z","iopub.execute_input":"2025-10-31T21:43:11.042480Z","iopub.status.idle":"2025-10-31T21:43:11.943674Z","shell.execute_reply.started":"2025-10-31T21:43:11.042449Z","shell.execute_reply":"2025-10-31T21:43:11.942994Z"}},"outputs":[{"name":"stdout","text":"\nScanning for subjects...\nLooking for data in: /kaggle/input/physioneteegmi\nFound 82 clean subjects\nWill process 20 subjects: ['S001', 'S002', 'S005', 'S006', 'S007', 'S008', 'S011', 'S014', 'S015', 'S016', 'S020', 'S030', 'S031', 'S032', 'S033', 'S034', 'S035', 'S036', 'S037', 'S038']\nUsing runs: ['R07', 'R08', 'R09', 'R10', 'R11', 'R12', 'R13', 'R14', 'R03', 'R04', 'R05', 'R06']\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"## 5. PyTorch Dataset","metadata":{}},{"cell_type":"code","source":"class EEGDataset(Dataset):\n    def __init__(self, x, y):\n        self.x = torch.FloatTensor(x).unsqueeze(1)\n        self.y = torch.LongTensor(y)\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, i):\n        return self.x[i], self.y[i]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T21:43:11.944500Z","iopub.execute_input":"2025-10-31T21:43:11.944733Z","iopub.status.idle":"2025-10-31T21:43:11.950040Z","shell.execute_reply.started":"2025-10-31T21:43:11.944716Z","shell.execute_reply":"2025-10-31T21:43:11.949272Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## 6. SparseGCN-CARM Architecture\n\n### 6.1 Channel Attention Module\nMulti-head attention to learn channel importance explicitly","metadata":{}},{"cell_type":"code","source":"class ChannelAttention(nn.Module):\n    def __init__(self, num_channels, num_heads=4):\n        super().__init__()\n        self.num_channels = num_channels\n        self.num_heads = num_heads\n        self.head_dim = max(1, num_channels // num_heads)\n\n        # Channel embedding\n        self.channel_embed = nn.Parameter(torch.randn(num_channels, self.head_dim * num_heads))\n        nn.init.xavier_uniform_(self.channel_embed)\n\n        # Attention weights\n        self.query = nn.Linear(self.head_dim * num_heads, self.head_dim * num_heads)\n        self.key = nn.Linear(self.head_dim * num_heads, self.head_dim * num_heads)\n\n        # Final projection to importance scores\n        self.importance_proj = nn.Sequential(\n            nn.Linear(self.head_dim * num_heads, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x=None):\n        embed = self.channel_embed  # (C, D)\n\n        Q = self.query(embed)  # (C, D)\n        K = self.key(embed)    # (C, D)\n\n        # Multi-head attention\n        Q = Q.view(self.num_channels, self.num_heads, self.head_dim)\n        K = K.view(self.num_channels, self.num_heads, self.head_dim)\n\n        # Attention scores\n        attn = torch.einsum('chd,khd->chk', Q, K) / np.sqrt(self.head_dim)\n        attn = F.softmax(attn, dim=-1)\n\n        # Aggregate across heads\n        attn_pooled = attn.mean(dim=1)  # (C, C)\n        channel_scores = attn_pooled.sum(dim=1)  # (C,)\n        channel_scores = channel_scores / channel_scores.sum()\n\n        # Also use learned projection\n        importance = self.importance_proj(embed).squeeze(-1) if embed.dim() > 1 else self.importance_proj(embed)\n\n        # Combine both signals\n        final_importance = 0.5 * channel_scores + 0.5 * importance\n        final_importance = final_importance / (final_importance.sum() + 1e-8)\n\n        return final_importance, attn\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T21:43:11.950665Z","iopub.execute_input":"2025-10-31T21:43:11.950923Z","iopub.status.idle":"2025-10-31T21:43:11.964404Z","shell.execute_reply.started":"2025-10-31T21:43:11.950905Z","shell.execute_reply":"2025-10-31T21:43:11.963648Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"### 6.2 Multi-Scale Temporal Convolution","metadata":{}},{"cell_type":"code","source":"class MultiScaleTemporalConv(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_sizes=[8, 16, 32], pool=True):\n        super().__init__()\n        self.pool = pool\n        k_num = len(kernel_sizes)\n        base = out_channels // k_num\n        rem = out_channels - base * k_num\n        out_list = [base + (1 if i < rem else 0) for i in range(k_num)]\n\n        branches = []\n        for i, k in enumerate(kernel_sizes):\n            oc = out_list[i]\n            branches.append(\n                nn.Sequential(\n                    nn.Conv2d(in_channels, oc, kernel_size=(1, k), padding=(0, k//2), bias=False),\n                    nn.BatchNorm2d(oc),\n                    nn.ELU()\n                )\n            )\n        self.branches = nn.ModuleList(branches)\n        self.pool_layer = nn.AvgPool2d(kernel_size=(1, 2)) if pool else None\n\n    def forward(self, x):\n        branch_outputs = [branch(x) for branch in self.branches]\n        x = torch.cat(branch_outputs, dim=1)\n        return self.pool_layer(x) if self.pool else x\n\n\ndef pairnorm(x, node_dim=2, eps=1e-6):\n    m = x.mean(dim=node_dim, keepdim=True)\n    xc = x - m\n    v = (xc * xc).mean(dim=node_dim, keepdim=True)\n    return xc / torch.sqrt(v + eps)\n\n\ndef build_feat_topk_adj(x, k, active_channels=None):\n    B, H, C, T = x.shape\n\n    if active_channels is not None:\n        mask = active_channels.view(C, 1).to(x.device)\n        x_masked = x * mask.view(1, 1, C, 1)\n    else:\n        x_masked = x\n\n    E = x_masked.permute(2, 1, 0, 3).contiguous().view(C, H, B*T).mean(2)\n    En = F.normalize(E, p=2, dim=1)\n    S = (En @ En.t()).clamp_min(0.0)\n    k = max(1, min(int(k), C))\n    vals, idx = torch.topk(S, k, dim=1)\n    M = torch.zeros_like(S)\n    M.scatter_(1, idx, 1.0)\n    A = S * M\n    A = torch.softmax(A, 1)\n    A = 0.5 * (A + A.t())\n    return A\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T21:43:11.965255Z","iopub.execute_input":"2025-10-31T21:43:11.965598Z","iopub.status.idle":"2025-10-31T21:43:11.981349Z","shell.execute_reply.started":"2025-10-31T21:43:11.965577Z","shell.execute_reply":"2025-10-31T21:43:11.980642Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"### 6.3 Feature-Adaptive GCN Layer (from CARMv2)","metadata":{}},{"cell_type":"code","source":"class AdaptiveGCNLayer(nn.Module):\n    def __init__(self, C, H, topk_k=8, lambda_feat=0.3, hop_alpha=0.5, edge_dropout=0.1,\n                 use_pairnorm=True, use_residual=True):\n        super().__init__()\n        self.C = C\n        self.H = H\n        self.k = topk_k\n        self.lf = lambda_feat\n        self.ha = hop_alpha\n        self.ed = edge_dropout\n        self.pn = use_pairnorm\n        self.res = use_residual\n\n        self.W = nn.Parameter(torch.empty(C, C))\n        nn.init.xavier_uniform_(self.W)\n\n        self.th = nn.Linear(H, H, bias=False)\n        self.bn = nn.BatchNorm2d(H)\n        self.act = nn.ELU()\n        self.last = None\n\n    def _learned(self, dev, active_channels=None):\n        A = torch.sigmoid(self.W)\n        A = 0.5 * (A + A.t())\n\n        if active_channels is not None:\n            mask = active_channels.view(self.C, 1).to(dev)\n            A = A * mask * mask.t()\n\n        I = torch.eye(self.C, device=dev, dtype=A.dtype)\n        At = A + I\n        d = torch.pow(At.sum(1).clamp_min(1e-6), -0.5)\n        D = torch.diag(d)\n        return D @ At @ D\n\n    def forward(self, x, active_channels=None):\n        B, H, C, T = x.shape\n\n        Al = self._learned(x.device, active_channels)\n        A2 = Al @ Al\n        Ah = (1 - self.ha) * Al + self.ha * A2\n        Af = build_feat_topk_adj(x, self.k, active_channels)\n        A = (1 - self.lf) * Ah + self.lf * Af\n\n        if self.training and self.ed > 0:\n            M = (torch.rand_like(A) > self.ed).float()\n            A = 0.5 * ((A * M) + (A * M).t())\n            A = A + torch.eye(C, device=A.device, dtype=A.dtype)\n\n        d = torch.pow(A.sum(1).clamp_min(1e-6), -0.5)\n        D = torch.diag(d)\n        A = D @ A @ D\n\n        xb = x.permute(0, 3, 2, 1).contiguous().view(B*T, C, H)\n        xg = A @ xb\n        xg = self.th(xg)\n        xg = xg.view(B, T, C, H).permute(0, 3, 2, 1)\n\n        if self.res:\n            if active_channels is not None:\n                x_res = x * active_channels.view(1, 1, C, 1)\n            else:\n                x_res = x\n            out = xg + x_res\n        else:\n            out = xg\n        out = pairnorm(out, 2) if self.pn else out\n        out = self.bn(out)\n        out = self.act(out)\n\n        self.last = {'learned': Al.detach().cpu().numpy()}\n        return out\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T21:43:11.982165Z","iopub.execute_input":"2025-10-31T21:43:11.982378Z","iopub.status.idle":"2025-10-31T21:43:11.997969Z","shell.execute_reply.started":"2025-10-31T21:43:11.982360Z","shell.execute_reply":"2025-10-31T21:43:11.997243Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"### 6.4 SparseGCN-CARM Main Model","metadata":{}},{"cell_type":"code","source":"class SparseGCNCARMModel(nn.Module):\n    def __init__(self, C, T, K, H, config):\n        super().__init__()\n        self.C = C\n        self.config = config\n\n        # Channel attention\n        if config.get('use_channel_attention', True):\n            self.channel_attention = ChannelAttention(C, config.get('attention_heads', 4))\n        else:\n            self.channel_attention = None\n\n        # Multi-scale temporal + adaptive GCN layers\n        scales = config.get('temporal_scales', [8, 16, 32])\n        self.t1 = MultiScaleTemporalConv(1, H, scales, False)\n        self.g1 = AdaptiveGCNLayer(C, H, config.get('topk_k', 8), config.get('lambda_feat', 0.3),\n                                   config.get('hop_alpha', 0.5), config.get('edge_dropout', 0.1),\n                                   config.get('use_pairnorm', True), config.get('use_residual', True))\n        self.t2 = MultiScaleTemporalConv(H, H, scales, True)\n        self.g2 = AdaptiveGCNLayer(C, H, config.get('topk_k', 8), config.get('lambda_feat', 0.3),\n                                   config.get('hop_alpha', 0.5), config.get('edge_dropout', 0.1),\n                                   config.get('use_pairnorm', True), config.get('use_residual', True))\n        self.t3 = MultiScaleTemporalConv(H, H, scales, True)\n        self.g3 = AdaptiveGCNLayer(C, H, config.get('topk_k', 8), config.get('lambda_feat', 0.3),\n                                   config.get('hop_alpha', 0.5), config.get('edge_dropout', 0.1),\n                                   config.get('use_pairnorm', True), config.get('use_residual', True))\n\n        with torch.no_grad():\n            ft = self._forward_features(torch.zeros(1, 1, C, T), None)\n            fs = ft.view(1, -1).size(1)\n\n        self.fc1 = nn.Linear(fs, 256)\n        self.dropout = nn.Dropout(0.5)\n        self.fc2 = nn.Linear(256, K)\n\n        # Active channels mask\n        self.register_buffer('active_channels', torch.ones(C))\n        self.channel_importance = None\n\n    def _forward_features(self, x, active_channels):\n        x = self.g1(self.t1(x), active_channels)\n        x = self.g2(self.t2(x), active_channels)\n        x = self.g3(self.t3(x), active_channels)\n        return x\n\n    def forward(self, x):\n        # Get channel importance\n        if self.channel_attention is not None:\n            importance, attn = self.channel_attention(x)\n            self.channel_importance = importance\n\n            # Apply soft channel gating during training\n            if self.training:\n                x = x * importance.view(1, 1, self.C, 1)\n\n        # Forward through network\n        x = self._forward_features(x, self.active_channels)\n\n        x = x.view(x.size(0), -1)\n        x = F.relu(self.fc1(x))\n        x = self.dropout(x)\n        return self.fc2(x)\n\n    def prune_channels(self, prune_ratio, min_channels=20):\n        if self.channel_importance is None:\n            return 0\n\n        num_active = int(self.active_channels.sum().item())\n        num_to_prune = max(1, int(num_active * prune_ratio))\n\n        if num_active - num_to_prune < min_channels:\n            return 0\n\n        importance = self.channel_importance.detach().cpu()\n        active_importance = importance * self.active_channels.cpu()\n\n        _, sorted_indices = torch.sort(active_importance)\n\n        pruned = 0\n        for idx in sorted_indices:\n            if self.active_channels[idx] > 0:\n                self.active_channels[idx] = 0\n                pruned += 1\n                if pruned >= num_to_prune:\n                    break\n\n        return pruned\n\n    def get_channel_importance(self):\n        if self.channel_importance is not None:\n            return self.channel_importance.detach().cpu().numpy()\n        return None\n\n    def get_active_channels_mask(self):\n        return self.active_channels.cpu().numpy()\n\n    def get_final_adjacency(self):\n        return self.g3.last.get('learned', None) if self.g3.last else None\n\n\n# Training functions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T21:43:11.998832Z","iopub.execute_input":"2025-10-31T21:43:11.999483Z","iopub.status.idle":"2025-10-31T21:43:12.015436Z","shell.execute_reply.started":"2025-10-31T21:43:11.999463Z","shell.execute_reply":"2025-10-31T21:43:12.014724Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"## 7. Training with Progressive Pruning","metadata":{}},{"cell_type":"code","source":"def train_epoch_sparse(model, dataloader, criterion, optimizer, device, config):\n    model.train()\n    total_loss = 0.0\n    all_preds, all_labels = [], []\n\n    for x, y in dataloader:\n        x, y = x.to(device), y.to(device)\n        optimizer.zero_grad()\n\n        logits = model(x)\n        loss = criterion(logits, y)\n\n        # Add channel importance regularization\n        if model.channel_importance is not None:\n            importance_loss = config.get('channel_importance_loss', 1e-3) * model.channel_importance.abs().mean()\n            loss = loss + importance_loss\n\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n        all_preds += torch.argmax(logits, 1).cpu().tolist()\n        all_labels += y.cpu().tolist()\n\n    return total_loss / max(1, len(dataloader)), accuracy_score(all_labels, all_preds)\n\n\n@torch.no_grad()\ndef evaluate(model, dataloader, criterion, device):\n    model.eval()\n    total_loss = 0.0\n    all_preds, all_labels = [], []\n\n    for x, y in dataloader:\n        x, y = x.to(device), y.to(device)\n        logits = model(x)\n        loss = criterion(logits, y)\n\n        total_loss += loss.item()\n        all_preds += torch.argmax(logits, 1).cpu().tolist()\n        all_labels += y.cpu().tolist()\n\n    return total_loss / max(1, len(dataloader)), accuracy_score(all_labels, all_preds)\n\n\ndef train_with_pruning(model, train_loader, val_loader, device, epochs, lr, patience, config):\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, mode='min', factor=0.5, patience=3, verbose=False\n    )\n\n    best_acc = 0.0\n    best_state = None\n    no_improve = 0\n\n    prune_enabled = config.get('prune_enabled', True)\n    prune_start = config.get('prune_start_epoch', 10)\n    prune_every = config.get('prune_every', 2)\n    prune_ratio = config.get('prune_ratio', 0.05)\n    min_channels = config.get('min_channels', 20)\n\n    for epoch in range(epochs):\n        train_loss, train_acc = train_epoch_sparse(model, train_loader, criterion, optimizer, device, config)\n        val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n\n        scheduler.step(val_loss)\n\n        # Progressive pruning\n        if prune_enabled and epoch >= prune_start and (epoch - prune_start) % prune_every == 0:\n            num_pruned = model.prune_channels(prune_ratio, min_channels)\n            if num_pruned > 0:\n                active = int(model.active_channels.sum().item())\n                print(f\"    Epoch {epoch+1}: Pruned {num_pruned} channels, {active} remaining\")\n\n        if val_acc > best_acc:\n            best_acc = val_acc\n            best_state = deepcopy(model.state_dict())\n            no_improve = 0\n        else:\n            no_improve += 1\n\n        if no_improve >= patience:\n            break\n\n    if best_state is None:\n        best_state = deepcopy(model.state_dict())\n\n    model.load_state_dict(best_state)\n    return best_state, best_acc\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T21:43:12.017667Z","iopub.execute_input":"2025-10-31T21:43:12.017896Z","iopub.status.idle":"2025-10-31T21:43:12.034480Z","shell.execute_reply.started":"2025-10-31T21:43:12.017880Z","shell.execute_reply":"2025-10-31T21:43:12.033685Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"## 8. Improved Channel Selector","metadata":{}},{"cell_type":"code","source":"class ImprovedChannelSelector:\n    def __init__(self, adjacency, importance_scores, channel_names):\n        self.A = adjacency\n        self.importance = importance_scores\n        self.names = np.array(channel_names)\n        self.C = adjacency.shape[0]\n\n    def importance_selection(self, k):\n        k = min(int(k), self.C)\n        indices = np.argsort(self.importance)[-k:]\n        indices = np.sort(indices)\n        return self.names[indices].tolist(), indices\n\n    def hybrid_selection(self, k):\n        connectivity = np.sum(np.abs(self.A), 1)\n        combined_score = 0.7 * self.importance + 0.3 * (connectivity / connectivity.max())\n\n        k = min(int(k), self.C)\n        indices = np.argsort(combined_score)[-k:]\n        indices = np.sort(indices)\n        return self.names[indices].tolist(), indices\n\n    def edge_selection(self, k):\n        edges = []\n        for i in range(self.C):\n            for j in range(i+1, self.C):\n                edges.append((i, j, abs(self.A[i, j]) + abs(self.A[j, i])))\n        edges.sort(key=lambda t: t[2], reverse=True)\n        top_edges = edges[:int(k)]\n        indices = sorted(set([i for i, _, _ in top_edges] + [j for _, j, _ in top_edges]))\n        return self.names[indices].tolist(), np.array(indices)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T21:43:12.035356Z","iopub.execute_input":"2025-10-31T21:43:12.035591Z","iopub.status.idle":"2025-10-31T21:43:12.051198Z","shell.execute_reply.started":"2025-10-31T21:43:12.035573Z","shell.execute_reply":"2025-10-31T21:43:12.050463Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"## 9. Main Training Loop","metadata":{}},{"cell_type":"code","source":"all_results = []\n\nfor subject_id in tqdm(subjects, desc='Training SparseGCN'):\n    print(f\"\\nProcessing {subject_id}...\")\n\n    X, Y, channel_names = load_subject_data(data_dir, subject_id, ALL_TASK_RUNS, CONFIG)\n    if X is None:\n        print(f\"  Skipped (no data)\")\n        continue\n\n    C, T = X.shape[1], X.shape[2]\n    K = len(set(CONFIG['data']['selected_classes']))\n    print(f\"  Data shape: {X.shape}, Classes: {K}\")\n\n    skf = StratifiedKFold(n_splits=CONFIG['model']['n_folds'], shuffle=True, random_state=42)\n    fold_results = []\n    adjacencies = []\n    importance_scores = []\n\n    for fold, (train_idx, val_idx) in enumerate(skf.split(X, Y)):\n        X_train, X_val = normalize(X[train_idx]), normalize(X[val_idx])\n        Y_train, Y_val = Y[train_idx], Y[val_idx]\n\n        train_loader = DataLoader(\n            EEGDataset(X_train, Y_train),\n            batch_size=CONFIG['model']['batch_size'],\n            shuffle=True,\n            num_workers=0\n        )\n        val_loader = DataLoader(\n            EEGDataset(X_val, Y_val),\n            batch_size=CONFIG['model']['batch_size'],\n            shuffle=False,\n            num_workers=0\n        )\n\n        model = SparseGCNCARMModel(C, T, K, CONFIG['model']['hidden_dim'], CONFIG['sparsegcn']).to(device)\n\n        best_state, best_acc = train_with_pruning(\n            model, train_loader, val_loader, device,\n            CONFIG['model']['epochs'],\n            CONFIG['model']['learning_rate'],\n            CONFIG['model']['patience'],\n            CONFIG['sparsegcn']\n        )\n\n        model.load_state_dict(best_state)\n        _, accuracy = evaluate(model, val_loader, nn.CrossEntropyLoss(), device)\n\n        adjacency = model.get_final_adjacency()\n        importance = model.get_channel_importance()\n        active_mask = model.get_active_channels_mask()\n\n        adjacencies.append(adjacency)\n        importance_scores.append(importance)\n\n        fold_results.append({\n            'fold': fold,\n            'val_acc': accuracy,\n            'num_active': int(active_mask.sum())\n        })\n\n        print(f\"  Fold {fold+1}: {accuracy:.4f} ({int(active_mask.sum())}/{C} channels)\")\n\n    avg_acc = np.mean([f['val_acc'] for f in fold_results])\n    avg_active = np.mean([f['num_active'] for f in fold_results])\n\n    all_results.append({\n        'subject': subject_id,\n        'accuracy': avg_acc,\n        'avg_active_channels': avg_active,\n        'adjacency_matrix': np.mean(adjacencies, 0),\n        'channel_importance': np.mean(importance_scores, 0),\n        'channel_names': channel_names\n    })\n\n    print(f\"  Final: {avg_acc:.4f} ({avg_active:.0f} active channels)\")\n\nprint(\"\\nTraining complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T21:43:12.051972Z","iopub.execute_input":"2025-10-31T21:43:12.052182Z","iopub.status.idle":"2025-10-31T23:52:14.242359Z","shell.execute_reply.started":"2025-10-31T21:43:12.052161Z","shell.execute_reply":"2025-10-31T23:52:14.241443Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Training SparseGCN:   0%|          | 0/20 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69d7d016177e403093f6de969a01a689"}},"metadata":{}},{"name":"stdout","text":"\nProcessing S001...\n  Data shape: (252, 64, 769), Classes: 2\n    Epoch 11: Pruned 3 channels, 61 remaining\n    Epoch 13: Pruned 3 channels, 58 remaining\n  Fold 1: 0.9286 (64/64 channels)\n    Epoch 11: Pruned 3 channels, 61 remaining\n    Epoch 13: Pruned 3 channels, 58 remaining\n  Fold 2: 0.9167 (64/64 channels)\n    Epoch 11: Pruned 3 channels, 61 remaining\n    Epoch 13: Pruned 3 channels, 58 remaining\n    Epoch 15: Pruned 2 channels, 56 remaining\n    Epoch 17: Pruned 2 channels, 54 remaining\n    Epoch 19: Pruned 2 channels, 52 remaining\n  Fold 3: 0.9167 (64/64 channels)\n  Final: 0.9206 (64 active channels)\n\nProcessing S002...\n  Data shape: (252, 64, 769), Classes: 2\n    Epoch 11: Pruned 3 channels, 61 remaining\n    Epoch 13: Pruned 3 channels, 58 remaining\n    Epoch 15: Pruned 2 channels, 56 remaining\n    Epoch 17: Pruned 2 channels, 54 remaining\n  Fold 1: 0.7976 (64/64 channels)\n    Epoch 11: Pruned 3 channels, 61 remaining\n    Epoch 13: Pruned 3 channels, 58 remaining\n    Epoch 15: Pruned 2 channels, 56 remaining\n    Epoch 17: Pruned 2 channels, 54 remaining\n    Epoch 19: Pruned 2 channels, 52 remaining\n  Fold 2: 0.7857 (64/64 channels)\n    Epoch 11: Pruned 3 channels, 61 remaining\n    Epoch 13: Pruned 3 channels, 58 remaining\n    Epoch 15: Pruned 2 channels, 56 remaining\n    Epoch 17: Pruned 2 channels, 54 remaining\n    Epoch 19: Pruned 2 channels, 52 remaining\n  Fold 3: 0.8571 (64/64 channels)\n  Final: 0.8135 (64 active channels)\n\nProcessing S005...\n  Data shape: (252, 64, 769), Classes: 2\n    Epoch 11: Pruned 3 channels, 61 remaining\n    Epoch 13: Pruned 3 channels, 58 remaining\n    Epoch 15: Pruned 2 channels, 56 remaining\n  Fold 1: 0.8690 (64/64 channels)\n    Epoch 11: Pruned 3 channels, 61 remaining\n  Fold 2: 0.8214 (64/64 channels)\n    Epoch 11: Pruned 3 channels, 61 remaining\n    Epoch 13: Pruned 3 channels, 58 remaining\n    Epoch 15: Pruned 2 channels, 56 remaining\n    Epoch 17: Pruned 2 channels, 54 remaining\n  Fold 3: 0.8810 (64/64 channels)\n  Final: 0.8571 (64 active channels)\n\nProcessing S006...\n  Data shape: (252, 64, 769), Classes: 2\n    Epoch 11: Pruned 3 channels, 61 remaining\n    Epoch 13: Pruned 3 channels, 58 remaining\n    Epoch 15: Pruned 2 channels, 56 remaining\n  Fold 1: 0.8333 (64/64 channels)\n    Epoch 11: Pruned 3 channels, 61 remaining\n    Epoch 13: Pruned 3 channels, 58 remaining\n    Epoch 15: Pruned 2 channels, 56 remaining\n    Epoch 17: Pruned 2 channels, 54 remaining\n    Epoch 19: Pruned 2 channels, 52 remaining\n    Epoch 21: Pruned 2 channels, 50 remaining\n  Fold 2: 0.8690 (61/64 channels)\n    Epoch 11: Pruned 3 channels, 61 remaining\n    Epoch 13: Pruned 3 channels, 58 remaining\n    Epoch 15: Pruned 2 channels, 56 remaining\n    Epoch 17: Pruned 2 channels, 54 remaining\n    Epoch 19: Pruned 2 channels, 52 remaining\n    Epoch 21: Pruned 2 channels, 50 remaining\n    Epoch 23: Pruned 2 channels, 48 remaining\n    Epoch 25: Pruned 2 channels, 46 remaining\n  Fold 3: 0.9167 (56/64 channels)\n  Final: 0.8730 (60 active channels)\n\nProcessing S007...\n  Data shape: (252, 64, 769), Classes: 2\n    Epoch 11: Pruned 3 channels, 61 remaining\n    Epoch 13: Pruned 3 channels, 58 remaining\n  Fold 1: 0.9405 (64/64 channels)\n    Epoch 11: Pruned 3 channels, 61 remaining\n    Epoch 13: Pruned 3 channels, 58 remaining\n    Epoch 15: Pruned 2 channels, 56 remaining\n    Epoch 17: Pruned 2 channels, 54 remaining\n    Epoch 19: Pruned 2 channels, 52 remaining\n  Fold 2: 0.9167 (64/64 channels)\n    Epoch 11: Pruned 3 channels, 61 remaining\n    Epoch 13: Pruned 3 channels, 58 remaining\n    Epoch 15: Pruned 2 channels, 56 remaining\n    Epoch 17: Pruned 2 channels, 54 remaining\n    Epoch 19: Pruned 2 channels, 52 remaining\n    Epoch 21: Pruned 2 channels, 50 remaining\n    Epoch 23: Pruned 2 channels, 48 remaining\n  Fold 3: 0.9643 (58/64 channels)\n  Final: 0.9405 (62 active channels)\n\nProcessing S008...\n  Data shape: (252, 64, 769), Classes: 2\n    Epoch 11: Pruned 3 channels, 61 remaining\n    Epoch 13: Pruned 3 channels, 58 remaining\n    Epoch 15: Pruned 2 channels, 56 remaining\n  Fold 1: 0.9405 (64/64 channels)\n    Epoch 11: Pruned 3 channels, 61 remaining\n    Epoch 13: Pruned 3 channels, 58 remaining\n  Fold 2: 0.9286 (64/64 channels)\n    Epoch 11: Pruned 3 channels, 61 remaining\n  Fold 3: 0.9643 (64/64 channels)\n  Final: 0.9444 (64 active channels)\n\nProcessing S011...\n  Data shape: (252, 64, 769), Classes: 2\n    Epoch 11: Pruned 3 channels, 61 remaining\n  Fold 1: 0.8929 (64/64 channels)\n    Epoch 11: Pruned 3 channels, 61 remaining\n  Fold 2: 0.8571 (64/64 channels)\n    Epoch 11: Pruned 3 channels, 61 remaining\n    Epoch 13: Pruned 3 channels, 58 remaining\n    Epoch 15: Pruned 2 channels, 56 remaining\n  Fold 3: 0.8214 (64/64 channels)\n  Final: 0.8571 (64 active channels)\n\nProcessing S014...\n  Data shape: (252, 64, 769), Classes: 2\n    Epoch 11: Pruned 3 channels, 61 remaining\n    Epoch 13: Pruned 3 channels, 58 remaining\n  Fold 1: 0.7976 (64/64 channels)\n    Epoch 11: Pruned 3 channels, 61 remaining\n    Epoch 13: Pruned 3 channels, 58 remaining\n    Epoch 15: Pruned 2 channels, 56 remaining\n    Epoch 17: Pruned 2 channels, 54 remaining\n  Fold 2: 0.8690 (64/64 channels)\n    Epoch 11: Pruned 3 channels, 61 remaining\n    Epoch 13: Pruned 3 channels, 58 remaining\n    Epoch 15: Pruned 2 channels, 56 remaining\n    Epoch 17: Pruned 2 channels, 54 remaining\n    Epoch 19: Pruned 2 channels, 52 remaining\n    Epoch 21: Pruned 2 channels, 50 remaining\n    Epoch 23: Pruned 2 channels, 48 remaining\n    Epoch 25: Pruned 2 channels, 46 remaining\n    Epoch 27: Pruned 2 channels, 44 remaining\n    Epoch 29: Pruned 2 channels, 42 remaining\n    Epoch 31: Pruned 2 channels, 40 remaining\n  Fold 3: 0.7976 (50/64 channels)\n  Final: 0.8214 (59 active channels)\n\nProcessing S015...\n  Data shape: (252, 64, 769), Classes: 2\n    Epoch 11: Pruned 3 channels, 61 remaining\n    Epoch 13: Pruned 3 channels, 58 remaining\n    Epoch 15: Pruned 2 channels, 56 remaining\n    Epoch 17: Pruned 2 channels, 54 remaining\n    Epoch 19: Pruned 2 channels, 52 remaining\n    Epoch 21: Pruned 2 channels, 50 remaining\n  Fold 1: 0.8333 (61/64 channels)\n    Epoch 11: Pruned 3 channels, 61 remaining\n    Epoch 13: Pruned 3 channels, 58 remaining\n    Epoch 15: Pruned 2 channels, 56 remaining\n  Fold 2: 0.9167 (64/64 channels)\n    Epoch 11: Pruned 3 channels, 61 remaining\n    Epoch 13: Pruned 3 channels, 58 remaining\n  Fold 3: 0.8810 (64/64 channels)\n  Final: 0.8770 (63 active channels)\n\nProcessing S016...\n  Data shape: (252, 64, 769), Classes: 2\n    Epoch 11: Pruned 3 channels, 61 remaining\n    Epoch 13: Pruned 3 channels, 58 remaining\n  Fold 1: 0.7262 (64/64 channels)\n    Epoch 11: Pruned 3 channels, 61 remaining\n    Epoch 13: Pruned 3 channels, 58 remaining\n    Epoch 15: Pruned 2 channels, 56 remaining\n    Epoch 17: Pruned 2 channels, 54 remaining\n    Epoch 19: Pruned 2 channels, 52 remaining\n    Epoch 21: Pruned 2 channels, 50 remaining\n  Fold 2: 0.8214 (61/64 channels)\n    Epoch 11: Pruned 3 channels, 61 remaining\n    Epoch 13: Pruned 3 channels, 58 remaining\n    Epoch 15: Pruned 2 channels, 56 remaining\n    Epoch 17: Pruned 2 channels, 54 remaining\n    Epoch 19: Pruned 2 channels, 52 remaining\n  Fold 3: 0.7381 (64/64 channels)\n  Final: 0.7619 (63 active channels)\n\nProcessing S020...\n  Data shape: (252, 64, 769), Classes: 2\n    Epoch 11: Pruned 3 channels, 61 remaining\n    Epoch 13: Pruned 3 channels, 58 remaining\n  Fold 1: 0.9762 (64/64 channels)\n    Epoch 11: Pruned 3 channels, 61 remaining\n  Fold 2: 0.9881 (64/64 channels)\n    Epoch 11: Pruned 3 channels, 61 remaining\n    Epoch 13: Pruned 3 channels, 58 remaining\n    Epoch 15: Pruned 2 channels, 56 remaining\n    Epoch 17: Pruned 2 channels, 54 remaining\n  Fold 3: 0.9405 (64/64 channels)\n  Final: 0.9683 (64 active channels)\n\nProcessing S030...\n  Data shape: (252, 64, 769), Classes: 2\n    Epoch 11: Pruned 3 channels, 61 remaining\n    Epoch 13: Pruned 3 channels, 58 remaining\n  Fold 1: 0.8810 (64/64 channels)\n    Epoch 11: Pruned 3 channels, 61 remaining\n    Epoch 13: Pruned 3 channels, 58 remaining\n    Epoch 15: Pruned 2 channels, 56 remaining\n  Fold 2: 0.8929 (64/64 channels)\n    Epoch 11: Pruned 3 channels, 61 remaining\n    Epoch 13: Pruned 3 channels, 58 remaining\n    Epoch 15: Pruned 2 channels, 56 remaining\n    Epoch 17: Pruned 2 channels, 54 remaining\n    Epoch 19: Pruned 2 channels, 52 remaining\n  Fold 3: 0.8810 (64/64 channels)\n  Final: 0.8849 (64 active channels)\n\nProcessing S031...\n  Data shape: (252, 64, 769), Classes: 2\n    Epoch 11: Pruned 3 channels, 61 remaining\n    Epoch 13: Pruned 3 channels, 58 remaining\n    Epoch 15: Pruned 2 channels, 56 remaining\n    Epoch 17: Pruned 2 channels, 54 remaining\n    Epoch 19: Pruned 2 channels, 52 remaining\n  Fold 1: 0.8571 (64/64 channels)\n    Epoch 11: Pruned 3 channels, 61 remaining\n    Epoch 13: Pruned 3 channels, 58 remaining\n  Fold 2: 0.8929 (64/64 channels)\n    Epoch 11: Pruned 3 channels, 61 remaining\n  Fold 3: 0.9048 (64/64 channels)\n  Final: 0.8849 (64 active channels)\n\nProcessing S032...\n  Data shape: (252, 64, 769), Classes: 2\n    Epoch 11: Pruned 3 channels, 61 remaining\n    Epoch 13: Pruned 3 channels, 58 remaining\n    Epoch 15: Pruned 2 channels, 56 remaining\n  Fold 1: 0.9524 (64/64 channels)\n    Epoch 11: Pruned 3 channels, 61 remaining\n    Epoch 13: Pruned 3 channels, 58 remaining\n    Epoch 15: Pruned 2 channels, 56 remaining\n    Epoch 17: Pruned 2 channels, 54 remaining\n    Epoch 19: Pruned 2 channels, 52 remaining\n    Epoch 21: Pruned 2 channels, 50 remaining\n  Fold 2: 0.9524 (61/64 channels)\n    Epoch 11: Pruned 3 channels, 61 remaining\n    Epoch 13: Pruned 3 channels, 58 remaining\n    Epoch 15: Pruned 2 channels, 56 remaining\n    Epoch 17: Pruned 2 channels, 54 remaining\n    Epoch 19: Pruned 2 channels, 52 remaining\n    Epoch 21: Pruned 2 channels, 50 remaining\n  Fold 3: 0.9643 (61/64 channels)\n  Final: 0.9563 (62 active channels)\n\nProcessing S033...\n  Data shape: (252, 64, 769), Classes: 2\n    Epoch 11: Pruned 3 channels, 61 remaining\n    Epoch 13: Pruned 3 channels, 58 remaining\n    Epoch 15: Pruned 2 channels, 56 remaining\n  Fold 1: 0.8929 (64/64 channels)\n    Epoch 11: Pruned 3 channels, 61 remaining\n    Epoch 13: Pruned 3 channels, 58 remaining\n    Epoch 15: Pruned 2 channels, 56 remaining\n    Epoch 17: Pruned 2 channels, 54 remaining\n    Epoch 19: Pruned 2 channels, 52 remaining\n    Epoch 21: Pruned 2 channels, 50 remaining\n    Epoch 23: Pruned 2 channels, 48 remaining\n  Fold 2: 0.9167 (58/64 channels)\n    Epoch 11: Pruned 3 channels, 61 remaining\n    Epoch 13: Pruned 3 channels, 58 remaining\n    Epoch 15: Pruned 2 channels, 56 remaining\n    Epoch 17: Pruned 2 channels, 54 remaining\n  Fold 3: 0.9405 (64/64 channels)\n  Final: 0.9167 (62 active channels)\n\nProcessing S034...\n  Data shape: (252, 64, 769), Classes: 2\n    Epoch 11: Pruned 3 channels, 61 remaining\n    Epoch 13: Pruned 3 channels, 58 remaining\n    Epoch 15: Pruned 2 channels, 56 remaining\n    Epoch 17: Pruned 2 channels, 54 remaining\n    Epoch 19: Pruned 2 channels, 52 remaining\n  Fold 1: 0.8810 (64/64 channels)\n    Epoch 11: Pruned 3 channels, 61 remaining\n    Epoch 13: Pruned 3 channels, 58 remaining\n    Epoch 15: Pruned 2 channels, 56 remaining\n  Fold 2: 0.8452 (64/64 channels)\n    Epoch 11: Pruned 3 channels, 61 remaining\n    Epoch 13: Pruned 3 channels, 58 remaining\n    Epoch 15: Pruned 2 channels, 56 remaining\n    Epoch 17: Pruned 2 channels, 54 remaining\n    Epoch 19: Pruned 2 channels, 52 remaining\n  Fold 3: 0.8929 (64/64 channels)\n  Final: 0.8730 (64 active channels)\n\nProcessing S035...\n  Data shape: (252, 64, 769), Classes: 2\n    Epoch 11: Pruned 3 channels, 61 remaining\n    Epoch 13: Pruned 3 channels, 58 remaining\n    Epoch 15: Pruned 2 channels, 56 remaining\n    Epoch 17: Pruned 2 channels, 54 remaining\n    Epoch 19: Pruned 2 channels, 52 remaining\n    Epoch 21: Pruned 2 channels, 50 remaining\n  Fold 1: 0.9286 (61/64 channels)\n    Epoch 11: Pruned 3 channels, 61 remaining\n    Epoch 13: Pruned 3 channels, 58 remaining\n    Epoch 15: Pruned 2 channels, 56 remaining\n    Epoch 17: Pruned 2 channels, 54 remaining\n    Epoch 19: Pruned 2 channels, 52 remaining\n    Epoch 21: Pruned 2 channels, 50 remaining\n    Epoch 23: Pruned 2 channels, 48 remaining\n    Epoch 25: Pruned 2 channels, 46 remaining\n    Epoch 27: Pruned 2 channels, 44 remaining\n  Fold 2: 0.9405 (54/64 channels)\n    Epoch 11: Pruned 3 channels, 61 remaining\n    Epoch 13: Pruned 3 channels, 58 remaining\n    Epoch 15: Pruned 2 channels, 56 remaining\n    Epoch 17: Pruned 2 channels, 54 remaining\n    Epoch 19: Pruned 2 channels, 52 remaining\n    Epoch 21: Pruned 2 channels, 50 remaining\n  Fold 3: 0.9524 (61/64 channels)\n  Final: 0.9405 (59 active channels)\n\nProcessing S036...\n  Data shape: (252, 64, 769), Classes: 2\n    Epoch 11: Pruned 3 channels, 61 remaining\n    Epoch 13: Pruned 3 channels, 58 remaining\n    Epoch 15: Pruned 2 channels, 56 remaining\n    Epoch 17: Pruned 2 channels, 54 remaining\n    Epoch 19: Pruned 2 channels, 52 remaining\n    Epoch 21: Pruned 2 channels, 50 remaining\n  Fold 1: 0.8810 (61/64 channels)\n    Epoch 11: Pruned 3 channels, 61 remaining\n    Epoch 13: Pruned 3 channels, 58 remaining\n  Fold 2: 0.7976 (64/64 channels)\n    Epoch 11: Pruned 3 channels, 61 remaining\n    Epoch 13: Pruned 3 channels, 58 remaining\n    Epoch 15: Pruned 2 channels, 56 remaining\n    Epoch 17: Pruned 2 channels, 54 remaining\n    Epoch 19: Pruned 2 channels, 52 remaining\n  Fold 3: 0.9405 (64/64 channels)\n  Final: 0.8730 (63 active channels)\n\nProcessing S037...\n  Data shape: (252, 64, 769), Classes: 2\n    Epoch 11: Pruned 3 channels, 61 remaining\n    Epoch 13: Pruned 3 channels, 58 remaining\n    Epoch 15: Pruned 2 channels, 56 remaining\n    Epoch 17: Pruned 2 channels, 54 remaining\n    Epoch 19: Pruned 2 channels, 52 remaining\n  Fold 1: 0.8214 (64/64 channels)\n    Epoch 11: Pruned 3 channels, 61 remaining\n    Epoch 13: Pruned 3 channels, 58 remaining\n    Epoch 15: Pruned 2 channels, 56 remaining\n    Epoch 17: Pruned 2 channels, 54 remaining\n  Fold 2: 0.8214 (64/64 channels)\n    Epoch 11: Pruned 3 channels, 61 remaining\n    Epoch 13: Pruned 3 channels, 58 remaining\n    Epoch 15: Pruned 2 channels, 56 remaining\n    Epoch 17: Pruned 2 channels, 54 remaining\n    Epoch 19: Pruned 2 channels, 52 remaining\n    Epoch 21: Pruned 2 channels, 50 remaining\n  Fold 3: 0.7857 (61/64 channels)\n  Final: 0.8095 (63 active channels)\n\nProcessing S038...\n  Data shape: (252, 64, 769), Classes: 2\n    Epoch 11: Pruned 3 channels, 61 remaining\n  Fold 1: 0.8095 (64/64 channels)\n    Epoch 11: Pruned 3 channels, 61 remaining\n    Epoch 13: Pruned 3 channels, 58 remaining\n    Epoch 15: Pruned 2 channels, 56 remaining\n    Epoch 17: Pruned 2 channels, 54 remaining\n    Epoch 19: Pruned 2 channels, 52 remaining\n    Epoch 21: Pruned 2 channels, 50 remaining\n  Fold 2: 0.8452 (61/64 channels)\n    Epoch 11: Pruned 3 channels, 61 remaining\n    Epoch 13: Pruned 3 channels, 58 remaining\n    Epoch 15: Pruned 2 channels, 56 remaining\n  Fold 3: 0.7738 (64/64 channels)\n  Final: 0.8095 (63 active channels)\n\nTraining complete!\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"## 10. Results and Comparison","metadata":{}},{"cell_type":"code","source":"results_df = pd.DataFrame(all_results)\n\nprint(\"SparseGCN-CARM Results:\")\nprint(f\"Mean accuracy: {results_df['accuracy'].mean():.4f}\")\nprint(f\"Std accuracy: {results_df['accuracy'].std():.4f}\")\nprint(f\"Avg active channels: {results_df['avg_active_channels'].mean():.1f}\")\n\n# Display results\nprint(\"\\nPer-subject results:\")\nprint(results_df[['subject', 'accuracy', 'avg_active_channels']])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T23:52:14.243270Z","iopub.execute_input":"2025-10-31T23:52:14.243720Z","iopub.status.idle":"2025-10-31T23:52:14.267058Z","shell.execute_reply.started":"2025-10-31T23:52:14.243700Z","shell.execute_reply":"2025-10-31T23:52:14.266407Z"}},"outputs":[{"name":"stdout","text":"SparseGCN-CARM Results:\nMean accuracy: 0.8792\nStd accuracy: 0.0567\nAvg active channels: 62.8\n\nPer-subject results:\n   subject  accuracy  avg_active_channels\n0     S001  0.920635            64.000000\n1     S002  0.813492            64.000000\n2     S005  0.857143            64.000000\n3     S006  0.873016            60.333333\n4     S007  0.940476            62.000000\n5     S008  0.944444            64.000000\n6     S011  0.857143            64.000000\n7     S014  0.821429            59.333333\n8     S015  0.876984            63.000000\n9     S016  0.761905            63.000000\n10    S020  0.968254            64.000000\n11    S030  0.884921            64.000000\n12    S031  0.884921            64.000000\n13    S032  0.956349            62.000000\n14    S033  0.916667            62.000000\n15    S034  0.873016            64.000000\n16    S035  0.940476            58.666667\n17    S036  0.873016            63.000000\n18    S037  0.809524            63.000000\n19    S038  0.809524            63.000000\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"## 11. Channel Selection Experiments","metadata":{}},{"cell_type":"code","source":"k_values = CONFIG['channel_selection']['k_values']\nselection_results = []\n\nfor result in all_results:\n    subject_id = result['subject']\n    full_acc = result['accuracy']\n    adjacency = result['adjacency_matrix']\n    importance = result['channel_importance']\n    channel_names = result['channel_names']\n\n    # Load subject data again\n    X, Y, _ = load_subject_data(data_dir, subject_id, ALL_TASK_RUNS, CONFIG)\n    if X is None:\n        continue\n\n    C, T = X.shape[1], X.shape[2]\n    K = len(set(CONFIG['data']['selected_classes']))\n\n    selector = ImprovedChannelSelector(adjacency, importance, channel_names)\n\n    for k in k_values:\n        # Try different selection methods\n        for method_name, method_func in [\n            ('IMP', selector.importance_selection),\n            ('HYB', selector.hybrid_selection),\n            ('ES', selector.edge_selection)\n        ]:\n            selected_names, selected_indices = method_func(k)\n\n            # Retrain with selected channels\n            X_subset = X[:, selected_indices, :]\n\n            skf = StratifiedKFold(n_splits=CONFIG['model']['n_folds'], shuffle=True, random_state=42)\n            fold_accs = []\n\n            for fold, (train_idx, val_idx) in enumerate(skf.split(X_subset, Y)):\n                X_train = normalize(X_subset[train_idx])\n                X_val = normalize(X_subset[val_idx])\n                Y_train, Y_val = Y[train_idx], Y[val_idx]\n\n                train_loader = DataLoader(\n                    EEGDataset(X_train, Y_train),\n                    batch_size=CONFIG['model']['batch_size'],\n                    shuffle=True,\n                    num_workers=0\n                )\n                val_loader = DataLoader(\n                    EEGDataset(X_val, Y_val),\n                    batch_size=CONFIG['model']['batch_size'],\n                    shuffle=False,\n                    num_workers=0\n                )\n\n                # Use smaller model for selected channels\n                model = SparseGCNCARMModel(\n                    len(selected_indices), T, K,\n                    CONFIG['model']['hidden_dim'],\n                    {**CONFIG['sparsegcn'], 'prune_enabled': False}\n                ).to(device)\n\n                best_state, best_acc = train_with_pruning(\n                    model, train_loader, val_loader, device,\n                    CONFIG['model']['epochs'],\n                    CONFIG['model']['learning_rate'],\n                    CONFIG['model']['patience'],\n                    {**CONFIG['sparsegcn'], 'prune_enabled': False}\n                )\n\n                model.load_state_dict(best_state)\n                _, accuracy = evaluate(model, val_loader, nn.CrossEntropyLoss(), device)\n                fold_accs.append(accuracy)\n\n            subset_acc = np.mean(fold_accs)\n            drop = full_acc - subset_acc\n\n            selection_results.append({\n                'subject': subject_id,\n                'method': method_name,\n                'k': k,\n                'full_acc': full_acc,\n                'subset_acc': subset_acc,\n                'drop': drop,\n                'channels': ','.join(selected_names)\n            })\n\n            print(f\"{subject_id} {method_name} k={k}: {subset_acc:.4f} (drop: {drop:.4f})\")\n\nselection_df = pd.DataFrame(selection_results)\nprint(\"\\nChannel selection complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T23:52:14.267768Z","iopub.execute_input":"2025-10-31T23:52:14.268008Z","execution_failed":"2025-11-01T05:05:00.408Z"}},"outputs":[{"name":"stdout","text":"S001 IMP k=10: 0.8770 (drop: 0.0437)\nS001 HYB k=10: 0.9246 (drop: -0.0040)\nS001 ES k=10: 0.9048 (drop: 0.0159)\nS001 IMP k=15: 0.9048 (drop: 0.0159)\nS001 HYB k=15: 0.9206 (drop: 0.0000)\nS001 ES k=15: 0.9127 (drop: 0.0079)\nS001 IMP k=20: 0.9206 (drop: 0.0000)\nS001 HYB k=20: 0.9008 (drop: 0.0198)\nS001 ES k=20: 0.9167 (drop: 0.0040)\nS001 IMP k=25: 0.9048 (drop: 0.0159)\nS001 HYB k=25: 0.9008 (drop: 0.0198)\nS001 ES k=25: 0.9048 (drop: 0.0159)\nS002 IMP k=10: 0.7579 (drop: 0.0556)\nS002 HYB k=10: 0.7183 (drop: 0.0952)\nS002 ES k=10: 0.7183 (drop: 0.0952)\nS002 IMP k=15: 0.7619 (drop: 0.0516)\nS002 HYB k=15: 0.8016 (drop: 0.0119)\nS002 ES k=15: 0.7857 (drop: 0.0278)\nS002 IMP k=20: 0.7778 (drop: 0.0357)\nS002 HYB k=20: 0.7897 (drop: 0.0238)\nS002 ES k=20: 0.8095 (drop: 0.0040)\nS002 IMP k=25: 0.7857 (drop: 0.0278)\nS002 HYB k=25: 0.8056 (drop: 0.0079)\nS002 ES k=25: 0.8214 (drop: -0.0079)\nS005 IMP k=10: 0.8294 (drop: 0.0278)\nS005 HYB k=10: 0.8214 (drop: 0.0357)\nS005 ES k=10: 0.7698 (drop: 0.0873)\nS005 IMP k=15: 0.8294 (drop: 0.0278)\nS005 HYB k=15: 0.8492 (drop: 0.0079)\nS005 ES k=15: 0.7976 (drop: 0.0595)\nS005 IMP k=20: 0.8175 (drop: 0.0397)\nS005 HYB k=20: 0.8373 (drop: 0.0198)\nS005 ES k=20: 0.8294 (drop: 0.0278)\nS005 IMP k=25: 0.8175 (drop: 0.0397)\nS005 HYB k=25: 0.8492 (drop: 0.0079)\nS005 ES k=25: 0.8373 (drop: 0.0198)\nS006 IMP k=10: 0.8175 (drop: 0.0556)\nS006 HYB k=10: 0.8333 (drop: 0.0397)\nS006 ES k=10: 0.8056 (drop: 0.0675)\nS006 IMP k=15: 0.8333 (drop: 0.0397)\nS006 HYB k=15: 0.8571 (drop: 0.0159)\nS006 ES k=15: 0.8333 (drop: 0.0397)\nS006 IMP k=20: 0.8810 (drop: -0.0079)\nS006 HYB k=20: 0.8452 (drop: 0.0278)\nS006 ES k=20: 0.8770 (drop: -0.0040)\nS006 IMP k=25: 0.8929 (drop: -0.0198)\nS006 HYB k=25: 0.8690 (drop: 0.0040)\nS006 ES k=25: 0.8413 (drop: 0.0317)\nS007 IMP k=10: 0.8968 (drop: 0.0437)\nS007 HYB k=10: 0.9325 (drop: 0.0079)\nS007 ES k=10: 0.9286 (drop: 0.0119)\nS007 IMP k=15: 0.9087 (drop: 0.0317)\nS007 HYB k=15: 0.9246 (drop: 0.0159)\nS007 ES k=15: 0.9444 (drop: -0.0040)\nS007 IMP k=20: 0.9167 (drop: 0.0238)\nS007 HYB k=20: 0.9206 (drop: 0.0198)\nS007 ES k=20: 0.9286 (drop: 0.0119)\nS007 IMP k=25: 0.8929 (drop: 0.0476)\nS007 HYB k=25: 0.9365 (drop: 0.0040)\nS007 ES k=25: 0.9365 (drop: 0.0040)\nS008 IMP k=10: 0.9008 (drop: 0.0437)\nS008 HYB k=10: 0.9484 (drop: -0.0040)\nS008 ES k=10: 0.9008 (drop: 0.0437)\nS008 IMP k=15: 0.9246 (drop: 0.0198)\nS008 HYB k=15: 0.9325 (drop: 0.0119)\nS008 ES k=15: 0.9167 (drop: 0.0278)\nS008 IMP k=20: 0.9484 (drop: -0.0040)\nS008 HYB k=20: 0.9524 (drop: -0.0079)\nS008 ES k=20: 0.9206 (drop: 0.0238)\nS008 IMP k=25: 0.9246 (drop: 0.0198)\nS008 HYB k=25: 0.9444 (drop: 0.0000)\nS008 ES k=25: 0.9286 (drop: 0.0159)\nS011 IMP k=10: 0.8214 (drop: 0.0357)\nS011 HYB k=10: 0.8730 (drop: -0.0159)\nS011 ES k=10: 0.8532 (drop: 0.0040)\nS011 IMP k=15: 0.8571 (drop: 0.0000)\nS011 HYB k=15: 0.8611 (drop: -0.0040)\nS011 ES k=15: 0.8730 (drop: -0.0159)\nS011 IMP k=20: 0.8532 (drop: 0.0040)\nS011 HYB k=20: 0.8413 (drop: 0.0159)\nS011 ES k=20: 0.8690 (drop: -0.0119)\nS011 IMP k=25: 0.8611 (drop: -0.0040)\nS011 HYB k=25: 0.8492 (drop: 0.0079)\nS011 ES k=25: 0.8810 (drop: -0.0238)\nS014 IMP k=10: 0.7857 (drop: 0.0357)\nS014 HYB k=10: 0.7619 (drop: 0.0595)\nS014 ES k=10: 0.7897 (drop: 0.0317)\nS014 IMP k=15: 0.7738 (drop: 0.0476)\nS014 HYB k=15: 0.7659 (drop: 0.0556)\nS014 ES k=15: 0.8175 (drop: 0.0040)\nS014 IMP k=20: 0.8016 (drop: 0.0198)\nS014 HYB k=20: 0.8413 (drop: -0.0198)\nS014 ES k=20: 0.8135 (drop: 0.0079)\nS014 IMP k=25: 0.7937 (drop: 0.0278)\nS014 HYB k=25: 0.7579 (drop: 0.0635)\nS014 ES k=25: 0.8214 (drop: 0.0000)\nS015 IMP k=10: 0.8452 (drop: 0.0317)\nS015 HYB k=10: 0.8730 (drop: 0.0040)\nS015 ES k=10: 0.8889 (drop: -0.0119)\nS015 IMP k=15: 0.8810 (drop: -0.0040)\nS015 HYB k=15: 0.8730 (drop: 0.0040)\nS015 ES k=15: 0.8810 (drop: -0.0040)\nS015 IMP k=20: 0.8929 (drop: -0.0159)\nS015 HYB k=20: 0.8889 (drop: -0.0119)\nS015 ES k=20: 0.8968 (drop: -0.0198)\nS015 IMP k=25: 0.8770 (drop: 0.0000)\nS015 HYB k=25: 0.8690 (drop: 0.0079)\nS015 ES k=25: 0.9206 (drop: -0.0437)\nS016 IMP k=10: 0.7579 (drop: 0.0040)\nS016 HYB k=10: 0.7500 (drop: 0.0119)\nS016 ES k=10: 0.7778 (drop: -0.0159)\nS016 IMP k=15: 0.7659 (drop: -0.0040)\nS016 HYB k=15: 0.7778 (drop: -0.0159)\nS016 ES k=15: 0.7659 (drop: -0.0040)\nS016 IMP k=20: 0.7341 (drop: 0.0278)\nS016 HYB k=20: 0.7698 (drop: -0.0079)\nS016 ES k=20: 0.7817 (drop: -0.0198)\nS016 IMP k=25: 0.7460 (drop: 0.0159)\nS016 HYB k=25: 0.7579 (drop: 0.0040)\nS016 ES k=25: 0.7817 (drop: -0.0198)\nS020 IMP k=10: 0.9603 (drop: 0.0079)\nS020 HYB k=10: 0.9444 (drop: 0.0238)\nS020 ES k=10: 0.9484 (drop: 0.0198)\nS020 IMP k=15: 0.9802 (drop: -0.0119)\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"## 12. Summary and Visualization","metadata":{}},{"cell_type":"code","source":"# Summarize by method and k\nsummary = selection_df.groupby(['method', 'k']).agg({\n    'drop': ['mean', 'std']\n}).reset_index()\n\nprint(\"\\nChannel Selection Summary:\")\nprint(summary)\n\n# Plot comparison\nfig, ax = plt.subplots(1, 1, figsize=(10, 6))\n\nfor method in ['IMP', 'HYB', 'ES']:\n    method_data = selection_df[selection_df['method'] == method]\n    grouped = method_data.groupby('k')['drop'].mean()\n    ax.plot(grouped.index, grouped.values, marker='o', label=method)\n\nax.set_xlabel('Number of Selected Channels (k)')\nax.set_ylabel('Accuracy Drop')\nax.set_title('SparseGCN-CARM: Channel Selection Performance')\nax.legend()\nax.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nBest selection method at k=20: {selection_df[selection_df['k']==20].groupby('method')['drop'].mean().idxmin()}\")\nprint(f\"Mean drop at k=20: {selection_df[selection_df['k']==20]['drop'].mean():.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T05:25:47.977264Z","iopub.execute_input":"2025-11-01T05:25:47.977823Z","iopub.status.idle":"2025-11-01T05:25:47.987761Z","shell.execute_reply.started":"2025-11-01T05:25:47.977801Z","shell.execute_reply":"2025-11-01T05:25:47.986648Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_37/2073451453.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Summarize by method and k\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m summary = selection_df.groupby(['method', 'k']).agg({\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;34m'drop'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'mean'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'std'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m }).reset_index()\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'selection_df' is not defined"],"ename":"NameError","evalue":"name 'selection_df' is not defined","output_type":"error"}],"execution_count":1},{"cell_type":"markdown","source":"## 13. Save Results","metadata":{}},{"cell_type":"code","source":"results_df.to_csv('results/sparsegcn_results.csv', index=False)\nselection_df.to_csv('results/sparsegcn_selection.csv', index=False)\nprint(\"Results saved!\")\nprint(f\"\\nFinal SparseGCN-CARM Performance:\")\nprint(f\"- Mean accuracy: {results_df['accuracy'].mean():.4f}\")\nprint(f\"- Mean active channels: {results_df['avg_active_channels'].mean():.1f}\")\nprint(f\"- Best selection drop @ k=20: {selection_df[selection_df['k']==20]['drop'].min():.4f}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-01T05:05:00.409Z"}},"outputs":[],"execution_count":null}]}