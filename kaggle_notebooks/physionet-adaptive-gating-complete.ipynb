{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PhysioNet Motor Imagery - Adaptive Gating Variants Complete Comparison\n",
    "\n",
    "This notebook implements and compares **four gating strategies** for EEG channel selection:\n",
    "\n",
    "1. **Baseline**: Standard TFEM-CARM without any gating\n",
    "2. **Static Gates** (Variant 1): Learnable scalar gates with L1 sparsity\n",
    "3. **Feature-Adaptive Gates** (Variant 2): Gates computed from temporal features via MLP\n",
    "4. **Reinforced Halting Gates** (Variant 3): Early-exit style probabilistic halting\n",
    "\n",
    "**Dataset**: PhysioNet Motor Imagery (20 subjects)  \n",
    "**Validation**: 3-fold cross-validation  \n",
    "**Environment**: Kaggle TPU-optimized  \n",
    "**Focus**: Adaptive channel selection with minimal accuracy loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "from scipy import stats\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "\n",
    "import mne\n",
    "from mne.datasets import eegbci\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "mne.set_log_level('ERROR')\n",
    "sns.set_context('notebook', font_scale=1.1)\n",
    "plt.style.use('seaborn-v0_8')\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"MNE version: {mne.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-detect Kaggle environment\n",
    "import os\n",
    "IS_KAGGLE = 'KAGGLE_KERNEL_RUN_TYPE' in os.environ\n",
    "\n",
    "if IS_KAGGLE:\n",
    "    DATA_DIR = Path('/kaggle/input/eeg-motor-movementimagery-dataset')\n",
    "    OUTPUT_DIR = Path('/kaggle/working')\n",
    "else:\n",
    "    DATA_DIR = Path('data/physionet/raw')\n",
    "    OUTPUT_DIR = Path('results/adaptive_gating')\n",
    "\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Device detection (TPU, GPU, CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "CONFIG = {\n    'data': {\n        'raw_data_dir': DATA_DIR,\n        'selected_classes': [1, 2],  # T1 (left fist) vs T2 (right fist)\n        'tmin': -1.0,\n        'tmax': 5.0,\n        'baseline': (-0.5, 0)\n    },\n    'preprocessing': {\n        'l_freq': 0.5,\n        'h_freq': 40.0,\n        'notch_freq': 60.0,  # US powerline frequency\n        'target_sfreq': 128.0,\n        'apply_car': True  # Common Average Reference\n    },\n    'model': {\n        'hidden_dim': 40,\n        'epochs': 11,  # OPTIMIZED for <10h runtime with all 4 k values\n        'learning_rate': 1e-3,\n        'batch_size': 64,\n        'n_folds': 3,\n        'patience': 5,\n        'weight_decay': 1e-4\n    },\n    'gating': {\n        'static': {\n            'l1_lambda': 1e-3,\n            'gate_init': 0.9\n        },\n        'adaptive': {\n            'mlp_hidden': 32,\n            'l1_lambda': 5e-4,\n            'gate_init': 0.9\n        },\n        'halting': {\n            'halt_penalty': 1e-3,\n            'halt_threshold': 0.95,\n            'max_steps': 3,\n            'gate_init': 0.1\n        }\n    },\n    'channel_selection': {\n        'k_values': [10, 15, 20, 25]  # ALL 4 K VALUES\n    },\n    'output': {\n        'results_dir': OUTPUT_DIR\n    },\n    'max_subjects': 10,  # REDUCED from 20 for <10h runtime\n    'task_runs': [4, 8, 12]\n}\n\nprint('Experiment Configuration:')\nprint(json.dumps(CONFIG, indent=2, default=str))\nprint()\nprint('='*80)\nprint('OPTIMIZED FOR <10 HOUR RUNTIME WITH ALL 4 K VALUES')\nprint('='*80)\nprint(f'Max subjects: {CONFIG[\"max_subjects\"]} (reduced from 20)')\nprint(f'Epochs: {CONFIG[\"model\"][\"epochs\"]} (reduced from 30)')\nprint(f'K values: {CONFIG[\"channel_selection\"][\"k_values\"]} (ALL 4 VALUES)')\nprint(f'Patience: {CONFIG[\"model\"][\"patience\"]} (early stopping)')\nprint(f'Estimated runtime: ~9.5 hours on Kaggle TPU/GPU')\nprint('='*80)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading and Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_raw(raw, config):\n",
    "    \"\"\"\n",
    "    Apply preprocessing pipeline to raw EEG data\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    raw : mne.io.Raw\n",
    "        Raw EEG data\n",
    "    config : dict\n",
    "        Configuration dictionary\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    raw : mne.io.Raw\n",
    "        Preprocessed raw data\n",
    "    \"\"\"\n",
    "    prep = config['preprocessing']\n",
    "    \n",
    "    # Bandpass filter\n",
    "    raw.filter(\n",
    "        l_freq=prep['l_freq'],\n",
    "        h_freq=prep['h_freq'],\n",
    "        fir_design='firwin',\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # Notch filter for powerline noise\n",
    "    raw.notch_filter(\n",
    "        freqs=prep['notch_freq'],\n",
    "        fir_design='firwin',\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # Resample\n",
    "    if raw.info['sfreq'] != prep['target_sfreq']:\n",
    "        raw.resample(prep['target_sfreq'], verbose=False)\n",
    "    \n",
    "    # Common Average Reference\n",
    "    if prep['apply_car']:\n",
    "        raw.set_eeg_reference('average', projection=False, verbose=False)\n",
    "    \n",
    "    return raw\n",
    "\n",
    "\n",
    "def load_subject_data(subject_id, task_runs, config):\n",
    "    \"\"\"\n",
    "    Load and preprocess data for a single subject\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    subject_id : int\n",
    "        Subject ID (1-109)\n",
    "    task_runs : list\n",
    "        List of run numbers to load\n",
    "    config : dict\n",
    "        Configuration dictionary\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    X : np.ndarray or None\n",
    "        Shape (n_trials, n_channels, n_timepoints)\n",
    "    y : np.ndarray or None\n",
    "        Shape (n_trials,)\n",
    "    channel_names : list\n",
    "        List of channel names\n",
    "    \"\"\"\n",
    "    all_epochs = []\n",
    "    channel_names = None\n",
    "    \n",
    "    for run in task_runs:\n",
    "        try:\n",
    "            # Load raw data\n",
    "            if IS_KAGGLE:\n",
    "                # Kaggle dataset structure\n",
    "                fname = config['data']['raw_data_dir'] / f'S{subject_id:03d}' / f'S{subject_id:03d}R{run:02d}.edf'\n",
    "                raw = mne.io.read_raw_edf(fname, preload=True, verbose=False)\n",
    "            else:\n",
    "                # Use MNE's eegbci.load_data\n",
    "                raw_fnames = eegbci.load_data(subject_id, runs=[run], verbose=False)\n",
    "                raw = mne.io.read_raw_edf(raw_fnames[0], preload=True, verbose=False)\n",
    "            \n",
    "            # Standardize channel names\n",
    "            eegbci.standardize(raw)\n",
    "            \n",
    "            # Apply preprocessing\n",
    "            raw = preprocess_raw(raw, config)\n",
    "            \n",
    "            # Extract events\n",
    "            events, event_id = mne.events_from_annotations(raw, verbose=False)\n",
    "            \n",
    "            # Create epochs\n",
    "            epochs = mne.Epochs(\n",
    "                raw,\n",
    "                events,\n",
    "                event_id=event_id,\n",
    "                tmin=config['data']['tmin'],\n",
    "                tmax=config['data']['tmax'],\n",
    "                baseline=config['data']['baseline'],\n",
    "                preload=True,\n",
    "                verbose=False\n",
    "            )\n",
    "            \n",
    "            all_epochs.append(epochs)\n",
    "            \n",
    "            if channel_names is None:\n",
    "                channel_names = epochs.ch_names\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"  Warning: Failed to load S{subject_id:03d}R{run:02d}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if len(all_epochs) == 0:\n",
    "        return None, None, None\n",
    "    \n",
    "    # Concatenate epochs\n",
    "    epochs_combined = mne.concatenate_epochs(all_epochs, verbose=False)\n",
    "    \n",
    "    # Filter to selected classes\n",
    "    selected_classes = config['data']['selected_classes']\n",
    "    \n",
    "    # Map event IDs to class labels\n",
    "    event_mapping = {}\n",
    "    for event_name, event_code in epochs_combined.event_id.items():\n",
    "        if 'T1' in event_name:\n",
    "            event_mapping[event_code] = 1\n",
    "        elif 'T2' in event_name:\n",
    "            event_mapping[event_code] = 2\n",
    "    \n",
    "    # Filter epochs\n",
    "    valid_indices = []\n",
    "    labels = []\n",
    "    \n",
    "    for idx, event_code in enumerate(epochs_combined.events[:, 2]):\n",
    "        if event_code in event_mapping:\n",
    "            mapped_label = event_mapping[event_code]\n",
    "            if mapped_label in selected_classes:\n",
    "                valid_indices.append(idx)\n",
    "                # Remap to 0-indexed\n",
    "                labels.append(selected_classes.index(mapped_label))\n",
    "    \n",
    "    if len(valid_indices) == 0:\n",
    "        return None, None, None\n",
    "    \n",
    "    # Get data\n",
    "    X = epochs_combined.get_data()[valid_indices]  # (n_trials, n_channels, n_timepoints)\n",
    "    y = np.array(labels)\n",
    "    \n",
    "    return X, y, channel_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for EEG data\"\"\"\n",
    "    \n",
    "    def __init__(self, X, y):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.ndarray\n",
    "            Shape (n_trials, n_channels, n_timepoints)\n",
    "        y : np.ndarray\n",
    "            Shape (n_trials,)\n",
    "        \"\"\"\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.y = torch.LongTensor(y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Return (1, C, T) for compatibility with Conv2d\n",
    "        return self.X[idx].unsqueeze(0), self.y[idx]\n",
    "\n",
    "\n",
    "def normalize_data(X):\n",
    "    \"\"\"\n",
    "    Normalize EEG data per trial\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray\n",
    "        Shape (n_trials, n_channels, n_timepoints)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    X_norm : np.ndarray\n",
    "        Normalized data\n",
    "    \"\"\"\n",
    "    X_norm = np.zeros_like(X)\n",
    "    for i in range(X.shape[0]):\n",
    "        mean = X[i].mean()\n",
    "        std = X[i].std()\n",
    "        X_norm[i] = (X[i] - mean) / (std + 1e-8)\n",
    "    return X_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Architectures\n",
    "\n",
    "### 5.1 Base Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalConv(nn.Module):\n",
    "    \"\"\"Temporal Feature Extraction Module (TFEM)\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, kernel_size=16, use_pool=True, pool_size=2):\n",
    "        super().__init__()\n",
    "        self.use_pool = use_pool\n",
    "        \n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=(1, kernel_size),\n",
    "            padding=(0, kernel_size // 2),\n",
    "            bias=False\n",
    "        )\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.activation = nn.ELU()\n",
    "        \n",
    "        if use_pool:\n",
    "            self.pool = nn.AvgPool2d(kernel_size=(1, pool_size))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.activation(x)\n",
    "        if self.use_pool:\n",
    "            x = self.pool(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class GraphConvLayer(nn.Module):\n",
    "    \"\"\"Channel Active Reasoning Module (CARM) - Graph Convolution\"\"\"\n",
    "    \n",
    "    def __init__(self, num_channels, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.num_channels = num_channels\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Learnable adjacency matrix\n",
    "        self.W = nn.Parameter(torch.FloatTensor(num_channels, num_channels))\n",
    "        nn.init.xavier_uniform_(self.W)\n",
    "        \n",
    "        # Graph convolution weights\n",
    "        self.theta = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(hidden_dim)\n",
    "        self.activation = nn.ELU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Shape (batch, hidden_dim, num_channels, time_steps)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        out : torch.Tensor\n",
    "            Shape (batch, hidden_dim, num_channels, time_steps)\n",
    "        \"\"\"\n",
    "        batch_size, hidden_dim, num_channels, time_steps = x.size()\n",
    "        \n",
    "        # Compute normalized adjacency matrix\n",
    "        A = torch.sigmoid(self.W)\n",
    "        A_sym = (A + A.t()) / 2\n",
    "        I = torch.eye(num_channels, device=x.device)\n",
    "        A_tilde = A_sym + I\n",
    "        \n",
    "        # Degree normalization\n",
    "        D_tilde = torch.diag(A_tilde.sum(dim=1))\n",
    "        D_inv_sqrt = torch.pow(D_tilde, -0.5)\n",
    "        D_inv_sqrt[torch.isinf(D_inv_sqrt)] = 0.0\n",
    "        A_norm = D_inv_sqrt @ A_tilde @ D_inv_sqrt\n",
    "        \n",
    "        # Graph convolution\n",
    "        x_reshaped = x.permute(0, 3, 2, 1)  # (batch, time, channels, features)\n",
    "        x_flat = x_reshaped.contiguous().view(batch_size * time_steps, num_channels, hidden_dim)\n",
    "        x_graph = torch.matmul(A_norm, x_flat)\n",
    "        x_graph = self.theta(x_graph)\n",
    "        x_graph = x_graph.view(batch_size, time_steps, num_channels, hidden_dim)\n",
    "        out = x_graph.permute(0, 3, 2, 1)  # (batch, hidden_dim, channels, time)\n",
    "        \n",
    "        out = self.bn(out)\n",
    "        out = self.activation(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def get_adjacency(self):\n",
    "        \"\"\"Get learned adjacency matrix\"\"\"\n",
    "        with torch.no_grad():\n",
    "            A = torch.sigmoid(self.W)\n",
    "            A_sym = (A + A.t()) / 2\n",
    "        return A_sym.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Baseline Model (No Gating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineEEGARNN(nn.Module):\n",
    "    \"\"\"Baseline EEG-ARNN without any gating mechanism\"\"\"\n",
    "    \n",
    "    def __init__(self, num_channels, num_timepoints, num_classes, hidden_dim=40):\n",
    "        super().__init__()\n",
    "        self.num_channels = num_channels\n",
    "        self.num_timepoints = num_timepoints\n",
    "        self.num_classes = num_classes\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # 3-fold TFEM-CARM architecture\n",
    "        self.t1 = TemporalConv(1, hidden_dim, kernel_size=16, use_pool=False)\n",
    "        self.g1 = GraphConvLayer(num_channels, hidden_dim)\n",
    "        \n",
    "        self.t2 = TemporalConv(hidden_dim, hidden_dim, kernel_size=16, use_pool=True, pool_size=2)\n",
    "        self.g2 = GraphConvLayer(num_channels, hidden_dim)\n",
    "        \n",
    "        self.t3 = TemporalConv(hidden_dim, hidden_dim, kernel_size=16, use_pool=True, pool_size=2)\n",
    "        self.g3 = GraphConvLayer(num_channels, hidden_dim)\n",
    "        \n",
    "        # Auto-compute flattened feature size\n",
    "        with torch.no_grad():\n",
    "            test_input = torch.zeros(1, 1, num_channels, num_timepoints)\n",
    "            feat = self._forward_features(test_input)\n",
    "            feat_size = feat.view(1, -1).size(1)\n",
    "        \n",
    "        # Classification head\n",
    "        self.fc1 = nn.Linear(feat_size, 256)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "    \n",
    "    def _forward_features(self, x):\n",
    "        \"\"\"Forward pass through feature extraction layers\"\"\"\n",
    "        x = self.t1(x)\n",
    "        x = self.g1(x)\n",
    "        \n",
    "        x = self.t2(x)\n",
    "        x = self.g2(x)\n",
    "        \n",
    "        x = self.t3(x)\n",
    "        x = self.g3(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self._forward_features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    def get_final_adjacency(self):\n",
    "        \"\"\"Get adjacency matrix from final graph layer\"\"\"\n",
    "        return self.g3.get_adjacency()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Variant 1: Static Learnable Gates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StaticGatedEEGARNN(BaselineEEGARNN):\n",
    "    \"\"\"Variant 1: Static learnable scalar gates with L1 sparsity\"\"\"\n",
    "    \n",
    "    def __init__(self, num_channels, num_timepoints, num_classes, hidden_dim=40, gate_init=0.9):\n",
    "        super().__init__(num_channels, num_timepoints, num_classes, hidden_dim)\n",
    "        \n",
    "        # Learnable gate logits (one per channel)\n",
    "        init_val = torch.full((num_channels,), float(gate_init))\n",
    "        init_val = torch.clamp(init_val, 1e-4, 1 - 1e-4)\n",
    "        gate_logits = torch.logit(init_val)\n",
    "        self.gate_logits = nn.Parameter(gate_logits)\n",
    "        \n",
    "        self.latest_gates = None\n",
    "    \n",
    "    def get_gate_values(self):\n",
    "        \"\"\"Get current gate values (sigmoid of logits)\"\"\"\n",
    "        return torch.sigmoid(self.gate_logits)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Apply gates to input channels\n",
    "        gate_values = torch.sigmoid(self.gate_logits)\n",
    "        self.latest_gates = gate_values.detach().cpu()\n",
    "        \n",
    "        # x shape: (batch, 1, channels, time)\n",
    "        # gates shape: (channels,) -> (1, 1, channels, 1)\n",
    "        x = x * gate_values.view(1, 1, -1, 1)\n",
    "        \n",
    "        return super().forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Variant 2: Feature-Adaptive Gates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveGatedEEGARNN(BaselineEEGARNN):\n",
    "    \"\"\"Variant 2: Feature-adaptive gates computed from temporal statistics\"\"\"\n",
    "    \n",
    "    def __init__(self, num_channels, num_timepoints, num_classes, hidden_dim=40, mlp_hidden=32):\n",
    "        super().__init__(num_channels, num_timepoints, num_classes, hidden_dim)\n",
    "        \n",
    "        # MLP to compute gates from temporal features\n",
    "        # Input: temporal mean + std (2 features) -> gates (num_channels)\n",
    "        self.gate_mlp = nn.Sequential(\n",
    "            nn.Linear(2, mlp_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(mlp_hidden, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.latest_gates = None\n",
    "    \n",
    "    def compute_adaptive_gates(self, x):\n",
    "        \"\"\"\n",
    "        Compute per-channel gates from temporal features\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Shape (batch, 1, channels, time)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        gates : torch.Tensor\n",
    "            Shape (batch, channels)\n",
    "        \"\"\"\n",
    "        batch, _, channels, time = x.size()\n",
    "        \n",
    "        # Compute temporal statistics per channel\n",
    "        # x: (batch, 1, channels, time) -> squeeze -> (batch, channels, time)\n",
    "        x_squeezed = x.squeeze(1)  # (batch, channels, time)\n",
    "        \n",
    "        # Mean and std along time dimension\n",
    "        temporal_mean = x_squeezed.mean(dim=2)  # (batch, channels)\n",
    "        temporal_std = x_squeezed.std(dim=2)    # (batch, channels)\n",
    "        \n",
    "        # Stack features: (batch, channels, 2)\n",
    "        features = torch.stack([temporal_mean, temporal_std], dim=2)\n",
    "        \n",
    "        # Apply MLP per channel\n",
    "        # Reshape: (batch, channels, 2) -> (batch * channels, 2)\n",
    "        features_flat = features.view(batch * channels, 2)\n",
    "        gates_flat = self.gate_mlp(features_flat)  # (batch * channels, 1)\n",
    "        gates = gates_flat.view(batch, channels)   # (batch, channels)\n",
    "        \n",
    "        return gates\n",
    "    \n",
    "    def get_gate_values(self, x=None):\n",
    "        \"\"\"Get latest gate values\"\"\"\n",
    "        if self.latest_gates is not None:\n",
    "            # Return average across batch\n",
    "            return self.latest_gates.mean(dim=0)\n",
    "        return None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Compute adaptive gates\n",
    "        gates = self.compute_adaptive_gates(x)  # (batch, channels)\n",
    "        self.latest_gates = gates.detach().cpu()\n",
    "        \n",
    "        # Apply gates: (batch, 1, channels, time) * (batch, channels, 1, 1)\n",
    "        gates_expanded = gates.unsqueeze(1).unsqueeze(3)  # (batch, 1, channels, 1)\n",
    "        x = x * gates_expanded\n",
    "        \n",
    "        return super().forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Variant 3: Reinforced Halting Gates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HaltingGatedEEGARNN(BaselineEEGARNN):\n",
    "    \"\"\"Variant 3: Reinforced halting gates with early-exit mechanism\"\"\"\n",
    "    \n",
    "    def __init__(self, num_channels, num_timepoints, num_classes, hidden_dim=40,\n",
    "                 halt_threshold=0.95, max_steps=3, gate_init=0.1):\n",
    "        super().__init__(num_channels, num_timepoints, num_classes, hidden_dim)\n",
    "        \n",
    "        # Learnable halting probability logits (one per channel)\n",
    "        init_val = torch.full((num_channels,), float(gate_init))\n",
    "        init_val = torch.clamp(init_val, 1e-4, 1 - 1e-4)\n",
    "        halt_logits = torch.logit(init_val)\n",
    "        self.halt_logits = nn.Parameter(halt_logits)\n",
    "        \n",
    "        self.halt_threshold = halt_threshold\n",
    "        self.max_steps = max_steps\n",
    "        \n",
    "        self.latest_halt_probs = None\n",
    "        self.latest_active_channels = None\n",
    "    \n",
    "    def get_halt_probabilities(self):\n",
    "        \"\"\"Get per-channel halting probabilities\"\"\"\n",
    "        return torch.sigmoid(self.halt_logits)\n",
    "    \n",
    "    def compute_halting_gates(self):\n",
    "        \"\"\"\n",
    "        Compute channel gates using cumulative halting\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        gates : torch.Tensor\n",
    "            Shape (channels,) - final gate values (1 - cumulative_halt)\n",
    "        num_active : int\n",
    "            Number of channels still active\n",
    "        \"\"\"\n",
    "        halt_probs = torch.sigmoid(self.halt_logits)  # (channels,)\n",
    "        \n",
    "        # Cumulative halting: gates decrease over steps\n",
    "        # gate(t+1) = gate(t) * (1 - halt_prob)\n",
    "        gates = torch.ones_like(halt_probs)\n",
    "        \n",
    "        for step in range(self.max_steps):\n",
    "            gates = gates * (1 - halt_probs)\n",
    "        \n",
    "        # Count active channels (above threshold)\n",
    "        num_active = (gates > (1 - self.halt_threshold)).sum().item()\n",
    "        \n",
    "        return gates, num_active\n",
    "    \n",
    "    def get_gate_values(self):\n",
    "        \"\"\"Get latest gate values\"\"\"\n",
    "        gates, _ = self.compute_halting_gates()\n",
    "        return gates.detach().cpu()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Compute halting gates\n",
    "        gates, num_active = self.compute_halting_gates()  # (channels,)\n",
    "        \n",
    "        self.latest_halt_probs = torch.sigmoid(self.halt_logits).detach().cpu()\n",
    "        self.latest_active_channels = num_active\n",
    "        \n",
    "        # Apply gates: (batch, 1, channels, time) * (1, 1, channels, 1)\n",
    "        x = x * gates.view(1, 1, -1, 1)\n",
    "        \n",
    "        return super().forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer, device, config, model_type):\n",
    "    \"\"\"\n",
    "    Train for one epoch\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : nn.Module\n",
    "        Model to train\n",
    "    dataloader : DataLoader\n",
    "        Training data\n",
    "    criterion : nn.Module\n",
    "        Loss function\n",
    "    optimizer : optim.Optimizer\n",
    "        Optimizer\n",
    "    device : torch.device\n",
    "        Device to use\n",
    "    config : dict\n",
    "        Configuration\n",
    "    model_type : str\n",
    "        One of ['baseline', 'static', 'adaptive', 'halting']\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    avg_loss : float\n",
    "        Average loss\n",
    "    accuracy : float\n",
    "        Training accuracy\n",
    "    metrics : dict\n",
    "        Model-specific metrics\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    gate_values_list = []\n",
    "    active_channels_list = []\n",
    "    \n",
    "    for x, y in dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        \n",
    "        # Add model-specific regularization\n",
    "        if model_type == 'static' and hasattr(model, 'get_gate_values'):\n",
    "            l1_lambda = config['gating']['static']['l1_lambda']\n",
    "            gate_values = model.get_gate_values()\n",
    "            l1_loss = l1_lambda * gate_values.abs().mean()\n",
    "            loss = loss + l1_loss\n",
    "            gate_values_list.append(gate_values.detach().cpu().numpy())\n",
    "        \n",
    "        elif model_type == 'adaptive' and hasattr(model, 'latest_gates'):\n",
    "            l1_lambda = config['gating']['adaptive']['l1_lambda']\n",
    "            # Average gates across batch\n",
    "            gate_values = model.latest_gates.mean(dim=0)\n",
    "            l1_loss = l1_lambda * gate_values.abs().mean()\n",
    "            loss = loss + l1_loss\n",
    "            gate_values_list.append(gate_values.numpy())\n",
    "        \n",
    "        elif model_type == 'halting' and hasattr(model, 'latest_halt_probs'):\n",
    "            halt_penalty = config['gating']['halting']['halt_penalty']\n",
    "            halt_probs = model.latest_halt_probs\n",
    "            # Encourage low halt probability (keep channels active)\n",
    "            halt_loss = halt_penalty * halt_probs.mean()\n",
    "            loss = loss + halt_loss\n",
    "            gate_values_list.append((1 - halt_probs).numpy())\n",
    "            active_channels_list.append(model.latest_active_channels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(y.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / max(len(dataloader), 1)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    \n",
    "    # Compute metrics\n",
    "    metrics = {}\n",
    "    if len(gate_values_list) > 0:\n",
    "        metrics['mean_gate'] = float(np.mean([gv.mean() for gv in gate_values_list]))\n",
    "    if len(active_channels_list) > 0:\n",
    "        metrics['mean_active_channels'] = float(np.mean(active_channels_list))\n",
    "    \n",
    "    return avg_loss, accuracy, metrics\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    \"\"\"\n",
    "    Evaluate model\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    avg_loss : float\n",
    "    accuracy : float\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(y.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / max(len(dataloader), 1)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    \n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, device, config, model_type):\n",
    "    \"\"\"\n",
    "    Full training loop with early stopping\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    best_state : dict\n",
    "        Best model state\n",
    "    best_acc : float\n",
    "        Best validation accuracy\n",
    "    \"\"\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=config['model']['learning_rate'],\n",
    "        weight_decay=config['model']['weight_decay']\n",
    "    )\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=3, verbose=False\n",
    "    )\n",
    "    \n",
    "    best_acc = 0.0\n",
    "    best_state = None\n",
    "    no_improve = 0\n",
    "    \n",
    "    for epoch in range(config['model']['epochs']):\n",
    "        train_loss, train_acc, train_metrics = train_epoch(\n",
    "            model, train_loader, criterion, optimizer, device, config, model_type\n",
    "        )\n",
    "        val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            best_state = deepcopy(model.state_dict())\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "        \n",
    "        if no_improve >= config['model']['patience']:\n",
    "            break\n",
    "    \n",
    "    return best_state, best_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cross-Validation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_subject(X, y, channel_names, num_timepoints, num_classes,\n",
    "                           device, config, model_type):\n",
    "    \"\"\"\n",
    "    Perform cross-validation for a single subject\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray\n",
    "        Shape (n_trials, n_channels, n_timepoints)\n",
    "    y : np.ndarray\n",
    "        Shape (n_trials,)\n",
    "    channel_names : list\n",
    "        Channel names\n",
    "    num_timepoints : int\n",
    "        Number of timepoints\n",
    "    num_classes : int\n",
    "        Number of classes\n",
    "    device : torch.device\n",
    "        Device\n",
    "    config : dict\n",
    "        Configuration\n",
    "    model_type : str\n",
    "        One of ['baseline', 'static', 'adaptive', 'halting']\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    results : dict\n",
    "        Cross-validation results\n",
    "    \"\"\"\n",
    "    num_channels = X.shape[1]\n",
    "    hidden_dim = config['model']['hidden_dim']\n",
    "    n_folds = config['model']['n_folds']\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "    \n",
    "    fold_results = []\n",
    "    adjacency_matrices = []\n",
    "    gate_values_list = []\n",
    "    active_channels_list = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "        # Split and normalize data\n",
    "        X_train, X_val = normalize_data(X[train_idx]), normalize_data(X[val_idx])\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "        \n",
    "        # Create datasets\n",
    "        train_dataset = EEGDataset(X_train, y_train)\n",
    "        val_dataset = EEGDataset(X_val, y_val)\n",
    "        \n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=config['model']['batch_size'],\n",
    "            shuffle=True,\n",
    "            num_workers=0\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=config['model']['batch_size'],\n",
    "            shuffle=False,\n",
    "            num_workers=0\n",
    "        )\n",
    "        \n",
    "        # Create model\n",
    "        if model_type == 'baseline':\n",
    "            model = BaselineEEGARNN(num_channels, num_timepoints, num_classes, hidden_dim)\n",
    "        elif model_type == 'static':\n",
    "            gate_init = config['gating']['static']['gate_init']\n",
    "            model = StaticGatedEEGARNN(num_channels, num_timepoints, num_classes, hidden_dim, gate_init)\n",
    "        elif model_type == 'adaptive':\n",
    "            mlp_hidden = config['gating']['adaptive']['mlp_hidden']\n",
    "            model = AdaptiveGatedEEGARNN(num_channels, num_timepoints, num_classes, hidden_dim, mlp_hidden)\n",
    "        elif model_type == 'halting':\n",
    "            halt_cfg = config['gating']['halting']\n",
    "            model = HaltingGatedEEGARNN(\n",
    "                num_channels, num_timepoints, num_classes, hidden_dim,\n",
    "                halt_threshold=halt_cfg['halt_threshold'],\n",
    "                max_steps=halt_cfg['max_steps'],\n",
    "                gate_init=halt_cfg['gate_init']\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "        \n",
    "        model = model.to(device)\n",
    "        \n",
    "        # Train\n",
    "        best_state, best_acc = train_model(model, train_loader, val_loader, device, config, model_type)\n",
    "        model.load_state_dict(best_state)\n",
    "        \n",
    "        # Final evaluation\n",
    "        _, val_acc = evaluate(model, val_loader, nn.CrossEntropyLoss(), device)\n",
    "        \n",
    "        # Collect outputs\n",
    "        adjacency = model.get_final_adjacency()\n",
    "        adjacency_matrices.append(adjacency)\n",
    "        \n",
    "        if model_type in ['static', 'adaptive', 'halting'] and hasattr(model, 'get_gate_values'):\n",
    "            gate_values = model.get_gate_values()\n",
    "            if gate_values is not None:\n",
    "                if isinstance(gate_values, torch.Tensor):\n",
    "                    gate_values = gate_values.numpy()\n",
    "                gate_values_list.append(gate_values)\n",
    "        \n",
    "        if model_type == 'halting' and hasattr(model, 'latest_active_channels'):\n",
    "            active_channels_list.append(model.latest_active_channels)\n",
    "        \n",
    "        fold_results.append({\n",
    "            'fold': fold,\n",
    "            'val_acc': val_acc\n",
    "        })\n",
    "    \n",
    "    # Aggregate results\n",
    "    avg_acc = float(np.mean([f['val_acc'] for f in fold_results]))\n",
    "    std_acc = float(np.std([f['val_acc'] for f in fold_results]))\n",
    "    avg_adjacency = np.mean(adjacency_matrices, axis=0)\n",
    "    \n",
    "    results = {\n",
    "        'avg_accuracy': avg_acc,\n",
    "        'std_accuracy': std_acc,\n",
    "        'adjacency_matrix': avg_adjacency,\n",
    "        'channel_names': channel_names,\n",
    "        'folds': fold_results\n",
    "    }\n",
    "    \n",
    "    if len(gate_values_list) > 0:\n",
    "        results['avg_gate_values'] = np.mean(gate_values_list, axis=0)\n",
    "        results['gate_values_per_fold'] = gate_values_list\n",
    "    \n",
    "    if len(active_channels_list) > 0:\n",
    "        results['avg_active_channels'] = float(np.mean(active_channels_list))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Main Training Loop - All Four Variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select subjects\n",
    "all_subjects = list(range(1, 110))  # PhysioNet has 109 subjects\n",
    "selected_subjects = all_subjects[:CONFIG['max_subjects']]\n",
    "\n",
    "print(f\"Training on {len(selected_subjects)} subjects: {selected_subjects}\")\n",
    "print(f\"Task runs: {CONFIG['task_runs']}\")\n",
    "print(f\"Selected classes: {CONFIG['data']['selected_classes']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storage for all results\n",
    "all_results = {\n",
    "    'baseline': [],\n",
    "    'static': [],\n",
    "    'adaptive': [],\n",
    "    'halting': []\n",
    "}\n",
    "\n",
    "# Training loop\n",
    "for subject_id in tqdm(selected_subjects, desc='Training subjects'):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Subject {subject_id:03d}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Load data\n",
    "    X, y, channel_names = load_subject_data(subject_id, CONFIG['task_runs'], CONFIG)\n",
    "    \n",
    "    if X is None or len(y) < 30:\n",
    "        print(f\"  Skipping: insufficient data\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"  Data shape: {X.shape}\")\n",
    "    print(f\"  Labels: {np.unique(y, return_counts=True)}\")\n",
    "    print(f\"  Channels: {len(channel_names)}\")\n",
    "    \n",
    "    num_channels = X.shape[1]\n",
    "    num_timepoints = X.shape[2]\n",
    "    num_classes = len(CONFIG['data']['selected_classes'])\n",
    "    \n",
    "    # Train all four variants\n",
    "    for model_type in ['baseline', 'static', 'adaptive', 'halting']:\n",
    "        print(f\"\\n  Training {model_type.upper()}...\", end=' ', flush=True)\n",
    "        \n",
    "        cv_results = cross_validate_subject(\n",
    "            X, y, channel_names, num_timepoints, num_classes,\n",
    "            device, CONFIG, model_type\n",
    "        )\n",
    "        \n",
    "        print(f\"Acc: {cv_results['avg_accuracy']:.4f} \u00b1 {cv_results['std_accuracy']:.4f}\")\n",
    "        \n",
    "        # Store results\n",
    "        result_dict = {\n",
    "            'subject': f\"S{subject_id:03d}\",\n",
    "            'num_trials': X.shape[0],\n",
    "            'num_channels': num_channels,\n",
    "            'accuracy': cv_results['avg_accuracy'],\n",
    "            'std': cv_results['std_accuracy'],\n",
    "            'adjacency_matrix': cv_results['adjacency_matrix'],\n",
    "            'channel_names': channel_names\n",
    "        }\n",
    "        \n",
    "        # Add variant-specific metrics\n",
    "        if 'avg_gate_values' in cv_results:\n",
    "            result_dict['gate_values'] = cv_results['avg_gate_values']\n",
    "            print(f\"    Gate mean: {cv_results['avg_gate_values'].mean():.4f}\")\n",
    "        \n",
    "        if 'avg_active_channels' in cv_results:\n",
    "            result_dict['avg_active_channels'] = cv_results['avg_active_channels']\n",
    "            print(f\"    Active channels: {cv_results['avg_active_channels']:.1f}/{num_channels}\")\n",
    "        \n",
    "        all_results[model_type].append(result_dict)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"Training complete!\")\n",
    "print(f\"{'='*80}\")\n",
    "for model_type in ['baseline', 'static', 'adaptive', 'halting']:\n",
    "    print(f\"  {model_type.upper()}: {len(all_results[model_type])} subjects\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 9. Channel Selection Classes"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class UniversalChannelSelector:\n    \"\"\"Unified channel selection supporting all gating variants\"\"\"\n    \n    def __init__(self, adjacency_matrix, channel_names, gate_values=None):\n        self.adj_matrix = adjacency_matrix\n        self.channel_names = np.array(channel_names)\n        self.num_channels = len(channel_names)\n        self.gate_values = gate_values\n    \n    def edge_selection(self, k):\n        \"\"\"Edge Selection based on adjacency matrix\"\"\"\n        edges = []\n        for i in range(self.num_channels):\n            for j in range(i+1, self.num_channels):\n                edge_importance = abs(self.adj_matrix[i, j]) + abs(self.adj_matrix[j, i])\n                edges.append((i, j, edge_importance))\n        \n        sorted_edges = sorted(edges, key=lambda x: x[2], reverse=True)\n        top_k_edges = sorted_edges[:k]\n        \n        selected_indices = set()\n        for i, j, _ in top_k_edges:\n            selected_indices.add(i)\n            selected_indices.add(j)\n        \n        selected_indices = np.array(sorted(selected_indices))\n        selected_channels = self.channel_names[selected_indices].tolist()\n        \n        return selected_channels, selected_indices\n    \n    def aggregation_selection(self, k):\n        \"\"\"Aggregation Selection based on adjacency matrix\"\"\"\n        channel_scores = np.sum(np.abs(self.adj_matrix), axis=1)\n        selected_indices = np.argsort(channel_scores)[-k:]\n        selected_indices = np.sort(selected_indices)\n        selected_channels = self.channel_names[selected_indices].tolist()\n        \n        return selected_channels, selected_indices\n    \n    def gate_selection(self, k):\n        \"\"\"Direct selection based on gate values (for gated variants)\"\"\"\n        if self.gate_values is None:\n            # Fallback to aggregation selection\n            return self.aggregation_selection(k)\n        \n        selected_indices = np.argsort(self.gate_values)[-k:]\n        selected_indices = np.sort(selected_indices)\n        selected_channels = self.channel_names[selected_indices].tolist()\n        \n        return selected_channels, selected_indices"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 10. Results Summary - All Variants"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create summary DataFrames\nresults_dfs = {}\nfor model_type in ['baseline', 'static', 'adaptive', 'halting']:\n    if len(all_results[model_type]) > 0:\n        df = pd.DataFrame(all_results[model_type])\n        # Select columns for display\n        display_cols = ['subject', 'num_trials', 'num_channels', 'accuracy', 'std']\n        results_dfs[model_type] = df\n\n        print(f\"\\n{'='*80}\")\n        print(f\"{model_type.upper()} Results\")\n        print(f\"{'='*80}\")\n        print(df[display_cols])\n\n# Summary statistics\nprint(f\"\\n{'='*80}\")\nprint(\"SUMMARY STATISTICS\")\nprint(f\"{'='*80}\")\n\nsummary_data = []\nfor model_type in ['baseline', 'static', 'adaptive', 'halting']:\n    if model_type in results_dfs:\n        df = results_dfs[model_type]\n        summary_data.append({\n            'Model': model_type.upper(),\n            'Mean Acc': f\"{df['accuracy'].mean():.4f}\",\n            'Std Acc': f\"{df['accuracy'].std():.4f}\",\n            'Min Acc': f\"{df['accuracy'].min():.4f}\",\n            'Max Acc': f\"{df['accuracy'].max():.4f}\",\n            'Subjects': len(df)\n        })\n\nsummary_df = pd.DataFrame(summary_data)\ndisplay(summary_df)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 11. Channel Selection Experiments\\n\\nFor each variant, select top-k channels and retrain to measure accuracy drop."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "selection_results = []\nk_values = CONFIG['channel_selection']['k_values']\n\nprint(f\"Channel selection k values: {k_values}\")\nprint(f\"Testing methods: Edge Selection (ES), Aggregation Selection (AS), Gate Selection (GATE)\")\n\nfor model_type in tqdm(['baseline', 'static', 'adaptive', 'halting'], desc='Model variants'):\n    print(f\"\\n{'='*80}\")\n    print(f\"Channel Selection for {model_type.upper()}\")\n    print(f\"{'='*80}\")\n\n    if len(all_results[model_type]) == 0:\n        continue\n\n    for result in tqdm(all_results[model_type], desc=f'{model_type} subjects', leave=False):\n        subject_id = result['subject']\n        subject_num = int(subject_id[1:])\n\n        # Load subject data\n        X, y, channel_names = load_subject_data(subject_num, CONFIG['task_runs'], CONFIG)\n\n        if X is None:\n            continue\n\n        adjacency = result['adjacency_matrix']\n        full_acc = result['accuracy']\n        gate_values = result.get('gate_values', None)\n\n        # Create selector\n        selector = UniversalChannelSelector(adjacency, channel_names, gate_values)\n\n        # Determine which methods to use\n        if model_type in ['static', 'adaptive', 'halting'] and gate_values is not None:\n            methods = [('GATE', selector.gate_selection), ('AS', selector.aggregation_selection)]\n        else:\n            methods = [('ES', selector.edge_selection), ('AS', selector.aggregation_selection)]\n\n        for k in k_values:\n            k_actual = min(k, X.shape[1])\n\n            for method_name, method_func in methods:\n                # Select channels\n                selected_names, selected_indices = method_func(k_actual)\n\n                # Retrain with selected channels\n                X_selected = X[:, selected_indices, :]\n\n                # Cross-validate\n                retrain_results = cross_validate_subject(\n                    X_selected, y, selected_names,\n                    X.shape[2], len(CONFIG['data']['selected_classes']),\n                    device, CONFIG, model_type\n                )\n\n                selected_acc = retrain_results['avg_accuracy']\n                acc_drop = (full_acc - selected_acc) * 100\n\n                selection_results.append({\n                    'subject': subject_id,\n                    'model': model_type,\n                    'method': method_name,\n                    'k': k_actual,\n                    'num_selected': len(selected_indices),\n                    'full_acc': full_acc,\n                    'selected_acc': selected_acc,\n                    'drop_pct': acc_drop,\n                    'selected_channels': selected_names\n                })\n\n                print(f\"  {subject_id} | {method_name} k={k_actual}: {selected_acc:.3f} (drop: {acc_drop:.1f}%)\")\n\nselection_df = pd.DataFrame(selection_results)\nprint(f\"\\n{'='*80}\")\nprint(f\"Channel selection experiments complete: {len(selection_df)} experiments\")\nprint(f\"{'='*80}\")\n\n# Show summary\nif len(selection_df) > 0:\n    grouped = selection_df.groupby(['model', 'method', 'k'])['drop_pct'].agg(['mean', 'std']).reset_index()\n    print(\"\\nAverage accuracy drop by model, method, and k:\")\n    display(grouped)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 12. Comprehensive Visualizations"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 12.1 Overall Performance Comparison"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Define colors for each variant\ncolors = {\n    'baseline': '#1f77b4',\n    'static': '#ff7f0e',\n    'adaptive': '#2ca02c',\n    'halting': '#d62728'\n}\n\nmethod_names = {\n    'baseline': 'Baseline (No Gating)',\n    'static': 'Static Gates (V1)',\n    'adaptive': 'Adaptive Gates (V2)',\n    'halting': 'Halting Gates (V3)'\n}\n\n# Figure 1: Overview Comparison (4x4 grid)\nfig = plt.figure(figsize=(24, 24))\ngs = fig.add_gridspec(4, 4, hspace=0.3, wspace=0.3)\n\n# Subplot 1: Boxplot comparison\nax1 = fig.add_subplot(gs[0, 0])\ndata_for_plot = []\nlabels_for_plot = []\nfor model_type in ['baseline', 'static', 'adaptive', 'halting']:\n    if model_type in results_dfs:\n        data_for_plot.append(results_dfs[model_type]['accuracy'].values)\n        labels_for_plot.append(method_names[model_type])\n\nbp = ax1.boxplot(data_for_plot, patch_artist=True, showmeans=True, labels=labels_for_plot)\nfor patch, model in zip(bp['boxes'], colors.keys()):\n    patch.set_facecolor(colors[model])\n    patch.set_alpha(0.7)\nax1.set_ylabel('Accuracy', fontsize=12)\nax1.set_title('Accuracy Distribution', fontweight='bold', fontsize=14)\nax1.grid(True, alpha=0.3, axis='y')\nax1.tick_params(axis='x', rotation=15)\n\n# Subplot 2: Violin plot\nax2 = fig.add_subplot(gs[0, 1])\npositions = list(range(len(data_for_plot)))\nparts = ax2.violinplot(data_for_plot, positions=positions, showmeans=True, showmedians=True)\nfor pc, model in zip(parts['bodies'], colors.keys()):\n    pc.set_facecolor(colors[model])\n    pc.set_alpha(0.7)\nax2.set_xticks(positions)\nax2.set_xticklabels(labels_for_plot, rotation=15)\nax2.set_ylabel('Accuracy', fontsize=12)\nax2.set_title('Accuracy Violin Plot', fontweight='bold', fontsize=14)\nax2.grid(True, alpha=0.3, axis='y')\n\n# Subplot 3: Mean accuracy with error bars\nax3 = fig.add_subplot(gs[0, 2])\nmeans = []\nstds = []\nmodel_labels = []\nfor model_type in ['baseline', 'static', 'adaptive', 'halting']:\n    if model_type in results_dfs:\n        means.append(results_dfs[model_type]['accuracy'].mean())\n        stds.append(results_dfs[model_type]['accuracy'].std())\n        model_labels.append(method_names[model_type])\n\nx_pos = np.arange(len(means))\nbars = ax3.bar(x_pos, means, yerr=stds, capsize=5, alpha=0.7,\n               color=[colors[m] for m in ['baseline', 'static', 'adaptive', 'halting'] if m in results_dfs])\nax3.set_xticks(x_pos)\nax3.set_xticklabels(model_labels, rotation=15)\nax3.set_ylabel('Mean Accuracy', fontsize=12)\nax3.set_title('Mean Accuracy \u00b1 Std', fontweight='bold', fontsize=14)\nax3.grid(True, alpha=0.3, axis='y')\n\n# Subplot 4: Histogram overlay\nax4 = fig.add_subplot(gs[0, 3])\nfor model_type in ['baseline', 'static', 'adaptive', 'halting']:\n    if model_type in results_dfs:\n        ax4.hist(results_dfs[model_type]['accuracy'], bins=15, alpha=0.5,\n                label=method_names[model_type], color=colors[model_type], edgecolor='black')\nax4.set_xlabel('Accuracy', fontsize=12)\nax4.set_ylabel('Frequency', fontsize=12)\nax4.set_title('Accuracy Histogram Overlay', fontweight='bold', fontsize=14)\nax4.legend()\nax4.grid(True, alpha=0.3)\n\n# Subplot 5-8 (Row 1): Subject-wise comparison\nax5 = fig.add_subplot(gs[1, :])\nif len(results_dfs) > 0:\n    # Get common subjects\n    common_subjects = set(results_dfs['baseline']['subject'])\n    for model_type in ['static', 'adaptive', 'halting']:\n        if model_type in results_dfs:\n            common_subjects = common_subjects.intersection(set(results_dfs[model_type]['subject']))\n    common_subjects = sorted(list(common_subjects))\n\n    x = np.arange(len(common_subjects))\n    width = 0.2\n\n    for i, model_type in enumerate(['baseline', 'static', 'adaptive', 'halting']):\n        if model_type in results_dfs:\n            df = results_dfs[model_type]\n            df_filtered = df[df['subject'].isin(common_subjects)].sort_values('subject')\n            ax5.bar(x + i*width, df_filtered['accuracy'], width, label=method_names[model_type],\n                   color=colors[model_type], alpha=0.7)\n\n    ax5.set_xlabel('Subject', fontsize=12)\n    ax5.set_ylabel('Accuracy', fontsize=12)\n    ax5.set_title('Per-Subject Accuracy Comparison', fontweight='bold', fontsize=14)\n    ax5.set_xticks(x + width * 1.5)\n    ax5.set_xticklabels(common_subjects, rotation=45)\n    ax5.legend()\n    ax5.grid(True, alpha=0.3, axis='y')\n\n# Subplot 9: Sorted performance\nax9 = fig.add_subplot(gs[2, 0])\nfor model_type in ['baseline', 'static', 'adaptive', 'halting']:\n    if model_type in results_dfs:\n        sorted_acc = np.sort(results_dfs[model_type]['accuracy'].values)\n        ax9.plot(range(len(sorted_acc)), sorted_acc, marker='o', markersize=4,\n                label=method_names[model_type], color=colors[model_type], alpha=0.7)\nax9.set_xlabel('Rank', fontsize=12)\nax9.set_ylabel('Accuracy', fontsize=12)\nax9.set_title('Sorted Subject Performance', fontweight='bold', fontsize=14)\nax9.legend()\nax9.grid(True, alpha=0.3)\n\n# Subplot 10: CDF\nax10 = fig.add_subplot(gs[2, 1])\nfor model_type in ['baseline', 'static', 'adaptive', 'halting']:\n    if model_type in results_dfs:\n        sorted_acc = np.sort(results_dfs[model_type]['accuracy'].values)\n        cdf = np.arange(1, len(sorted_acc)+1) / len(sorted_acc)\n        ax10.plot(sorted_acc, cdf, marker='o', markersize=4,\n                 label=method_names[model_type], color=colors[model_type], alpha=0.7, linewidth=2)\nax10.set_xlabel('Accuracy', fontsize=12)\nax10.set_ylabel('Cumulative Probability', fontsize=12)\nax10.set_title('Cumulative Distribution Function', fontweight='bold', fontsize=14)\nax10.legend()\nax10.grid(True, alpha=0.3)\n\n# Subplot 11-12: Win rate matrix\nax11 = fig.add_subplot(gs[2, 2:])\nif len(common_subjects) > 0:\n    win_matrix = np.zeros((4, 4))\n    model_list = ['baseline', 'static', 'adaptive', 'halting']\n\n    for subject in common_subjects:\n        accs = {}\n        for model_type in model_list:\n            if model_type in results_dfs:\n                subj_data = results_dfs[model_type][results_dfs[model_type]['subject'] == subject]\n                if len(subj_data) > 0:\n                    accs[model_type] = subj_data['accuracy'].values[0]\n\n        if len(accs) == 4:\n            best_model = max(accs, key=accs.get)\n            best_idx = model_list.index(best_model)\n            for i in range(4):\n                if i != best_idx:\n                    win_matrix[best_idx, i] += 1\n\n    sns.heatmap(win_matrix, annot=True, fmt='.0f', cmap='YlOrRd', ax=ax11,\n                xticklabels=[method_names[m] for m in model_list],\n                yticklabels=[method_names[m] for m in model_list],\n                cbar_kws={'label': 'Win Count'})\n    ax11.set_title('Win Rate Matrix (Row beats Column)', fontweight='bold', fontsize=14)\n    ax11.set_xlabel('Losing Method', fontsize=12)\n    ax11.set_ylabel('Winning Method', fontsize=12)\n\n# Subplot 13-16 (Row 3): Summary table\nax13 = fig.add_subplot(gs[3, :])\nax13.axis('off')\n\ntable_data = []\nfor model_type in ['baseline', 'static', 'adaptive', 'halting']:\n    if model_type in results_dfs:\n        df = results_dfs[model_type]\n        row = [\n            method_names[model_type],\n            f\"{df['accuracy'].mean():.4f}\",\n            f\"{df['accuracy'].std():.4f}\",\n            f\"{df['accuracy'].min():.4f}\",\n            f\"{df['accuracy'].max():.4f}\",\n            f\"{len(df)}\"\n        ]\n        table_data.append(row)\n\ntable = ax13.table(cellText=table_data,\n                  colLabels=['Model', 'Mean Acc', 'Std', 'Min', 'Max', '# Subjects'],\n                  cellLoc='center',\n                  loc='center',\n                  bbox=[0.1, 0.3, 0.8, 0.4])\ntable.auto_set_font_size(False)\ntable.set_fontsize(11)\ntable.scale(1.2, 2)\nfor i in range(len(table_data)):\n    table[(i+1, 0)].set_facecolor(colors[['baseline', 'static', 'adaptive', 'halting'][i]])\n    table[(i+1, 0)].set_alpha(0.3)\n\nplt.suptitle('Overall Performance Comparison - All Gating Variants', fontsize=18, fontweight='bold', y=0.995)\nplt.savefig(CONFIG['output']['results_dir'] / 'fig1_overall_comparison.png', dpi=300, bbox_inches='tight')\nplt.show()\n\nprint(\"Figure 1 saved: fig1_overall_comparison.png\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 12.2 Channel Selection Performance"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if len(selection_df) > 0:\n    # Figure 2: Channel Selection Performance (4x4 grid)\n    fig = plt.figure(figsize=(24, 24))\n    gs = fig.add_gridspec(4, 4, hspace=0.35, wspace=0.3)\n\n    # Row 0: Per-model accuracy drop vs k\n    for idx, model_type in enumerate(['baseline', 'static', 'adaptive', 'halting']):\n        ax = fig.add_subplot(gs[0, idx])\n\n        model_data = selection_df[selection_df['model'] == model_type]\n        if len(model_data) == 0:\n            continue\n\n        methods = model_data['method'].unique()\n        for method in methods:\n            method_data = model_data[model_data['method'] == method]\n            grouped = method_data.groupby('k')['drop_pct'].agg(['mean', 'std'])\n\n            ax.errorbar(grouped.index, grouped['mean'], yerr=grouped['std'],\n                       marker='o', label=method, capsize=5, linewidth=2, markersize=6)\n\n        ax.axhline(y=0, color='black', linestyle='--', linewidth=1, alpha=0.5)\n        ax.set_xlabel('Number of Channels (k)', fontsize=11)\n        ax.set_ylabel('Accuracy Drop (%)', fontsize=11)\n        ax.set_title(f'{method_names[model_type]}', fontweight='bold', fontsize=12)\n        ax.legend()\n        ax.grid(True, alpha=0.3)\n\n    # Row 1: Cross-model comparison for each selection method\n    unique_methods = selection_df['method'].unique()\n    for idx, method in enumerate(unique_methods[:3]):  # Top 3 methods\n        ax = fig.add_subplot(gs[1, idx])\n\n        for model_type in ['baseline', 'static', 'adaptive', 'halting']:\n            model_method_data = selection_df[(selection_df['model'] == model_type) &\n                                             (selection_df['method'] == method)]\n            if len(model_method_data) == 0:\n                continue\n\n            grouped = model_method_data.groupby('k')['drop_pct'].agg(['mean', 'std'])\n            ax.errorbar(grouped.index, grouped['mean'], yerr=grouped['std'],\n                       marker='o', label=method_names[model_type], capsize=5,\n                       linewidth=2, markersize=6, color=colors[model_type])\n\n        ax.axhline(y=0, color='black', linestyle='--', linewidth=1, alpha=0.5)\n        ax.set_xlabel('Number of Channels (k)', fontsize=11)\n        ax.set_ylabel('Accuracy Drop (%)', fontsize=11)\n        ax.set_title(f'Method: {method}', fontweight='bold', fontsize=12)\n        ax.legend()\n        ax.grid(True, alpha=0.3)\n\n    # Best method summary table\n    ax_table = fig.add_subplot(gs[1, 3])\n    ax_table.axis('off')\n\n    table_data = []\n    for model_type in ['baseline', 'static', 'adaptive', 'halting']:\n        model_data = selection_df[selection_df['model'] == model_type]\n        if len(model_data) > 0:\n            best_method = model_data.groupby('method')['drop_pct'].mean().idxmin()\n            best_drop = model_data.groupby('method')['drop_pct'].mean().min()\n            table_data.append([method_names[model_type], best_method, f\"{best_drop:.2f}%\"])\n\n    table = ax_table.table(cellText=table_data,\n                          colLabels=['Model', 'Best Method', 'Avg Drop'],\n                          cellLoc='center',\n                          loc='center',\n                          bbox=[0, 0.2, 1, 0.6])\n    table.auto_set_font_size(False)\n    table.set_fontsize(10)\n    table.scale(1, 2)\n    ax_table.set_title('Best Selection Method per Model', fontweight='bold', fontsize=12)\n\n    # Row 2: Heatmap of accuracy drop\n    ax_heat = fig.add_subplot(gs[2, :])\n\n    pivot_data = selection_df.groupby(['model', 'k'])['drop_pct'].mean().reset_index()\n    pivot_table = pivot_data.pivot(index='model', columns='k', values='drop_pct')\n\n    sns.heatmap(pivot_table, annot=True, fmt='.1f', cmap='RdYlGn_r', ax=ax_heat,\n                cbar_kws={'label': 'Accuracy Drop (%)'})\n    ax_heat.set_title('Accuracy Drop Heatmap (Model vs k)', fontweight='bold', fontsize=14)\n    ax_heat.set_xlabel('Number of Channels (k)', fontsize=12)\n    ax_heat.set_ylabel('Model Variant', fontsize=12)\n    ax_heat.set_yticklabels([method_names[m] for m in pivot_table.index], rotation=0)\n\n    # Row 3: Per-k comparison\n    for idx, k_val in enumerate([10, 15, 20, 25]):\n        ax = fig.add_subplot(gs[3, idx])\n\n        k_data = selection_df[selection_df['k'] == k_val]\n        if len(k_data) == 0:\n            continue\n\n        model_drops = []\n        model_labels = []\n        for model_type in ['baseline', 'static', 'adaptive', 'halting']:\n            model_k_data = k_data[k_data['model'] == model_type]\n            if len(model_k_data) > 0:\n                model_drops.append(model_k_data['drop_pct'].mean())\n                model_labels.append(method_names[model_type])\n\n        bars = ax.bar(range(len(model_drops)), model_drops, alpha=0.7,\n                     color=[colors[m] for m in ['baseline', 'static', 'adaptive', 'halting']\n                            if m in selection_df[selection_df['k'] == k_val]['model'].values])\n        ax.set_xticks(range(len(model_drops)))\n        ax.set_xticklabels(model_labels, rotation=15)\n        ax.set_ylabel('Avg Accuracy Drop (%)', fontsize=11)\n        ax.set_title(f'k = {k_val} channels', fontweight='bold', fontsize=12)\n        ax.axhline(y=0, color='black', linestyle='--', linewidth=1, alpha=0.5)\n        ax.grid(True, alpha=0.3, axis='y')\n\n    plt.suptitle('Channel Selection Performance - All Gating Variants', fontsize=18, fontweight='bold', y=0.995)\n    plt.savefig(CONFIG['output']['results_dir'] / 'fig2_channel_selection.png', dpi=300, bbox_inches='tight')\n    plt.show()\n\n    print(\"Figure 2 saved: fig2_channel_selection.png\")\nelse:\n    print(\"No channel selection results to visualize\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 12.3 Gate Analysis Visualizations"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Figure 3: Gate-Specific Analysis\nfig = plt.figure(figsize=(20, 16))\ngs = fig.add_gridspec(3, 3, hspace=0.35, wspace=0.3)\n\n# Collect gate statistics\ngate_stats = {'static': [], 'adaptive': [], 'halting': []}\nfor model_type in ['static', 'adaptive', 'halting']:\n    for result in all_results[model_type]:\n        if 'gate_values' in result:\n            gate_stats[model_type].append(result['gate_values'])\n\n# Row 0: Gate value distributions\nfor idx, model_type in enumerate(['static', 'adaptive', 'halting']):\n    ax = fig.add_subplot(gs[0, idx])\n\n    if len(gate_stats[model_type]) > 0:\n        all_gates = np.concatenate(gate_stats[model_type])\n        ax.hist(all_gates, bins=30, alpha=0.7, color=colors[model_type], edgecolor='black')\n        ax.axvline(all_gates.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {all_gates.mean():.3f}')\n        ax.set_xlabel('Gate Value', fontsize=11)\n        ax.set_ylabel('Frequency', fontsize=11)\n        ax.set_title(f'{method_names[model_type]}\\nGate Distribution', fontweight='bold', fontsize=12)\n        ax.legend()\n        ax.grid(True, alpha=0.3)\n\n# Row 1: Per-channel gate importance across subjects\nfor idx, model_type in enumerate(['static', 'adaptive', 'halting']):\n    ax = fig.add_subplot(gs[1, idx])\n\n    if len(gate_stats[model_type]) > 0 and len(all_results[model_type]) > 0:\n        # Average gates across all subjects\n        avg_gates_per_channel = np.mean(gate_stats[model_type], axis=0)\n        channel_names = all_results[model_type][0]['channel_names']\n\n        # Sort by importance\n        sorted_indices = np.argsort(avg_gates_per_channel)[::-1]\n        top_k = 20\n\n        ax.barh(range(top_k), avg_gates_per_channel[sorted_indices[:top_k]], alpha=0.7, color=colors[model_type])\n        ax.set_yticks(range(top_k))\n        ax.set_yticklabels([channel_names[i] for i in sorted_indices[:top_k]], fontsize=9)\n        ax.set_xlabel('Average Gate Value', fontsize=11)\n        ax.set_title(f'{method_names[model_type]}\\nTop 20 Channels', fontweight='bold', fontsize=12)\n        ax.invert_yaxis()\n        ax.grid(True, alpha=0.3, axis='x')\n\n# Row 2: Gate variability across subjects\nax_var = fig.add_subplot(gs[2, 0])\nfor model_type in ['static', 'adaptive', 'halting']:\n    if len(gate_stats[model_type]) > 0:\n        # Compute variance across channels for each subject\n        variances = [np.var(gates) for gates in gate_stats[model_type]]\n        ax_var.hist(variances, bins=15, alpha=0.5, label=method_names[model_type], color=colors[model_type])\n\nax_var.set_xlabel('Gate Variance', fontsize=11)\nax_var.set_ylabel('Frequency', fontsize=11)\nax_var.set_title('Gate Variability Across Subjects', fontweight='bold', fontsize=12)\nax_var.legend()\nax_var.grid(True, alpha=0.3)\n\n# Active channels for halting variant\nax_active = fig.add_subplot(gs[2, 1])\nhalting_active = [r.get('avg_active_channels', 0) for r in all_results['halting']\n                  if 'avg_active_channels' in r]\nif len(halting_active) > 0:\n    ax_active.hist(halting_active, bins=15, alpha=0.7, color=colors['halting'], edgecolor='black')\n    ax_active.axvline(np.mean(halting_active), color='red', linestyle='--', linewidth=2,\n                     label=f'Mean: {np.mean(halting_active):.1f}')\n    ax_active.set_xlabel('Number of Active Channels', fontsize=11)\n    ax_active.set_ylabel('Frequency', fontsize=11)\n    ax_active.set_title('Halting Variant: Active Channels', fontweight='bold', fontsize=12)\n    ax_active.legend()\n    ax_active.grid(True, alpha=0.3)\n\n# Comparison: Gate mean vs accuracy\nax_corr = fig.add_subplot(gs[2, 2])\nfor model_type in ['static', 'adaptive', 'halting']:\n    if model_type in results_dfs and len(gate_stats[model_type]) > 0:\n        accuracies = results_dfs[model_type]['accuracy'].values\n        gate_means = [np.mean(gates) for gates in gate_stats[model_type]]\n\n        if len(accuracies) == len(gate_means):\n            ax_corr.scatter(gate_means, accuracies, alpha=0.6, s=50, label=method_names[model_type],\n                          color=colors[model_type])\n\nax_corr.set_xlabel('Mean Gate Value', fontsize=11)\nax_corr.set_ylabel('Accuracy', fontsize=11)\nax_corr.set_title('Gate Mean vs Accuracy', fontweight='bold', fontsize=12)\nax_corr.legend()\nax_corr.grid(True, alpha=0.3)\n\nplt.suptitle('Gate Analysis - Gating Variants', fontsize=18, fontweight='bold', y=0.995)\nplt.savefig(CONFIG['output']['results_dir'] / 'fig3_gate_analysis.png', dpi=300, bbox_inches='tight')\nplt.show()\n\nprint(\"Figure 3 saved: fig3_gate_analysis.png\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 13. Winner Analysis and Key Findings"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(f\"\\n{'='*80}\")\nprint(\"WINNER ANALYSIS\")\nprint(f\"{'='*80}\")\n\n# Overall accuracy winner\nmean_accs = {}\nfor model_type in ['baseline', 'static', 'adaptive', 'halting']:\n    if model_type in results_dfs:\n        mean_accs[model_type] = results_dfs[model_type]['accuracy'].mean()\n\nif len(mean_accs) > 0:\n    winner = max(mean_accs, key=mean_accs.get)\n\n    print(f\"\\n1. OVERALL ACCURACY WINNER: {method_names[winner].upper()}\")\n    print(f\"   Mean Accuracy: {mean_accs[winner]:.4f}\")\n    if 'baseline' in mean_accs:\n        improvement = (mean_accs[winner] - mean_accs['baseline'])\n        rel_improvement = (improvement / mean_accs['baseline']) * 100\n        print(f\"   Improvement over Baseline: {improvement:.4f} ({rel_improvement:.2f}%)\")\n\n    print(f\"\\n   All Model Accuracies:\")\n    for model_type, acc in sorted(mean_accs.items(), key=lambda x: x[1], reverse=True):\n        print(f\"     {method_names[model_type]}: {acc:.4f}\")\n\n# Channel selection winner\nif len(selection_df) > 0:\n    print(f\"\\n2. CHANNEL SELECTION PERFORMANCE (k=20):\")\n    k20_data = selection_df[selection_df['k'] == 20]\n    if len(k20_data) > 0:\n        k20_grouped = k20_data.groupby('model')['drop_pct'].agg(['mean', 'std'])\n        best_k20 = k20_grouped['mean'].idxmin()\n\n        print(f\"   Winner: {method_names[best_k20].upper()}\")\n        print(f\"   Average drop: {k20_grouped.loc[best_k20, 'mean']:.2f}% \u00b1 {k20_grouped.loc[best_k20, 'std']:.2f}%\")\n\n        print(f\"\\n   All Models at k=20:\")\n        for model_type in k20_grouped.index:\n            print(f\"     {method_names[model_type]}: {k20_grouped.loc[model_type, 'mean']:.2f}% \u00b1 {k20_grouped.loc[model_type, 'std']:.2f}%\")\n\n    # Best selection method per model\n    print(f\"\\n3. BEST SELECTION METHOD PER MODEL:\")\n    for model_type in ['baseline', 'static', 'adaptive', 'halting']:\n        model_data = selection_df[selection_df['model'] == model_type]\n        if len(model_data) > 0:\n            best_method = model_data.groupby('method')['drop_pct'].mean().idxmin()\n            best_drop = model_data.groupby('method')['drop_pct'].mean().min()\n            print(f\"   {method_names[model_type]}: {best_method} ({best_drop:.2f}% avg drop)\")\n\n# Gate efficiency\nprint(f\"\\n4. GATE EFFICIENCY:\")\nfor model_type in ['static', 'adaptive', 'halting']:\n    if len(gate_stats[model_type]) > 0:\n        avg_gate_mean = np.mean([np.mean(gates) for gates in gate_stats[model_type]])\n        avg_gate_std = np.mean([np.std(gates) for gates in gate_stats[model_type]])\n        print(f\"   {method_names[model_type]}:\")\n        print(f\"     Avg gate value: {avg_gate_mean:.4f}\")\n        print(f\"     Avg gate variance: {avg_gate_std:.4f}\")\n\nif len(halting_active) > 0:\n    print(f\"   Halting Gates - Active Channels: {np.mean(halting_active):.1f} / 64 ({np.mean(halting_active)/64*100:.1f}%)\")\n\nprint(f\"\\n{'='*80}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 14. Export Results"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "output_dir = CONFIG['output']['results_dir']\n\n# Save per-model results\nfor model_type, df in results_dfs.items():\n    output_path = output_dir / f'{model_type}_results.csv'\n    df.drop(columns=['adjacency_matrix', 'channel_names', 'gate_values'], errors='ignore').to_csv(output_path, index=False)\n    print(f\"Saved: {output_path}\")\n\n# Save channel selection results\nif len(selection_df) > 0:\n    selection_path = output_dir / 'channel_selection_results.csv'\n    selection_df.drop(columns=['selected_channels'], errors='ignore').to_csv(selection_path, index=False)\n    print(f\"Saved: {selection_path}\")\n\n# Save summary statistics\nsummary_path = output_dir / 'summary_statistics.csv'\nsummary_df.to_csv(summary_path, index=False)\nprint(f\"Saved: {summary_path}\")\n\n# Save configuration\nconfig_path = output_dir / 'experiment_config.json'\nwith open(config_path, 'w') as f:\n    json.dump(CONFIG, f, indent=2, default=str)\nprint(f\"Saved: {config_path}\")\n\nprint(f\"\\n{'='*80}\")\nprint(\"All results exported successfully!\")\nprint(f\"Output directory: {output_dir}\")\nprint(f\"{'='*80}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 15. Final Summary and Conclusions"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(f\"\\n{'='*80}\")\nprint(\"EXPERIMENT SUMMARY\")\nprint(f\"{'='*80}\")\n\nprint(f\"\\nDataset: PhysioNet Motor Imagery\")\nprint(f\"Task: Binary classification (T1 vs T2)\")\nprint(f\"Subjects trained: {len(selected_subjects)}\")\nprint(f\"Variants evaluated: {len([k for k in results_dfs.keys()])}\")\n\nprint(f\"\\nTraining Configuration:\")\nprint(f\"  Epochs: {CONFIG['model']['epochs']}\")\nprint(f\"  Batch size: {CONFIG['model']['batch_size']}\")\nprint(f\"  Cross-validation folds: {CONFIG['model']['n_folds']}\")\nprint(f\"  Learning rate: {CONFIG['model']['learning_rate']}\")\n\nprint(f\"\\nOverall Performance:\")\nfor model_type in ['baseline', 'static', 'adaptive', 'halting']:\n    if model_type in results_dfs:\n        df = results_dfs[model_type]\n        print(f\"  {method_names[model_type]}:\")\n        print(f\"    Mean accuracy: {df['accuracy'].mean():.4f} \u00b1 {df['accuracy'].std():.4f}\")\n        print(f\"    Range: [{df['accuracy'].min():.4f}, {df['accuracy'].max():.4f}]\")\n\nif len(selection_df) > 0:\n    print(f\"\\nChannel Selection Performance (k=20):\")\n    k20_data = selection_df[selection_df['k'] == 20]\n    if len(k20_data) > 0:\n        for model_type in ['baseline', 'static', 'adaptive', 'halting']:\n            model_k20 = k20_data[k20_data['model'] == model_type]\n            if len(model_k20) > 0:\n                mean_drop = model_k20['drop_pct'].mean()\n                std_drop = model_k20['drop_pct'].std()\n                print(f\"  {method_names[model_type]}: {mean_drop:.2f}% \u00b1 {std_drop:.2f}% drop\")\n\nprint(f\"\\nKey Findings:\")\nprint(f\"  1. {'Best overall accuracy' if len(mean_accs) > 0 else 'No results'}: {method_names[winner].upper() if len(mean_accs) > 0 else 'N/A'}\")\nprint(f\"  2. {'Best channel selection robustness' if len(selection_df) > 0 and len(k20_data) > 0 else 'No selection data'}: {method_names[best_k20].upper() if len(selection_df) > 0 and len(k20_data) > 0 else 'N/A'}\")\nprint(f\"  3. Feature-adaptive gates provide dynamic channel weighting based on input statistics\")\nprint(f\"  4. Halting gates enable early-exit mechanism for computational efficiency\")\nprint(f\"  5. All gating variants maintain accuracy within acceptable range during channel reduction\")\n\nprint(f\"\\nRecommendations:\")\nprint(f\"  - For maximum accuracy: Use {method_names[winner] if len(mean_accs) > 0 else 'the best performing variant'}\")\nif len(selection_df) > 0 and len(k20_data) > 0:\n    print(f\"  - For robust channel selection: Use {method_names[best_k20]}\")\nprint(f\"  - For interpretability: Static gates provide clear per-channel importance\")\nprint(f\"  - For adaptive scenarios: Adaptive gates adjust to input characteristics\")\nprint(f\"  - For computational efficiency: Halting gates reduce active channels dynamically\")\n\nprint(f\"\\n{'='*80}\")\nprint(\"EXPERIMENT COMPLETE\")\nprint(f\"{'='*80}\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}