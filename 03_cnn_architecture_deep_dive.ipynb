{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Architecture Deep Dive: Understanding Every Layer\n",
    "\n",
    "This notebook provides an exhaustive understanding of Convolutional Neural Networks (CNNs) with detailed explanations of every component.\n",
    "\n",
    "## Complete Learning Objectives:\n",
    "1. **Convolution Operation**: Mathematics, padding, stride, dilation\n",
    "2. **Pooling Layers**: Max, average, global pooling mechanics\n",
    "3. **Feature Maps**: What CNNs actually learn and see\n",
    "4. **Receptive Fields**: How information flows through layers\n",
    "5. **Architecture Patterns**: LeNet, AlexNet, VGG, ResNet principles\n",
    "6. **Parameter Calculations**: Exact formulas for memory and computation\n",
    "7. **Implementation**: From scratch understanding with TensorFlow\n",
    "\n",
    "**Prerequisites**: Complete `01_deep_learning_foundations.ipynb` and `02_neural_network_fundamentals.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Cell 1: Comprehensive CNN Library Setup\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03mCNN-SPECIFIC LIBRARY EXPLANATIONS:\u001b[39;00m\n\u001b[32m      4\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     20\u001b[39m \u001b[33;03m- tensorflow.keras.utils: Model visualization and utilities\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m keras\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m layers, models, applications, preprocessing\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# Cell 1: Comprehensive CNN Library Setup\n",
    "\"\"\"\n",
    "CNN-SPECIFIC LIBRARY EXPLANATIONS:\n",
    "\n",
    "Computer Vision Libraries:\n",
    "- tensorflow: Core deep learning framework with excellent CNN support\n",
    "- opencv-python (cv2): Advanced image processing operations\n",
    "- PIL/Pillow: Python Imaging Library for basic image operations\n",
    "- imageio: Reading and writing various image formats\n",
    "\n",
    "Scientific Computing for CNNs:\n",
    "- numpy: Multi-dimensional array operations (essential for image tensors)\n",
    "- scipy: Signal processing functions (convolution operations)\n",
    "- matplotlib: Visualization of images, filters, feature maps\n",
    "- seaborn: Statistical plots for model performance analysis\n",
    "\n",
    "Specialized CNN Tools:\n",
    "- tensorflow.keras.applications: Pre-trained models (VGG, ResNet, etc.)\n",
    "- tensorflow.keras.preprocessing: Image augmentation and preprocessing\n",
    "- tensorflow.keras.utils: Model visualization and utilities\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, applications, preprocessing\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import ndimage\n",
    "import cv2\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "\n",
    "print(f\"üîß CNN ENVIRONMENT SETUP\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Keras version: {keras.__version__}\")\n",
    "print(f\"OpenCV version: {cv2.__version__}\")\n",
    "print(f\"GPU available: {len(tf.config.list_physical_devices('GPU')) > 0}\")\n",
    "if len(tf.config.list_physical_devices('GPU')) > 0:\n",
    "    print(f\"GPU devices: {tf.config.list_physical_devices('GPU')}\")\n",
    "print(f\"Random seed: {RANDOM_SEED}\")\n",
    "\n",
    "# Configure plotting for high-quality CNN visualizations\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (14, 10)\n",
    "plt.rcParams['font.size'] = 11\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams['grid.alpha'] = 0.3\n",
    "\n",
    "print(\"\\n‚úÖ All CNN-specific libraries imported and configured successfully!\")\n",
    "print(\"üéØ Ready for deep CNN exploration!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Convolution Operation: The Heart of CNNs\n",
    "\n",
    "**What is Convolution?**\n",
    "- **Mathematical operation**: Sliding a filter (kernel) over an input to detect features\n",
    "- **Biological inspiration**: How visual cortex processes images\n",
    "- **Key insight**: Local features (edges, textures) matter more than global position\n",
    "\n",
    "**Convolution Mathematics:**\n",
    "```\n",
    "Output[i,j] = Œ£ Œ£ Input[i+m,j+n] * Filter[m,n]\n",
    "               m n\n",
    "```\n",
    "\n",
    "**Critical Parameters:**\n",
    "1. **Filter Size**: Usually 3x3, 5x5, 7x7 (odd numbers for symmetry)\n",
    "2. **Stride**: How much to move filter each step (1, 2, 3...)\n",
    "3. **Padding**: Add zeros around input to control output size\n",
    "4. **Dilation**: Spacing between filter elements (for larger receptive field)\n",
    "\n",
    "**Output Size Formula:**\n",
    "```\n",
    "Output_size = (Input_size + 2*Padding - Filter_size) / Stride + 1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Convolution Operation Deep Analysis\n",
    "\n",
    "print(\"=== CONVOLUTION OPERATION MATHEMATICS ===\")\n",
    "\n",
    "def manual_convolution_2d(input_array, filter_array, stride=1, padding=0):\n",
    "    \"\"\"\n",
    "    Manual implementation of 2D convolution to understand the mathematics\n",
    "    \n",
    "    Args:\n",
    "        input_array: 2D numpy array (height, width)\n",
    "        filter_array: 2D numpy array (filter_height, filter_width)\n",
    "        stride: Step size for filter movement\n",
    "        padding: Number of zero-padding pixels around input\n",
    "    \n",
    "    Returns:\n",
    "        output_array: 2D numpy array with convolution result\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add padding if specified\n",
    "    if padding > 0:\n",
    "        input_array = np.pad(input_array, padding, mode='constant', constant_values=0)\n",
    "    \n",
    "    input_h, input_w = input_array.shape\n",
    "    filter_h, filter_w = filter_array.shape\n",
    "    \n",
    "    # Calculate output dimensions\n",
    "    output_h = (input_h - filter_h) // stride + 1\n",
    "    output_w = (input_w - filter_w) // stride + 1\n",
    "    \n",
    "    # Initialize output array\n",
    "    output_array = np.zeros((output_h, output_w))\n",
    "    \n",
    "    # Perform convolution\n",
    "    for i in range(0, output_h):\n",
    "        for j in range(0, output_w):\n",
    "            # Extract the current window\n",
    "            window = input_array[i*stride:i*stride+filter_h, j*stride:j*stride+filter_w]\n",
    "            # Element-wise multiplication and sum\n",
    "            output_array[i, j] = np.sum(window * filter_array)\n",
    "    \n",
    "    return output_array\n",
    "\n",
    "# Create example input and filters\n",
    "print(\"\\nüñºÔ∏è EXAMPLE INPUT IMAGE (8x8):\")\n",
    "example_input = np.array([\n",
    "    [1, 1, 1, 0, 0, 0, 1, 1],\n",
    "    [1, 1, 1, 0, 0, 0, 1, 1],\n",
    "    [1, 1, 1, 0, 0, 0, 1, 1],\n",
    "    [0, 0, 0, 1, 1, 1, 0, 0],\n",
    "    [0, 0, 0, 1, 1, 1, 0, 0],\n",
    "    [0, 0, 0, 1, 1, 1, 0, 0],\n",
    "    [1, 1, 1, 0, 0, 0, 1, 1],\n",
    "    [1, 1, 1, 0, 0, 0, 1, 1]\n",
    "], dtype=np.float32)\n",
    "\n",
    "print(f\"Input shape: {example_input.shape}\")\n",
    "print(f\"Input pattern: Checkerboard-like pattern with clear edges\")\n",
    "\n",
    "# Define different types of filters\n",
    "filters = {\n",
    "    'vertical_edge': np.array([[-1, 0, 1],\n",
    "                              [-1, 0, 1],\n",
    "                              [-1, 0, 1]], dtype=np.float32),\n",
    "    \n",
    "    'horizontal_edge': np.array([[-1, -1, -1],\n",
    "                                [ 0,  0,  0],\n",
    "                                [ 1,  1,  1]], dtype=np.float32),\n",
    "    \n",
    "    'blur': np.array([[1, 1, 1],\n",
    "                     [1, 1, 1],\n",
    "                     [1, 1, 1]], dtype=np.float32) / 9,\n",
    "    \n",
    "    'sharpen': np.array([[ 0, -1,  0],\n",
    "                        [-1,  5, -1],\n",
    "                        [ 0, -1,  0]], dtype=np.float32),\n",
    "    \n",
    "    'diagonal_edge': np.array([[-1, -1,  0],\n",
    "                              [-1,  0,  1],\n",
    "                              [ 0,  1,  1]], dtype=np.float32)\n",
    "}\n",
    "\n",
    "print(f\"\\nüîß DEFINED FILTERS:\")\n",
    "for name, filter_array in filters.items():\n",
    "    print(f\"  {name:15s}: {filter_array.shape} - {filter_array.sum():.2f} (sum)\")\n",
    "\n",
    "# Demonstrate convolution with different parameters\n",
    "print(f\"\\nüßÆ CONVOLUTION PARAMETER ANALYSIS:\")\n",
    "\n",
    "# Test different stride values\n",
    "strides_to_test = [1, 2, 3]\n",
    "padding_values = [0, 1, 2]\n",
    "\n",
    "print(f\"\\nSTRIDE EFFECTS (using vertical_edge filter, no padding):\")\n",
    "for stride in strides_to_test:\n",
    "    output = manual_convolution_2d(example_input, filters['vertical_edge'], stride=stride, padding=0)\n",
    "    print(f\"  Stride {stride}: Input {example_input.shape} ‚Üí Output {output.shape}\")\n",
    "    print(f\"           Formula: ({example_input.shape[0]} - {filters['vertical_edge'].shape[0]}) / {stride} + 1 = {output.shape[0]}\")\n",
    "\n",
    "print(f\"\\nPADDING EFFECTS (using vertical_edge filter, stride=1):\")\n",
    "for padding in padding_values:\n",
    "    output = manual_convolution_2d(example_input, filters['vertical_edge'], stride=1, padding=padding)\n",
    "    effective_input_size = example_input.shape[0] + 2 * padding\n",
    "    print(f\"  Padding {padding}: Effective input {effective_input_size}x{effective_input_size} ‚Üí Output {output.shape}\")\n",
    "    print(f\"            Formula: ({effective_input_size} - {filters['vertical_edge'].shape[0]}) / 1 + 1 = {output.shape[0]}\")\n",
    "\n",
    "# Visualize convolution operations\n",
    "fig, axes = plt.subplots(3, 3, figsize=(18, 15))\n",
    "\n",
    "# Plot original input\n",
    "axes[0, 0].imshow(example_input, cmap='gray', interpolation='nearest')\n",
    "axes[0, 0].set_title('Original Input\\n(8x8 Checkerboard Pattern)')\n",
    "axes[0, 0].set_xlabel('Width')\n",
    "axes[0, 0].set_ylabel('Height')\n",
    "\n",
    "# Add grid to show pixels clearly\n",
    "for i in range(example_input.shape[0] + 1):\n",
    "    axes[0, 0].axhline(i - 0.5, color='white', linewidth=0.5)\n",
    "for j in range(example_input.shape[1] + 1):\n",
    "    axes[0, 0].axvline(j - 0.5, color='white', linewidth=0.5)\n",
    "\n",
    "# Plot different filter effects\n",
    "filter_names = ['vertical_edge', 'horizontal_edge', 'blur', 'sharpen', 'diagonal_edge']\n",
    "positions = [(0, 1), (0, 2), (1, 0), (1, 1), (1, 2)]\n",
    "\n",
    "for i, (name, (row, col)) in enumerate(zip(filter_names, positions)):\n",
    "    filter_array = filters[name]\n",
    "    output = manual_convolution_2d(example_input, filter_array, stride=1, padding=0)\n",
    "    \n",
    "    # Plot filter\n",
    "    filter_plot = axes[row, col]\n",
    "    im = filter_plot.imshow(filter_array, cmap='RdBu', interpolation='nearest', vmin=-1, vmax=1)\n",
    "    filter_plot.set_title(f'{name.replace(\"_\", \" \").title()} Filter\\n{filter_array.shape}')\n",
    "    \n",
    "    # Add values to filter visualization\n",
    "    for fi in range(filter_array.shape[0]):\n",
    "        for fj in range(filter_array.shape[1]):\n",
    "            filter_plot.text(fj, fi, f'{filter_array[fi, fj]:.1f}', \n",
    "                           ha='center', va='center', color='white', fontweight='bold')\n",
    "    \n",
    "    # Plot convolution output\n",
    "    output_plot = axes[2, i % 3] if i < 3 else axes[2, i - 3] if i == 3 else None\n",
    "    if output_plot is not None:\n",
    "        output_plot.imshow(output, cmap='viridis', interpolation='nearest')\n",
    "        output_plot.set_title(f'Output: {name.replace(\"_\", \" \").title()}\\n{output.shape}')\n",
    "        \n",
    "        # Show some output values\n",
    "        if output.shape[0] <= 6:  # Only for small outputs\n",
    "            for oi in range(min(3, output.shape[0])):\n",
    "                for oj in range(min(3, output.shape[1])):\n",
    "                    output_plot.text(oj, oi, f'{output[oi, oj]:.1f}', \n",
    "                                   ha='center', va='center', color='white', fontsize=8)\n",
    "\n",
    "# Add colorbar for filters\n",
    "plt.colorbar(im, ax=axes[0:2, :].ravel().tolist(), shrink=0.6, label='Filter Weight')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Convolution Operation: Different Filters and Their Effects', y=1.02, fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "# Analyze what each filter detects\n",
    "print(f\"\\nüîç FILTER ANALYSIS:\")\n",
    "for name, filter_array in filters.items():\n",
    "    output = manual_convolution_2d(example_input, filter_array, stride=1, padding=0)\n",
    "    max_response = np.max(output)\n",
    "    min_response = np.min(output)\n",
    "    \n",
    "    print(f\"\\n{name.upper()} FILTER:\")\n",
    "    print(f\"  Purpose: {get_filter_purpose(name)}\")\n",
    "    print(f\"  Response range: [{min_response:.2f}, {max_response:.2f}]\")\n",
    "    print(f\"  Strong responses at: {np.unravel_index(np.argmax(output), output.shape)}\")\n",
    "    print(f\"  Filter characteristics: {analyze_filter_characteristics(filter_array)}\")\n",
    "\n",
    "def get_filter_purpose(name):\n",
    "    purposes = {\n",
    "        'vertical_edge': 'Detects vertical edges and transitions',\n",
    "        'horizontal_edge': 'Detects horizontal edges and transitions', \n",
    "        'blur': 'Smooths image by averaging neighboring pixels',\n",
    "        'sharpen': 'Enhances edges by emphasizing differences',\n",
    "        'diagonal_edge': 'Detects diagonal edges and corners'\n",
    "    }\n",
    "    return purposes.get(name, 'Unknown purpose')\n",
    "\n",
    "def analyze_filter_characteristics(filter_array):\n",
    "    characteristics = []\n",
    "    \n",
    "    # Check if filter is symmetric\n",
    "    if np.allclose(filter_array, filter_array.T):\n",
    "        characteristics.append('Symmetric')\n",
    "    \n",
    "    # Check filter sum (important for brightness preservation)\n",
    "    filter_sum = np.sum(filter_array)\n",
    "    if abs(filter_sum) < 0.1:\n",
    "        characteristics.append('Zero-sum (edge detector)')\n",
    "    elif filter_sum > 0.9:\n",
    "        characteristics.append('Positive-sum (feature enhancer)')\n",
    "    \n",
    "    # Check for directionality\n",
    "    if np.max(filter_array) - np.min(filter_array) > 1:\n",
    "        characteristics.append('High contrast')\n",
    "    \n",
    "    return ', '.join(characteristics) if characteristics else 'Basic filter'\n",
    "\n",
    "print(f\"\\nüí° CONVOLUTION KEY INSIGHTS:\")\n",
    "print(f\"\\n1. MATHEMATICAL OPERATION:\")\n",
    "print(f\"   ‚Ä¢ Convolution = Element-wise multiplication + Sum\")\n",
    "print(f\"   ‚Ä¢ Filter slides across entire input image\")\n",
    "print(f\"   ‚Ä¢ Each position produces one output value\")\n",
    "\n",
    "print(f\"\\n2. PARAMETER EFFECTS:\")\n",
    "print(f\"   ‚Ä¢ Larger stride ‚Üí Smaller output, faster computation\")\n",
    "print(f\"   ‚Ä¢ Padding ‚Üí Control output size, preserve border information\")\n",
    "print(f\"   ‚Ä¢ Filter size ‚Üí Receptive field, computational cost\")\n",
    "\n",
    "print(f\"\\n3. FEATURE DETECTION:\")\n",
    "print(f\"   ‚Ä¢ Different filters detect different features\")\n",
    "print(f\"   ‚Ä¢ Zero-sum filters ‚Üí Edge detectors\")\n",
    "print(f\"   ‚Ä¢ Positive-sum filters ‚Üí Feature enhancers\")\n",
    "print(f\"   ‚Ä¢ CNNs learn optimal filter weights automatically!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pooling Operations: Spatial Dimension Reduction\n",
    "\n",
    "**Why Pooling?**\n",
    "- **Dimensionality reduction**: Reduce spatial size ‚Üí Less computation\n",
    "- **Translation invariance**: Small shifts in input don't change output much\n",
    "- **Feature abstraction**: Focus on presence of features, not exact location\n",
    "\n",
    "**Types of Pooling:**\n",
    "\n",
    "1. **Max Pooling**: Take maximum value in each window\n",
    "   - Most common in CNNs\n",
    "   - Preserves strongest features\n",
    "   - Creates translation invariance\n",
    "\n",
    "2. **Average Pooling**: Take average value in each window\n",
    "   - Smoother downsampling\n",
    "   - Less aggressive feature selection\n",
    "   - Better for fine-grained features\n",
    "\n",
    "3. **Global Pooling**: Pool entire feature map to single value\n",
    "   - Extreme dimensionality reduction\n",
    "   - Often used before final classification layer\n",
    "   - Completely position-invariant\n",
    "\n",
    "**Pooling Mathematics:**\n",
    "```\n",
    "Max Pool: Output[i,j] = max(Input[i*stride:i*stride+pool_size, j*stride:j*stride+pool_size])\n",
    "Avg Pool: Output[i,j] = mean(Input[i*stride:i*stride+pool_size, j*stride:j*stride+pool_size])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Pooling Operations Comprehensive Analysis\n",
    "\n",
    "print(\"=== POOLING OPERATIONS DEEP DIVE ===\")\n",
    "\n",
    "def manual_pooling_2d(input_array, pool_size=2, stride=None, pool_type='max'):\n",
    "    \"\"\"\n",
    "    Manual implementation of pooling operations\n",
    "    \n",
    "    Args:\n",
    "        input_array: 2D numpy array\n",
    "        pool_size: Size of pooling window (assumes square)\n",
    "        stride: Step size (defaults to pool_size for non-overlapping)\n",
    "        pool_type: 'max', 'average', or 'min'\n",
    "    \n",
    "    Returns:\n",
    "        pooled_array: 2D numpy array after pooling\n",
    "    \"\"\"\n",
    "    \n",
    "    if stride is None:\n",
    "        stride = pool_size\n",
    "    \n",
    "    input_h, input_w = input_array.shape\n",
    "    \n",
    "    # Calculate output dimensions\n",
    "    output_h = (input_h - pool_size) // stride + 1\n",
    "    output_w = (input_w - pool_size) // stride + 1\n",
    "    \n",
    "    # Initialize output array\n",
    "    pooled_array = np.zeros((output_h, output_w))\n",
    "    \n",
    "    # Perform pooling\n",
    "    for i in range(output_h):\n",
    "        for j in range(output_w):\n",
    "            # Extract the current window\n",
    "            window = input_array[i*stride:i*stride+pool_size, j*stride:j*stride+pool_size]\n",
    "            \n",
    "            # Apply pooling operation\n",
    "            if pool_type == 'max':\n",
    "                pooled_array[i, j] = np.max(window)\n",
    "            elif pool_type == 'average':\n",
    "                pooled_array[i, j] = np.mean(window)\n",
    "            elif pool_type == 'min':\n",
    "                pooled_array[i, j] = np.min(window)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown pool_type: {pool_type}\")\n",
    "    \n",
    "    return pooled_array\n",
    "\n",
    "# Create a more complex test image\n",
    "print(\"\\nüñºÔ∏è CREATING TEST IMAGE FOR POOLING:\")\n",
    "\n",
    "# Create an 8x8 image with various patterns\n",
    "test_image = np.array([\n",
    "    [1.0, 0.8, 0.2, 0.1, 0.9, 0.7, 0.3, 0.0],\n",
    "    [0.9, 1.0, 0.0, 0.3, 0.8, 1.0, 0.1, 0.2],\n",
    "    [0.1, 0.2, 0.7, 0.9, 0.0, 0.3, 0.8, 0.6],\n",
    "    [0.0, 0.4, 1.0, 0.8, 0.2, 0.1, 0.9, 1.0],\n",
    "    [0.8, 0.9, 0.3, 0.0, 1.0, 0.6, 0.2, 0.4],\n",
    "    [1.0, 0.7, 0.1, 0.5, 0.8, 1.0, 0.0, 0.3],\n",
    "    [0.2, 0.0, 0.9, 1.0, 0.1, 0.4, 0.7, 0.8],\n",
    "    [0.4, 0.3, 0.8, 0.6, 0.0, 0.2, 1.0, 0.9]\n",
    "], dtype=np.float32)\n",
    "\n",
    "print(f\"Test image shape: {test_image.shape}\")\n",
    "print(f\"Value range: [{test_image.min():.1f}, {test_image.max():.1f}]\")\n",
    "print(f\"Contains: Random values simulating feature map activations\")\n",
    "\n",
    "# Test different pooling operations\n",
    "pooling_types = ['max', 'average', 'min']\n",
    "pool_sizes = [2, 3, 4]\n",
    "\n",
    "print(f\"\\nüìä POOLING PARAMETER ANALYSIS:\")\n",
    "\n",
    "# Analyze effect of different pool sizes\n",
    "print(f\"\\nPOOL SIZE EFFECTS (Max Pooling):\")\n",
    "for pool_size in pool_sizes:\n",
    "    pooled = manual_pooling_2d(test_image, pool_size=pool_size, pool_type='max')\n",
    "    reduction_ratio = (test_image.size / pooled.size)\n",
    "    print(f\"  Pool size {pool_size}x{pool_size}: {test_image.shape} ‚Üí {pooled.shape} (reduction: {reduction_ratio:.1f}x)\")\n",
    "\n",
    "# Analyze different pooling types\n",
    "print(f\"\\nPOOLING TYPE COMPARISON (2x2 pools):\")\n",
    "for pool_type in pooling_types:\n",
    "    pooled = manual_pooling_2d(test_image, pool_size=2, pool_type=pool_type)\n",
    "    mean_val = np.mean(pooled)\n",
    "    std_val = np.std(pooled)\n",
    "    print(f\"  {pool_type:8s}: Mean={mean_val:.3f}, Std={std_val:.3f}, Range=[{pooled.min():.3f}, {pooled.max():.3f}]\")\n",
    "\n",
    "# Demonstrate overlapping vs non-overlapping pooling\n",
    "print(f\"\\nOVERLAPPING vs NON-OVERLAPPING POOLING:\")\n",
    "non_overlapping = manual_pooling_2d(test_image, pool_size=2, stride=2, pool_type='max')\n",
    "overlapping = manual_pooling_2d(test_image, pool_size=2, stride=1, pool_type='max')\n",
    "print(f\"  Non-overlapping (stride=pool_size): {test_image.shape} ‚Üí {non_overlapping.shape}\")\n",
    "print(f\"  Overlapping (stride=1):             {test_image.shape} ‚Üí {overlapping.shape}\")\n",
    "print(f\"  Overlap effect: More spatial resolution retained with overlapping\")\n",
    "\n",
    "# Comprehensive visualization\n",
    "fig, axes = plt.subplots(3, 4, figsize=(20, 15))\n",
    "\n",
    "# Plot original image\n",
    "im0 = axes[0, 0].imshow(test_image, cmap='viridis', interpolation='nearest', vmin=0, vmax=1)\n",
    "axes[0, 0].set_title('Original Image\\n(8x8 Feature Map)')\n",
    "axes[0, 0].set_xlabel('Width')\n",
    "axes[0, 0].set_ylabel('Height')\n",
    "\n",
    "# Add value annotations for clarity\n",
    "for i in range(test_image.shape[0]):\n",
    "    for j in range(test_image.shape[1]):\n",
    "        axes[0, 0].text(j, i, f'{test_image[i, j]:.1f}', \n",
    "                       ha='center', va='center', color='white', fontsize=8)\n",
    "\n",
    "# Plot different pooling types (2x2)\n",
    "for idx, pool_type in enumerate(pooling_types):\n",
    "    pooled = manual_pooling_2d(test_image, pool_size=2, pool_type=pool_type)\n",
    "    \n",
    "    im = axes[0, idx+1].imshow(pooled, cmap='viridis', interpolation='nearest', vmin=0, vmax=1)\n",
    "    axes[0, idx+1].set_title(f'{pool_type.title()} Pooling 2x2\\n{pooled.shape}')\n",
    "    \n",
    "    # Add value annotations\n",
    "    for i in range(pooled.shape[0]):\n",
    "        for j in range(pooled.shape[1]):\n",
    "            axes[0, idx+1].text(j, i, f'{pooled[i, j]:.2f}', \n",
    "                               ha='center', va='center', color='white', fontsize=10)\n",
    "\n",
    "# Plot different pool sizes (max pooling)\n",
    "for idx, pool_size in enumerate([2, 3, 4]):\n",
    "    pooled = manual_pooling_2d(test_image, pool_size=pool_size, pool_type='max')\n",
    "    \n",
    "    im = axes[1, idx].imshow(pooled, cmap='viridis', interpolation='nearest', vmin=0, vmax=1)\n",
    "    axes[1, idx].set_title(f'Max Pool {pool_size}x{pool_size}\\n{pooled.shape}')\n",
    "    \n",
    "    # Add value annotations\n",
    "    for i in range(pooled.shape[0]):\n",
    "        for j in range(pooled.shape[1]):\n",
    "            axes[1, idx].text(j, i, f'{pooled[i, j]:.2f}', \n",
    "                             ha='center', va='center', color='white', fontsize=10)\n",
    "\n",
    "# Demonstrate stride effects\n",
    "stride_examples = [(2, 2), (2, 1), (3, 1)]\n",
    "for idx, (pool_size, stride) in enumerate(stride_examples):\n",
    "    pooled = manual_pooling_2d(test_image, pool_size=pool_size, stride=stride, pool_type='max')\n",
    "    \n",
    "    im = axes[1, 3] if idx == 0 else axes[2, idx-1] if idx < 3 else None\n",
    "    if im is not None:\n",
    "        img = im.imshow(pooled, cmap='viridis', interpolation='nearest', vmin=0, vmax=1)\n",
    "        im.set_title(f'Pool {pool_size}x{pool_size}, Stride {stride}\\n{pooled.shape}')\n",
    "        \n",
    "        # Add value annotations for smaller outputs\n",
    "        if pooled.size <= 16:\n",
    "            for i in range(pooled.shape[0]):\n",
    "                for j in range(pooled.shape[1]):\n",
    "                    im.text(j, i, f'{pooled[i, j]:.2f}', \n",
    "                           ha='center', va='center', color='white', fontsize=10)\n",
    "\n",
    "# Global pooling demonstration\n",
    "global_max = np.max(test_image)\n",
    "global_avg = np.mean(test_image)\n",
    "global_min = np.min(test_image)\n",
    "\n",
    "global_values = np.array([[global_max], [global_avg], [global_min]])\n",
    "im_global = axes[2, 2].imshow(global_values, cmap='viridis', interpolation='nearest', vmin=0, vmax=1)\n",
    "axes[2, 2].set_title('Global Pooling\\n(Max, Avg, Min)')\n",
    "axes[2, 2].set_ylabel('Pool Type')\n",
    "axes[2, 2].set_yticks([0, 1, 2])\n",
    "axes[2, 2].set_yticklabels(['Max', 'Avg', 'Min'])\n",
    "axes[2, 2].set_xticks([])\n",
    "\n",
    "for i, val in enumerate([global_max, global_avg, global_min]):\n",
    "    axes[2, 2].text(0, i, f'{val:.3f}', ha='center', va='center', color='white', fontsize=12)\n",
    "\n",
    "# Information loss analysis\n",
    "axes[2, 3].axis('off')\n",
    "info_text = f\"\"\"\n",
    "INFORMATION ANALYSIS:\n",
    "\n",
    "Original: {test_image.size} values\n",
    "Max Pool 2x2: {manual_pooling_2d(test_image, 2, pool_type='max').size} values\n",
    "Compression: {test_image.size / manual_pooling_2d(test_image, 2, pool_type='max').size:.1f}x\n",
    "\n",
    "Information Preserved:\n",
    "‚Ä¢ Max Pool: Strongest activations\n",
    "‚Ä¢ Avg Pool: General feature strength\n",
    "‚Ä¢ Min Pool: Weakest activations\n",
    "\n",
    "Trade-offs:\n",
    "‚Ä¢ Smaller size ‚Üí Faster computation\n",
    "‚Ä¢ Lost detail ‚Üí Less precise localization\n",
    "‚Ä¢ Translation invariance gained\n",
    "\"\"\"\n",
    "axes[2, 3].text(0.05, 0.95, info_text, transform=axes[2, 3].transAxes, \n",
    "               verticalalignment='top', fontsize=10, fontfamily='monospace')\n",
    "\n",
    "# Add colorbars\n",
    "plt.colorbar(im0, ax=axes[0, :].ravel().tolist(), shrink=0.8, label='Activation Value')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Pooling Operations: Types, Sizes, and Effects', y=1.02, fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "# Analyze pooling effects on different image patterns\n",
    "print(f\"\\nüîç POOLING EFFECTS ON DIFFERENT PATTERNS:\")\n",
    "\n",
    "# Create different test patterns\n",
    "patterns = {\n",
    "    'uniform': np.ones((4, 4)) * 0.5,\n",
    "    'gradient': np.array([[i*j/9 for j in range(4)] for i in range(4)]),\n",
    "    'checkerboard': np.array([[((i+j) % 2) for j in range(4)] for i in range(4)]),\n",
    "    'sparse_features': np.zeros((4, 4))\n",
    "}\n",
    "patterns['sparse_features'][1, 1] = 1.0\n",
    "patterns['sparse_features'][2, 3] = 0.8\n",
    "\n",
    "for pattern_name, pattern in patterns.items():\n",
    "    max_pooled = manual_pooling_2d(pattern, pool_size=2, pool_type='max')\n",
    "    avg_pooled = manual_pooling_2d(pattern, pool_size=2, pool_type='average')\n",
    "    \n",
    "    original_info = np.sum(pattern > 0.1)  # Count significant activations\n",
    "    max_info = np.sum(max_pooled > 0.1)\n",
    "    avg_info = np.sum(avg_pooled > 0.1)\n",
    "    \n",
    "    print(f\"\\n{pattern_name.upper()} PATTERN:\")\n",
    "    print(f\"  Original significant features: {original_info}\")\n",
    "    print(f\"  Max pooling preserves: {max_info} features\")\n",
    "    print(f\"  Avg pooling preserves: {avg_info} features\")\n",
    "    print(f\"  Best pooling for this pattern: {'Max' if max_info >= avg_info else 'Average'}\")\n",
    "\n",
    "print(f\"\\nüí° POOLING KEY INSIGHTS:\")\n",
    "print(f\"\\n1. DIMENSIONALITY REDUCTION:\")\n",
    "print(f\"   ‚Ä¢ Pool size 2x2 ‚Üí 4x reduction in spatial dimensions\")\n",
    "print(f\"   ‚Ä¢ Pool size 3x3 ‚Üí 9x reduction in spatial dimensions\")\n",
    "print(f\"   ‚Ä¢ Critical for computational efficiency in deep networks\")\n",
    "\n",
    "print(f\"\\n2. POOLING TYPE SELECTION:\")\n",
    "print(f\"   ‚Ä¢ Max pooling: Best for sparse, distinct features (most common)\")\n",
    "print(f\"   ‚Ä¢ Average pooling: Better for distributed, texture-like features\")\n",
    "print(f\"   ‚Ä¢ Global pooling: Complete spatial invariance for classification\")\n",
    "\n",
    "print(f\"\\n3. TRANSLATION INVARIANCE:\")\n",
    "print(f\"   ‚Ä¢ Small shifts in input ‚Üí Same pooled output\")\n",
    "print(f\"   ‚Ä¢ Essential for robust image recognition\")\n",
    "print(f\"   ‚Ä¢ Trade-off: Lost spatial precision\")\n",
    "\n",
    "print(f\"\\n4. ARCHITECTURAL CONSIDERATIONS:\")\n",
    "print(f\"   ‚Ä¢ Usually placed after convolution layers\")\n",
    "print(f\"   ‚Ä¢ Stride typically equals pool size (non-overlapping)\")\n",
    "print(f\"   ‚Ä¢ Modern architectures sometimes replace with strided convolutions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Maps and Receptive Fields: What CNNs Really See\n",
    "\n",
    "**Feature Maps Explained:**\n",
    "- **Output of convolution**: Each filter produces one feature map\n",
    "- **Multiple filters**: Create multiple feature maps per layer\n",
    "- **Feature hierarchy**: Early layers‚Üíedges, Later layers‚Üícomplex objects\n",
    "- **Spatial arrangement**: Feature maps preserve spatial relationships\n",
    "\n",
    "**Receptive Field:**\n",
    "- **Definition**: Input region that affects a particular output neuron\n",
    "- **Growth through layers**: Deeper layers see larger input regions\n",
    "- **Critical concept**: Determines what patterns the network can detect\n",
    "\n",
    "**Receptive Field Calculation:**\n",
    "```\n",
    "RF = 1 (for first layer)\n",
    "For each subsequent layer:\n",
    "RF_new = RF_old + (kernel_size - 1) * stride_product\n",
    "```\n",
    "\n",
    "**Feature Learning Hierarchy:**\n",
    "1. **Layer 1**: Edges, colors, simple textures\n",
    "2. **Layer 2-3**: Shapes, patterns, object parts\n",
    "3. **Layer 4-5**: Objects, complex features\n",
    "4. **Final layers**: High-level semantic concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Feature Maps and Receptive Field Analysis\n",
    "\n",
    "print(\"=== FEATURE MAPS AND RECEPTIVE FIELD ANALYSIS ===\")\n",
    "\n",
    "def calculate_receptive_field(layers_config):\n",
    "    \"\"\"\n",
    "    Calculate receptive field for a CNN architecture\n",
    "    \n",
    "    Args:\n",
    "        layers_config: List of tuples (layer_type, kernel_size, stride, padding)\n",
    "    \n",
    "    Returns:\n",
    "        List of receptive field sizes for each layer\n",
    "    \"\"\"\n",
    "    receptive_fields = [1]  # Start with RF=1 for input\n",
    "    stride_product = 1\n",
    "    \n",
    "    for layer_type, kernel_size, stride, padding in layers_config:\n",
    "        if layer_type in ['conv', 'pool']:\n",
    "            # Update receptive field\n",
    "            current_rf = receptive_fields[-1] + (kernel_size - 1) * stride_product\n",
    "            receptive_fields.append(current_rf)\n",
    "            \n",
    "            # Update stride product for next layer\n",
    "            stride_product *= stride\n",
    "        else:\n",
    "            # For other layers (like dense), RF doesn't change\n",
    "            receptive_fields.append(receptive_fields[-1])\n",
    "    \n",
    "    return receptive_fields, stride_product\n",
    "\n",
    "# Define several CNN architectures to analyze\n",
    "architectures = {\n",
    "    'Simple CNN': [\n",
    "        ('conv', 3, 1, 0),  # 3x3 conv, stride 1\n",
    "        ('pool', 2, 2, 0),  # 2x2 max pool, stride 2\n",
    "        ('conv', 3, 1, 0),  # 3x3 conv, stride 1  \n",
    "        ('pool', 2, 2, 0),  # 2x2 max pool, stride 2\n",
    "        ('conv', 3, 1, 0),  # 3x3 conv, stride 1\n",
    "    ],\n",
    "    \n",
    "    'Deep CNN': [\n",
    "        ('conv', 3, 1, 1),  # 3x3 conv, stride 1, padding 1\n",
    "        ('conv', 3, 1, 1),  # 3x3 conv, stride 1, padding 1\n",
    "        ('pool', 2, 2, 0),  # 2x2 max pool, stride 2\n",
    "        ('conv', 3, 1, 1),  # 3x3 conv, stride 1, padding 1\n",
    "        ('conv', 3, 1, 1),  # 3x3 conv, stride 1, padding 1\n",
    "        ('pool', 2, 2, 0),  # 2x2 max pool, stride 2\n",
    "        ('conv', 3, 1, 1),  # 3x3 conv, stride 1, padding 1\n",
    "        ('conv', 3, 1, 1),  # 3x3 conv, stride 1, padding 1\n",
    "        ('pool', 2, 2, 0),  # 2x2 max pool, stride 2\n",
    "    ],\n",
    "    \n",
    "    'Large Kernel CNN': [\n",
    "        ('conv', 7, 1, 0),  # 7x7 conv, stride 1\n",
    "        ('pool', 2, 2, 0),  # 2x2 max pool, stride 2\n",
    "        ('conv', 5, 1, 0),  # 5x5 conv, stride 1\n",
    "        ('pool', 2, 2, 0),  # 2x2 max pool, stride 2\n",
    "        ('conv', 3, 1, 0),  # 3x3 conv, stride 1\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(f\"\\nüìê RECEPTIVE FIELD ANALYSIS:\")\n",
    "\n",
    "# Analyze each architecture\n",
    "rf_results = {}\n",
    "for arch_name, config in architectures.items():\n",
    "    rfs, final_stride = calculate_receptive_field(config)\n",
    "    rf_results[arch_name] = {'rfs': rfs, 'stride': final_stride, 'config': config}\n",
    "    \n",
    "    print(f\"\\n{arch_name.upper()}:\")\n",
    "    print(f\"  Layer-by-layer receptive field growth:\")\n",
    "    \n",
    "    layer_names = ['Input'] + [f\"{layer[0].title()} {i+1}\" for i, layer in enumerate(config)]\n",
    "    for i, (name, rf) in enumerate(zip(layer_names, rfs)):\n",
    "        if i < len(config):\n",
    "            layer_info = config[i]\n",
    "            print(f\"    {name:12s}: RF = {rf:2d}x{rf:2d} | Kernel: {layer_info[1]}x{layer_info[1]}, Stride: {layer_info[2]}\")\n",
    "        else:\n",
    "            print(f\"    {name:12s}: RF = {rf:2d}x{rf:2d}\")\n",
    "    \n",
    "    print(f\"  Final receptive field: {rfs[-1]}x{rfs[-1]} pixels\")\n",
    "    print(f\"  Total downsampling: {final_stride}x\")\n",
    "\n",
    "# Visualize receptive field growth\n",
    "plt.figure(figsize=(18, 12))\n",
    "\n",
    "# Plot 1: Receptive field growth curves\n",
    "plt.subplot(2, 3, 1)\n",
    "for arch_name, results in rf_results.items():\n",
    "    rfs = results['rfs']\n",
    "    plt.plot(range(len(rfs)), rfs, marker='o', linewidth=2, label=arch_name)\n",
    "\n",
    "plt.title('Receptive Field Growth Through Layers')\n",
    "plt.xlabel('Layer Number')\n",
    "plt.ylabel('Receptive Field Size (pixels)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')\n",
    "\n",
    "# Plot 2: Final receptive field comparison\n",
    "plt.subplot(2, 3, 2)\n",
    "arch_names = list(rf_results.keys())\n",
    "final_rfs = [results['rfs'][-1] for results in rf_results.values()]\n",
    "num_layers = [len(results['config']) for results in rf_results.values()]\n",
    "\n",
    "bars = plt.bar(arch_names, final_rfs, color=['skyblue', 'lightgreen', 'lightcoral'])\n",
    "plt.title('Final Receptive Field Size')\n",
    "plt.ylabel('Receptive Field Size (pixels)')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, rf, layers in zip(bars, final_rfs, num_layers):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "             f'{rf}x{rf}\\n({layers} layers)', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Plot 3: Create a real CNN to visualize feature maps\n",
    "print(f\"\\nüß† BUILDING REAL CNN FOR FEATURE MAP VISUALIZATION:\")\n",
    "\n",
    "# Load CIFAR-10 for demonstration\n",
    "(x_train_demo, y_train_demo), (x_test_demo, y_test_demo) = keras.datasets.cifar10.load_data()\n",
    "x_train_demo = x_train_demo.astype('float32') / 255.0\n",
    "x_test_demo = x_test_demo.astype('float32') / 255.0\n",
    "\n",
    "# Build a simple CNN for feature visualization\n",
    "demo_cnn = models.Sequential([\n",
    "    layers.Conv2D(16, 3, activation='relu', input_shape=(32, 32, 3), name='conv1'),\n",
    "    layers.Conv2D(32, 3, activation='relu', name='conv2'),\n",
    "    layers.MaxPooling2D(2, name='pool1'),\n",
    "    layers.Conv2D(64, 3, activation='relu', name='conv3'),\n",
    "    layers.MaxPooling2D(2, name='pool2'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "demo_cnn.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(f\"Demo CNN architecture:\")\n",
    "for i, layer in enumerate(demo_cnn.layers):\n",
    "    if hasattr(layer, 'kernel_size'):\n",
    "        print(f\"  Layer {i+1}: {layer.name:8s} - Filters: {layer.filters if hasattr(layer, 'filters') else 'N/A':2}, \"\n",
    "              f\"Kernel: {layer.kernel_size if hasattr(layer, 'kernel_size') else 'N/A'}\")\n",
    "    else:\n",
    "        print(f\"  Layer {i+1}: {layer.name:8s} - {type(layer).__name__}\")\n",
    "\n",
    "# Train briefly to get meaningful filters\n",
    "print(f\"\\nTraining briefly to learn meaningful filters...\")\n",
    "demo_cnn.fit(x_train_demo[:1000], y_train_demo[:1000], epochs=3, batch_size=32, verbose=0)\n",
    "\n",
    "# Function to extract and visualize feature maps\n",
    "def visualize_feature_maps(model, input_image, layer_names=None):\n",
    "    \"\"\"Extract and visualize feature maps from specified layers\"\"\"\n",
    "    \n",
    "    if layer_names is None:\n",
    "        layer_names = [layer.name for layer in model.layers if 'conv' in layer.name]\n",
    "    \n",
    "    # Create a model that outputs feature maps\n",
    "    layer_outputs = []\n",
    "    for layer_name in layer_names:\n",
    "        for layer in model.layers:\n",
    "            if layer.name == layer_name:\n",
    "                layer_outputs.append(layer.output)\n",
    "                break\n",
    "    \n",
    "    if not layer_outputs:\n",
    "        print(\"No matching layers found!\")\n",
    "        return\n",
    "    \n",
    "    activation_model = models.Model(inputs=model.input, outputs=layer_outputs)\n",
    "    \n",
    "    # Get activations\n",
    "    activations = activation_model.predict(input_image[np.newaxis, ...], verbose=0)\n",
    "    \n",
    "    return activations, layer_names\n",
    "\n",
    "# Select an interesting test image\n",
    "test_idx = 0\n",
    "test_image = x_test_demo[test_idx]\n",
    "true_label = y_test_demo[test_idx][0]\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "print(f\"\\nAnalyzing image: {class_names[true_label]}\")\n",
    "\n",
    "# Extract feature maps\n",
    "conv_layer_names = ['conv1', 'conv2', 'conv3']\n",
    "activations, layer_names = visualize_feature_maps(demo_cnn, test_image, conv_layer_names)\n",
    "\n",
    "# Plot original image\n",
    "plt.subplot(2, 3, 3)\n",
    "plt.imshow(test_image)\n",
    "plt.title(f'Original Image\\n{class_names[true_label]}')\n",
    "plt.axis('off')\n",
    "\n",
    "# Plot feature maps from different layers\n",
    "for layer_idx, (activation, layer_name) in enumerate(zip(activations, layer_names)):\n",
    "    if layer_idx < 3:  # Show first 3 layers\n",
    "        plt.subplot(2, 3, 4 + layer_idx)\n",
    "        \n",
    "        # Show first 6 feature maps from this layer\n",
    "        feature_maps_to_show = min(6, activation.shape[-1])\n",
    "        \n",
    "        # Create a grid to show multiple feature maps\n",
    "        grid_size = int(np.ceil(np.sqrt(feature_maps_to_show)))\n",
    "        combined_map = np.zeros((activation.shape[1] * grid_size, activation.shape[2] * grid_size))\n",
    "        \n",
    "        for i in range(feature_maps_to_show):\n",
    "            row = i // grid_size\n",
    "            col = i % grid_size\n",
    "            start_row = row * activation.shape[1]\n",
    "            end_row = start_row + activation.shape[1]\n",
    "            start_col = col * activation.shape[2]\n",
    "            end_col = start_col + activation.shape[2]\n",
    "            \n",
    "            feature_map = activation[0, :, :, i]\n",
    "            # Normalize for visualization\n",
    "            feature_map = (feature_map - feature_map.min()) / (feature_map.max() - feature_map.min() + 1e-8)\n",
    "            combined_map[start_row:end_row, start_col:end_col] = feature_map\n",
    "        \n",
    "        plt.imshow(combined_map, cmap='viridis')\n",
    "        plt.title(f'{layer_name} Feature Maps\\n{activation.shape[1]}x{activation.shape[2]}x{activation.shape[3]}')\n",
    "        plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze feature map statistics\n",
    "print(f\"\\nüìä FEATURE MAP ANALYSIS:\")\n",
    "for layer_idx, (activation, layer_name) in enumerate(zip(activations, layer_names)):\n",
    "    mean_activation = np.mean(activation)\n",
    "    std_activation = np.std(activation)\n",
    "    sparsity = np.mean(activation == 0)  # Fraction of zero activations (due to ReLU)\n",
    "    max_activation = np.max(activation)\n",
    "    \n",
    "    print(f\"\\n{layer_name.upper()}:\")\n",
    "    print(f\"  Shape: {activation.shape[1:]} (H x W x Channels)\")\n",
    "    print(f\"  Total activations: {activation.size:,}\")\n",
    "    print(f\"  Mean activation: {mean_activation:.4f}\")\n",
    "    print(f\"  Std activation: {std_activation:.4f}\")\n",
    "    print(f\"  Sparsity (zeros): {sparsity:.2%}\")\n",
    "    print(f\"  Max activation: {max_activation:.4f}\")\n",
    "    \n",
    "    # Analyze what this layer might be detecting\n",
    "    if layer_idx == 0:\n",
    "        print(f\"  Likely detects: Edges, colors, simple textures\")\n",
    "    elif layer_idx == 1:\n",
    "        print(f\"  Likely detects: Shapes, patterns, combinations of edges\")\n",
    "    else:\n",
    "        print(f\"  Likely detects: Object parts, complex features\")\n",
    "\n",
    "# Calculate actual receptive fields for our demo CNN\n",
    "demo_config = [\n",
    "    ('conv', 3, 1, 0),  # conv1\n",
    "    ('conv', 3, 1, 0),  # conv2  \n",
    "    ('pool', 2, 2, 0),  # pool1\n",
    "    ('conv', 3, 1, 0),  # conv3\n",
    "    ('pool', 2, 2, 0),  # pool2\n",
    "]\n",
    "\n",
    "demo_rfs, demo_stride = calculate_receptive_field(demo_config)\n",
    "\n",
    "print(f\"\\nüîç DEMO CNN RECEPTIVE FIELD ANALYSIS:\")\n",
    "layer_names_rf = ['Input', 'Conv1', 'Conv2', 'Pool1', 'Conv3', 'Pool2']\n",
    "for name, rf in zip(layer_names_rf, demo_rfs):\n",
    "    print(f\"  {name:8s}: {rf:2d}x{rf:2d} pixels\")\n",
    "\n",
    "print(f\"\\nThis means:\")\n",
    "print(f\"  ‚Ä¢ Conv1 neurons see {demo_rfs[1]}x{demo_rfs[1]} pixel regions\")\n",
    "print(f\"  ‚Ä¢ Conv2 neurons see {demo_rfs[2]}x{demo_rfs[2]} pixel regions\")\n",
    "print(f\"  ‚Ä¢ Conv3 neurons see {demo_rfs[4]}x{demo_rfs[4]} pixel regions\")\n",
    "print(f\"  ‚Ä¢ Final layer sees {demo_rfs[-1]}x{demo_rfs[-1]} = {demo_rfs[-1]**2} pixels\")\n",
    "print(f\"  ‚Ä¢ On 32x32 input, that's {(demo_rfs[-1]**2)/(32*32):.1%} of the image\")\n",
    "\n",
    "print(f\"\\nüí° FEATURE MAP AND RECEPTIVE FIELD INSIGHTS:\")\n",
    "print(f\"\\n1. FEATURE HIERARCHY:\")\n",
    "print(f\"   ‚Ä¢ Early layers: Local features (edges, textures)\")\n",
    "print(f\"   ‚Ä¢ Middle layers: Mid-level features (shapes, patterns)\")\n",
    "print(f\"   ‚Ä¢ Late layers: High-level features (objects, concepts)\")\n",
    "\n",
    "print(f\"\\n2. RECEPTIVE FIELD GROWTH:\")\n",
    "print(f\"   ‚Ä¢ Convolution: Adds (kernel_size - 1) to receptive field\")\n",
    "print(f\"   ‚Ä¢ Pooling: Multiplies effective stride for subsequent layers\")\n",
    "print(f\"   ‚Ä¢ Larger kernels: Faster receptive field growth\")\n",
    "\n",
    "print(f\"\\n3. ARCHITECTURAL DESIGN:\")\n",
    "print(f\"   ‚Ä¢ Want large final receptive field to see whole objects\")\n",
    "print(f\"   ‚Ä¢ But not too large early on (lose local detail)\")\n",
    "print(f\"   ‚Ä¢ Balance: Gradual growth through multiple layers\")\n",
    "\n",
    "print(f\"\\n4. FEATURE MAP SPARSITY:\")\n",
    "print(f\"   ‚Ä¢ ReLU activation ‚Üí Many zero values (sparsity)\")\n",
    "print(f\"   ‚Ä¢ Good: Efficient computation and storage\")\n",
    "print(f\"   ‚Ä¢ Indicates selective feature detection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. CNN Architecture Patterns: From LeNet to Modern Networks\n",
    "\n",
    "**Historical Evolution:**\n",
    "\n",
    "1. **LeNet-5 (1998)**: First successful CNN\n",
    "   - 2 conv layers, 2 fully connected\n",
    "   - Sigmoid/tanh activations\n",
    "   - Proved CNNs work for digit recognition\n",
    "\n",
    "2. **AlexNet (2012)**: ImageNet breakthrough\n",
    "   - 5 conv layers, 3 fully connected\n",
    "   - ReLU activations, dropout, data augmentation\n",
    "   - GPU acceleration\n",
    "\n",
    "3. **VGG (2014)**: Deeper with small filters\n",
    "   - Only 3x3 convolutions\n",
    "   - 16-19 layers deep\n",
    "   - Showed depth matters\n",
    "\n",
    "4. **ResNet (2015)**: Skip connections\n",
    "   - Residual learning\n",
    "   - 50-152 layers\n",
    "   - Solved vanishing gradient problem\n",
    "\n",
    "**Key Design Patterns:**\n",
    "- **Feature extraction**: Conv + Pool layers\n",
    "- **Classification**: Global pooling + Dense layers\n",
    "- **Regularization**: Dropout, batch normalization\n",
    "- **Skip connections**: For very deep networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: CNN Architecture Patterns Implementation and Analysis\n",
    "\n",
    "print(\"=== CNN ARCHITECTURE PATTERNS THROUGH HISTORY ===\")\n",
    "\n",
    "def create_lenet_style(input_shape=(32, 32, 1), num_classes=10):\n",
    "    \"\"\"\n",
    "    LeNet-5 style architecture (adapted for modern frameworks)\n",
    "    Original: Designed for 32x32 grayscale images (MNIST-style)\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        # Feature extraction\n",
    "        layers.Conv2D(6, 5, activation='tanh', input_shape=input_shape, name='conv1'),\n",
    "        layers.AveragePooling2D(2, name='pool1'),\n",
    "        layers.Conv2D(16, 5, activation='tanh', name='conv2'),\n",
    "        layers.AveragePooling2D(2, name='pool2'),\n",
    "        \n",
    "        # Classification\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(120, activation='tanh', name='fc1'),\n",
    "        layers.Dense(84, activation='tanh', name='fc2'),\n",
    "        layers.Dense(num_classes, activation='softmax', name='output')\n",
    "    ], name='LeNet_Style')\n",
    "    \n",
    "    return model\n",
    "\n",
    "def create_alexnet_style(input_shape=(224, 224, 3), num_classes=10):\n",
    "    \"\"\"\n",
    "    AlexNet style architecture (simplified for smaller inputs)\n",
    "    Original: Designed for 224x224 color images (ImageNet)\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        # Feature extraction\n",
    "        layers.Conv2D(64, 11, strides=4, activation='relu', input_shape=input_shape, name='conv1'),\n",
    "        layers.MaxPooling2D(3, strides=2, name='pool1'),\n",
    "        \n",
    "        layers.Conv2D(192, 5, padding='same', activation='relu', name='conv2'),\n",
    "        layers.MaxPooling2D(3, strides=2, name='pool2'),\n",
    "        \n",
    "        layers.Conv2D(384, 3, padding='same', activation='relu', name='conv3'),\n",
    "        layers.Conv2D(256, 3, padding='same', activation='relu', name='conv4'),\n",
    "        layers.Conv2D(256, 3, padding='same', activation='relu', name='conv5'),\n",
    "        layers.MaxPooling2D(3, strides=2, name='pool3'),\n",
    "        \n",
    "        # Classification\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(4096, activation='relu', name='fc1'),\n",
    "        layers.Dropout(0.5, name='dropout1'),\n",
    "        layers.Dense(4096, activation='relu', name='fc2'),\n",
    "        layers.Dropout(0.5, name='dropout2'),\n",
    "        layers.Dense(num_classes, activation='softmax', name='output')\n",
    "    ], name='AlexNet_Style')\n",
    "    \n",
    "    return model\n",
    "\n",
    "def create_vgg_style(input_shape=(224, 224, 3), num_classes=10, depth='11'):\n",
    "    \"\"\"\n",
    "    VGG style architecture (VGG-11 variant)\n",
    "    Key innovation: Only 3x3 convolutions\n",
    "    \"\"\"\n",
    "    model = models.Sequential(name=f'VGG_{depth}_Style')\n",
    "    model.add(layers.Input(shape=input_shape))\n",
    "    \n",
    "    # VGG-11 configuration: [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M']\n",
    "    # Where 'M' means MaxPooling\n",
    "    \n",
    "    # Block 1\n",
    "    model.add(layers.Conv2D(64, 3, padding='same', activation='relu', name='conv1_1'))\n",
    "    model.add(layers.MaxPooling2D(2, strides=2, name='pool1'))\n",
    "    \n",
    "    # Block 2  \n",
    "    model.add(layers.Conv2D(128, 3, padding='same', activation='relu', name='conv2_1'))\n",
    "    model.add(layers.MaxPooling2D(2, strides=2, name='pool2'))\n",
    "    \n",
    "    # Block 3\n",
    "    model.add(layers.Conv2D(256, 3, padding='same', activation='relu', name='conv3_1'))\n",
    "    model.add(layers.Conv2D(256, 3, padding='same', activation='relu', name='conv3_2'))\n",
    "    model.add(layers.MaxPooling2D(2, strides=2, name='pool3'))\n",
    "    \n",
    "    # Block 4\n",
    "    model.add(layers.Conv2D(512, 3, padding='same', activation='relu', name='conv4_1'))\n",
    "    model.add(layers.Conv2D(512, 3, padding='same', activation='relu', name='conv4_2'))\n",
    "    model.add(layers.MaxPooling2D(2, strides=2, name='pool4'))\n",
    "    \n",
    "    # Block 5\n",
    "    model.add(layers.Conv2D(512, 3, padding='same', activation='relu', name='conv5_1'))\n",
    "    model.add(layers.Conv2D(512, 3, padding='same', activation='relu', name='conv5_2'))\n",
    "    model.add(layers.MaxPooling2D(2, strides=2, name='pool5'))\n",
    "    \n",
    "    # Classification\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(4096, activation='relu', name='fc1'))\n",
    "    model.add(layers.Dropout(0.5, name='dropout1'))\n",
    "    model.add(layers.Dense(4096, activation='relu', name='fc2'))\n",
    "    model.add(layers.Dropout(0.5, name='dropout2'))\n",
    "    model.add(layers.Dense(num_classes, activation='softmax', name='output'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "def create_modern_cnn(input_shape=(32, 32, 3), num_classes=10):\n",
    "    \"\"\"\n",
    "    Modern CNN with best practices:\n",
    "    - Batch normalization\n",
    "    - Residual-like connections (simplified)\n",
    "    - Global average pooling\n",
    "    - Data augmentation layers\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        # Data augmentation (built into model)\n",
    "        layers.RandomFlip('horizontal'),\n",
    "        layers.RandomRotation(0.1),\n",
    "        layers.RandomZoom(0.1),\n",
    "        \n",
    "        # Block 1\n",
    "        layers.Conv2D(32, 3, padding='same', input_shape=input_shape, name='conv1_1'),\n",
    "        layers.BatchNormalization(name='bn1_1'),\n",
    "        layers.Activation('relu', name='relu1_1'),\n",
    "        layers.Conv2D(32, 3, padding='same', name='conv1_2'),\n",
    "        layers.BatchNormalization(name='bn1_2'),\n",
    "        layers.Activation('relu', name='relu1_2'),\n",
    "        layers.MaxPooling2D(2, name='pool1'),\n",
    "        layers.Dropout(0.25, name='dropout1'),\n",
    "        \n",
    "        # Block 2\n",
    "        layers.Conv2D(64, 3, padding='same', name='conv2_1'),\n",
    "        layers.BatchNormalization(name='bn2_1'),\n",
    "        layers.Activation('relu', name='relu2_1'),\n",
    "        layers.Conv2D(64, 3, padding='same', name='conv2_2'),\n",
    "        layers.BatchNormalization(name='bn2_2'),\n",
    "        layers.Activation('relu', name='relu2_2'),\n",
    "        layers.MaxPooling2D(2, name='pool2'),\n",
    "        layers.Dropout(0.25, name='dropout2'),\n",
    "        \n",
    "        # Block 3\n",
    "        layers.Conv2D(128, 3, padding='same', name='conv3_1'),\n",
    "        layers.BatchNormalization(name='bn3_1'),\n",
    "        layers.Activation('relu', name='relu3_1'),\n",
    "        layers.Conv2D(128, 3, padding='same', name='conv3_2'),\n",
    "        layers.BatchNormalization(name='bn3_2'),\n",
    "        layers.Activation('relu', name='relu3_2'),\n",
    "        layers.MaxPooling2D(2, name='pool3'),\n",
    "        layers.Dropout(0.25, name='dropout3'),\n",
    "        \n",
    "        # Global pooling instead of flatten + dense\n",
    "        layers.GlobalAveragePooling2D(name='global_pool'),\n",
    "        layers.Dropout(0.5, name='dropout_final'),\n",
    "        layers.Dense(num_classes, activation='softmax', name='output')\n",
    "    ], name='Modern_CNN')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create all architectures for comparison\n",
    "print(f\"\\nüèóÔ∏è CREATING HISTORICAL CNN ARCHITECTURES:\")\n",
    "\n",
    "# Adjust input shapes for fair comparison (all using CIFAR-10 size)\n",
    "input_shape_cifar = (32, 32, 3)\n",
    "num_classes = 10\n",
    "\n",
    "architectures_historical = {\n",
    "    'LeNet-Style': create_lenet_style(input_shape_cifar, num_classes),\n",
    "    'AlexNet-Style': create_alexnet_style(input_shape_cifar, num_classes), \n",
    "    'VGG-Style': create_vgg_style(input_shape_cifar, num_classes),\n",
    "    'Modern-CNN': create_modern_cnn(input_shape_cifar, num_classes)\n",
    "}\n",
    "\n",
    "# Analyze each architecture\n",
    "print(f\"\\nüìä ARCHITECTURE COMPARISON:\")\n",
    "analysis_data = []\n",
    "\n",
    "for name, model in architectures_historical.items():\n",
    "    # Count different types of layers\n",
    "    conv_layers = sum(1 for layer in model.layers if isinstance(layer, layers.Conv2D))\n",
    "    pool_layers = sum(1 for layer in model.layers if isinstance(layer, (layers.MaxPooling2D, layers.AveragePooling2D, layers.GlobalAveragePooling2D)))\n",
    "    dense_layers = sum(1 for layer in model.layers if isinstance(layer, layers.Dense))\n",
    "    total_params = model.count_params()\n",
    "    \n",
    "    analysis_data.append({\n",
    "        'name': name,\n",
    "        'conv_layers': conv_layers,\n",
    "        'pool_layers': pool_layers, \n",
    "        'dense_layers': dense_layers,\n",
    "        'total_layers': len(model.layers),\n",
    "        'total_params': total_params\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n{name.upper()}:\")\n",
    "    print(f\"  Total layers: {len(model.layers)}\")\n",
    "    print(f\"  Conv layers: {conv_layers}\")\n",
    "    print(f\"  Pooling layers: {pool_layers}\")\n",
    "    print(f\"  Dense layers: {dense_layers}\")\n",
    "    print(f\"  Total parameters: {total_params:,}\")\n",
    "    print(f\"  Key innovations: {get_architecture_innovations(name)}\")\n",
    "\n",
    "def get_architecture_innovations(name):\n",
    "    \"\"\"Get key innovations for each architecture\"\"\"\n",
    "    innovations = {\n",
    "        'LeNet-Style': 'First successful CNN, proved concept works',\n",
    "        'AlexNet-Style': 'ReLU activation, dropout, data augmentation, GPU training',\n",
    "        'VGG-Style': 'Small 3x3 filters only, very deep networks',\n",
    "        'Modern-CNN': 'Batch normalization, global pooling, built-in augmentation'\n",
    "    }\n",
    "    return innovations.get(name, 'Unknown innovations')\n",
    "\n",
    "# Create comprehensive visualization\n",
    "fig, axes = plt.subplots(3, 2, figsize=(16, 18))\n",
    "\n",
    "# Plot 1: Parameter count comparison\n",
    "names = [data['name'] for data in analysis_data]\n",
    "param_counts = [data['total_params'] for data in analysis_data]\n",
    "\n",
    "bars1 = axes[0, 0].bar(names, param_counts, color=['lightblue', 'lightgreen', 'lightcoral', 'gold'])\n",
    "axes[0, 0].set_title('Total Parameters by Architecture')\n",
    "axes[0, 0].set_ylabel('Number of Parameters')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "axes[0, 0].set_yscale('log')\n",
    "\n",
    "# Add value labels\n",
    "for bar, count in zip(bars1, param_counts):\n",
    "    axes[0, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height(),\n",
    "                   f'{count:,}', ha='center', va='bottom', rotation=45, fontsize=8)\n",
    "\n",
    "# Plot 2: Layer composition\n",
    "layer_types = ['Conv Layers', 'Pool Layers', 'Dense Layers']\n",
    "x = np.arange(len(names))\n",
    "width = 0.25\n",
    "\n",
    "conv_counts = [data['conv_layers'] for data in analysis_data]\n",
    "pool_counts = [data['pool_layers'] for data in analysis_data]\n",
    "dense_counts = [data['dense_layers'] for data in analysis_data]\n",
    "\n",
    "axes[0, 1].bar(x - width, conv_counts, width, label='Conv Layers', color='skyblue')\n",
    "axes[0, 1].bar(x, pool_counts, width, label='Pool Layers', color='lightgreen')\n",
    "axes[0, 1].bar(x + width, dense_counts, width, label='Dense Layers', color='lightcoral')\n",
    "\n",
    "axes[0, 1].set_title('Layer Composition by Architecture')\n",
    "axes[0, 1].set_xlabel('Architecture')\n",
    "axes[0, 1].set_ylabel('Number of Layers')\n",
    "axes[0, 1].set_xticks(x)\n",
    "axes[0, 1].set_xticklabels(names, rotation=45)\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Plot 3: Architecture evolution timeline\n",
    "years = [1998, 2012, 2014, 2020]  # Approximate years\n",
    "accuracies = [99.2, 84.7, 92.7, 95.0]  # Approximate accuracies on their respective datasets\n",
    "\n",
    "axes[1, 0].plot(years, accuracies, 'o-', linewidth=3, markersize=8, color='darkblue')\n",
    "axes[1, 0].set_title('CNN Evolution: Performance Over Time')\n",
    "axes[1, 0].set_xlabel('Year')\n",
    "axes[1, 0].set_ylabel('Approximate Accuracy (%)')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Add architecture labels\n",
    "arch_names_short = ['LeNet', 'AlexNet', 'VGG', 'Modern']\n",
    "for year, acc, name in zip(years, accuracies, arch_names_short):\n",
    "    axes[1, 0].annotate(name, (year, acc), xytext=(5, 5), \n",
    "                       textcoords='offset points', fontsize=10)\n",
    "\n",
    "# Plot 4: Detailed architecture comparison table\n",
    "axes[1, 1].axis('off')\n",
    "table_data = []\n",
    "for data in analysis_data:\n",
    "    table_data.append([\n",
    "        data['name'],\n",
    "        str(data['conv_layers']),\n",
    "        str(data['pool_layers']),\n",
    "        str(data['dense_layers']),\n",
    "        f\"{data['total_params']:,}\"\n",
    "    ])\n",
    "\n",
    "table = axes[1, 1].table(\n",
    "    cellText=table_data,\n",
    "    colLabels=['Architecture', 'Conv', 'Pool', 'Dense', 'Parameters'],\n",
    "    cellLoc='center',\n",
    "    loc='center'\n",
    ")\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(9)\n",
    "table.scale(1.2, 1.5)\n",
    "axes[1, 1].set_title('Architecture Comparison Table')\n",
    "\n",
    "# Plot 5: Show detailed layer structure for one architecture\n",
    "modern_cnn = architectures_historical['Modern-CNN']\n",
    "layer_info = []\n",
    "current_shape = input_shape_cifar\n",
    "\n",
    "for i, layer in enumerate(modern_cnn.layers[:15]):  # Show first 15 layers\n",
    "    layer_type = type(layer).__name__\n",
    "    if hasattr(layer, 'output_shape') and layer.built:\n",
    "        try:\n",
    "            # This is a simplified shape calculation\n",
    "            if 'Conv2D' in layer_type:\n",
    "                if hasattr(layer, 'filters'):\n",
    "                    current_shape = (*current_shape[:2], layer.filters)\n",
    "            elif 'MaxPooling2D' in layer_type:\n",
    "                if hasattr(layer, 'pool_size'):\n",
    "                    pool_size = layer.pool_size[0] if isinstance(layer.pool_size, tuple) else layer.pool_size\n",
    "                    current_shape = (current_shape[0]//pool_size, current_shape[1]//pool_size, current_shape[2])\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    params = layer.count_params() if hasattr(layer, 'count_params') else 0\n",
    "    layer_info.append((i+1, layer_type, str(current_shape), f\"{params:,}\"))\n",
    "\n",
    "# Create layer structure visualization\n",
    "axes[2, 0].axis('off')\n",
    "layer_table = axes[2, 0].table(\n",
    "    cellText=layer_info,\n",
    "    colLabels=['#', 'Layer Type', 'Output Shape', 'Parameters'],\n",
    "    cellLoc='center',\n",
    "    loc='center'\n",
    ")\n",
    "layer_table.auto_set_font_size(False)\n",
    "layer_table.set_fontsize(8)\n",
    "layer_table.scale(1.2, 1.8)\n",
    "axes[2, 0].set_title('Modern CNN Layer Structure (First 15 Layers)')\n",
    "\n",
    "# Plot 6: Key insights and evolution\n",
    "axes[2, 1].axis('off')\n",
    "evolution_text = \"\"\"\n",
    "CNN EVOLUTION KEY INSIGHTS:\n",
    "\n",
    "LeNet (1998):\n",
    "‚Ä¢ First successful CNN\n",
    "‚Ä¢ Sigmoid/Tanh activations\n",
    "‚Ä¢ Average pooling\n",
    "‚Ä¢ Small datasets (MNIST)\n",
    "\n",
    "AlexNet (2012):\n",
    "‚Ä¢ ReLU breakthrough\n",
    "‚Ä¢ Dropout regularization\n",
    "‚Ä¢ Data augmentation\n",
    "‚Ä¢ GPU acceleration\n",
    "\n",
    "VGG (2014):\n",
    "‚Ä¢ Small 3x3 filters only\n",
    "‚Ä¢ Very deep (16-19 layers)\n",
    "‚Ä¢ Systematic architecture\n",
    "‚Ä¢ Showed depth importance\n",
    "\n",
    "Modern CNNs:\n",
    "‚Ä¢ Batch normalization\n",
    "‚Ä¢ Skip connections (ResNet)\n",
    "‚Ä¢ Global pooling\n",
    "‚Ä¢ Efficient architectures\n",
    "\"\"\"\n",
    "\n",
    "axes[2, 1].text(0.05, 0.95, evolution_text, transform=axes[2, 1].transAxes,\n",
    "               verticalalignment='top', fontsize=9, fontfamily='monospace')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate memory requirements\n",
    "print(f\"\\nüíæ MEMORY ANALYSIS:\")\n",
    "for name, model in architectures_historical.items():\n",
    "    # Estimate memory for batch size 32\n",
    "    batch_size = 32\n",
    "    \n",
    "    # Parameters memory (float32)\n",
    "    param_memory_mb = model.count_params() * 4 / (1024 * 1024)\n",
    "    \n",
    "    # Activation memory (approximate, for batch_size)\n",
    "    # This is a rough estimate\n",
    "    activation_elements = 0\n",
    "    current_shape = list(input_shape_cifar)\n",
    "    \n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, layers.Conv2D):\n",
    "            if hasattr(layer, 'filters'):\n",
    "                current_shape[2] = layer.filters\n",
    "                activation_elements += np.prod(current_shape)\n",
    "        elif isinstance(layer, layers.MaxPooling2D):\n",
    "            if hasattr(layer, 'pool_size'):\n",
    "                pool_size = layer.pool_size[0] if isinstance(layer.pool_size, tuple) else layer.pool_size\n",
    "                current_shape[0] //= pool_size\n",
    "                current_shape[1] //= pool_size\n",
    "                activation_elements += np.prod(current_shape)\n",
    "    \n",
    "    activation_memory_mb = activation_elements * batch_size * 4 / (1024 * 1024)\n",
    "    total_memory_mb = param_memory_mb + activation_memory_mb\n",
    "    \n",
    "    print(f\"\\n{name.upper()}:\")\n",
    "    print(f\"  Parameters: {param_memory_mb:.1f} MB\")\n",
    "    print(f\"  Activations (batch={batch_size}): {activation_memory_mb:.1f} MB\")\n",
    "    print(f\"  Total memory: {total_memory_mb:.1f} MB\")\n",
    "    \n",
    "    # Performance characteristics\n",
    "    if 'LeNet' in name:\n",
    "        print(f\"  Best for: Small datasets, simple patterns\")\n",
    "    elif 'AlexNet' in name:\n",
    "        print(f\"  Best for: Medium datasets, breakthrough performance\")\n",
    "    elif 'VGG' in name:\n",
    "        print(f\"  Best for: High accuracy, systematic design\")\n",
    "    elif 'Modern' in name:\n",
    "        print(f\"  Best for: Production use, efficient training\")\n",
    "\n",
    "print(f\"\\nüí° ARCHITECTURE DESIGN INSIGHTS:\")\n",
    "print(f\"\\n1. HISTORICAL PROGRESSION:\")\n",
    "print(f\"   ‚Ä¢ LeNet ‚Üí AlexNet: ReLU + Dropout + Scale\")\n",
    "print(f\"   ‚Ä¢ AlexNet ‚Üí VGG: Systematic design + Depth\")\n",
    "print(f\"   ‚Ä¢ VGG ‚Üí ResNet: Skip connections for very deep networks\")\n",
    "print(f\"   ‚Ä¢ Modern: Efficiency + Best practices\")\n",
    "\n",
    "print(f\"\\n2. KEY INNOVATIONS IMPACT:\")\n",
    "print(f\"   ‚Ä¢ ReLU: Solved vanishing gradients, faster training\")\n",
    "print(f\"   ‚Ä¢ Dropout: Reduced overfitting, better generalization\")\n",
    "print(f\"   ‚Ä¢ Batch Norm: Stable training, higher learning rates\")\n",
    "print(f\"   ‚Ä¢ Skip connections: Ultra-deep networks possible\")\n",
    "\n",
    "print(f\"\\n3. MODERN DESIGN PRINCIPLES:\")\n",
    "print(f\"   ‚Ä¢ Start with proven architectures\")\n",
    "print(f\"   ‚Ä¢ Use batch normalization after conv layers\")\n",
    "print(f\"   ‚Ä¢ Prefer global pooling over large dense layers\")\n",
    "print(f\"   ‚Ä¢ Add data augmentation for robustness\")\n",
    "print(f\"   ‚Ä¢ Monitor parameter count vs. performance\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
