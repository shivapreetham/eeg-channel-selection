{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2967738,"sourceType":"datasetVersion","datasetId":1819423}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# PhysioNet Motor Imagery - Legacy Baseline Methods\n\n## Comprehensive Evaluation of 5 Baseline Methods + Channel Selection\n\nThis notebook trains and evaluates:\n1. **FBCSP** - Filter Bank Common Spatial Patterns with LDA\n2. **CNN-SAE** - CNN with Spatial Attention\n3. **EEGNet** - Compact convolutional network\n4. **ACS-SE-CNN** - Attention + Squeeze-Excitation CNN\n5. **G-CARM** - Graph-based CARM\n\n## Channel Selection Methods:\n- **FBCSP**: CSP pattern-based selection\n- **G-CARM**: Edge Selection (ES) / Aggregation Selection (AS)\n- **CNN-SAE, EEGNet, ACS-SE-CNN**: Gradient-based attribution\n\n## Configuration:\n- **30 epochs**, **0.002 LR**, **NO EARLY STOPPING** (for PyTorch models)\n- **10 subjects**, **3-fold CV**\n- **9 filter banks**, **4 CSP components** (for FBCSP)\n- **Channel Selection**: k=[10,15,20,25,30]\n\n## Metrics:\n- Accuracy, Precision, Recall, F1-Score, AUC-ROC, Specificity\n\n## Output:\n- `legacy_*_results.csv` (full channel results)\n- `legacy_*_retrain_results.csv` (channel selection results)","metadata":{}},{"cell_type":"markdown","source":"## 1. Setup and Imports","metadata":{}},{"cell_type":"code","source":"import json\nimport random\nimport warnings\nfrom pathlib import Path\nfrom copy import deepcopy\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm.auto import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score,\n    roc_auc_score, confusion_matrix\n)\nfrom scipy.signal import butter, filtfilt\nimport gc\n\nimport mne\nfrom mne.decoding import CSP\n\nwarnings.filterwarnings('ignore')\nsns.set_context('notebook', font_scale=1.0)\nmne.set_log_level('WARNING')\n\ndef set_seed(s=42):\n    random.seed(s)\n    np.random.seed(s)\n    torch.manual_seed(s)\n    torch.cuda.manual_seed_all(s)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nset_seed(42)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'Device: {device}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T05:10:57.903380Z","iopub.execute_input":"2025-11-27T05:10:57.904090Z","iopub.status.idle":"2025-11-27T05:11:05.283413Z","shell.execute_reply.started":"2025-11-27T05:10:57.904060Z","shell.execute_reply":"2025-11-27T05:11:05.282489Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"## 2. Configuration","metadata":{}},{"cell_type":"code","source":"import os\nfrom pathlib import Path\n\nif os.path.exists('/kaggle/input'):\n    print(\"Running on Kaggle\")\n    kaggle_input = Path('/kaggle/input')\n    datasets = [d for d in kaggle_input.iterdir() if d.is_dir()]\n    print(f\"Available datasets: {[d.name for d in datasets]}\")\n\n    DATA_DIR = None\n    possible_names = ['physioneteegmi', 'eeg-motor-movementimagery-dataset']\n    for ds_name in possible_names:\n        test_path = kaggle_input / ds_name\n        if test_path.exists():\n            DATA_DIR = test_path\n            print(f\"Found dataset: {DATA_DIR}\")\n            break\n\n    if DATA_DIR is None and datasets:\n        DATA_DIR = datasets[0]\n        print(f\"Using first available dataset: {DATA_DIR}\")\nelse:\n    print(\"Running locally\")\n    DATA_DIR = Path('data/physionet/files')\n\nCONFIG = {\n    'data': {\n        'raw_data_dir': DATA_DIR,\n        'selected_classes': [1, 2],\n        'tmin': -1.0,\n        'tmax': 5.0,\n        'baseline': (-0.5, 0)\n    },\n    'preprocessing': {\n        'l_freq': 0.5,\n        'h_freq': 40.0,\n        'notch_freq': 50.0,\n        'target_sfreq': 128.0,\n        'apply_car': True\n    },\n    'model': {\n        'epochs': 30,\n        'learning_rate': 0.002,\n        'batch_size': 64,\n        'n_folds': 3,\n        'patience': 999\n    },\n    'fbcsp': {\n        'freq_bands': [\n            (4, 8), (8, 12), (12, 16), (16, 20), (20, 24),\n            (24, 28), (28, 32), (32, 36), (36, 40)\n        ],\n        'n_components': 4\n    },\n    'channel_selection': {\n        'k_values': [10, 15, 20, 25, 30]\n    },\n    'output': {\n        'results_dir': Path('results'),\n    },\n    'max_subjects': 10,\n    'min_runs_per_subject': 8\n}\n\nCONFIG['output']['results_dir'].mkdir(exist_ok=True, parents=True)\n\nprint(f\"\\nConfiguration loaded!\")\nprint(f\"Training: {CONFIG['max_subjects']} subjects, {CONFIG['model']['n_folds']}-fold CV, {CONFIG['model']['epochs']} epochs\")\nprint(f\"Learning rate: {CONFIG['model']['learning_rate']}, No early stopping (patience={CONFIG['model']['patience']})\")\nprint(f\"FBCSP: {len(CONFIG['fbcsp']['freq_bands'])} filter banks, {CONFIG['fbcsp']['n_components']} components\")\nprint(f\"Channel selection k values: {CONFIG['channel_selection']['k_values']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T05:11:05.284482Z","iopub.execute_input":"2025-11-27T05:11:05.284863Z","iopub.status.idle":"2025-11-27T05:11:05.296228Z","shell.execute_reply.started":"2025-11-27T05:11:05.284843Z","shell.execute_reply":"2025-11-27T05:11:05.295678Z"}},"outputs":[{"name":"stdout","text":"Running on Kaggle\nAvailable datasets: ['physioneteegmi']\nFound dataset: /kaggle/input/physioneteegmi\n\nConfiguration loaded!\nTraining: 10 subjects, 3-fold CV, 30 epochs\nLearning rate: 0.002, No early stopping (patience=999)\nFBCSP: 9 filter banks, 4 components\nChannel selection k values: [10, 15, 20, 25, 30]\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"## 3. Data Cleaning - Remove Faulty Subjects","metadata":{}},{"cell_type":"code","source":"KNOWN_BAD_SUBJECTS = [\n    'S088', 'S089', 'S092', 'S100', 'S104', 'S106', 'S107', 'S108', 'S109'\n]\n\nHIGH_ISSUE_SUBJECTS = [\n    'S003', 'S004', 'S009', 'S010', 'S012', 'S013', 'S017', 'S018', 'S019',\n    'S021', 'S022', 'S023', 'S024', 'S025', 'S026', 'S027', 'S028', 'S029'\n]\n\nEXCLUDED_SUBJECTS = set(KNOWN_BAD_SUBJECTS + HIGH_ISSUE_SUBJECTS)\n\nprint(f\"Total excluded subjects: {len(EXCLUDED_SUBJECTS)}\")\nprint(f\"Excluded subjects: {sorted(EXCLUDED_SUBJECTS)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T05:11:05.296809Z","iopub.execute_input":"2025-11-27T05:11:05.297011Z","iopub.status.idle":"2025-11-27T05:11:05.319139Z","shell.execute_reply.started":"2025-11-27T05:11:05.296996Z","shell.execute_reply":"2025-11-27T05:11:05.318365Z"}},"outputs":[{"name":"stdout","text":"Total excluded subjects: 27\nExcluded subjects: ['S003', 'S004', 'S009', 'S010', 'S012', 'S013', 'S017', 'S018', 'S019', 'S021', 'S022', 'S023', 'S024', 'S025', 'S026', 'S027', 'S028', 'S029', 'S088', 'S089', 'S092', 'S100', 'S104', 'S106', 'S107', 'S108', 'S109']\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"## 4. Data Loading and Preprocessing Functions","metadata":{}},{"cell_type":"code","source":"def preprocess_raw(raw, config):\n    cleaned_names = {name: name.rstrip('.') for name in raw.ch_names}\n    raw.rename_channels(cleaned_names)\n    raw.pick_types(eeg=True)\n    raw.set_montage('standard_1020', on_missing='ignore', match_case=False)\n    \n    nyquist = raw.info['sfreq'] / 2.0\n    if config['preprocessing']['notch_freq'] < nyquist:\n        raw.notch_filter(freqs=config['preprocessing']['notch_freq'], verbose=False)\n    \n    raw.filter(\n        l_freq=config['preprocessing']['l_freq'],\n        h_freq=config['preprocessing']['h_freq'],\n        method='fir',\n        fir_design='firwin',\n        verbose=False\n    )\n    \n    if config['preprocessing']['apply_car']:\n        raw.set_eeg_reference('average', projection=False, verbose=False)\n    \n    raw.resample(config['preprocessing']['target_sfreq'], npad='auto', verbose=False)\n    return raw\n\n\ndef load_and_preprocess_edf(edf_path, config):\n    raw = mne.io.read_raw_edf(edf_path, preload=True, verbose='ERROR')\n    raw = preprocess_raw(raw, config)\n    \n    events, event_ids = mne.events_from_annotations(raw, verbose='ERROR')\n    \n    if len(events) == 0:\n        return None, None, raw.ch_names\n    \n    epochs = mne.Epochs(\n        raw,\n        events,\n        event_id=event_ids,\n        tmin=config['data']['tmin'],\n        tmax=config['data']['tmax'],\n        baseline=tuple(config['data']['baseline']),\n        preload=True,\n        verbose='ERROR'\n    )\n    \n    return epochs.get_data(), epochs.events[:, 2], raw.ch_names\n\n\ndef filter_classes(x, y, selected_classes):\n    mask = np.isin(y, selected_classes)\n    y, x = y[mask], x[mask]\n    label_map = {old: new for new, old in enumerate(sorted(selected_classes))}\n    y = np.array([label_map[int(label)] for label in y], dtype=np.int64)\n    return x, y\n\n\ndef normalize(x):\n    mu = x.mean(axis=(0, 2), keepdims=True)\n    sd = x.std(axis=(0, 2), keepdims=True) + 1e-8\n    return (x - mu) / sd\n\n\ndef load_subject_data(data_dir, subject_id, run_ids, config):\n    subject_dir = data_dir / subject_id\n    if not subject_dir.exists():\n        return None, None, None\n    \n    all_x, all_y = [], []\n    channel_names = None\n    \n    for run_id in run_ids:\n        edf_path = subject_dir / f'{subject_id}{run_id}.edf'\n        if not edf_path.exists():\n            continue\n        \n        try:\n            x, y, ch_names = load_and_preprocess_edf(edf_path, config)\n            if x is None or len(y) == 0:\n                continue\n            \n            x, y = filter_classes(x, y, config['data']['selected_classes'])\n            if len(y) == 0:\n                continue\n            \n            channel_names = channel_names or ch_names\n            all_x.append(x)\n            all_y.append(y)\n        except Exception as e:\n            print(f\"  Warning: Failed to load {edf_path.name}: {e}\")\n            continue\n    \n    if len(all_x) == 0:\n        return None, None, channel_names\n    \n    return np.concatenate(all_x, 0), np.concatenate(all_y, 0), channel_names\n\n\ndef get_available_subjects(data_dir, min_runs=8, excluded=None):\n    if not data_dir.exists():\n        raise ValueError(f\"Data directory not found: {data_dir}\")\n    \n    excluded = excluded or set()\n    subjects = []\n    \n    for subject_dir in sorted(data_dir.iterdir()):\n        if not subject_dir.is_dir() or not subject_dir.name.startswith('S'):\n            continue\n        \n        if subject_dir.name in excluded:\n            continue\n        \n        edf_files = list(subject_dir.glob('*.edf'))\n        if len(edf_files) >= min_runs:\n            subjects.append(subject_dir.name)\n    \n    return subjects\n\n\nprint(\"\\nScanning for subjects...\")\ndata_dir = CONFIG['data']['raw_data_dir']\nprint(f\"Looking for data in: {data_dir}\")\n\nall_subjects = get_available_subjects(\n    data_dir, \n    min_runs=CONFIG['min_runs_per_subject'],\n    excluded=EXCLUDED_SUBJECTS\n)\nsubjects = all_subjects[:CONFIG['max_subjects']]\n\nprint(f\"Found {len(all_subjects)} clean subjects with >= {CONFIG['min_runs_per_subject']} runs\")\nprint(f\"Will process {len(subjects)} subjects: {subjects}\")\n\nMOTOR_IMAGERY_RUNS = ['R07', 'R08', 'R09', 'R10', 'R11', 'R12', 'R13', 'R14']\nMOTOR_EXECUTION_RUNS = ['R03', 'R04', 'R05', 'R06']\nALL_TASK_RUNS = MOTOR_IMAGERY_RUNS + MOTOR_EXECUTION_RUNS\nprint(f\"Using runs: {ALL_TASK_RUNS}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T05:11:05.320544Z","iopub.execute_input":"2025-11-27T05:11:05.320770Z","iopub.status.idle":"2025-11-27T05:11:05.801896Z","shell.execute_reply.started":"2025-11-27T05:11:05.320753Z","shell.execute_reply":"2025-11-27T05:11:05.801182Z"}},"outputs":[{"name":"stdout","text":"\nScanning for subjects...\nLooking for data in: /kaggle/input/physioneteegmi\nFound 82 clean subjects with >= 8 runs\nWill process 10 subjects: ['S001', 'S002', 'S005', 'S006', 'S007', 'S008', 'S011', 'S014', 'S015', 'S016']\nUsing runs: ['R07', 'R08', 'R09', 'R10', 'R11', 'R12', 'R13', 'R14', 'R03', 'R04', 'R05', 'R06']\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"## 5. PyTorch Dataset","metadata":{}},{"cell_type":"code","source":"class EEGDataset(Dataset):\n    def __init__(self, x, y):\n        self.x = torch.FloatTensor(x).unsqueeze(1)\n        self.y = torch.LongTensor(y)\n    \n    def __len__(self):\n        return len(self.y)\n    \n    def __getitem__(self, i):\n        return self.x[i], self.y[i]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T05:11:05.802542Z","iopub.execute_input":"2025-11-27T05:11:05.802820Z","iopub.status.idle":"2025-11-27T05:11:05.807300Z","shell.execute_reply.started":"2025-11-27T05:11:05.802791Z","shell.execute_reply":"2025-11-27T05:11:05.806573Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"## 6. Comprehensive Metrics Functions","metadata":{}},{"cell_type":"code","source":"@torch.no_grad()\ndef calculate_comprehensive_metrics(model, dataloader, device):\n    model.eval()\n    all_preds, all_labels, all_probs = [], [], []\n\n    for X_batch, y_batch in dataloader:\n        X_batch = X_batch.to(device)\n        outputs = model(X_batch)\n        probs = torch.softmax(outputs, dim=1)\n        _, predicted = torch.max(outputs, 1)\n\n        all_preds.extend(predicted.cpu().numpy())\n        all_labels.extend(y_batch.numpy())\n        all_probs.extend(probs[:, 1].cpu().numpy())\n\n    all_preds = np.array(all_preds)\n    all_labels = np.array(all_labels)\n    all_probs = np.array(all_probs)\n\n    metrics = {\n        'accuracy': accuracy_score(all_labels, all_preds),\n        'precision': precision_score(all_labels, all_preds, average='binary', zero_division=0),\n        'recall': recall_score(all_labels, all_preds, average='binary', zero_division=0),\n        'f1_score': f1_score(all_labels, all_preds, average='binary', zero_division=0),\n        'auc_roc': roc_auc_score(all_labels, all_probs) if len(np.unique(all_labels)) > 1 else 0.0,\n    }\n\n    cm = confusion_matrix(all_labels, all_preds)\n    if cm.shape == (2, 2):\n        tn, fp, fn, tp = cm.ravel()\n        metrics['specificity'] = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n        metrics['sensitivity'] = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n    else:\n        metrics['specificity'] = 0.0\n        metrics['sensitivity'] = metrics['recall']\n\n    return metrics\n\n\ndef calculate_sklearn_metrics(y_true, y_pred):\n    metrics = {\n        'accuracy': accuracy_score(y_true, y_pred),\n        'precision': precision_score(y_true, y_pred, average='binary', zero_division=0),\n        'recall': recall_score(y_true, y_pred, average='binary', zero_division=0),\n        'f1_score': f1_score(y_true, y_pred, average='binary', zero_division=0),\n        'auc_roc': 0.0,\n    }\n\n    cm = confusion_matrix(y_true, y_pred)\n    if cm.shape == (2, 2):\n        tn, fp, fn, tp = cm.ravel()\n        metrics['specificity'] = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n        metrics['sensitivity'] = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n    else:\n        metrics['specificity'] = 0.0\n        metrics['sensitivity'] = metrics['recall']\n\n    return metrics\n\n\nprint(\"Comprehensive metrics functions defined!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T05:11:05.807978Z","iopub.execute_input":"2025-11-27T05:11:05.808251Z","iopub.status.idle":"2025-11-27T05:11:05.826746Z","shell.execute_reply.started":"2025-11-27T05:11:05.808235Z","shell.execute_reply":"2025-11-27T05:11:05.826109Z"}},"outputs":[{"name":"stdout","text":"Comprehensive metrics functions defined!\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"## 7. Model Architectures","metadata":{}},{"cell_type":"markdown","source":"### 7.1 FBCSP","metadata":{}},{"cell_type":"code","source":"class FBCSP:\n    def __init__(self, freq_bands, n_components=4, sfreq=128.0):\n        self.freq_bands = freq_bands\n        self.n_components = n_components\n        self.sfreq = sfreq\n        self.csps = []\n        self.lda = LinearDiscriminantAnalysis()\n    \n    def _bandpass_filter(self, data, low_freq, high_freq):\n        nyquist = self.sfreq / 2.0\n        low = low_freq / nyquist\n        high = high_freq / nyquist\n        b, a = butter(5, [low, high], btype='band')\n        return filtfilt(b, a, data, axis=-1)\n    \n    def fit(self, X, y):\n        self.csps = []\n        all_features = []\n        \n        for low_freq, high_freq in self.freq_bands:\n            X_filtered = self._bandpass_filter(X.copy(), low_freq, high_freq)\n            \n            csp = CSP(n_components=self.n_components, reg='ledoit_wolf', log=True, norm_trace=False)\n            csp.fit(X_filtered, y)\n            self.csps.append(csp)\n            \n            features = csp.transform(X_filtered)\n            all_features.append(features)\n        \n        all_features = np.concatenate(all_features, axis=1)\n        self.lda.fit(all_features, y)\n        return self\n    \n    def predict(self, X):\n        all_features = []\n        \n        for idx, (low_freq, high_freq) in enumerate(self.freq_bands):\n            X_filtered = self._bandpass_filter(X.copy(), low_freq, high_freq)\n            features = self.csps[idx].transform(X_filtered)\n            all_features.append(features)\n        \n        all_features = np.concatenate(all_features, axis=1)\n        return self.lda.predict(all_features)\n\nprint(\"FBCSP defined!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T05:11:05.827459Z","iopub.execute_input":"2025-11-27T05:11:05.827730Z","iopub.status.idle":"2025-11-27T05:11:05.847309Z","shell.execute_reply.started":"2025-11-27T05:11:05.827708Z","shell.execute_reply":"2025-11-27T05:11:05.846698Z"}},"outputs":[{"name":"stdout","text":"FBCSP defined!\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"def train_epoch(model, dataloader, criterion, optimizer, device):\n    model.train()\n    total_loss = 0.0\n    all_preds, all_labels = [], []\n    \n    for x, y in dataloader:\n        x, y = x.to(device), y.to(device)\n        optimizer.zero_grad()\n        logits = model(x)\n        loss = criterion(logits, y)\n\n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n        all_preds += torch.argmax(logits, 1).cpu().tolist()\n        all_labels += y.cpu().tolist()\n    \n    return total_loss / max(1, len(dataloader)), accuracy_score(all_labels, all_preds)\n\n\n@torch.no_grad()\ndef evaluate(model, dataloader, criterion, device):\n    model.eval()\n    total_loss = 0.0\n    all_preds, all_labels = [], []\n    \n    for x, y in dataloader:\n        x, y = x.to(device), y.to(device)\n        logits = model(x)\n        loss = criterion(logits, y)\n        \n        total_loss += loss.item()\n        all_preds += torch.argmax(logits, 1).cpu().tolist()\n        all_labels += y.cpu().tolist()\n    \n    return total_loss / max(1, len(dataloader)), accuracy_score(all_labels, all_preds)\n\n\ndef train_model(model, train_loader, val_loader, device, epochs, lr, patience, verbose=True):\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, mode='min', factor=0.5, patience=3, verbose=False\n    )\n    \n    best_acc = 0.0\n    best_state = None\n    no_improve = 0\n    \n    epoch_iterator = tqdm(range(epochs), desc='    Epochs', leave=False) if verbose else range(epochs)\n    \n    for epoch in epoch_iterator:\n        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n        val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n        \n        scheduler.step(val_loss)\n        \n        if verbose:\n            epoch_iterator.set_postfix({\n                'train_loss': f'{train_loss:.4f}',\n                'train_acc': f'{train_acc:.4f}',\n                'val_loss': f'{val_loss:.4f}',\n                'val_acc': f'{val_acc:.4f}',\n                'best': f'{best_acc:.4f}'\n            })\n        \n        if val_acc > best_acc:\n            best_acc = val_acc\n            best_state = deepcopy(model.state_dict())\n            no_improve = 0\n        else:\n            no_improve += 1\n        \n        if no_improve >= patience:\n            if verbose:\n                print(f'      Early stopping at epoch {epoch+1}/{epochs}')\n            break\n    \n    if best_state is None:\n        best_state = deepcopy(model.state_dict())\n    \n    model.load_state_dict(best_state)\n    return best_state, best_acc\n\n\nprint(\"Training functions defined!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T05:11:05.848187Z","iopub.execute_input":"2025-11-27T05:11:05.848688Z","iopub.status.idle":"2025-11-27T05:11:05.868195Z","shell.execute_reply.started":"2025-11-27T05:11:05.848647Z","shell.execute_reply":"2025-11-27T05:11:05.867530Z"}},"outputs":[{"name":"stdout","text":"Training functions defined!\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"### 7.2 CNN-SAE","metadata":{}},{"cell_type":"code","source":"class SpatialAttention(nn.Module):\n    def __init__(self, n_channels):\n        super().__init__()\n        self.attention = nn.Sequential(\n            nn.Linear(n_channels, n_channels // 4),\n            nn.ReLU(),\n            nn.Linear(n_channels // 4, n_channels),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        pooled = torch.mean(x, dim=2)\n        weights = self.attention(pooled)\n        return x * weights.unsqueeze(2)\n\n\nclass CNNSAE(nn.Module):\n    def __init__(self, n_channels=64, n_classes=2, n_timepoints=769):\n        super().__init__()\n        self.spatial_attention = SpatialAttention(n_channels)\n        self.conv1 = nn.Conv1d(n_channels, 64, kernel_size=5, padding=2)\n        self.bn1 = nn.BatchNorm1d(64)\n        self.pool1 = nn.MaxPool1d(2)\n        self.conv2 = nn.Conv1d(64, 128, kernel_size=5, padding=2)\n        self.bn2 = nn.BatchNorm1d(128)\n        self.pool2 = nn.MaxPool1d(2)\n        self.conv3 = nn.Conv1d(128, 256, kernel_size=3, padding=1)\n        self.bn3 = nn.BatchNorm1d(256)\n        self.pool3 = nn.MaxPool1d(2)\n        self.dropout = nn.Dropout(0.5)\n\n        with torch.no_grad():\n            test_input = torch.zeros(1, n_channels, n_timepoints)\n            test_output = self._forward_features(test_input)\n            flattened_size = test_output.view(1, -1).size(1)\n\n        self.fc1 = nn.Linear(flattened_size, 256)\n        self.fc2 = nn.Linear(256, n_classes)\n\n    def _forward_features(self, x):\n        x = self.spatial_attention(x)\n        x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n        x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n        x = self.pool3(F.relu(self.bn3(self.conv3(x))))\n        return x\n\n    def forward(self, x):\n        if x.dim() == 4:\n            x = x.squeeze(1)\n        x = self._forward_features(x)\n        x = x.view(x.size(0), -1)\n        x = self.dropout(x)\n        x = F.relu(self.fc1(x))\n        x = self.dropout(x)\n        return self.fc2(x)\n\nprint(\"CNN-SAE defined!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T05:11:05.868872Z","iopub.execute_input":"2025-11-27T05:11:05.869200Z","iopub.status.idle":"2025-11-27T05:11:05.890434Z","shell.execute_reply.started":"2025-11-27T05:11:05.869182Z","shell.execute_reply":"2025-11-27T05:11:05.889719Z"}},"outputs":[{"name":"stdout","text":"CNN-SAE defined!\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"### 7.3 EEGNet","metadata":{}},{"cell_type":"code","source":"class EEGNet(nn.Module):\n    def __init__(self, n_channels=64, n_classes=2, n_timepoints=769, F1=8, D=2, F2=16):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, F1, (1, 64), padding=(0, 32), bias=False)\n        self.bn1 = nn.BatchNorm2d(F1)\n        self.conv2 = nn.Conv2d(F1, F1*D, (n_channels, 1), groups=F1, bias=False)\n        self.bn2 = nn.BatchNorm2d(F1*D)\n        self.pool1 = nn.AvgPool2d((1, 4))\n        self.dropout1 = nn.Dropout(0.5)\n        self.conv3 = nn.Conv2d(F1*D, F2, (1, 16), padding=(0, 8), bias=False)\n        self.bn3 = nn.BatchNorm2d(F2)\n        self.pool2 = nn.AvgPool2d((1, 8))\n        self.dropout2 = nn.Dropout(0.5)\n\n        with torch.no_grad():\n            test_input = torch.zeros(1, 1, n_channels, n_timepoints)\n            test_output = self._forward_features(test_input)\n            flattened_size = test_output.view(1, -1).size(1)\n\n        self.fc = nn.Linear(flattened_size, n_classes)\n\n    def _forward_features(self, x):\n        x = self.bn1(self.conv1(x))\n        x = self.dropout1(self.pool1(F.elu(self.bn2(self.conv2(x)))))\n        x = self.dropout2(self.pool2(F.elu(self.bn3(self.conv3(x)))))\n        return x\n\n    def forward(self, x):\n        if x.dim() == 3:\n            x = x.unsqueeze(1)\n        x = self._forward_features(x)\n        x = x.view(x.size(0), -1)\n        return self.fc(x)\n\nprint(\"EEGNet defined!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T05:11:05.892459Z","iopub.execute_input":"2025-11-27T05:11:05.892757Z","iopub.status.idle":"2025-11-27T05:11:05.911201Z","shell.execute_reply.started":"2025-11-27T05:11:05.892740Z","shell.execute_reply":"2025-11-27T05:11:05.910459Z"}},"outputs":[{"name":"stdout","text":"EEGNet defined!\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"### 7.4 ACS-SE-CNN","metadata":{}},{"cell_type":"code","source":"class SEBlock(nn.Module):\n    def __init__(self, channels, reduction=4):\n        super().__init__()\n        self.fc1 = nn.Linear(channels, max(1, channels // reduction))\n        self.fc2 = nn.Linear(max(1, channels // reduction), channels)\n\n    def forward(self, x):\n        squeeze = torch.mean(x, dim=2)\n        excitation = F.relu(self.fc1(squeeze))\n        excitation = torch.sigmoid(self.fc2(excitation))\n        return x * excitation.unsqueeze(2)\n\n\nclass ACSECNN(nn.Module):\n    def __init__(self, n_channels=64, n_classes=2, n_timepoints=769):\n        super().__init__()\n        self.channel_attention = nn.Sequential(\n            nn.Linear(n_timepoints, 128),\n            nn.ReLU(),\n            nn.Linear(128, 1),\n            nn.Sigmoid()\n        )\n        self.se1 = SEBlock(n_channels)\n        self.se2 = SEBlock(128)\n        self.se3 = SEBlock(256)\n        self.conv1 = nn.Conv1d(n_channels, 128, kernel_size=5, padding=2)\n        self.bn1 = nn.BatchNorm1d(128)\n        self.pool1 = nn.MaxPool1d(2)\n        self.conv2 = nn.Conv1d(128, 256, kernel_size=5, padding=2)\n        self.bn2 = nn.BatchNorm1d(256)\n        self.pool2 = nn.MaxPool1d(2)\n        self.conv3 = nn.Conv1d(256, 512, kernel_size=3, padding=1)\n        self.bn3 = nn.BatchNorm1d(512)\n        self.pool3 = nn.MaxPool1d(2)\n        self.dropout = nn.Dropout(0.5)\n\n        with torch.no_grad():\n            test_input = torch.zeros(1, n_channels, n_timepoints)\n            test_output = self._forward_features(test_input)\n            flattened_size = test_output.view(1, -1).size(1)\n\n        self.fc1 = nn.Linear(flattened_size, 256)\n        self.fc2 = nn.Linear(256, n_classes)\n\n    def _forward_features(self, x):\n        channel_weights = []\n        for i in range(x.size(1)):\n            w = self.channel_attention(x[:, i, :])\n            channel_weights.append(w)\n        channel_weights = torch.cat(channel_weights, dim=1)\n        x = x * channel_weights.unsqueeze(2)\n        x = self.se1(x)\n        x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n        x = self.se2(x)\n        x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n        x = self.se3(x)\n        x = self.pool3(F.relu(self.bn3(self.conv3(x))))\n        return x\n\n    def forward(self, x):\n        if x.dim() == 4:\n            x = x.squeeze(1)\n        x = self._forward_features(x)\n        x = x.view(x.size(0), -1)\n        x = self.dropout(x)\n        x = F.relu(self.fc1(x))\n        x = self.dropout(x)\n        return self.fc2(x)\n\nprint(\"ACS-SE-CNN defined!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T05:11:05.911985Z","iopub.execute_input":"2025-11-27T05:11:05.912336Z","iopub.status.idle":"2025-11-27T05:11:05.932903Z","shell.execute_reply.started":"2025-11-27T05:11:05.912309Z","shell.execute_reply":"2025-11-27T05:11:05.932200Z"}},"outputs":[{"name":"stdout","text":"ACS-SE-CNN defined!\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"### 7.5 G-CARM","metadata":{}},{"cell_type":"code","source":"class CARMBlock(nn.Module):\n    def __init__(self, n_channels):\n        super().__init__()\n        self.A = nn.Parameter(torch.randn(n_channels, n_channels) * 0.01)\n\n    def forward(self, x):\n        A_norm = torch.softmax(self.A, dim=1)\n        x_reshaped = x.permute(0, 2, 1)\n        x_graph = torch.matmul(x_reshaped, A_norm.t())\n        return x_graph.permute(0, 2, 1)\n    \n    def get_adjacency(self):\n        with torch.no_grad():\n            return torch.sigmoid(self.A).cpu().numpy()\n\n\nclass GCARM(nn.Module):\n    def __init__(self, n_channels=64, n_classes=2, n_timepoints=769):\n        super().__init__()\n        self.carm1 = CARMBlock(n_channels)\n        self.carm2 = CARMBlock(n_channels)\n        self.conv1 = nn.Conv1d(n_channels, 128, kernel_size=5, padding=2)\n        self.bn1 = nn.BatchNorm1d(128)\n        self.pool1 = nn.MaxPool1d(2)\n        self.conv2 = nn.Conv1d(128, 256, kernel_size=5, padding=2)\n        self.bn2 = nn.BatchNorm1d(256)\n        self.pool2 = nn.MaxPool1d(2)\n        self.conv3 = nn.Conv1d(256, 512, kernel_size=3, padding=1)\n        self.bn3 = nn.BatchNorm1d(512)\n        self.pool3 = nn.MaxPool1d(2)\n        self.dropout = nn.Dropout(0.5)\n\n        with torch.no_grad():\n            test_input = torch.zeros(1, n_channels, n_timepoints)\n            test_output = self._forward_features(test_input)\n            flattened_size = test_output.view(1, -1).size(1)\n\n        self.fc1 = nn.Linear(flattened_size, 256)\n        self.fc2 = nn.Linear(256, n_classes)\n\n    def _forward_features(self, x):\n        x = self.carm1(x)\n        x = self.carm2(x)\n        x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n        x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n        x = self.pool3(F.relu(self.bn3(self.conv3(x))))\n        return x\n\n    def forward(self, x):\n        if x.dim() == 4:\n            x = x.squeeze(1)\n        x = self._forward_features(x)\n        x = x.view(x.size(0), -1)\n        x = self.dropout(x)\n        x = F.relu(self.fc1(x))\n        x = self.dropout(x)\n        return self.fc2(x)\n    \n    def get_final_adjacency(self):\n        return self.carm2.get_adjacency()\n\nprint(\"G-CARM defined!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T05:11:05.933542Z","iopub.execute_input":"2025-11-27T05:11:05.933773Z","iopub.status.idle":"2025-11-27T05:11:05.952898Z","shell.execute_reply.started":"2025-11-27T05:11:05.933740Z","shell.execute_reply":"2025-11-27T05:11:05.952193Z"}},"outputs":[{"name":"stdout","text":"G-CARM defined!\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"## 8. Training Functions","metadata":{}},{"cell_type":"code","source":"def train_epoch(model, dataloader, criterion, optimizer, device):\n    model.train()\n    total_loss = 0.0\n    all_preds, all_labels = [], []\n    \n    for x, y in dataloader:\n        x, y = x.to(device), y.to(device)\n        optimizer.zero_grad()\n        logits = model(x)\n        loss = criterion(logits, y)\n\n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n        all_preds += torch.argmax(logits, 1).cpu().tolist()\n        all_labels += y.cpu().tolist()\n    \n    return total_loss / max(1, len(dataloader)), accuracy_score(all_labels, all_preds)\n\n\n@torch.no_grad()\ndef evaluate(model, dataloader, criterion, device):\n    model.eval()\n    total_loss = 0.0\n    all_preds, all_labels = [], []\n    \n    for x, y in dataloader:\n        x, y = x.to(device), y.to(device)\n        logits = model(x)\n        loss = criterion(logits, y)\n        \n        total_loss += loss.item()\n        all_preds += torch.argmax(logits, 1).cpu().tolist()\n        all_labels += y.cpu().tolist()\n    \n    return total_loss / max(1, len(dataloader)), accuracy_score(all_labels, all_preds)\n\n\ndef train_model(model, train_loader, val_loader, device, epochs, lr, patience):\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, mode='min', factor=0.5, patience=3, verbose=False\n    )\n    \n    best_acc = 0.0\n    best_state = None\n    no_improve = 0\n    \n    for epoch in range(epochs):\n        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n        val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n        \n        scheduler.step(val_loss)\n        \n        if val_acc > best_acc:\n            best_acc = val_acc\n            best_state = deepcopy(model.state_dict())\n            no_improve = 0\n        else:\n            no_improve += 1\n        \n        if no_improve >= patience:\n            break\n    \n    if best_state is None:\n        best_state = deepcopy(model.state_dict())\n    \n    model.load_state_dict(best_state)\n    return best_state, best_acc\n\n\nprint(\"Training functions defined!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T05:11:05.953523Z","iopub.execute_input":"2025-11-27T05:11:05.953758Z","iopub.status.idle":"2025-11-27T05:11:05.969020Z","shell.execute_reply.started":"2025-11-27T05:11:05.953742Z","shell.execute_reply":"2025-11-27T05:11:05.968230Z"}},"outputs":[{"name":"stdout","text":"Training functions defined!\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"## 9. Main Training Loop","metadata":{}},{"cell_type":"code","source":"all_results = {\n    'fbcsp': [],\n    'cnn_sae': [],\n    'eegnet': [],\n    'acs_se_cnn': [],\n    'g_carm': []\n}\n\nprint(\"\\nStarting training for legacy methods...\\n\")\n\nfor subject_id in tqdm(subjects, desc='Training subjects'):\n    print(f\"\\nProcessing {subject_id}...\")\n    \n    X, Y, channel_names = load_subject_data(\n        data_dir,\n        subject_id,\n        ALL_TASK_RUNS,\n        CONFIG\n    )\n    \n    if X is None or len(Y) == 0:\n        print(f\"  Skipped: No data available\")\n        continue\n    \n    C, T = X.shape[1], X.shape[2]\n    K = len(set(CONFIG['data']['selected_classes']))\n    \n    print(f\"  Data shape: {X.shape}\")\n    print(f\"  Label distribution: {np.bincount(Y)}\")\n    \n    for model_name in ['fbcsp', 'cnn_sae', 'eegnet', 'acs_se_cnn', 'g_carm']:\n        print(f\"\\n  Training {model_name.upper()}...\")\n        \n        skf = StratifiedKFold(n_splits=CONFIG['model']['n_folds'], shuffle=True, random_state=42)\n        fold_results = []\n        fold_models = []\n        fold_csp_patterns = []\n        fold_adjacencies = []\n        \n        for fold, (train_idx, val_idx) in enumerate(skf.split(X, Y)):\n            X_train, X_val = X[train_idx], X[val_idx]\n            Y_train, Y_val = Y[train_idx], Y[val_idx]\n            \n            if model_name == 'fbcsp':\n                model = FBCSP(\n                    freq_bands=CONFIG['fbcsp']['freq_bands'],\n                    n_components=CONFIG['fbcsp']['n_components'],\n                    sfreq=CONFIG['preprocessing']['target_sfreq']\n                )\n                model.fit(X_train, Y_train)\n                y_pred = model.predict(X_val)\n                metrics = calculate_sklearn_metrics(Y_val, y_pred)\n                fold_results.append(metrics)\n                \n                csp_patterns = []\n                for csp in model.csps:\n                    patterns = csp.patterns_\n                    csp_patterns.append(np.abs(patterns).mean(axis=1))\n                avg_csp_importance = np.mean(np.stack(csp_patterns, 0), 0)\n                fold_csp_patterns.append(avg_csp_importance)\n                fold_models.append(model)\n                \n            else:\n                X_train_norm = normalize(X_train)\n                X_val_norm = normalize(X_val)\n                \n                train_loader = DataLoader(\n                    EEGDataset(X_train_norm, Y_train),\n                    batch_size=CONFIG['model']['batch_size'],\n                    shuffle=True,\n                    num_workers=0\n                )\n                val_loader = DataLoader(\n                    EEGDataset(X_val_norm, Y_val),\n                    batch_size=CONFIG['model']['batch_size'],\n                    shuffle=False,\n                    num_workers=0\n                )\n                \n                if model_name == 'cnn_sae':\n                    model = CNNSAE(n_channels=C, n_classes=K, n_timepoints=T).to(device)\n                elif model_name == 'eegnet':\n                    model = EEGNet(n_channels=C, n_classes=K, n_timepoints=T).to(device)\n                elif model_name == 'acs_se_cnn':\n                    model = ACSECNN(n_channels=C, n_classes=K, n_timepoints=T).to(device)\n                elif model_name == 'g_carm':\n                    model = GCARM(n_channels=C, n_classes=K, n_timepoints=T).to(device)\n                \n                best_state, best_acc = train_model(\n                    model, train_loader, val_loader, device,\n                    CONFIG['model']['epochs'],\n                    CONFIG['model']['learning_rate'],\n                    CONFIG['model']['patience']\n                )\n                model.load_state_dict(best_state)\n                \n                metrics = calculate_comprehensive_metrics(model, val_loader, device)\n                fold_results.append(metrics)\n                \n                if model_name == 'g_carm':\n                    adjacency = model.get_final_adjacency()\n                    fold_adjacencies.append(adjacency)\n                \n                fold_models.append(deepcopy(model).cpu())\n                \n                del model\n                torch.cuda.empty_cache()\n                gc.collect()\n        \n        avg_metrics = {}\n        for key in ['accuracy', 'precision', 'recall', 'f1_score', 'auc_roc', 'specificity']:\n            values = [f[key] for f in fold_results]\n            avg_metrics[f'avg_{key}'] = float(np.mean(values))\n            avg_metrics[f'std_{key}'] = float(np.std(values))\n        \n        result = {\n            'subject': subject_id,\n            'num_trials': X.shape[0],\n            'num_channels': C,\n            **avg_metrics,\n            'channel_names': channel_names,\n            'fold_models': fold_models\n        }\n        \n        if model_name == 'fbcsp':\n            result['csp_importance'] = np.mean(np.stack(fold_csp_patterns, 0), 0)\n        elif model_name == 'g_carm':\n            result['adjacency_matrix'] = np.mean(np.stack(fold_adjacencies, 0), 0)\n        \n        all_results[model_name].append(result)\n        \n        print(f\"    Accuracy: {avg_metrics['avg_accuracy']:.4f} ± {avg_metrics['std_accuracy']:.4f}\")\n        print(f\"    F1-Score: {avg_metrics['avg_f1_score']:.4f} ± {avg_metrics['std_f1_score']:.4f}\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"Training Complete!\")\nprint(\"=\"*80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T05:11:05.969697Z","iopub.execute_input":"2025-11-27T05:11:05.969963Z","iopub.status.idle":"2025-11-27T05:28:30.777620Z","shell.execute_reply.started":"2025-11-27T05:11:05.969942Z","shell.execute_reply":"2025-11-27T05:28:30.776857Z"}},"outputs":[{"name":"stdout","text":"\nStarting training for legacy methods...\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training subjects:   0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"898c2f1002d148f9b9de5d964bd91edc"}},"metadata":{}},{"name":"stdout","text":"\nProcessing S001...\n  Data shape: (252, 64, 769)\n  Label distribution: [168  84]\n\n  Training FBCSP...\n    Accuracy: 0.6151 ± 0.0148\n    F1-Score: 0.3294 ± 0.0486\n\n  Training CNN_SAE...\n    Accuracy: 0.9286 ± 0.0097\n    F1-Score: 0.8886 ± 0.0168\n\n  Training EEGNET...\n    Accuracy: 0.9365 ± 0.0056\n    F1-Score: 0.9025 ± 0.0070\n\n  Training ACS_SE_CNN...\n    Accuracy: 0.8651 ± 0.0312\n    F1-Score: 0.7859 ± 0.0652\n\n  Training G_CARM...\n    Accuracy: 0.8095 ± 0.0168\n    F1-Score: 0.6883 ± 0.0473\n\nProcessing S002...\n  Data shape: (252, 64, 769)\n  Label distribution: [168  84]\n\n  Training FBCSP...\n    Accuracy: 0.6786 ± 0.0097\n    F1-Score: 0.4209 ± 0.0454\n\n  Training CNN_SAE...\n    Accuracy: 0.7103 ± 0.0297\n    F1-Score: 0.3236 ± 0.1868\n\n  Training EEGNET...\n    Accuracy: 0.7897 ± 0.0245\n    F1-Score: 0.6519 ± 0.0489\n\n  Training ACS_SE_CNN...\n    Accuracy: 0.6984 ± 0.0112\n    F1-Score: 0.4566 ± 0.1556\n\n  Training G_CARM...\n    Accuracy: 0.6746 ± 0.0245\n    F1-Score: 0.2197 ± 0.1150\n\nProcessing S005...\n  Data shape: (252, 64, 769)\n  Label distribution: [168  84]\n\n  Training FBCSP...\n    Accuracy: 0.6190 ± 0.0097\n    F1-Score: 0.2938 ± 0.0107\n\n  Training CNN_SAE...\n    Accuracy: 0.8333 ± 0.0424\n    F1-Score: 0.7339 ± 0.0683\n\n  Training EEGNET...\n    Accuracy: 0.9167 ± 0.0350\n    F1-Score: 0.8731 ± 0.0553\n\n  Training ACS_SE_CNN...\n    Accuracy: 0.7976 ± 0.0257\n    F1-Score: 0.6836 ± 0.0366\n\n  Training G_CARM...\n    Accuracy: 0.7857 ± 0.0257\n    F1-Score: 0.6633 ± 0.0842\n\nProcessing S006...\n  Data shape: (252, 64, 769)\n  Label distribution: [168  84]\n\n  Training FBCSP...\n    Accuracy: 0.5159 ± 0.0341\n    F1-Score: 0.1787 ± 0.1058\n\n  Training CNN_SAE...\n    Accuracy: 0.8016 ± 0.0405\n    F1-Score: 0.6746 ± 0.0775\n\n  Training EEGNET...\n    Accuracy: 0.9246 ± 0.0312\n    F1-Score: 0.8865 ± 0.0464\n\n  Training ACS_SE_CNN...\n    Accuracy: 0.7381 ± 0.0194\n    F1-Score: 0.4408 ± 0.0883\n\n  Training G_CARM...\n    Accuracy: 0.7302 ± 0.0056\n    F1-Score: 0.4646 ± 0.1143\n\nProcessing S007...\n  Data shape: (252, 64, 769)\n  Label distribution: [168  84]\n\n  Training FBCSP...\n    Accuracy: 0.7262 ± 0.0292\n    F1-Score: 0.5114 ± 0.0382\n\n  Training CNN_SAE...\n    Accuracy: 0.9008 ± 0.0368\n    F1-Score: 0.8429 ± 0.0587\n\n  Training EEGNET...\n    Accuracy: 0.9603 ± 0.0202\n    F1-Score: 0.9387 ± 0.0319\n\n  Training ACS_SE_CNN...\n    Accuracy: 0.8968 ± 0.0312\n    F1-Score: 0.8380 ± 0.0486\n\n  Training G_CARM...\n    Accuracy: 0.8571 ± 0.0541\n    F1-Score: 0.8000 ± 0.0542\n\nProcessing S008...\n  Data shape: (252, 64, 769)\n  Label distribution: [168  84]\n\n  Training FBCSP...\n    Accuracy: 0.6071 ± 0.0424\n    F1-Score: 0.3159 ± 0.0814\n\n  Training CNN_SAE...\n    Accuracy: 0.9087 ± 0.0438\n    F1-Score: 0.8493 ± 0.0728\n\n  Training EEGNET...\n    Accuracy: 0.9762 ± 0.0000\n    F1-Score: 0.9643 ± 0.0010\n\n  Training ACS_SE_CNN...\n    Accuracy: 0.8929 ± 0.0350\n    F1-Score: 0.8423 ± 0.0513\n\n  Training G_CARM...\n    Accuracy: 0.8730 ± 0.0368\n    F1-Score: 0.8009 ± 0.0533\n\nProcessing S011...\n  Data shape: (252, 64, 769)\n  Label distribution: [168  84]\n\n  Training FBCSP...\n    Accuracy: 0.5079 ± 0.0202\n    F1-Score: 0.1599 ± 0.0520\n\n  Training CNN_SAE...\n    Accuracy: 0.8889 ± 0.0202\n    F1-Score: 0.8188 ± 0.0400\n\n  Training EEGNET...\n    Accuracy: 0.9127 ± 0.0224\n    F1-Score: 0.8703 ± 0.0346\n\n  Training ACS_SE_CNN...\n    Accuracy: 0.8333 ± 0.0257\n    F1-Score: 0.7219 ± 0.0501\n\n  Training G_CARM...\n    Accuracy: 0.7103 ± 0.0245\n    F1-Score: 0.4622 ± 0.0769\n\nProcessing S014...\n  Data shape: (252, 64, 769)\n  Label distribution: [168  84]\n\n  Training FBCSP...\n    Accuracy: 0.6746 ± 0.0662\n    F1-Score: 0.3775 ± 0.1682\n\n  Training CNN_SAE...\n    Accuracy: 0.8532 ± 0.0112\n    F1-Score: 0.7574 ± 0.0251\n\n  Training EEGNET...\n    Accuracy: 0.8770 ± 0.0312\n    F1-Score: 0.7884 ± 0.0663\n\n  Training ACS_SE_CNN...\n    Accuracy: 0.8254 ± 0.0405\n    F1-Score: 0.7246 ± 0.0646\n\n  Training G_CARM...\n    Accuracy: 0.7183 ± 0.0148\n    F1-Score: 0.4027 ± 0.1540\n\nProcessing S015...\n  Data shape: (252, 64, 769)\n  Label distribution: [168  84]\n\n  Training FBCSP...\n    Accuracy: 0.7460 ± 0.0459\n    F1-Score: 0.5773 ± 0.1106\n\n  Training CNN_SAE...\n    Accuracy: 0.8452 ± 0.0257\n    F1-Score: 0.7292 ± 0.0510\n\n  Training EEGNET...\n    Accuracy: 0.9127 ± 0.0341\n    F1-Score: 0.8676 ± 0.0475\n\n  Training ACS_SE_CNN...\n    Accuracy: 0.8492 ± 0.0148\n    F1-Score: 0.7612 ± 0.0327\n\n  Training G_CARM...\n    Accuracy: 0.8571 ± 0.0292\n    F1-Score: 0.7779 ± 0.0559\n\nProcessing S016...\n  Data shape: (252, 64, 769)\n  Label distribution: [168  84]\n\n  Training FBCSP...\n    Accuracy: 0.6270 ± 0.0148\n    F1-Score: 0.4035 ± 0.0688\n\n  Training CNN_SAE...\n    Accuracy: 0.7540 ± 0.0148\n    F1-Score: 0.5553 ± 0.0635\n\n  Training EEGNET...\n    Accuracy: 0.8175 ± 0.0341\n    F1-Score: 0.7052 ± 0.0642\n\n  Training ACS_SE_CNN...\n    Accuracy: 0.7460 ± 0.0148\n    F1-Score: 0.4890 ± 0.1129\n\n  Training G_CARM...\n    Accuracy: 0.7817 ± 0.0312\n    F1-Score: 0.6138 ± 0.0733\n\n================================================================================\nTraining Complete!\n================================================================================\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"## 10. Channel Selection Functions","metadata":{}},{"cell_type":"code","source":"def compute_gradient_attribution(model, X, Y, device, use_true_target=True, max_samples=None):\n    \"\"\"\n    Compute per-channel gradient-based attribution scores averaged over samples.\n    Assumes X shape (n_samples, n_channels, n_time).\n    Tries both input shapes (batch, C, T) and (batch, 1, C, T) automatically.\n    \"\"\"\n    model.eval()\n    model.to(device)\n\n    X = np.asarray(X)\n    Y = np.asarray(Y)\n    n_total = X.shape[0]\n    n_used = n_total if max_samples is None else min(n_total, int(max_samples))\n\n    grad_scores = []\n\n    for i in range(n_used):\n        x_np = X[i:i+1]         # shape (1, C, T)\n        target = int(Y[i]) if (use_true_target and i < len(Y)) else int(Y[0])\n\n        # Prepare tensor candidate 1: (1, C, T)\n        x1 = torch.from_numpy(x_np).float().to(device)    # (1, C, T)\n        # Candidate 2: (1, 1, C, T)\n        x2 = x1.unsqueeze(1)\n\n        # We'll try forward with candidate shapes to detect which one works\n        chosen = None\n        out = None\n        for x_tensor in (x1, x2):\n            x_tensor = x_tensor.clone().detach().requires_grad_(True)\n            try:\n                with torch.enable_grad():\n                    model.zero_grad()\n                    out_try = model(x_tensor)\n                # if forward succeeded, choose this tensor\n                chosen = x_tensor\n                out = out_try\n                break\n            except Exception:\n                # not the right input shape / model choked; try next\n                try:\n                    # cleanup before trying next\n                    if x_tensor.grad is not None:\n                        x_tensor.grad.zero_()\n                    del x_tensor, out_try\n                    torch.cuda.empty_cache()\n                except Exception:\n                    pass\n                continue\n\n        if chosen is None or out is None:\n            # If both shapes fail, skip this sample but warn minimally\n            # (avoid printing in batch runs; user can debug by running a single forward)\n            continue\n\n        # Now compute gradient w.r.t chosen input and target\n        with torch.enable_grad():\n            try:\n                # pick logit for target class\n                if out.dim() == 0:\n                    score = out\n                elif out.dim() == 1:\n                    # shape (n_classes,) or (batch,)\n                    score = out.squeeze()\n                else:\n                    score = out[0, target]\n                # zero grads and backward\n                model.zero_grad()\n                if chosen.grad is not None:\n                    chosen.grad.zero_()\n                score.backward(retain_graph=False)\n            except Exception:\n                # If backward failed, skip this sample\n                try:\n                    del chosen, out, score\n                    torch.cuda.empty_cache()\n                except Exception:\n                    pass\n                continue\n\n            # extract gradient and reduce to per-channel importance\n            g = chosen.grad.detach().cpu()   # e.g., (1,C,T) or (1,1,C,T)\n            g = g.squeeze(0)\n            # If shape is (1,C,T) -> squeeze extra dim\n            if g.dim() == 3 and g.shape[0] == 1:\n                g = g.squeeze(0)   # (C,T)\n            # If shape now is (C,T) or (C,T,...) pick last dim as time\n            if g.dim() >= 2:\n                channel_imp = g.abs().mean(dim=-1).cpu().numpy().flatten()\n            else:\n                # fallback: mean over remaining dims\n                channel_imp = g.abs().mean().cpu().numpy().flatten()\n\n            grad_scores.append(channel_imp)\n\n        # cleanup per-sample tensors\n        try:\n            del chosen, out, score, g, x1, x2\n            torch.cuda.empty_cache()\n        except Exception:\n            pass\n\n    if len(grad_scores) == 0:\n        return np.zeros(X.shape[1], dtype=float)\n\n    return np.mean(np.stack(grad_scores, axis=0), axis=0)\n\n\nclass ChannelSelectorLegacy:\n    def __init__(self, channel_names, model_name, **kwargs):\n        self.names = np.array(channel_names)\n        self.C = len(channel_names)\n        self.model_name = model_name\n        \n        if model_name == 'fbcsp':\n            self.csp_importance = kwargs.get('csp_importance')\n        elif model_name == 'g_carm':\n            self.adjacency = kwargs.get('adjacency')\n        else:\n            self.gradient_scores = kwargs.get('gradient_scores')\n    \n    def select_channels(self, k, method='default'):\n        if self.model_name == 'fbcsp':\n            importance = self.csp_importance\n        elif self.model_name == 'g_carm':\n            if method == 'ES':\n                importance = self._edge_selection_scores()\n            else:\n                importance = self._aggregation_selection_scores()\n        else:\n            importance = self.gradient_scores\n\n        if importance is None:\n            # fallback: select first-k channels if no importance provided\n            indices = np.arange(min(int(k), self.C))\n            return self.names[indices].tolist(), indices\n        \n        indices = np.sort(np.argsort(importance)[-int(k):])\n        return self.names[indices].tolist(), indices\n    \n    def _edge_selection_scores(self):\n        # compute delta_ij = |f_ij| + |f_ji| for i<j, then accumulate to nodes (no double counting)\n        edge_importance = np.zeros(self.C)\n        if self.adjacency is None:\n            return edge_importance\n        for i in range(self.C):\n            for j in range(i+1, self.C):\n                val = abs(self.adjacency[i, j]) + abs(self.adjacency[j, i])\n                edge_importance[i] += val\n                edge_importance[j] += val\n        return edge_importance\n    \n    def _aggregation_selection_scores(self):\n        if self.adjacency is None:\n            return np.zeros(self.C)\n        return np.sum(np.abs(self.adjacency), axis=1)\n\n\ndef retrain_legacy_model(X, Y, selected_indices, model_name, config, device):\n    X_selected = X[:, selected_indices, :]\n    C, T = X_selected.shape[1], X_selected.shape[2]\n    K = len(set(config['data']['selected_classes']))\n    \n    skf = StratifiedKFold(n_splits=config['model']['n_folds'], shuffle=True, random_state=42)\n    fold_results = []\n    \n    for fold, (train_idx, val_idx) in enumerate(skf.split(X_selected, Y)):\n        X_train, X_val = X_selected[train_idx], X_selected[val_idx]\n        Y_train, Y_val = Y[train_idx], Y[val_idx]\n        \n        if model_name == 'fbcsp':\n            model = FBCSP(\n                freq_bands=config['fbcsp']['freq_bands'],\n                n_components=config['fbcsp']['n_components'],\n                sfreq=config['preprocessing']['target_sfreq']\n            )\n            model.fit(X_train, Y_train)\n            y_pred = model.predict(X_val)\n            metrics = calculate_sklearn_metrics(Y_val, y_pred)\n            fold_results.append(metrics)\n        else:\n            X_train_norm = normalize(X_train)\n            X_val_norm = normalize(X_val)\n            \n            train_loader = DataLoader(\n                EEGDataset(X_train_norm, Y_train),\n                batch_size=config['model']['batch_size'],\n                shuffle=True,\n                num_workers=0\n            )\n            val_loader = DataLoader(\n                EEGDataset(X_val_norm, Y_val),\n                batch_size=config['model']['batch_size'],\n                shuffle=False,\n                num_workers=0\n            )\n            \n            if model_name == 'cnn_sae':\n                model = CNNSAE(n_channels=C, n_classes=K, n_timepoints=T).to(device)\n            elif model_name == 'eegnet':\n                model = EEGNet(n_channels=C, n_classes=K, n_timepoints=T).to(device)\n            elif model_name == 'acs_se_cnn':\n                model = ACSECNN(n_channels=C, n_classes=K, n_timepoints=T).to(device)\n            elif model_name == 'g_carm':\n                model = GCARM(n_channels=C, n_classes=K, n_timepoints=T).to(device)\n            else:\n                # fallback generic model if none matched (attempt to use any available class name)\n                try:\n                    model = globals().get(model_name)\n                    if isinstance(model, type):\n                        model = model(n_channels=C, n_classes=K, n_timepoints=T).to(device)\n                    else:\n                        # create a trivial conv if model_name isn't found\n                        model = CNNSAE(n_channels=C, n_classes=K, n_timepoints=T).to(device)\n                except Exception:\n                    model = CNNSAE(n_channels=C, n_classes=K, n_timepoints=T).to(device)\n            \n            # Robust call to train_model: try with verbose kw, else without\n            try:\n                result = train_model(\n                    model, train_loader, val_loader, device,\n                    config['model']['epochs'],\n                    config['model']['learning_rate'],\n                    config['model']['patience'],\n                    verbose=False\n                )\n            except TypeError:\n                result = train_model(\n                    model, train_loader, val_loader, device,\n                    config['model']['epochs'],\n                    config['model']['learning_rate'],\n                    config['model']['patience']\n                )\n\n            # Normalize possible return formats to best_state and best_acc\n            best_state = None\n            best_acc = None\n            if isinstance(result, (tuple, list)):\n                if len(result) >= 2:\n                    best_state, best_acc = result[0], result[1]\n                elif len(result) == 1:\n                    candidate = result[0]\n                    if hasattr(candidate, 'state_dict'):\n                        best_state = candidate.state_dict()\n            elif isinstance(result, dict):\n                best_state = result.get('best_state') or result.get('state_dict') or result.get('model_state') or result.get('state')\n                best_acc = result.get('best_acc') or result.get('best_val_acc') or result.get('val_acc') or result.get('accuracy')\n            elif hasattr(result, 'state_dict'):\n                best_state = result.state_dict()\n\n            if best_state is None:\n                # try to see if result[0] is an nn.Module in tuple/list\n                if isinstance(result, (tuple, list)) and len(result) > 0 and hasattr(result[0], 'state_dict'):\n                    best_state = result[0].state_dict()\n\n            if best_state is None:\n                raise RuntimeError(\"Could not extract model state (best_state) from train_model result. Ensure train_model returns (state_dict, best_acc) or a dict containing 'best_state'/'state_dict'.\")\n\n            # load the best state into the model\n            try:\n                model.load_state_dict(best_state)\n            except Exception:\n                # if best_state is a model instance, try to use it directly\n                if hasattr(best_state, 'state_dict'):\n                    model.load_state_dict(best_state.state_dict())\n                else:\n                    raise\n\n            # If best_acc not provided, compute it from validation loader\n            if best_acc is None:\n                try:\n                    metrics_after = calculate_comprehensive_metrics(model, val_loader, device)\n                    # try common metric keys\n                    best_acc = metrics_after.get('accuracy') or metrics_after.get('avg_accuracy') or metrics_after.get('acc') or None\n                    if isinstance(best_acc, (np.ndarray, list)):\n                        best_acc = float(np.mean(best_acc))\n                    if hasattr(best_acc, 'item'):\n                        best_acc = float(best_acc.item())\n                except Exception:\n                    best_acc = None\n\n            metrics = calculate_comprehensive_metrics(model, val_loader, device)\n            fold_results.append(metrics)\n            \n            # cleanup\n            try:\n                del model\n                torch.cuda.empty_cache()\n                gc.collect()\n            except Exception:\n                pass\n    \n    avg_metrics = {}\n    for key in ['accuracy', 'precision', 'recall', 'f1_score', 'auc_roc', 'specificity']:\n        values = [f[key] for f in fold_results]\n        avg_metrics[f'avg_{key}'] = float(np.mean(values))\n        avg_metrics[f'std_{key}'] = float(np.std(values))\n    \n    return avg_metrics\n\n\nprint(\"Channel selection functions defined!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T05:42:34.789783Z","iopub.execute_input":"2025-11-27T05:42:34.790617Z","iopub.status.idle":"2025-11-27T05:42:34.818535Z","shell.execute_reply.started":"2025-11-27T05:42:34.790592Z","shell.execute_reply":"2025-11-27T05:42:34.817802Z"}},"outputs":[{"name":"stdout","text":"Channel selection functions defined!\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"retrain_results = {\n    'fbcsp': [],\n    'cnn_sae': [],\n    'eegnet': [],\n    'acs_se_cnn': [],\n    'g_carm': []\n}\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"CHANNEL SELECTION AND RETRAINING\")\nprint(\"=\"*80 + \"\\n\")\n\nfor subject_id in tqdm(subjects, desc='Retraining with channel selection'):\n    print(f\"\\nProcessing {subject_id}...\")\n    \n    X, Y, channel_names = load_subject_data(\n        data_dir,\n        subject_id,\n        ALL_TASK_RUNS,\n        CONFIG\n    )\n    \n    if X is None:\n        continue\n    \n    for model_name in ['fbcsp', 'cnn_sae', 'eegnet', 'acs_se_cnn', 'g_carm']:\n        subj_result = None\n        for res in all_results[model_name]:\n            if res['subject'] == subject_id:\n                subj_result = res\n                break\n        \n        if subj_result is None:\n            continue\n        \n        print(f\"\\n  {model_name.upper()} channel selection...\")\n        \n        if model_name == 'fbcsp':\n            selector = ChannelSelectorLegacy(\n                channel_names, model_name,\n                csp_importance=subj_result['csp_importance']\n            )\n            methods = ['default']\n            \n        elif model_name == 'g_carm':\n            selector = ChannelSelectorLegacy(\n                channel_names, model_name,\n                adjacency=subj_result['adjacency_matrix']\n            )\n            methods = ['ES', 'AS']\n            \n        else:\n            X_norm = normalize(X)\n            fold_models = subj_result['fold_models']\n            \n            gradient_scores_list = []\n            for model in fold_models:\n                model_gpu = model.to(device)\n                grad_scores = compute_gradient_attribution(model_gpu, X_norm[:10], Y[:10], device)\n                gradient_scores_list.append(grad_scores)\n                del model_gpu\n                torch.cuda.empty_cache()\n            \n            avg_gradient_scores = np.mean(np.stack(gradient_scores_list, 0), 0)\n            \n            selector = ChannelSelectorLegacy(\n                channel_names, model_name,\n                gradient_scores=avg_gradient_scores\n            )\n            methods = ['default']\n        \n        for method in methods:\n            for k in CONFIG['channel_selection']['k_values']:\n                method_label = f'{model_name.upper()}-{method}' if method != 'default' else model_name.upper()\n                \n                selected_channels, selected_indices = selector.select_channels(k, method)\n                \n                retrain_metrics = retrain_legacy_model(\n                    X, Y, selected_indices, model_name, CONFIG, device\n                )\n                \n                acc_drop = subj_result['avg_accuracy'] - retrain_metrics['avg_accuracy']\n                \n                retrain_results[model_name].append({\n                    'subject': subject_id,\n                    'method': method if method != 'default' else model_name.upper(),\n                    'k': k,\n                    'num_channels_selected': len(selected_channels),\n                    **retrain_metrics,\n                    'full_channels_acc': subj_result['avg_accuracy'],\n                    'accuracy_drop': acc_drop,\n                    'accuracy_drop_pct': (acc_drop / subj_result['avg_accuracy'] * 100) if subj_result['avg_accuracy'] > 0 else 0.0\n                })\n                \n                print(f\"    {method_label}, k={k}: {retrain_metrics['avg_accuracy']:.4f} (drop: {acc_drop:.4f})\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"Channel Selection Complete!\")\nprint(\"=\"*80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T05:42:36.883474Z","iopub.execute_input":"2025-11-27T05:42:36.883818Z","iopub.status.idle":"2025-11-27T06:24:00.949400Z","shell.execute_reply.started":"2025-11-27T05:42:36.883797Z","shell.execute_reply":"2025-11-27T06:24:00.948543Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nCHANNEL SELECTION AND RETRAINING\n================================================================================\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Retraining with channel selection:   0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45fbcddaa190416b93f9b56491e67a43"}},"metadata":{}},{"name":"stdout","text":"\nProcessing S001...\n\n  FBCSP channel selection...\n    FBCSP, k=10: 0.6667 (drop: -0.0516)\n    FBCSP, k=15: 0.6667 (drop: -0.0516)\n    FBCSP, k=20: 0.6746 (drop: -0.0595)\n    FBCSP, k=25: 0.6389 (drop: -0.0238)\n    FBCSP, k=30: 0.6746 (drop: -0.0595)\n\n  CNN_SAE channel selection...\n    CNN_SAE, k=10: 0.9405 (drop: -0.0119)\n    CNN_SAE, k=15: 0.9286 (drop: 0.0000)\n    CNN_SAE, k=20: 0.9206 (drop: 0.0079)\n    CNN_SAE, k=25: 0.9365 (drop: -0.0079)\n    CNN_SAE, k=30: 0.9008 (drop: 0.0278)\n\n  EEGNET channel selection...\n    EEGNET, k=10: 0.9484 (drop: -0.0119)\n    EEGNET, k=15: 0.9603 (drop: -0.0238)\n    EEGNET, k=20: 0.9365 (drop: 0.0000)\n    EEGNET, k=25: 0.9444 (drop: -0.0079)\n    EEGNET, k=30: 0.9524 (drop: -0.0159)\n\n  ACS_SE_CNN channel selection...\n    ACS_SE_CNN, k=10: 0.9167 (drop: -0.0516)\n    ACS_SE_CNN, k=15: 0.9008 (drop: -0.0357)\n    ACS_SE_CNN, k=20: 0.9008 (drop: -0.0357)\n    ACS_SE_CNN, k=25: 0.9087 (drop: -0.0437)\n    ACS_SE_CNN, k=30: 0.8929 (drop: -0.0278)\n\n  G_CARM channel selection...\n    G_CARM-ES, k=10: 0.7857 (drop: 0.0238)\n    G_CARM-ES, k=15: 0.7024 (drop: 0.1071)\n    G_CARM-ES, k=20: 0.7421 (drop: 0.0675)\n    G_CARM-ES, k=25: 0.7183 (drop: 0.0913)\n    G_CARM-ES, k=30: 0.7262 (drop: 0.0833)\n    G_CARM-AS, k=10: 0.8810 (drop: -0.0714)\n    G_CARM-AS, k=15: 0.8571 (drop: -0.0476)\n    G_CARM-AS, k=20: 0.8929 (drop: -0.0833)\n    G_CARM-AS, k=25: 0.8651 (drop: -0.0556)\n    G_CARM-AS, k=30: 0.8492 (drop: -0.0397)\n\nProcessing S002...\n\n  FBCSP channel selection...\n    FBCSP, k=10: 0.6310 (drop: 0.0476)\n    FBCSP, k=15: 0.6706 (drop: 0.0079)\n    FBCSP, k=20: 0.6825 (drop: -0.0040)\n    FBCSP, k=25: 0.6429 (drop: 0.0357)\n    FBCSP, k=30: 0.6786 (drop: 0.0000)\n\n  CNN_SAE channel selection...\n    CNN_SAE, k=10: 0.7341 (drop: -0.0238)\n    CNN_SAE, k=15: 0.7341 (drop: -0.0238)\n    CNN_SAE, k=20: 0.7579 (drop: -0.0476)\n    CNN_SAE, k=25: 0.7381 (drop: -0.0278)\n    CNN_SAE, k=30: 0.7222 (drop: -0.0119)\n\n  EEGNET channel selection...\n    EEGNET, k=10: 0.8333 (drop: -0.0437)\n    EEGNET, k=15: 0.8373 (drop: -0.0476)\n    EEGNET, k=20: 0.8254 (drop: -0.0357)\n    EEGNET, k=25: 0.7778 (drop: 0.0119)\n    EEGNET, k=30: 0.8175 (drop: -0.0278)\n\n  ACS_SE_CNN channel selection...\n    ACS_SE_CNN, k=10: 0.6944 (drop: 0.0040)\n    ACS_SE_CNN, k=15: 0.6905 (drop: 0.0079)\n    ACS_SE_CNN, k=20: 0.6865 (drop: 0.0119)\n    ACS_SE_CNN, k=25: 0.7103 (drop: -0.0119)\n    ACS_SE_CNN, k=30: 0.7143 (drop: -0.0159)\n\n  G_CARM channel selection...\n    G_CARM-ES, k=10: 0.6905 (drop: -0.0159)\n    G_CARM-ES, k=15: 0.6825 (drop: -0.0079)\n    G_CARM-ES, k=20: 0.6825 (drop: -0.0079)\n    G_CARM-ES, k=25: 0.6706 (drop: 0.0040)\n    G_CARM-ES, k=30: 0.6706 (drop: 0.0040)\n    G_CARM-AS, k=10: 0.6825 (drop: -0.0079)\n    G_CARM-AS, k=15: 0.6746 (drop: 0.0000)\n    G_CARM-AS, k=20: 0.6786 (drop: -0.0040)\n    G_CARM-AS, k=25: 0.6905 (drop: -0.0159)\n    G_CARM-AS, k=30: 0.6944 (drop: -0.0198)\n\nProcessing S005...\n\n  FBCSP channel selection...\n    FBCSP, k=10: 0.6667 (drop: -0.0476)\n    FBCSP, k=15: 0.6667 (drop: -0.0476)\n    FBCSP, k=20: 0.6190 (drop: 0.0000)\n    FBCSP, k=25: 0.6111 (drop: 0.0079)\n    FBCSP, k=30: 0.5913 (drop: 0.0278)\n\n  CNN_SAE channel selection...\n    CNN_SAE, k=10: 0.8413 (drop: -0.0079)\n    CNN_SAE, k=15: 0.8175 (drop: 0.0159)\n    CNN_SAE, k=20: 0.8770 (drop: -0.0437)\n    CNN_SAE, k=25: 0.8690 (drop: -0.0357)\n    CNN_SAE, k=30: 0.8730 (drop: -0.0397)\n\n  EEGNET channel selection...\n    EEGNET, k=10: 0.9325 (drop: -0.0159)\n    EEGNET, k=15: 0.9325 (drop: -0.0159)\n    EEGNET, k=20: 0.9365 (drop: -0.0198)\n    EEGNET, k=25: 0.9127 (drop: 0.0040)\n    EEGNET, k=30: 0.9246 (drop: -0.0079)\n\n  ACS_SE_CNN channel selection...\n    ACS_SE_CNN, k=10: 0.8095 (drop: -0.0119)\n    ACS_SE_CNN, k=15: 0.8254 (drop: -0.0278)\n    ACS_SE_CNN, k=20: 0.8016 (drop: -0.0040)\n    ACS_SE_CNN, k=25: 0.7937 (drop: 0.0040)\n    ACS_SE_CNN, k=30: 0.8056 (drop: -0.0079)\n\n  G_CARM channel selection...\n    G_CARM-ES, k=10: 0.7381 (drop: 0.0476)\n    G_CARM-ES, k=15: 0.7222 (drop: 0.0635)\n    G_CARM-ES, k=20: 0.7341 (drop: 0.0516)\n    G_CARM-ES, k=25: 0.7738 (drop: 0.0119)\n    G_CARM-ES, k=30: 0.7738 (drop: 0.0119)\n    G_CARM-AS, k=10: 0.7817 (drop: 0.0040)\n    G_CARM-AS, k=15: 0.7937 (drop: -0.0079)\n    G_CARM-AS, k=20: 0.7421 (drop: 0.0437)\n    G_CARM-AS, k=25: 0.7698 (drop: 0.0159)\n    G_CARM-AS, k=30: 0.7897 (drop: -0.0040)\n\nProcessing S006...\n\n  FBCSP channel selection...\n    FBCSP, k=10: 0.5913 (drop: -0.0754)\n    FBCSP, k=15: 0.5675 (drop: -0.0516)\n    FBCSP, k=20: 0.5833 (drop: -0.0675)\n    FBCSP, k=25: 0.5952 (drop: -0.0794)\n    FBCSP, k=30: 0.5556 (drop: -0.0397)\n\n  CNN_SAE channel selection...\n    CNN_SAE, k=10: 0.8571 (drop: -0.0556)\n    CNN_SAE, k=15: 0.8889 (drop: -0.0873)\n    CNN_SAE, k=20: 0.8214 (drop: -0.0198)\n    CNN_SAE, k=25: 0.8095 (drop: -0.0079)\n    CNN_SAE, k=30: 0.7817 (drop: 0.0198)\n\n  EEGNET channel selection...\n    EEGNET, k=10: 0.9286 (drop: -0.0040)\n    EEGNET, k=15: 0.9286 (drop: -0.0040)\n    EEGNET, k=20: 0.9444 (drop: -0.0198)\n    EEGNET, k=25: 0.9484 (drop: -0.0238)\n    EEGNET, k=30: 0.9563 (drop: -0.0317)\n\n  ACS_SE_CNN channel selection...\n    ACS_SE_CNN, k=10: 0.7341 (drop: 0.0040)\n    ACS_SE_CNN, k=15: 0.7421 (drop: -0.0040)\n    ACS_SE_CNN, k=20: 0.7579 (drop: -0.0198)\n    ACS_SE_CNN, k=25: 0.7817 (drop: -0.0437)\n    ACS_SE_CNN, k=30: 0.7540 (drop: -0.0159)\n\n  G_CARM channel selection...\n    G_CARM-ES, k=10: 0.7341 (drop: -0.0040)\n    G_CARM-ES, k=15: 0.7143 (drop: 0.0159)\n    G_CARM-ES, k=20: 0.7222 (drop: 0.0079)\n    G_CARM-ES, k=25: 0.6786 (drop: 0.0516)\n    G_CARM-ES, k=30: 0.7024 (drop: 0.0278)\n    G_CARM-AS, k=10: 0.7222 (drop: 0.0079)\n    G_CARM-AS, k=15: 0.7262 (drop: 0.0040)\n    G_CARM-AS, k=20: 0.7460 (drop: -0.0159)\n    G_CARM-AS, k=25: 0.7341 (drop: -0.0040)\n    G_CARM-AS, k=30: 0.7460 (drop: -0.0159)\n\nProcessing S007...\n\n  FBCSP channel selection...\n    FBCSP, k=10: 0.7222 (drop: 0.0040)\n    FBCSP, k=15: 0.7698 (drop: -0.0437)\n    FBCSP, k=20: 0.7460 (drop: -0.0198)\n    FBCSP, k=25: 0.7183 (drop: 0.0079)\n    FBCSP, k=30: 0.7460 (drop: -0.0198)\n\n  CNN_SAE channel selection...\n    CNN_SAE, k=10: 0.9365 (drop: -0.0357)\n    CNN_SAE, k=15: 0.9206 (drop: -0.0198)\n    CNN_SAE, k=20: 0.9286 (drop: -0.0278)\n    CNN_SAE, k=25: 0.9365 (drop: -0.0357)\n    CNN_SAE, k=30: 0.9167 (drop: -0.0159)\n\n  EEGNET channel selection...\n    EEGNET, k=10: 0.9643 (drop: -0.0040)\n    EEGNET, k=15: 0.9643 (drop: -0.0040)\n    EEGNET, k=20: 0.9643 (drop: -0.0040)\n    EEGNET, k=25: 0.9802 (drop: -0.0198)\n    EEGNET, k=30: 0.9841 (drop: -0.0238)\n\n  ACS_SE_CNN channel selection...\n    ACS_SE_CNN, k=10: 0.8849 (drop: 0.0119)\n    ACS_SE_CNN, k=15: 0.8968 (drop: 0.0000)\n    ACS_SE_CNN, k=20: 0.8373 (drop: 0.0595)\n    ACS_SE_CNN, k=25: 0.8810 (drop: 0.0159)\n    ACS_SE_CNN, k=30: 0.8810 (drop: 0.0159)\n\n  G_CARM channel selection...\n    G_CARM-ES, k=10: 0.8849 (drop: -0.0278)\n    G_CARM-ES, k=15: 0.8770 (drop: -0.0198)\n    G_CARM-ES, k=20: 0.8571 (drop: 0.0000)\n    G_CARM-ES, k=25: 0.8452 (drop: 0.0119)\n    G_CARM-ES, k=30: 0.8571 (drop: 0.0000)\n    G_CARM-AS, k=10: 0.9048 (drop: -0.0476)\n    G_CARM-AS, k=15: 0.8690 (drop: -0.0119)\n    G_CARM-AS, k=20: 0.8690 (drop: -0.0119)\n    G_CARM-AS, k=25: 0.8770 (drop: -0.0198)\n    G_CARM-AS, k=30: 0.8651 (drop: -0.0079)\n\nProcessing S008...\n\n  FBCSP channel selection...\n    FBCSP, k=10: 0.6310 (drop: -0.0238)\n    FBCSP, k=15: 0.6627 (drop: -0.0556)\n    FBCSP, k=20: 0.7063 (drop: -0.0992)\n    FBCSP, k=25: 0.6984 (drop: -0.0913)\n    FBCSP, k=30: 0.6706 (drop: -0.0635)\n\n  CNN_SAE channel selection...\n    CNN_SAE, k=10: 0.9603 (drop: -0.0516)\n    CNN_SAE, k=15: 0.9246 (drop: -0.0159)\n    CNN_SAE, k=20: 0.9286 (drop: -0.0198)\n    CNN_SAE, k=25: 0.9365 (drop: -0.0278)\n    CNN_SAE, k=30: 0.9246 (drop: -0.0159)\n\n  EEGNET channel selection...\n    EEGNET, k=10: 0.9683 (drop: 0.0079)\n    EEGNET, k=15: 0.9881 (drop: -0.0119)\n    EEGNET, k=20: 0.9683 (drop: 0.0079)\n    EEGNET, k=25: 0.9643 (drop: 0.0119)\n    EEGNET, k=30: 0.9802 (drop: -0.0040)\n\n  ACS_SE_CNN channel selection...\n    ACS_SE_CNN, k=10: 0.8452 (drop: 0.0476)\n    ACS_SE_CNN, k=15: 0.8571 (drop: 0.0357)\n    ACS_SE_CNN, k=20: 0.9008 (drop: -0.0079)\n    ACS_SE_CNN, k=25: 0.9008 (drop: -0.0079)\n    ACS_SE_CNN, k=30: 0.8810 (drop: 0.0119)\n\n  G_CARM channel selection...\n    G_CARM-ES, k=10: 0.7817 (drop: 0.0913)\n    G_CARM-ES, k=15: 0.7262 (drop: 0.1468)\n    G_CARM-ES, k=20: 0.8056 (drop: 0.0675)\n    G_CARM-ES, k=25: 0.7579 (drop: 0.1151)\n    G_CARM-ES, k=30: 0.8373 (drop: 0.0357)\n    G_CARM-AS, k=10: 0.8651 (drop: 0.0079)\n    G_CARM-AS, k=15: 0.7778 (drop: 0.0952)\n    G_CARM-AS, k=20: 0.8492 (drop: 0.0238)\n    G_CARM-AS, k=25: 0.8770 (drop: -0.0040)\n    G_CARM-AS, k=30: 0.8810 (drop: -0.0079)\n\nProcessing S011...\n\n  FBCSP channel selection...\n    FBCSP, k=10: 0.5714 (drop: -0.0635)\n    FBCSP, k=15: 0.5714 (drop: -0.0635)\n    FBCSP, k=20: 0.5238 (drop: -0.0159)\n    FBCSP, k=25: 0.5317 (drop: -0.0238)\n    FBCSP, k=30: 0.4960 (drop: 0.0119)\n\n  CNN_SAE channel selection...\n    CNN_SAE, k=10: 0.8968 (drop: -0.0079)\n    CNN_SAE, k=15: 0.8532 (drop: 0.0357)\n    CNN_SAE, k=20: 0.8651 (drop: 0.0238)\n    CNN_SAE, k=25: 0.8849 (drop: 0.0040)\n    CNN_SAE, k=30: 0.8929 (drop: -0.0040)\n\n  EEGNET channel selection...\n    EEGNET, k=10: 0.9127 (drop: 0.0000)\n    EEGNET, k=15: 0.9246 (drop: -0.0119)\n    EEGNET, k=20: 0.9365 (drop: -0.0238)\n    EEGNET, k=25: 0.9127 (drop: 0.0000)\n    EEGNET, k=30: 0.9206 (drop: -0.0079)\n\n  ACS_SE_CNN channel selection...\n    ACS_SE_CNN, k=10: 0.8254 (drop: 0.0079)\n    ACS_SE_CNN, k=15: 0.8056 (drop: 0.0278)\n    ACS_SE_CNN, k=20: 0.8056 (drop: 0.0278)\n    ACS_SE_CNN, k=25: 0.8413 (drop: -0.0079)\n    ACS_SE_CNN, k=30: 0.8095 (drop: 0.0238)\n\n  G_CARM channel selection...\n    G_CARM-ES, k=10: 0.6944 (drop: 0.0159)\n    G_CARM-ES, k=15: 0.6944 (drop: 0.0159)\n    G_CARM-ES, k=20: 0.7341 (drop: -0.0238)\n    G_CARM-ES, k=25: 0.7381 (drop: -0.0278)\n    G_CARM-ES, k=30: 0.7103 (drop: 0.0000)\n    G_CARM-AS, k=10: 0.7222 (drop: -0.0119)\n    G_CARM-AS, k=15: 0.6548 (drop: 0.0556)\n    G_CARM-AS, k=20: 0.6706 (drop: 0.0397)\n    G_CARM-AS, k=25: 0.7937 (drop: -0.0833)\n    G_CARM-AS, k=30: 0.7500 (drop: -0.0397)\n\nProcessing S014...\n\n  FBCSP channel selection...\n    FBCSP, k=10: 0.7063 (drop: -0.0317)\n    FBCSP, k=15: 0.7024 (drop: -0.0278)\n    FBCSP, k=20: 0.7024 (drop: -0.0278)\n    FBCSP, k=25: 0.7302 (drop: -0.0556)\n    FBCSP, k=30: 0.6746 (drop: 0.0000)\n\n  CNN_SAE channel selection...\n    CNN_SAE, k=10: 0.8810 (drop: -0.0278)\n    CNN_SAE, k=15: 0.8770 (drop: -0.0238)\n    CNN_SAE, k=20: 0.8730 (drop: -0.0198)\n    CNN_SAE, k=25: 0.8810 (drop: -0.0278)\n    CNN_SAE, k=30: 0.8651 (drop: -0.0119)\n\n  EEGNET channel selection...\n    EEGNET, k=10: 0.9048 (drop: -0.0278)\n    EEGNET, k=15: 0.8968 (drop: -0.0198)\n    EEGNET, k=20: 0.8810 (drop: -0.0040)\n    EEGNET, k=25: 0.8968 (drop: -0.0198)\n    EEGNET, k=30: 0.8929 (drop: -0.0159)\n\n  ACS_SE_CNN channel selection...\n    ACS_SE_CNN, k=10: 0.8175 (drop: 0.0079)\n    ACS_SE_CNN, k=15: 0.8333 (drop: -0.0079)\n    ACS_SE_CNN, k=20: 0.8254 (drop: 0.0000)\n    ACS_SE_CNN, k=25: 0.8333 (drop: -0.0079)\n    ACS_SE_CNN, k=30: 0.7976 (drop: 0.0278)\n\n  G_CARM channel selection...\n    G_CARM-ES, k=10: 0.7143 (drop: 0.0040)\n    G_CARM-ES, k=15: 0.7421 (drop: -0.0238)\n    G_CARM-ES, k=20: 0.6984 (drop: 0.0198)\n    G_CARM-ES, k=25: 0.7183 (drop: 0.0000)\n    G_CARM-ES, k=30: 0.7937 (drop: -0.0754)\n    G_CARM-AS, k=10: 0.7659 (drop: -0.0476)\n    G_CARM-AS, k=15: 0.8214 (drop: -0.1032)\n    G_CARM-AS, k=20: 0.7698 (drop: -0.0516)\n    G_CARM-AS, k=25: 0.7937 (drop: -0.0754)\n    G_CARM-AS, k=30: 0.7857 (drop: -0.0675)\n\nProcessing S015...\n\n  FBCSP channel selection...\n    FBCSP, k=10: 0.8135 (drop: -0.0675)\n    FBCSP, k=15: 0.8413 (drop: -0.0952)\n    FBCSP, k=20: 0.8333 (drop: -0.0873)\n    FBCSP, k=25: 0.8373 (drop: -0.0913)\n    FBCSP, k=30: 0.8413 (drop: -0.0952)\n\n  CNN_SAE channel selection...\n    CNN_SAE, k=10: 0.8294 (drop: 0.0159)\n    CNN_SAE, k=15: 0.8294 (drop: 0.0159)\n    CNN_SAE, k=20: 0.8532 (drop: -0.0079)\n    CNN_SAE, k=25: 0.8611 (drop: -0.0159)\n    CNN_SAE, k=30: 0.8651 (drop: -0.0198)\n\n  EEGNET channel selection...\n    EEGNET, k=10: 0.8968 (drop: 0.0159)\n    EEGNET, k=15: 0.9167 (drop: -0.0040)\n    EEGNET, k=20: 0.9008 (drop: 0.0119)\n    EEGNET, k=25: 0.9127 (drop: 0.0000)\n    EEGNET, k=30: 0.9325 (drop: -0.0198)\n\n  ACS_SE_CNN channel selection...\n    ACS_SE_CNN, k=10: 0.8294 (drop: 0.0198)\n    ACS_SE_CNN, k=15: 0.8571 (drop: -0.0079)\n    ACS_SE_CNN, k=20: 0.8571 (drop: -0.0079)\n    ACS_SE_CNN, k=25: 0.8254 (drop: 0.0238)\n    ACS_SE_CNN, k=30: 0.8452 (drop: 0.0040)\n\n  G_CARM channel selection...\n    G_CARM-ES, k=10: 0.8413 (drop: 0.0159)\n    G_CARM-ES, k=15: 0.7857 (drop: 0.0714)\n    G_CARM-ES, k=20: 0.7778 (drop: 0.0794)\n    G_CARM-ES, k=25: 0.8254 (drop: 0.0317)\n    G_CARM-ES, k=30: 0.8214 (drop: 0.0357)\n    G_CARM-AS, k=10: 0.8294 (drop: 0.0278)\n    G_CARM-AS, k=15: 0.8214 (drop: 0.0357)\n    G_CARM-AS, k=20: 0.8333 (drop: 0.0238)\n    G_CARM-AS, k=25: 0.8611 (drop: -0.0040)\n    G_CARM-AS, k=30: 0.8492 (drop: 0.0079)\n\nProcessing S016...\n\n  FBCSP channel selection...\n    FBCSP, k=10: 0.7302 (drop: -0.1032)\n    FBCSP, k=15: 0.7103 (drop: -0.0833)\n    FBCSP, k=20: 0.6825 (drop: -0.0556)\n    FBCSP, k=25: 0.6508 (drop: -0.0238)\n    FBCSP, k=30: 0.6984 (drop: -0.0714)\n\n  CNN_SAE channel selection...\n    CNN_SAE, k=10: 0.7738 (drop: -0.0198)\n    CNN_SAE, k=15: 0.7500 (drop: 0.0040)\n    CNN_SAE, k=20: 0.7579 (drop: -0.0040)\n    CNN_SAE, k=25: 0.7659 (drop: -0.0119)\n    CNN_SAE, k=30: 0.7778 (drop: -0.0238)\n\n  EEGNET channel selection...\n    EEGNET, k=10: 0.8095 (drop: 0.0079)\n    EEGNET, k=15: 0.7857 (drop: 0.0317)\n    EEGNET, k=20: 0.7619 (drop: 0.0556)\n    EEGNET, k=25: 0.7738 (drop: 0.0437)\n    EEGNET, k=30: 0.7341 (drop: 0.0833)\n\n  ACS_SE_CNN channel selection...\n    ACS_SE_CNN, k=10: 0.7063 (drop: 0.0397)\n    ACS_SE_CNN, k=15: 0.7024 (drop: 0.0437)\n    ACS_SE_CNN, k=20: 0.7143 (drop: 0.0317)\n    ACS_SE_CNN, k=25: 0.7024 (drop: 0.0437)\n    ACS_SE_CNN, k=30: 0.7500 (drop: -0.0040)\n\n  G_CARM channel selection...\n    G_CARM-ES, k=10: 0.7341 (drop: 0.0476)\n    G_CARM-ES, k=15: 0.7659 (drop: 0.0159)\n    G_CARM-ES, k=20: 0.7778 (drop: 0.0040)\n    G_CARM-ES, k=25: 0.7222 (drop: 0.0595)\n    G_CARM-ES, k=30: 0.7381 (drop: 0.0437)\n    G_CARM-AS, k=10: 0.7698 (drop: 0.0119)\n    G_CARM-AS, k=15: 0.7421 (drop: 0.0397)\n    G_CARM-AS, k=20: 0.7738 (drop: 0.0079)\n    G_CARM-AS, k=25: 0.7540 (drop: 0.0278)\n    G_CARM-AS, k=30: 0.7778 (drop: 0.0040)\n\n================================================================================\nChannel Selection Complete!\n================================================================================\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"## 10. Save Results","metadata":{}},{"cell_type":"code","source":"results_dir = CONFIG['output']['results_dir']\n\nfor model_name in ['fbcsp', 'cnn_sae', 'eegnet', 'acs_se_cnn', 'g_carm']:\n    if len(all_results[model_name]) > 0:\n        df = pd.DataFrame(all_results[model_name])\n        df.to_csv(results_dir / f'legacy_{model_name}_results.csv', index=False)\n        print(f\"Saved: legacy_{model_name}_results.csv\")\n\nprint(f\"\\nAll results saved to {results_dir}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T06:24:00.950731Z","iopub.execute_input":"2025-11-27T06:24:00.951000Z","iopub.status.idle":"2025-11-27T06:24:00.997402Z","shell.execute_reply.started":"2025-11-27T06:24:00.950983Z","shell.execute_reply":"2025-11-27T06:24:00.996811Z"}},"outputs":[{"name":"stdout","text":"Saved: legacy_fbcsp_results.csv\nSaved: legacy_cnn_sae_results.csv\nSaved: legacy_eegnet_results.csv\nSaved: legacy_acs_se_cnn_results.csv\nSaved: legacy_g_carm_results.csv\n\nAll results saved to results\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"## 11. Results Summary","metadata":{}},{"cell_type":"code","source":"print(\"\\n\" + \"=\"*80)\nprint(\"RESULTS SUMMARY\")\nprint(\"=\"*80 + \"\\n\")\n\nfor model_name in ['fbcsp', 'cnn_sae', 'eegnet', 'acs_se_cnn', 'g_carm']:\n    if len(all_results[model_name]) > 0:\n        accs = [r['avg_accuracy'] for r in all_results[model_name]]\n        f1s = [r['avg_f1_score'] for r in all_results[model_name]]\n        aucs = [r['avg_auc_roc'] for r in all_results[model_name]]\n        \n        print(f\"{model_name.upper()} Results:\")\n        print(f\"  Subjects: {len(all_results[model_name])}\")\n        print(f\"  Mean accuracy: {np.mean(accs):.4f} ± {np.std(accs):.4f}\")\n        print(f\"  Mean F1-Score: {np.mean(f1s):.4f} ± {np.std(f1s):.4f}\")\n        print(f\"  Mean AUC-ROC: {np.mean(aucs):.4f} ± {np.std(aucs):.4f}\")\n        print()\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"DONE!\")\nprint(\"=\"*80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T06:24:00.998065Z","iopub.execute_input":"2025-11-27T06:24:00.998328Z","iopub.status.idle":"2025-11-27T06:24:01.005913Z","shell.execute_reply.started":"2025-11-27T06:24:00.998311Z","shell.execute_reply":"2025-11-27T06:24:01.005083Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nRESULTS SUMMARY\n================================================================================\n\nFBCSP Results:\n  Subjects: 10\n  Mean accuracy: 0.6317 ± 0.0747\n  Mean F1-Score: 0.3568 ± 0.1249\n  Mean AUC-ROC: 0.0000 ± 0.0000\n\nCNN_SAE Results:\n  Subjects: 10\n  Mean accuracy: 0.8425 ± 0.0667\n  Mean F1-Score: 0.7174 ± 0.1606\n  Mean AUC-ROC: 0.8604 ± 0.0939\n\nEEGNET Results:\n  Subjects: 10\n  Mean accuracy: 0.9024 ± 0.0561\n  Mean F1-Score: 0.8449 ± 0.0949\n  Mean AUC-ROC: 0.9238 ± 0.0755\n\nACS_SE_CNN Results:\n  Subjects: 10\n  Mean accuracy: 0.8143 ± 0.0644\n  Mean F1-Score: 0.6744 ± 0.1470\n  Mean AUC-ROC: 0.8201 ± 0.0923\n\nG_CARM Results:\n  Subjects: 10\n  Mean accuracy: 0.7798 ± 0.0661\n  Mean F1-Score: 0.5893 ± 0.1854\n  Mean AUC-ROC: 0.7769 ± 0.0929\n\n\n================================================================================\nDONE!\n================================================================================\n","output_type":"stream"}],"execution_count":23}]}