{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline 2: EEG-ARNN Methods - FINAL CORRECTED VERSION\n",
    "\n",
    "**OPTIMIZED:** 20 epochs, 2-fold CV, Comprehensive metrics, Smart redundancy avoidance\n",
    "\n",
    "## Models\n",
    "1. Baseline-EEG-ARNN (without gating)\n",
    "2. Adaptive-Gating-EEG-ARNN (YOUR CONTRIBUTION!)\n",
    "\n",
    "## Workflow\n",
    "\n",
    "### Part 1: Train Both Models on 64 Channels\n",
    "- 2 models \u00d7 2 folds = 4 training runs (~1 hour)\n",
    "\n",
    "### Part 2: Channel Selection Experiments\n",
    "For each k in [10, 15, 20, 25, 30]:\n",
    "  1. Extract importance scores using all 3 methods (Edge, Aggregation, Gate)\n",
    "  2. Select top-k channels for each method\n",
    "  3. **Compare channel sets** - if identical, train only once!\n",
    "  4. Train model(s) on selected k channels\n",
    "  5. Evaluate with comprehensive metrics\n",
    "\n",
    "**Smart Optimization:** Skip redundant training when methods select same channels\n",
    "\n",
    "## Metrics Calculated\n",
    "- Accuracy, Precision, Recall (Sensitivity), F1-Score, AUC-ROC, Specificity\n",
    "\n",
    "## Runtime: ~9-11 hours\n",
    "- Initial training: ~1 hour (4 runs)\n",
    "- Channel selection: ~8-10 hours (varies based on channel overlap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "import gc\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import mne\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix\n",
    ")\n",
    "from copy import deepcopy\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "mne.set_log_level('ERROR')\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    'data_path': '/kaggle/input/eeg-preprocessed-data/derived',\n",
    "    'models_dir': './models',\n",
    "    'results_dir': './results',\n",
    "    \n",
    "    'n_folds': 2,\n",
    "    'random_seed': 42,\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    'batch_size': 32,\n",
    "    'epochs': 20,\n",
    "    'learning_rate': 0.002,\n",
    "    'weight_decay': 1e-4,\n",
    "    'patience': 5,\n",
    "    'scheduler_patience': 2,\n",
    "    'scheduler_factor': 0.5,\n",
    "    'use_early_stopping': True,\n",
    "    'min_lr': 1e-6,\n",
    "    'use_amp': True,\n",
    "    \n",
    "    # Data parameters\n",
    "    'n_channels': 64,\n",
    "    'n_classes': 2,\n",
    "    'sfreq': 128,\n",
    "    'tmin': 0.0,\n",
    "    'tmax': 4.0,\n",
    "    'n_timepoints': 513,\n",
    "    'hidden_dim': 128,\n",
    "    'mi_runs': [7, 8, 11, 12],\n",
    "    \n",
    "    # Gating parameters\n",
    "    'gating': {\n",
    "        'gate_init': 0.9,\n",
    "        'l1_lambda': 1e-3,\n",
    "    },\n",
    "    \n",
    "    # Single k-values list for channel selection\n",
    "    'k_values': [10, 15, 20, 25, 30],\n",
    "}\n",
    "\n",
    "os.makedirs(CONFIG['models_dir'], exist_ok=True)\n",
    "os.makedirs(CONFIG['results_dir'], exist_ok=True)\n",
    "\n",
    "np.random.seed(CONFIG['random_seed'])\n",
    "torch.manual_seed(CONFIG['random_seed'])\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(CONFIG['random_seed'])\n",
    "\n",
    "print(f\"Device: {CONFIG['device']}\")\n",
    "print(f\"Epochs: {CONFIG['epochs']}\")\n",
    "print(f\"Folds: {CONFIG['n_folds']}\")\n",
    "print(f\"Learning rate: {CONFIG['learning_rate']}\")\n",
    "print(f\"k-values: {CONFIG['k_values']}\")\n",
    "\n",
    "# Runtime estimates\n",
    "initial_runs = 2 * CONFIG['n_folds']\n",
    "max_cs_runs = 2 * 3 * len(CONFIG['k_values']) * CONFIG['n_folds']\n",
    "\n",
    "print(f\"\\nEstimated training runs:\")\n",
    "print(f\"  Initial (64 channels): {initial_runs}\")\n",
    "print(f\"  Channel selection (worst case): {max_cs_runs}\")\n",
    "print(f\"  Channel selection (best case - with skips): ~{int(max_cs_runs * 0.7)}\")\n",
    "print(f\"\\nEstimated runtime: ~9-11 hours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Metrics Calculation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_comprehensive_metrics(model, dataloader, device):\n",
    "    \"\"\"Calculate all metrics for PyTorch models.\"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in dataloader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            \n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(y_batch.numpy())\n",
    "            all_probs.extend(probs[:, 1].cpu().numpy())\n",
    "    \n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_probs = np.array(all_probs)\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(all_labels, all_preds),\n",
    "        'precision': precision_score(all_labels, all_preds, average='binary', zero_division=0),\n",
    "        'recall': recall_score(all_labels, all_preds, average='binary', zero_division=0),\n",
    "        'f1_score': f1_score(all_labels, all_preds, average='binary', zero_division=0),\n",
    "        'auc_roc': roc_auc_score(all_labels, all_probs) if len(np.unique(all_labels)) > 1 else 0.0,\n",
    "    }\n",
    "    \n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    if cm.shape == (2, 2):\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        metrics['specificity'] = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
    "        metrics['sensitivity'] = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    else:\n",
    "        metrics['specificity'] = 0.0\n",
    "        metrics['sensitivity'] = 0.0\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "print(\"Metrics calculation function loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_physionet_data(data_path):\n",
    "    \"\"\"Load preprocessed PhysioNet data.\"\"\"\n",
    "    data_root = os.path.abspath(data_path)\n",
    "    if not os.path.isdir(data_root):\n",
    "        raise FileNotFoundError(f\"Data path not found: {data_root}\")\n",
    "\n",
    "    tmin, tmax = CONFIG['tmin'], CONFIG['tmax']\n",
    "    mi_runs = CONFIG['mi_runs']\n",
    "    event_id = {'T1': 1, 'T2': 2}\n",
    "    label_map = {1: 0, 2: 1}\n",
    "\n",
    "    preprocessed_dir = os.path.join(data_root, 'preprocessed')\n",
    "    if os.path.isdir(preprocessed_dir):\n",
    "        data_root = preprocessed_dir\n",
    "    \n",
    "    subject_dirs = [d for d in sorted(os.listdir(data_root))\n",
    "                    if os.path.isdir(os.path.join(data_root, d)) and d.upper().startswith('S')]\n",
    "\n",
    "    all_X, all_y, all_subjects = [], [], []\n",
    "    print(f\"Loading data from {len(subject_dirs)} subjects...\")\n",
    "    \n",
    "    for subject_dir in subject_dirs:\n",
    "        subject_num = int(subject_dir[1:]) if len(subject_dir) > 1 else -1\n",
    "        subject_path = os.path.join(data_root, subject_dir)\n",
    "        \n",
    "        for run_id in mi_runs:\n",
    "            run_file = f\"{subject_dir}R{run_id:02d}_preproc_raw.fif\"\n",
    "            run_path = os.path.join(subject_path, run_file)\n",
    "            \n",
    "            if not os.path.exists(run_path):\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                raw = mne.io.read_raw_fif(run_path, preload=True, verbose=False)\n",
    "                picks = mne.pick_types(raw.info, eeg=True, meg=False, stim=False, eog=False)\n",
    "                if len(picks) == 0:\n",
    "                    continue\n",
    "                \n",
    "                events, _ = mne.events_from_annotations(raw, event_id=event_id)\n",
    "                if len(events) == 0:\n",
    "                    continue\n",
    "                \n",
    "                epochs = mne.Epochs(raw, events, event_id=event_id, tmin=tmin, tmax=tmax,\n",
    "                                    baseline=None, preload=True, picks=picks, verbose=False)\n",
    "                \n",
    "                data = epochs.get_data()\n",
    "                labels = np.array([label_map.get(epochs.events[i, 2], -1) for i in range(len(epochs))])\n",
    "                valid = labels >= 0\n",
    "                \n",
    "                if np.any(valid):\n",
    "                    all_X.append(data[valid])\n",
    "                    all_y.append(labels[valid])\n",
    "                    all_subjects.append(np.full(np.sum(valid), subject_num))\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    X = np.concatenate(all_X, axis=0)\n",
    "    y = np.concatenate(all_y, axis=0)\n",
    "    subjects = np.concatenate(all_subjects, axis=0)\n",
    "    \n",
    "    print(f\"Loaded {len(X)} trials from {len(np.unique(subjects))} subjects\")\n",
    "    print(f\"Data shape: {X.shape}\")\n",
    "    print(f\"Labels: {np.bincount(y)}\")\n",
    "    \n",
    "    return X, y, subjects\n",
    "\n",
    "\n",
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph Convolution Layer\n",
    "class GraphConvLayer(nn.Module):\n",
    "    def __init__(self, num_channels, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.A = nn.Parameter(torch.randn(num_channels, num_channels) * 0.01)\n",
    "        self.theta = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(hidden_dim)\n",
    "        self.act = nn.ELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, H, C, T = x.shape\n",
    "        A = torch.sigmoid(self.A)\n",
    "        A = 0.5 * (A + A.t())\n",
    "        I = torch.eye(C, device=A.device)\n",
    "        A_hat = A + I\n",
    "        D = torch.diag(torch.pow(A_hat.sum(1).clamp_min(1e-6), -0.5))\n",
    "        A_norm = D @ A_hat @ D\n",
    "\n",
    "        x_perm = x.permute(0, 3, 2, 1).contiguous().view(B * T, C, H)\n",
    "        x_g = A_norm @ x_perm\n",
    "        x_g = self.theta(x_g)\n",
    "        x_g = x_g.view(B, T, C, H).permute(0, 3, 2, 1)\n",
    "        return self.act(self.bn(x_g))\n",
    "\n",
    "    def get_adjacency(self):\n",
    "        with torch.no_grad():\n",
    "            A = torch.sigmoid(self.A)\n",
    "            A = 0.5 * (A + A.t())\n",
    "            return A.cpu().numpy()\n",
    "\n",
    "\n",
    "# Temporal Convolution\n",
    "class TemporalConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=16, pool=True):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=(1, kernel_size),\n",
    "                              padding=(0, kernel_size // 2), bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.act = nn.ELU()\n",
    "        self.pool_layer = nn.AvgPool2d(kernel_size=(1, 2)) if pool else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.act(self.bn(self.conv(x)))\n",
    "        if self.pool_layer is not None:\n",
    "            x = self.pool_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline EEG-ARNN (without gating)\n",
    "class BaselineEEGARNN(nn.Module):\n",
    "    def __init__(self, n_channels=64, n_classes=2, n_timepoints=513, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.use_gate_regularizer = False\n",
    "\n",
    "        self.t1 = TemporalConv(1, hidden_dim, 16, pool=False)\n",
    "        self.g1 = GraphConvLayer(n_channels, hidden_dim)\n",
    "        self.t2 = TemporalConv(hidden_dim, hidden_dim, 16, pool=True)\n",
    "        self.g2 = GraphConvLayer(n_channels, hidden_dim)\n",
    "        self.t3 = TemporalConv(hidden_dim, hidden_dim, 16, pool=True)\n",
    "        self.g3 = GraphConvLayer(n_channels, hidden_dim)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, n_channels, n_timepoints)\n",
    "            feat = self._forward_features(self._prepare_input(dummy))\n",
    "            self.feature_dim = feat.view(1, -1).size(1)\n",
    "\n",
    "        self.fc1 = nn.Linear(self.feature_dim, 256)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(256, n_classes)\n",
    "\n",
    "    def _prepare_input(self, x):\n",
    "        if x.dim() == 3:\n",
    "            x = x.unsqueeze(1)\n",
    "        return x\n",
    "\n",
    "    def _forward_features(self, x):\n",
    "        x = self.g1(self.t1(x))\n",
    "        x = self.g2(self.t2(x))\n",
    "        x = self.g3(self.t3(x))\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        prepared = self._prepare_input(x)\n",
    "        features = self._forward_features(prepared)\n",
    "        x = features.view(features.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        return self.fc2(x)\n",
    "\n",
    "    def get_final_adjacency(self):\n",
    "        return self.g3.get_adjacency()\n",
    "\n",
    "    def get_channel_importance_edge(self):\n",
    "        adjacency = self.get_final_adjacency()\n",
    "        return np.sum(adjacency, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adaptive Gating EEG-ARNN (YOUR CONTRIBUTION!)\n",
    "class AdaptiveGatingEEGARNN(BaselineEEGARNN):\n",
    "    def __init__(self, n_channels=64, n_classes=2, n_timepoints=513, hidden_dim=128, gate_init=0.9):\n",
    "        super().__init__(n_channels, n_classes, n_timepoints, hidden_dim)\n",
    "        self.use_gate_regularizer = True\n",
    "        \n",
    "        self.gate_net = nn.Sequential(\n",
    "            nn.Linear(n_channels * 2, n_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_channels, n_channels),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        init_value = float(np.clip(gate_init, 1e-3, 1 - 1e-3))\n",
    "        init_bias = math.log(init_value / (1.0 - init_value))\n",
    "        with torch.no_grad():\n",
    "            self.gate_net[-2].bias.fill_(init_bias)\n",
    "        \n",
    "        self.latest_gate_values = None\n",
    "        self.gate_penalty_tensor = None\n",
    "\n",
    "    def compute_gates(self, x):\n",
    "        x_s = x.squeeze(1)\n",
    "        ch_mean = x_s.mean(dim=2)\n",
    "        ch_std = x_s.std(dim=2)\n",
    "        stats = torch.cat([ch_mean, ch_std], dim=1)\n",
    "        return self.gate_net(stats)\n",
    "\n",
    "    def forward(self, x):\n",
    "        prepared = self._prepare_input(x)\n",
    "        gates = self.compute_gates(prepared)\n",
    "        self.gate_penalty_tensor = gates\n",
    "        self.latest_gate_values = gates.detach()\n",
    "        gated = prepared * gates.view(gates.size(0), 1, gates.size(1), 1)\n",
    "        \n",
    "        features = self._forward_features(gated)\n",
    "        x = features.view(features.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        return self.fc2(x)\n",
    "\n",
    "    def get_channel_importance_gate(self):\n",
    "        if self.latest_gate_values is None:\n",
    "            return None\n",
    "        return self.latest_gate_values.mean(dim=0).cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer, device, l1_lambda=0.0, scaler=None):\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "    \n",
    "    use_amp = scaler is not None\n",
    "    \n",
    "    for X_batch, y_batch in dataloader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Mixed Precision Training\n",
    "        if use_amp:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                \n",
    "                gate_penalty = getattr(model, 'gate_penalty_tensor', None)\n",
    "                if l1_lambda > 0 and gate_penalty is not None:\n",
    "                    loss = loss + l1_lambda * gate_penalty.abs().mean()\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            \n",
    "            gate_penalty = getattr(model, 'gate_penalty_tensor', None)\n",
    "            if l1_lambda > 0 and gate_penalty is not None:\n",
    "                loss = loss + l1_lambda * gate_penalty.abs().mean()\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += y_batch.size(0)\n",
    "        correct += (predicted == y_batch).sum().item()\n",
    "        \n",
    "        # Aggressive memory cleanup\n",
    "        del outputs, loss\n",
    "    \n",
    "    return total_loss / max(1, len(dataloader)), correct / max(1, total)\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in dataloader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += y_batch.size(0)\n",
    "            correct += (predicted == y_batch).sum().item()\n",
    "        \n",
    "        # Aggressive memory cleanup\n",
    "        del outputs, loss\n",
    "    \n",
    "    return total_loss / max(1, len(dataloader)), correct / max(1, total)\n",
    "\n",
    "\n",
    "def train_pytorch_model(model, train_loader, val_loader, config, model_name=''):\n",
    "    device = config['device']\n",
    "    model = model.to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'], \n",
    "                          weight_decay=config['weight_decay'])\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=config['scheduler_factor'], \n",
    "        patience=config['scheduler_patience'], min_lr=config['min_lr']\n",
    "    )\n",
    "    \n",
    "    l1_lambda = config['gating']['l1_lambda'] if getattr(model, 'use_gate_regularizer', False) else 0.0\n",
    "    \n",
    "    # Initialize Scaler for AMP\n",
    "    scaler = torch.cuda.amp.GradScaler() if config.get('use_amp', False) and device != 'cpu' else None\n",
    "    \n",
    "    best_state = {k: v.cpu() if hasattr(v, 'cpu') else v for k, v in model.state_dict().items()}\n",
    "    best_val_acc = 0.0\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(config['epochs']):\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device, l1_lambda, scaler)\n",
    "        val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        improved = val_acc > best_val_acc or (val_acc == best_val_acc and val_loss < best_val_loss)\n",
    "        if improved:\n",
    "    best_state = {k: v.cpu() if hasattr(v, 'cpu') else v for k, v in model.state_dict().items()}\n",
    "            best_val_acc = val_acc\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if epoch % 5 == 0 or improved:\n",
    "            print(f\"[{model_name}] Epoch {epoch+1}/{config['epochs']} - \"\n",
    "                  f\"Train: {train_acc:.4f} | Val: {val_acc:.4f} | Best: {best_val_acc:.4f}\")\n",
    "        \n",
    "        if config['use_early_stopping'] and patience_counter >= config['patience']:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    model.load_state_dict(best_state)\n",
    "    return best_state, model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Channel Selection Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_channel_importance_aggregation(model, dataloader, device):\n",
    "    \"\"\"Aggregation Selection: based on feature activations.\"\"\"\n",
    "    model.eval()\n",
    "    channel_stats = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, _ in dataloader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            prepared = model._prepare_input(X_batch)\n",
    "            features = model._forward_features(prepared)\n",
    "            activations = torch.mean(torch.abs(features), dim=(1, 3))\n",
    "            channel_stats.append(activations.cpu())\n",
    "\n",
    "    if not channel_stats:\n",
    "        return np.zeros(model.n_channels)\n",
    "    stacked = torch.cat(channel_stats, dim=0)\n",
    "    return stacked.mean(dim=0).numpy()\n",
    "\n",
    "\n",
    "def compute_gate_importance(model, dataloader, device):\n",
    "    \"\"\"Gate Selection: average gate values across dataset.\"\"\"\n",
    "    model.eval()\n",
    "    gate_batches = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, _ in dataloader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            _ = model(X_batch)\n",
    "            latest = getattr(model, 'latest_gate_values', None)\n",
    "            if latest is not None:\n",
    "                gate_batches.append(latest.cpu())\n",
    "\n",
    "    if not gate_batches:\n",
    "        return np.ones(model.n_channels) / model.n_channels\n",
    "    stacked = torch.cat(gate_batches, dim=0)\n",
    "    return stacked.mean(dim=0).numpy()\n",
    "\n",
    "\n",
    "def select_top_k_channels(importance_scores, k):\n",
    "    \"\"\"Select top k channels and return as sorted list.\"\"\"\n",
    "    top_k_indices = np.argsort(importance_scores)[-k:]\n",
    "    return sorted(top_k_indices.tolist())\n",
    "\n",
    "\n",
    "def apply_channel_selection(X, selected_channels):\n",
    "    \"\"\"Apply channel selection to data.\"\"\"\n",
    "    return X[:, selected_channels, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading PhysioNet data...\")\n",
    "X, y, subjects = load_physionet_data(CONFIG['data_path'])\n",
    "print(f\"\\nData ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Train Initial Models (64 Channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup cross-validation\n",
    "skf = StratifiedKFold(n_splits=CONFIG['n_folds'], shuffle=True, random_state=CONFIG['random_seed'])\n",
    "\n",
    "models_to_train = [\n",
    "    {'name': 'Baseline-EEG-ARNN', 'class': BaselineEEGARNN},\n",
    "    {'name': 'Adaptive-Gating-EEG-ARNN', 'class': AdaptiveGatingEEGARNN},\n",
    "]\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"TRAINING INITIAL MODELS (64 CHANNELS)\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "all_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop for 64-channel models\n",
    "for model_info in models_to_train:\n",
    "    model_name = model_info['name']\n",
    "    model_class = model_info['class']\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training {model_name}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    fold_results = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "        print(f\"\\nFold {fold + 1}/{CONFIG['n_folds']}\")\n",
    "        \n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "        \n",
    "        train_dataset = EEGDataset(X_train, y_train)\n",
    "        val_dataset = EEGDataset(X_val, y_val)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'], shuffle=False)\n",
    "        \n",
    "        if model_class == AdaptiveGatingEEGARNN:\n",
    "            model = model_class(\n",
    "                n_channels=CONFIG['n_channels'],\n",
    "                n_classes=CONFIG['n_classes'],\n",
    "                n_timepoints=CONFIG['n_timepoints'],\n",
    "                hidden_dim=CONFIG['hidden_dim'],\n",
    "                gate_init=CONFIG['gating']['gate_init']\n",
    "            )\n",
    "        else:\n",
    "            model = model_class(\n",
    "                n_channels=CONFIG['n_channels'],\n",
    "                n_classes=CONFIG['n_classes'],\n",
    "                n_timepoints=CONFIG['n_timepoints'],\n",
    "                hidden_dim=CONFIG['hidden_dim']\n",
    "            )\n",
    "        \n",
    "        best_state, model = train_pytorch_model(model, train_loader, val_loader, CONFIG, model_name)\n",
    "        \n",
    "        # Calculate comprehensive metrics\n",
    "        metrics = calculate_comprehensive_metrics(model, val_loader, CONFIG['device'])\n",
    "        \n",
    "        # Save model\n",
    "        model_path = os.path.join(CONFIG['models_dir'], f\"eegarnn_{model_name}_fold{fold+1}.pt\")\n",
    "        torch.save(best_state, model_path)\n",
    "        \n",
    "        result = {'fold': fold + 1}\n",
    "        result.update(metrics)\n",
    "        fold_results.append(result)\n",
    "        \n",
    "        print(f\"\\nFold {fold + 1} Results:\")\n",
    "        print(f\"  Accuracy: {metrics['accuracy']:.4f}\")\n",
    "        print(f\"  F1-Score: {metrics['f1_score']:.4f}\")\n",
    "        print(f\"  AUC-ROC: {metrics['auc_roc']:.4f}\")\n",
    "        \n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    all_results[model_name] = fold_results\n",
    "    \n",
    "    mean_acc = np.mean([r['accuracy'] for r in fold_results])\n",
    "    print(f\"\\n{model_name} Mean Accuracy: {mean_acc:.4f}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"INITIAL MODELS TRAINED!\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Channel Selection Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*60}\")\n",
    "print(\"CHANNEL SELECTION EXPERIMENTS\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "channel_selection_results = []\n",
    "training_count = 0\n",
    "skipped_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Channel selection experiments\n",
    "for model_info in models_to_train:\n",
    "    model_name = model_info['name']\n",
    "    model_class = model_info['class']\n",
    "    \n",
    "    # Define which methods apply to this model\n",
    "    if 'Baseline' in model_name:\n",
    "        methods = ['edge', 'aggregation']\n",
    "    else:\n",
    "        methods = ['edge', 'aggregation', 'gate']\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{model_name}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    for k in CONFIG['k_values']:\n",
    "        print(f\"\\nk={k} channels:\")\n",
    "        \n",
    "        for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "            X_train, X_val = X[train_idx], X[val_idx]\n",
    "            y_train, y_val = y[train_idx], y[val_idx]\n",
    "            \n",
    "            # Load the trained 64-channel model\n",
    "            if model_class == AdaptiveGatingEEGARNN:\n",
    "                full_model = model_class(\n",
    "                    n_channels=CONFIG['n_channels'],\n",
    "                    n_classes=CONFIG['n_classes'],\n",
    "                    n_timepoints=CONFIG['n_timepoints'],\n",
    "                    hidden_dim=CONFIG['hidden_dim'],\n",
    "                    gate_init=CONFIG['gating']['gate_init']\n",
    "                )\n",
    "            else:\n",
    "                full_model = model_class(\n",
    "                    n_channels=CONFIG['n_channels'],\n",
    "                    n_classes=CONFIG['n_classes'],\n",
    "                    n_timepoints=CONFIG['n_timepoints'],\n",
    "                    hidden_dim=CONFIG['hidden_dim']\n",
    "                )\n",
    "            \n",
    "            model_path = os.path.join(CONFIG['models_dir'], f\"eegarnn_{model_name}_fold{fold+1}.pt\")\n",
    "            state_dict = torch.load(model_path, map_location=CONFIG['device'])\n",
    "            full_model.load_state_dict(state_dict)\n",
    "            full_model = full_model.to(CONFIG['device'])\n",
    "            full_model.eval()\n",
    "            \n",
    "            # Get channel importance for all methods\n",
    "            train_dataset = EEGDataset(X_train, y_train)\n",
    "            train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=False)\n",
    "            \n",
    "            method_channels = {}\n",
    "            for method in methods:\n",
    "                if method == 'edge':\n",
    "                    importance = full_model.get_channel_importance_edge()\n",
    "                elif method == 'aggregation':\n",
    "                    importance = get_channel_importance_aggregation(full_model, train_loader, CONFIG['device'])\n",
    "                else:  # gate\n",
    "                    importance = compute_gate_importance(full_model, train_loader, CONFIG['device'])\n",
    "                \n",
    "                selected = select_top_k_channels(importance, k)\n",
    "                method_channels[method] = selected\n",
    "            \n",
    "            del full_model\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            \n",
    "            # Group methods by identical channel sets to avoid redundant training\n",
    "            unique_channel_sets = {}\n",
    "            for method in methods:\n",
    "                channels_tuple = tuple(method_channels[method])\n",
    "                if channels_tuple not in unique_channel_sets:\n",
    "                    unique_channel_sets[channels_tuple] = []\n",
    "                unique_channel_sets[channels_tuple].append(method)\n",
    "            \n",
    "            # Train only once per unique channel set\n",
    "            for channels_tuple, method_group in unique_channel_sets.items():\n",
    "                channels_list = list(channels_tuple)\n",
    "                \n",
    "                if len(method_group) > 1:\n",
    "                    print(f\"  Fold {fold+1}: Methods {method_group} selected same channels - training once\")\n",
    "                    skipped_count += len(method_group) - 1\n",
    "                \n",
    "                # Apply channel selection\n",
    "                X_train_selected = apply_channel_selection(X_train, channels_list)\n",
    "                X_val_selected = apply_channel_selection(X_val, channels_list)\n",
    "                \n",
    "                # Train model with k channels\n",
    "                train_dataset = EEGDataset(X_train_selected, y_train)\n",
    "                val_dataset = EEGDataset(X_val_selected, y_val)\n",
    "                train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True)\n",
    "                val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'], shuffle=False)\n",
    "                \n",
    "                if model_class == AdaptiveGatingEEGARNN:\n",
    "                    new_model = model_class(\n",
    "                        n_channels=k,\n",
    "                        n_classes=CONFIG['n_classes'],\n",
    "                        n_timepoints=CONFIG['n_timepoints'],\n",
    "                        hidden_dim=CONFIG['hidden_dim'],\n",
    "                        gate_init=CONFIG['gating']['gate_init']\n",
    "                    )\n",
    "                else:\n",
    "                    new_model = model_class(\n",
    "                        n_channels=k,\n",
    "                        n_classes=CONFIG['n_classes'],\n",
    "                        n_timepoints=CONFIG['n_timepoints'],\n",
    "                        hidden_dim=CONFIG['hidden_dim']\n",
    "                    )\n",
    "                \n",
    "                best_state, new_model = train_pytorch_model(\n",
    "                    new_model, train_loader, val_loader, CONFIG, \n",
    "                    f\"{model_name}-{'+'.join(method_group)}-k{k}-fold{fold+1}\"\n",
    "                )\n",
    "                \n",
    "                training_count += 1\n",
    "                \n",
    "                # Calculate comprehensive metrics\n",
    "                metrics = calculate_comprehensive_metrics(new_model, val_loader, CONFIG['device'])\n",
    "                \n",
    "                # Save results for all methods in the group\n",
    "                for method in method_group:\n",
    "                    result = {\n",
    "                        'model': model_name,\n",
    "                        'method': method,\n",
    "                        'k': k,\n",
    "                        'fold': fold + 1,\n",
    "                        'selected_channels': str(channels_list)\n",
    "                    }\n",
    "                    result.update(metrics)\n",
    "                    channel_selection_results.append(result)\n",
    "                \n",
    "                del new_model\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "        \n",
    "        # Print summary for this k\n",
    "        k_results = [r for r in channel_selection_results \n",
    "                     if r['model'] == model_name and r['k'] == k]\n",
    "        for method in methods:\n",
    "            method_results = [r for r in k_results if r['method'] == method]\n",
    "            if method_results:\n",
    "                mean_acc = np.mean([r['accuracy'] for r in method_results])\n",
    "                print(f\"  {method}: {mean_acc:.4f}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"CHANNEL SELECTION COMPLETE!\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nTotal trainings performed: {training_count}\")\n",
    "print(f\"Trainings skipped (redundant): {skipped_count}\")\n",
    "print(f\"Time saved: ~{skipped_count * 10 / 60:.1f} hours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save All Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save initial 64-channel results\n",
    "for model_name, fold_results in all_results.items():\n",
    "    detailed_results = []\n",
    "    for result in fold_results:\n",
    "        detailed_results.append({\n",
    "            'model': model_name,\n",
    "            'fold': result['fold'],\n",
    "            'accuracy': result['accuracy'],\n",
    "            'precision': result['precision'],\n",
    "            'recall': result['recall'],\n",
    "            'f1_score': result['f1_score'],\n",
    "            'auc_roc': result['auc_roc'],\n",
    "            'specificity': result['specificity'],\n",
    "            'sensitivity': result['sensitivity']\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(detailed_results)\n",
    "    filename = model_name.lower().replace('-', '_').replace(' ', '_')\n",
    "    df.to_csv(os.path.join(CONFIG['results_dir'], f'eegarnn_{filename}_results.csv'), index=False)\n",
    "    print(f\"Saved: results/eegarnn_{filename}_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save channel selection results\n",
    "cs_df = pd.DataFrame(channel_selection_results)\n",
    "cs_df.to_csv(os.path.join(CONFIG['results_dir'], 'channel_selection_results.csv'), index=False)\n",
    "\n",
    "print(\"\\nChannel Selection Results (sample):\")\n",
    "print(cs_df[['model', 'method', 'k', 'fold', 'accuracy', 'f1_score', 'auc_roc']].head(20))\n",
    "print(\"\\nSaved: results/channel_selection_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*60}\")\n",
    "print(\"COMPLETE SUMMARY\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Initial models (64 channels)\n",
    "print(\"1. Initial Models (64 channels):\")\n",
    "for model_name, fold_results in all_results.items():\n",
    "    accs = [r['accuracy'] for r in fold_results]\n",
    "    f1s = [r['f1_score'] for r in fold_results]\n",
    "    print(f\"   {model_name}:\")\n",
    "    print(f\"     Accuracy: {np.mean(accs):.4f} +/- {np.std(accs):.4f}\")\n",
    "    print(f\"     F1-Score: {np.mean(f1s):.4f} +/- {np.std(f1s):.4f}\")\n",
    "\n",
    "# Best channel selection results\n",
    "print(\"\\n2. Best Channel Selection Results:\")\n",
    "for k in CONFIG['k_values']:\n",
    "    k_results = cs_df[cs_df['k'] == k]\n",
    "    best_idx = k_results['accuracy'].idxmax()\n",
    "    best = k_results.loc[best_idx]\n",
    "    print(f\"   k={k}: {best['method']} ({best['model']}) - Acc={best['accuracy']:.4f}\")\n",
    "\n",
    "# Method comparison\n",
    "print(\"\\n3. Method Comparison (averaged across all k):\")\n",
    "for method in ['edge', 'aggregation', 'gate']:\n",
    "    method_results = cs_df[cs_df['method'] == method]\n",
    "    if len(method_results) > 0:\n",
    "        mean_acc = method_results['accuracy'].mean()\n",
    "        print(f\"   {method.capitalize()}: {mean_acc:.4f}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"PIPELINE 2 COMPLETE!\")\n",
    "print(f\"{'='*60}\")\n",
    "print(\"\\nAll comprehensive metrics calculated and saved.\")\n",
    "print(\"\\nOutputs:\")\n",
    "print(\"  1. eegarnn_baseline_eeg_arnn_results.csv\")\n",
    "print(\"  2. eegarnn_adaptive_gating_eeg_arnn_results.csv\")\n",
    "print(\"  3. channel_selection_results.csv\")\n",
    "print(\"\\nReady for comparison with Pipeline 1!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}