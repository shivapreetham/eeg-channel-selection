{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PhysioNet EEG: Generate Paper-Ready Results\n",
    "\n",
    "This notebook generates publication-ready tables and figures from the experimental results.\n",
    "\n",
    "**Expected Runtime**: 5-10 minutes\n",
    "\n",
    "**Input**: \n",
    "- `results/summary_all_models.csv` (from notebook 01)\n",
    "- `results/channel_selection_results.csv` (from notebook 02)\n",
    "- `results/retention_analysis.csv` (from notebook 02)\n",
    "\n",
    "**Output**: \n",
    "- `table_ii_model_comparison.tex` - LaTeX table for model comparison\n",
    "- `table_iii_retention.tex` - LaTeX table for retention analysis\n",
    "- `figure_model_comparison.pdf` - Bar chart comparing all models\n",
    "- `figure_retention_curves.pdf` - Line plots for retention analysis\n",
    "- `paper_summary.json` - Summary statistics for paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-paper')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.rcParams['font.size'] = 10\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "\n",
    "# Paths\n",
    "RESULTS_DIR = './results'\n",
    "FIGURES_DIR = './figures'\n",
    "\n",
    "os.makedirs(FIGURES_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all results\n",
    "model_results = pd.read_csv(os.path.join(RESULTS_DIR, 'summary_all_models.csv'))\n",
    "channel_selection_results = pd.read_csv(os.path.join(RESULTS_DIR, 'channel_selection_results.csv'))\n",
    "retention_results = pd.read_csv(os.path.join(RESULTS_DIR, 'retention_analysis.csv'))\n",
    "\n",
    "print(\"Results loaded successfully!\")\n",
    "print(f\"\\nModel results shape: {model_results.shape}\")\n",
    "print(f\"Channel selection results shape: {channel_selection_results.shape}\")\n",
    "print(f\"Retention results shape: {retention_results.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table II: Model Comparison\n",
    "\n",
    "Generate LaTeX table comparing all baseline models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by accuracy\n",
    "model_results_sorted = model_results.sort_values('mean_accuracy', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Add rank column\n",
    "model_results_sorted['rank'] = range(1, len(model_results_sorted) + 1)\n",
    "\n",
    "# Format for display\n",
    "model_results_sorted['accuracy_str'] = model_results_sorted.apply(\n",
    "    lambda row: f\"{row['mean_accuracy']*100:.2f} ± {row['std_accuracy']*100:.2f}\",\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(\"Model Comparison Table:\")\n",
    "print(model_results_sorted[['rank', 'model', 'accuracy_str']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate LaTeX table\n",
    "latex_table = r\"\"\"\n",
    "\\begin{table}[htbp]\n",
    "\\centering\n",
    "\\caption{Comparison of baseline methods on PhysioNet Motor Imagery dataset}\n",
    "\\label{tab:model_comparison}\n",
    "\\begin{tabular}{clc}\n",
    "\\toprule\n",
    "\\textbf{Rank} & \\textbf{Method} & \\textbf{Accuracy (\\%)} \\\\\\\\\n",
    "\\midrule\n",
    "\"\"\"\n",
    "\n",
    "for _, row in model_results_sorted.iterrows():\n",
    "    # Bold the winner\n",
    "    if row['rank'] == 1:\n",
    "        latex_table += f\"{row['rank']} & \\\\textbf{{{row['model']}}} & \\\\textbf{{{row['accuracy_str']}}} \\\\\\\\\\\\\n\"\n",
    "    else:\n",
    "        latex_table += f\"{row['rank']} & {row['model']} & {row['accuracy_str']} \\\\\\\\\\\\\n\"\n",
    "\n",
    "latex_table += r\"\"\"\n",
    "\\bottomrule\n",
    "\\end{tabular}\n",
    "\\end{table}\n",
    "\"\"\"\n",
    "\n",
    "# Save LaTeX table\n",
    "table_path = os.path.join(RESULTS_DIR, 'table_ii_model_comparison.tex')\n",
    "with open(table_path, 'w') as f:\n",
    "    f.write(latex_table)\n",
    "\n",
    "print(f\"\\nLaTeX table saved to: {table_path}\")\n",
    "print(\"\\nTable preview:\")\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table III: Channel Selection and Retention Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create retention table for different k values\n",
    "retention_k_display = [10, 20, 30, 40, 50, 64]\n",
    "retention_display = retention_results[retention_results['k'].isin(retention_k_display)].copy()\n",
    "\n",
    "retention_display['accuracy_str'] = retention_display.apply(\n",
    "    lambda row: f\"{row['mean_accuracy']*100:.2f} ± {row['std_accuracy']*100:.2f}\",\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Also add best channel selection results\n",
    "cs_summary = channel_selection_results.groupby(['model', 'method']).apply(\n",
    "    lambda x: x.loc[x['mean_accuracy'].idxmax()]\n",
    ").reset_index(drop=True)\n",
    "\n",
    "cs_summary['accuracy_str'] = cs_summary.apply(\n",
    "    lambda row: f\"{row['mean_accuracy']*100:.2f} ± {row['std_accuracy']*100:.2f}\",\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(\"Best Channel Selection Results:\")\n",
    "print(cs_summary[['model', 'method', 'k', 'accuracy_str']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate LaTeX table for retention analysis\n",
    "latex_retention = r\"\"\"\n",
    "\\begin{table}[htbp]\n",
    "\\centering\n",
    "\\caption{Performance retention with channel selection using Gate Selection method}\n",
    "\\label{tab:retention_analysis}\n",
    "\\begin{tabular}{cc}\n",
    "\\toprule\n",
    "\\textbf{Channels (k)} & \\textbf{Accuracy (\\%)} \\\\\\\\\n",
    "\\midrule\n",
    "\"\"\"\n",
    "\n",
    "for _, row in retention_display.iterrows():\n",
    "    latex_retention += f\"{row['k']} & {row['accuracy_str']} \\\\\\\\\\\\\n\"\n",
    "\n",
    "latex_retention += r\"\"\"\n",
    "\\bottomrule\n",
    "\\end{tabular}\n",
    "\\end{table}\n",
    "\"\"\"\n",
    "\n",
    "# Save LaTeX table\n",
    "retention_table_path = os.path.join(RESULTS_DIR, 'table_iii_retention.tex')\n",
    "with open(retention_table_path, 'w') as f:\n",
    "    f.write(latex_retention)\n",
    "\n",
    "print(f\"\\nRetention table saved to: {retention_table_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 1: Model Comparison Bar Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bar chart comparing all models\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plot bars\n",
    "x_pos = np.arange(len(model_results_sorted))\n",
    "bars = ax.bar(x_pos, model_results_sorted['mean_accuracy'] * 100, \n",
    "              yerr=model_results_sorted['std_accuracy'] * 100,\n",
    "              capsize=5, alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "\n",
    "# Highlight the winner in different color\n",
    "bars[0].set_color('#FF6B6B')\n",
    "bars[0].set_alpha(1.0)\n",
    "\n",
    "# Labels and formatting\n",
    "ax.set_xlabel('Method', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Accuracy (%)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Comparison of Baseline Methods on PhysioNet Motor Imagery Dataset', \n",
    "             fontsize=14, fontweight='bold', pad=20)\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(model_results_sorted['model'], rotation=45, ha='right')\n",
    "ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "ax.set_ylim([75, 90])\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (bar, row) in enumerate(zip(bars, model_results_sorted.itertuples())):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "            f'{row.mean_accuracy*100:.2f}',\n",
    "            ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save figure\n",
    "fig_path = os.path.join(FIGURES_DIR, 'figure_model_comparison.pdf')\n",
    "plt.savefig(fig_path, format='pdf', bbox_inches='tight', dpi=300)\n",
    "plt.savefig(os.path.join(FIGURES_DIR, 'figure_model_comparison.png'), \n",
    "            format='png', bbox_inches='tight', dpi=300)\n",
    "\n",
    "print(f\"\\nModel comparison figure saved to: {fig_path}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 2: Retention Analysis Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create retention curve\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plot retention curve\n",
    "ax.plot(retention_results['k'], retention_results['mean_accuracy'] * 100,\n",
    "        marker='o', linewidth=2, markersize=8, label='Adaptive-Gating-EEG-ARNN (Gate Selection)',\n",
    "        color='#4ECDC4')\n",
    "\n",
    "# Fill error region\n",
    "ax.fill_between(retention_results['k'],\n",
    "                (retention_results['mean_accuracy'] - retention_results['std_accuracy']) * 100,\n",
    "                (retention_results['mean_accuracy'] + retention_results['std_accuracy']) * 100,\n",
    "                alpha=0.2, color='#4ECDC4')\n",
    "\n",
    "# Add baseline (64 channels)\n",
    "baseline_acc = model_results_sorted[model_results_sorted['model'] == 'Adaptive-Gating-EEG-ARNN']['mean_accuracy'].values[0]\n",
    "ax.axhline(y=baseline_acc * 100, color='red', linestyle='--', linewidth=2, \n",
    "           label=f'Full 64 channels ({baseline_acc*100:.2f}%)', alpha=0.7)\n",
    "\n",
    "# Labels and formatting\n",
    "ax.set_xlabel('Number of Channels (k)', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Accuracy (%)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Performance Retention with Channel Selection', \n",
    "             fontsize=14, fontweight='bold', pad=20)\n",
    "ax.grid(True, alpha=0.3, linestyle='--')\n",
    "ax.legend(fontsize=10, loc='lower right')\n",
    "ax.set_xlim([0, 70])\n",
    "ax.set_ylim([75, 90])\n",
    "\n",
    "# Annotate key points\n",
    "key_k_values = [10, 20, 30]\n",
    "for k in key_k_values:\n",
    "    row = retention_results[retention_results['k'] == k].iloc[0]\n",
    "    ax.annotate(f'k={k}\\n{row[\"mean_accuracy\"]*100:.2f}%',\n",
    "                xy=(k, row['mean_accuracy']*100),\n",
    "                xytext=(k, row['mean_accuracy']*100 - 3),\n",
    "                ha='center', fontsize=8,\n",
    "                bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.3))\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save figure\n",
    "retention_fig_path = os.path.join(FIGURES_DIR, 'figure_retention_curves.pdf')\n",
    "plt.savefig(retention_fig_path, format='pdf', bbox_inches='tight', dpi=300)\n",
    "plt.savefig(os.path.join(FIGURES_DIR, 'figure_retention_curves.png'),\n",
    "            format='png', bbox_inches='tight', dpi=300)\n",
    "\n",
    "print(f\"\\nRetention curve figure saved to: {retention_fig_path}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 3: Channel Selection Method Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different channel selection methods for Adaptive-Gating-EEG-ARNN\n",
    "adaptive_gating_cs = channel_selection_results[\n",
    "    channel_selection_results['model'] == 'Adaptive-Gating-EEG-ARNN'\n",
    "]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plot each method\n",
    "for method in ['edge', 'aggregation', 'gate']:\n",
    "    method_data = adaptive_gating_cs[adaptive_gating_cs['method'] == method]\n",
    "    ax.plot(method_data['k'], method_data['mean_accuracy'] * 100,\n",
    "            marker='o', linewidth=2, markersize=6, label=method.upper())\n",
    "    \n",
    "    # Add error bars\n",
    "    ax.fill_between(method_data['k'],\n",
    "                    (method_data['mean_accuracy'] - method_data['std_accuracy']) * 100,\n",
    "                    (method_data['mean_accuracy'] + method_data['std_accuracy']) * 100,\n",
    "                    alpha=0.15)\n",
    "\n",
    "# Labels and formatting\n",
    "ax.set_xlabel('Number of Channels (k)', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Accuracy (%)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Comparison of Channel Selection Methods\\n(Adaptive-Gating-EEG-ARNN)', \n",
    "             fontsize=14, fontweight='bold', pad=20)\n",
    "ax.grid(True, alpha=0.3, linestyle='--')\n",
    "ax.legend(fontsize=10, loc='lower right', title='Selection Method')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save figure\n",
    "cs_fig_path = os.path.join(FIGURES_DIR, 'figure_channel_selection_comparison.pdf')\n",
    "plt.savefig(cs_fig_path, format='pdf', bbox_inches='tight', dpi=300)\n",
    "plt.savefig(os.path.join(FIGURES_DIR, 'figure_channel_selection_comparison.png'),\n",
    "            format='png', bbox_inches='tight', dpi=300)\n",
    "\n",
    "print(f\"\\nChannel selection comparison figure saved to: {cs_fig_path}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics for Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary dictionary\n",
    "summary = {\n",
    "    'winner': {\n",
    "        'model': model_results_sorted.iloc[0]['model'],\n",
    "        'accuracy': float(model_results_sorted.iloc[0]['mean_accuracy']),\n",
    "        'std': float(model_results_sorted.iloc[0]['std_accuracy']),\n",
    "        'accuracy_pct': f\"{model_results_sorted.iloc[0]['mean_accuracy']*100:.2f}\",\n",
    "        'std_pct': f\"{model_results_sorted.iloc[0]['std_accuracy']*100:.2f}\"\n",
    "    },\n",
    "    'all_models': {},\n",
    "    'channel_selection': {\n",
    "        'best_method': None,\n",
    "        'best_k': None,\n",
    "        'best_accuracy': None\n",
    "    },\n",
    "    'retention': {\n",
    "        'channels_for_90pct_retention': None,\n",
    "        'accuracy_with_30_channels': None\n",
    "    }\n",
    "}\n",
    "\n",
    "# Add all model accuracies\n",
    "for _, row in model_results_sorted.iterrows():\n",
    "    summary['all_models'][row['model']] = {\n",
    "        'rank': int(row['rank']),\n",
    "        'accuracy': float(row['mean_accuracy']),\n",
    "        'std': float(row['std_accuracy']),\n",
    "        'accuracy_pct': f\"{row['mean_accuracy']*100:.2f}\",\n",
    "        'std_pct': f\"{row['std_accuracy']*100:.2f}\"\n",
    "    }\n",
    "\n",
    "# Best channel selection method\n",
    "best_cs = cs_summary.loc[cs_summary['mean_accuracy'].idxmax()]\n",
    "summary['channel_selection']['best_method'] = best_cs['method']\n",
    "summary['channel_selection']['best_k'] = int(best_cs['k'])\n",
    "summary['channel_selection']['best_accuracy'] = f\"{best_cs['mean_accuracy']*100:.2f}\"\n",
    "\n",
    "# Retention analysis\n",
    "baseline_acc = model_results_sorted.iloc[0]['mean_accuracy']\n",
    "target_acc = baseline_acc * 0.9  # 90% retention\n",
    "\n",
    "retention_90pct = retention_results[retention_results['mean_accuracy'] >= target_acc]\n",
    "if len(retention_90pct) > 0:\n",
    "    min_k = retention_90pct['k'].min()\n",
    "    summary['retention']['channels_for_90pct_retention'] = int(min_k)\n",
    "\n",
    "acc_30 = retention_results[retention_results['k'] == 30]\n",
    "if len(acc_30) > 0:\n",
    "    summary['retention']['accuracy_with_30_channels'] = f\"{acc_30.iloc[0]['mean_accuracy']*100:.2f}\"\n",
    "\n",
    "# Save summary\n",
    "summary_path = os.path.join(RESULTS_DIR, 'paper_summary.json')\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(\"\\nPaper Summary:\")\n",
    "print(json.dumps(summary, indent=2))\n",
    "print(f\"\\nSummary saved to: {summary_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Ranking Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final ranking CSV for easy reference\n",
    "final_ranking = model_results_sorted[['rank', 'model', 'mean_accuracy', 'std_accuracy']].copy()\n",
    "final_ranking['accuracy_pct'] = final_ranking['mean_accuracy'] * 100\n",
    "final_ranking['std_pct'] = final_ranking['std_accuracy'] * 100\n",
    "\n",
    "ranking_path = os.path.join(RESULTS_DIR, 'final_ranking.csv')\n",
    "final_ranking.to_csv(ranking_path, index=False)\n",
    "\n",
    "print(\"\\nFinal Model Ranking:\")\n",
    "print(final_ranking[['rank', 'model', 'accuracy_pct', 'std_pct']])\n",
    "print(f\"\\nRanking saved to: {ranking_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verification\n",
    "\n",
    "Verify that all outputs are ready for the paper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check all required files\n",
    "required_files = [\n",
    "    ('results/table_ii_model_comparison.tex', 'LaTeX Table II - Model Comparison'),\n",
    "    ('results/table_iii_retention.tex', 'LaTeX Table III - Retention Analysis'),\n",
    "    ('figures/figure_model_comparison.pdf', 'Figure 1 - Model Comparison'),\n",
    "    ('figures/figure_retention_curves.pdf', 'Figure 2 - Retention Curves'),\n",
    "    ('figures/figure_channel_selection_comparison.pdf', 'Figure 3 - Channel Selection Methods'),\n",
    "    ('results/paper_summary.json', 'Paper Summary Statistics'),\n",
    "    ('results/final_ranking.csv', 'Final Model Ranking'),\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"VERIFICATION: Paper-Ready Outputs\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "all_present = True\n",
    "for filepath, description in required_files:\n",
    "    if os.path.exists(filepath):\n",
    "        file_size = os.path.getsize(filepath)\n",
    "        print(f\"[OK] {description}\")\n",
    "        print(f\"     Path: {filepath}\")\n",
    "        print(f\"     Size: {file_size:,} bytes\\n\")\n",
    "    else:\n",
    "        print(f\"[MISSING] {description}\")\n",
    "        print(f\"          Expected: {filepath}\\n\")\n",
    "        all_present = False\n",
    "\n",
    "if all_present:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SUCCESS: All paper outputs generated successfully!\")\n",
    "    print(\"=\"*60)\n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"WARNING: Some outputs are missing!\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Findings Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"KEY FINDINGS FOR PAPER\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "winner = summary['winner']\n",
    "print(f\"1. BEST MODEL: {winner['model']}\")\n",
    "print(f\"   Accuracy: {winner['accuracy_pct']}% ± {winner['std_pct']}%\\n\")\n",
    "\n",
    "print(f\"2. CHANNEL SELECTION:\")\n",
    "print(f\"   Best Method: {summary['channel_selection']['best_method'].upper()}\")\n",
    "print(f\"   Optimal k: {summary['channel_selection']['best_k']}\")\n",
    "print(f\"   Accuracy: {summary['channel_selection']['best_accuracy']}%\\n\")\n",
    "\n",
    "if summary['retention']['channels_for_90pct_retention']:\n",
    "    print(f\"3. RETENTION:\")\n",
    "    print(f\"   90% retention achieved with: {summary['retention']['channels_for_90pct_retention']} channels\")\n",
    "    print(f\"   Accuracy with 30 channels: {summary['retention']['accuracy_with_30_channels']}%\")\n",
    "    reduction = (1 - 30/64) * 100\n",
    "    print(f\"   Channel reduction: {reduction:.1f}%\\n\")\n",
    "\n",
    "print(f\"4. MODEL RANKING:\")\n",
    "for i, row in enumerate(model_results_sorted.itertuples(), 1):\n",
    "    print(f\"   {i}. {row.model}: {row.mean_accuracy*100:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"All results ready for paper submission!\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelnel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
