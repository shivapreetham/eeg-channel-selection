{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13546721,"sourceType":"datasetVersion","datasetId":8603408}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# PhysioNet EEG: Train All Baseline Models\n\nThis notebook trains 7 different models on the PhysioNet Motor Imagery dataset:\n1. FBCSP (Filter Bank Common Spatial Patterns)\n2. CNN-SAE (CNN with Spatial Attention)\n3. EEGNet\n4. ACS-SE-CNN (Adaptive Channel Selection SE-CNN)\n5. G-CARM (Graph Channel Active Reasoning Module)\n6. Baseline EEG-ARNN (without adaptive gating)\n7. Adaptive Gating EEG-ARNN (our proposed method)\n\n**Expected Runtime**: 10-12 hours on Kaggle GPU\n\n**Input**: `/kaggle/input/physionet-preprocessed/derived/` (preprocessed EEG data)\n\n**Output**: \n- Trained models in `models/` folder\n- Results CSV files in `results/` folder","metadata":{}},{"cell_type":"markdown","source":"## Configuration","metadata":{}},{"cell_type":"code","source":"import os\nimport math\nfrom copy import deepcopy\n\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport mne\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nimport pickle\nfrom tqdm.auto import tqdm\nimport warnings\nwarnings.filterwarnings('ignore')\n\nmne.set_log_level('ERROR')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T17:08:15.636425Z","iopub.execute_input":"2025-11-15T17:08:15.636688Z","iopub.status.idle":"2025-11-15T17:08:21.041240Z","shell.execute_reply.started":"2025-11-15T17:08:15.636659Z","shell.execute_reply":"2025-11-15T17:08:21.040644Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Configuration\nCONFIG = {\n    'data_path': '/kaggle/input/eeg-preprocessed-data/derived',  # Change this for local testing\n    'output_dir': './',\n    'results_dir': './results',\n    'models_dir': './models',\n    'figures_dir': './figures',\n\n    'n_folds': 3,\n    'random_seed': 42,\n    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n\n    # Training hyperparameters\n    'batch_size': 64,\n    'epochs': 20,\n    'learning_rate': 0.001,\n    'weight_decay': 1e-4,\n    'patience': 10,\n    'scheduler_patience': 3,\n    'use_early_stopping': False,\n\n    # Data parameters\n    'n_channels': 64,\n    'n_classes': 2,\n    'sfreq': 128,\n    'tmin': 0.0,\n    'tmax': 4.0,\n    'n_timepoints': 513,  # 4 seconds at 128 Hz + 1\n    'hidden_dim': 128,\n    'mi_runs': [7, 8, 11, 12],\n\n    # FBCSP parameters\n    'fbcsp_bands': [(4, 8), (8, 12), (12, 16), (16, 20), (20, 24), (24, 28), (28, 32), (32, 36), (36, 40)],\n    'fbcsp_n_components': 4,\n\n    # Gating regularization\n    'gating': {\n        'gate_init': 0.9,\n        'l1_lambda': 1e-3,\n    },\n}\n\n# Create output directories\nos.makedirs(CONFIG['results_dir'], exist_ok=True)\nos.makedirs(CONFIG['models_dir'], exist_ok=True)\nos.makedirs(CONFIG['figures_dir'], exist_ok=True)\n\n# Set random seeds\nnp.random.seed(CONFIG['random_seed'])\ntorch.manual_seed(CONFIG['random_seed'])\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(CONFIG['random_seed'])\n\nprint(f\"Device: {CONFIG['device']}\")\nprint(f\"Data path: {CONFIG['data_path']}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T17:08:21.042204Z","iopub.execute_input":"2025-11-15T17:08:21.042606Z","iopub.status.idle":"2025-11-15T17:08:21.110145Z","shell.execute_reply.started":"2025-11-15T17:08:21.042587Z","shell.execute_reply":"2025-11-15T17:08:21.109490Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\nData path: /kaggle/input/eeg-preprocessed-data/derived\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## Data Loading Utilities","metadata":{}},{"cell_type":"code","source":"def load_physionet_data(data_path, subject_ids=None):\n    \"\"\"\n    Load preprocessed PhysioNet motor imagery data from the derived folder.\n    Supports both the newer folder structure (derived/preprocessed/S***/S***R**_preproc_raw.fif)\n    and the legacy flat directory containing epoch files.\n    \"\"\"\n    data_root = os.path.abspath(data_path)\n    if not os.path.isdir(data_root):\n        raise FileNotFoundError(f\"Data path not found: {data_root}\")\n\n    config = globals().get('CONFIG', {})\n    tmin = float(config.get('tmin', 0.0))\n    tmax = float(config.get('tmax', 4.0))\n    mi_runs = [int(r) for r in config.get('mi_runs', [7, 8, 11, 12])]\n    event_id = {'T1': 1, 'T2': 2}\n\n    def normalize_subject(value):\n        if value is None:\n            return None\n        if isinstance(value, str) and value.upper().startswith('S'):\n            value = value[1:]\n        try:\n            return int(value)\n        except Exception:\n            return None\n\n    subject_filter = None\n    if subject_ids is not None:\n        subject_filter = set()\n        for sid in subject_ids:\n            norm = normalize_subject(sid)\n            if norm is not None:\n                subject_filter.add(norm)\n        if not subject_filter:\n            subject_filter = None\n\n    def aggregate_results(blocks_X, blocks_y, blocks_subjects):\n        X = np.concatenate(blocks_X, axis=0)\n        y = np.concatenate(blocks_y, axis=0)\n        subjects = np.concatenate(blocks_subjects, axis=0)\n        print(f\"Loaded {len(X)} trials from {len(np.unique(subjects))} subjects\")\n        print(f\"Data shape: {X.shape}\")\n        print(f\"Label distribution: {np.bincount(y)}\")\n        return X, y, subjects\n\n    subject_root = data_root\n    preprocessed_dir = os.path.join(data_root, 'preprocessed')\n    if os.path.isdir(preprocessed_dir):\n        subject_root = preprocessed_dir\n    subject_dirs = [d for d in sorted(os.listdir(subject_root))\n                    if os.path.isdir(os.path.join(subject_root, d)) and d.upper().startswith('S')]\n\n    all_X, all_y, all_subjects = [], [], []\n    if subject_dirs:\n        print(f\"Detected {len(subject_dirs)} preprocessed subject folders under {subject_root}\")\n        label_map = {event_id['T1']: 0, event_id['T2']: 1}\n        for subject_dir in subject_dirs:\n            subject_numeric = normalize_subject(subject_dir)\n            if subject_filter and subject_numeric not in subject_filter:\n                continue\n            subject_path = os.path.join(subject_root, subject_dir)\n            for run_id in mi_runs:\n                candidate_names = [\n                    f\"{subject_dir}R{run_id:02d}_preproc_raw.fif\",\n                    f\"{subject_dir}R{run_id:02d}_raw.fif\",\n                    f\"{subject_dir}R{run_id:02d}.fif\",\n                    f\"{subject_dir}_R{run_id:02d}.fif\",\n                ]\n                run_path = None\n                for name in candidate_names:\n                    candidate = os.path.join(subject_path, name)\n                    if os.path.exists(candidate):\n                        run_path = candidate\n                        break\n                if run_path is None:\n                    continue\n                try:\n                    raw = mne.io.read_raw_fif(run_path, preload=True, verbose=False)\n                except Exception as e:\n                    print(f\"Error loading {run_path}: {e}\")\n                    continue\n                picks = mne.pick_types(raw.info, eeg=True, meg=False, stim=False, eog=False)\n                if len(picks) == 0:\n                    continue\n                try:\n                    events, _ = mne.events_from_annotations(raw, event_id=event_id)\n                except Exception as e:\n                    print(f\"Error parsing annotations for {run_path}: {e}\")\n                    continue\n                if len(events) == 0:\n                    continue\n                try:\n                    epochs = mne.Epochs(raw, events, event_id=event_id, tmin=tmin, tmax=tmax,\n                                        baseline=None, preload=True, picks=picks, verbose=False)\n                except Exception as e:\n                    print(f\"Error epoching {run_path}: {e}\")\n                    continue\n                data = epochs.get_data()\n                labels = epochs.events[:, 2]\n                mapped = np.array([label_map.get(lbl, -1) for lbl in labels])\n                valid_mask = mapped >= 0\n                if not np.any(valid_mask):\n                    continue\n                all_X.append(data[valid_mask])\n                all_y.append(mapped[valid_mask])\n                subj_label = subject_numeric if subject_numeric is not None else -1\n                all_subjects.append(np.full(np.sum(valid_mask), subj_label))\n        if all_X:\n            return aggregate_results(all_X, all_y, all_subjects)\n        print(\"No data loaded from preprocessed folders, falling back to legacy format...\")\n\n    # Legacy format fallback (flat directory with epoch files)\n    legacy_files = [f for f in os.listdir(data_root) if f.endswith('.fif')]\n    if not legacy_files:\n        raise ValueError(\n            \"No valid PhysioNet files found. Ensure the derived folder contains either \"\n            \"preprocessed subject subfolders or .fif epoch files.\"\n        )\n    if subject_filter:\n        filtered = []\n        for fname in legacy_files:\n            parts = fname.split('_')\n            if not parts:\n                continue\n            subj = normalize_subject(parts[0])\n            if subj is not None and subj in subject_filter:\n                filtered.append(fname)\n        legacy_files = filtered\n        if not legacy_files:\n            raise ValueError(\"No files matched the requested subject IDs in legacy format.\")\n\n    print(f\"Found {len(legacy_files)} legacy epoch files. Loading...\")\n    for fname in legacy_files:\n        filepath = os.path.join(data_root, fname)\n        try:\n            epochs = mne.read_epochs(filepath, preload=True, verbose=False)\n        except Exception as e:\n            print(f\"Error loading {filepath}: {e}\")\n            continue\n        current_event_id = epochs.event_id\n        if not current_event_id:\n            continue\n        label_lookup = {}\n        if 'T1' in current_event_id:\n            label_lookup[current_event_id['T1']] = 0\n        if 'T2' in current_event_id:\n            label_lookup[current_event_id['T2']] = 1\n        if not label_lookup:\n            continue\n        labels = np.array([label_lookup.get(epochs.events[i, -1], -1) for i in range(len(epochs))])\n        valid = labels >= 0\n        if not np.any(valid):\n            continue\n        data = epochs.get_data()[valid]\n        labels = labels[valid]\n        subj = normalize_subject(fname.split('_')[0])\n        subj_arr = np.full(len(labels), subj if subj is not None else -1)\n        all_X.append(data)\n        all_y.append(labels)\n        all_subjects.append(subj_arr)\n    if not all_X:\n        raise ValueError(\"No valid trials were loaded from the provided PhysioNet files.\")\n    return aggregate_results(all_X, all_y, all_subjects)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T17:08:21.111162Z","iopub.execute_input":"2025-11-15T17:08:21.111413Z","iopub.status.idle":"2025-11-15T17:08:21.129988Z","shell.execute_reply.started":"2025-11-15T17:08:21.111390Z","shell.execute_reply":"2025-11-15T17:08:21.129294Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"## Model Architectures","metadata":{}},{"cell_type":"code","source":"# Model 1: FBCSP\nclass FBCSP:\n    \"\"\"Filter Bank Common Spatial Patterns with LDA classifier.\"\"\"\n    def __init__(self, freq_bands, n_components=4, sfreq=128):\n        self.freq_bands = freq_bands\n        self.n_components = n_components\n        self.sfreq = sfreq\n        self.csp_list = []\n        self.classifier = None\n        \n    def fit(self, X, y):\n        \"\"\"X: (n_trials, n_channels, n_timepoints), y: (n_trials,)\"\"\"\n        from mne.decoding import CSP\n        \n        all_features = []\n        \n        for low, high in self.freq_bands:\n            # Filter data\n            X_filtered = self._bandpass_filter(X, low, high)\n            \n            # Apply CSP\n            csp = CSP(n_components=self.n_components, reg=None, log=True, norm_trace=False)\n            features = csp.fit_transform(X_filtered, y)\n            \n            self.csp_list.append(csp)\n            all_features.append(features)\n        \n        # Concatenate features from all bands\n        all_features = np.concatenate(all_features, axis=1)\n        \n        # Train LDA classifier\n        self.classifier = LinearDiscriminantAnalysis()\n        self.classifier.fit(all_features, y)\n        \n        return self\n    \n    def predict(self, X):\n        all_features = []\n        \n        for idx, (low, high) in enumerate(self.freq_bands):\n            X_filtered = self._bandpass_filter(X, low, high)\n            features = self.csp_list[idx].transform(X_filtered)\n            all_features.append(features)\n        \n        all_features = np.concatenate(all_features, axis=1)\n        return self.classifier.predict(all_features)\n    \n    def score(self, X, y):\n        predictions = self.predict(X)\n        return np.mean(predictions == y)\n    \n    def _bandpass_filter(self, X, low, high):\n        \"\"\"Apply bandpass filter to data.\"\"\"\n        from scipy.signal import butter, filtfilt\n        \n        nyq = self.sfreq / 2\n        low_norm = low / nyq\n        high_norm = high / nyq\n        \n        b, a = butter(4, [low_norm, high_norm], btype='band')\n        \n        X_filtered = np.zeros_like(X)\n        for i in range(X.shape[0]):\n            for j in range(X.shape[1]):\n                X_filtered[i, j, :] = filtfilt(b, a, X[i, j, :])\n        \n        return X_filtered","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T17:08:21.131342Z","iopub.execute_input":"2025-11-15T17:08:21.131900Z","iopub.status.idle":"2025-11-15T17:08:21.147221Z","shell.execute_reply.started":"2025-11-15T17:08:21.131882Z","shell.execute_reply":"2025-11-15T17:08:21.146642Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Model 2: CNN-SAE (CNN with Spatial Attention)\nclass SpatialAttention(nn.Module):\n    def __init__(self, n_channels):\n        super().__init__()\n        self.attention = nn.Sequential(\n            nn.Linear(n_channels, n_channels // 4),\n            nn.ReLU(),\n            nn.Linear(n_channels // 4, n_channels),\n            nn.Sigmoid()\n        )\n    \n    def forward(self, x):\n        # x: (batch, channels, time)\n        pooled = torch.mean(x, dim=2)  # (batch, channels)\n        weights = self.attention(pooled)  # (batch, channels)\n        return x * weights.unsqueeze(2)  # (batch, channels, time)\n\nclass CNNSAE(nn.Module):\n    def __init__(self, n_channels=64, n_classes=2, n_timepoints=513):\n        super().__init__()\n        \n        self.spatial_attention = SpatialAttention(n_channels)\n        \n        self.conv1 = nn.Conv1d(n_channels, 64, kernel_size=5, padding=2)\n        self.bn1 = nn.BatchNorm1d(64)\n        self.pool1 = nn.MaxPool1d(2)\n        \n        self.conv2 = nn.Conv1d(64, 128, kernel_size=5, padding=2)\n        self.bn2 = nn.BatchNorm1d(128)\n        self.pool2 = nn.MaxPool1d(2)\n        \n        self.conv3 = nn.Conv1d(128, 256, kernel_size=3, padding=1)\n        self.bn3 = nn.BatchNorm1d(256)\n        self.pool3 = nn.MaxPool1d(2)\n        \n        self.dropout = nn.Dropout(0.5)\n        \n        # Calculate flattened size\n        test_input = torch.zeros(1, n_channels, n_timepoints)\n        test_output = self._forward_features(test_input)\n        flattened_size = test_output.view(1, -1).size(1)\n        \n        self.fc1 = nn.Linear(flattened_size, 256)\n        self.fc2 = nn.Linear(256, n_classes)\n    \n    def _forward_features(self, x):\n        x = self.spatial_attention(x)\n        \n        x = self.pool1(torch.relu(self.bn1(self.conv1(x))))\n        x = self.pool2(torch.relu(self.bn2(self.conv2(x))))\n        x = self.pool3(torch.relu(self.bn3(self.conv3(x))))\n        \n        return x\n    \n    def forward(self, x):\n        x = self._forward_features(x)\n        x = x.view(x.size(0), -1)\n        x = self.dropout(x)\n        x = torch.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.fc2(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T17:08:21.147845Z","iopub.execute_input":"2025-11-15T17:08:21.148057Z","iopub.status.idle":"2025-11-15T17:08:21.162763Z","shell.execute_reply.started":"2025-11-15T17:08:21.148036Z","shell.execute_reply":"2025-11-15T17:08:21.162157Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"**Updated EEGNet**","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass EEGNet(nn.Module):\n    def __init__(self, n_channels=64, n_classes=2, n_timepoints=513, F1=8, D=2, F2=16):\n        super().__init__()\n\n        # Temporal convolution\n        self.conv1 = nn.Conv2d(1, F1, (1, 64), padding=(0, 32), bias=False)\n        self.bn1 = nn.BatchNorm2d(F1)\n\n        # Depthwise convolution\n        self.conv2 = nn.Conv2d(F1, F1 * D, (n_channels, 1), groups=F1, bias=False)\n        self.bn2 = nn.BatchNorm2d(F1 * D)\n        self.pool1 = nn.AvgPool2d((1, 4))\n        self.dropout1 = nn.Dropout(0.5)\n\n        # Separable convolution\n        self.conv3 = nn.Conv2d(F1 * D, F2, (1, 16), padding=(0, 8), bias=False)\n        self.bn3 = nn.BatchNorm2d(F2)\n        self.pool2 = nn.AvgPool2d((1, 8))\n        self.dropout2 = nn.Dropout(0.5)\n\n        # Calculate flattened size\n        test_input = torch.zeros(1, 1, n_channels, n_timepoints)\n        test_output = self._forward_features(test_input)\n        flattened_size = test_output.view(1, -1).size(1)\n\n        self.fc = nn.Linear(flattened_size, n_classes)\n\n    def _forward_features(self, x):\n        x = self.bn1(self.conv1(x))\n        # REPLACED torch.elu(...) with F.elu(...)\n        x = self.dropout1(self.pool1(F.elu(self.bn2(self.conv2(x)))))\n        x = self.dropout2(self.pool2(F.elu(self.bn3(self.conv3(x)))))\n        return x\n\n    def forward(self, x):\n        # Input: (batch, channels, time)\n        x = x.unsqueeze(1)  # (batch, 1, channels, time)\n        x = self._forward_features(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T18:56:36.604727Z","iopub.execute_input":"2025-11-15T18:56:36.604996Z","iopub.status.idle":"2025-11-15T18:56:36.613288Z","shell.execute_reply.started":"2025-11-15T18:56:36.604976Z","shell.execute_reply":"2025-11-15T18:56:36.612416Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"m = EEGNet(n_channels=64, n_classes=2, n_timepoints=513)\nprint(\"Instantiated EEGNet OK. Params:\", sum(p.numel() for p in m.parameters()))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T18:56:58.561226Z","iopub.execute_input":"2025-11-15T18:56:58.561819Z","iopub.status.idle":"2025-11-15T18:56:58.588466Z","shell.execute_reply.started":"2025-11-15T18:56:58.561795Z","shell.execute_reply":"2025-11-15T18:56:58.587864Z"}},"outputs":[{"name":"stdout","text":"Instantiated EEGNet OK. Params: 6226\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"# Model 4: ACS-SE-CNN (Adaptive Channel Selection with Squeeze-Excitation CNN)\nclass SEBlock(nn.Module):\n    \"\"\"Squeeze-and-Excitation block for channel attention.\"\"\"\n    def __init__(self, channels, reduction=4):\n        super().__init__()\n        hidden = max(1, channels // reduction)\n        self.fc1 = nn.Linear(channels, channels // reduction)\n        self.fc2 = nn.Linear(channels // reduction, channels)\n    \n    def forward(self, x):\n        # x: (batch, channels, time)xx\n        squeeze = torch.mean(x, dim=2)  # (batch, channels)\n        excitation = torch.relu(self.fc1(squeeze))\n        excitation = torch.sigmoid(self.fc2(excitation))  # (batch, channels)\n        return x * excitation.unsqueeze(2)\n\nclass ACSECNN(nn.Module):\n    def __init__(self, n_channels=64, n_classes=2, n_timepoints=513):\n        super().__init__()\n        \n        # Channel selection module\n        self.channel_attention = nn.Sequential(\n            nn.Linear(n_timepoints, 128),\n            nn.ReLU(),\n            nn.Linear(128, 1),\n            nn.Sigmoid()\n        )\n        \n        # SE blocks\n        self.se1 = SEBlock(n_channels)\n        self.se2 = SEBlock(128)\n        self.se3 = SEBlock(256)\n        \n        # Convolutional layers\n        self.conv1 = nn.Conv1d(n_channels, 128, kernel_size=5, padding=2)\n        self.bn1 = nn.BatchNorm1d(128)\n        self.pool1 = nn.MaxPool1d(2)\n        \n        self.conv2 = nn.Conv1d(128, 256, kernel_size=5, padding=2)\n        self.bn2 = nn.BatchNorm1d(256)\n        self.pool2 = nn.MaxPool1d(2)\n        \n        self.conv3 = nn.Conv1d(256, 512, kernel_size=3, padding=1)\n        self.bn3 = nn.BatchNorm1d(512)\n        self.pool3 = nn.MaxPool1d(2)\n        \n        self.dropout = nn.Dropout(0.5)\n        \n        # Calculate flattened size\n        test_input = torch.zeros(1, n_channels, n_timepoints)\n        test_output = self._forward_features(test_input)\n        flattened_size = test_output.view(1, -1).size(1)\n        \n        self.fc1 = nn.Linear(flattened_size, 256)\n        self.fc2 = nn.Linear(256, n_classes)\n        \n        self.channel_weights = None\n    \n    def _forward_features(self, x):\n        # Adaptive channel selection\n        channel_weights = []\n        for i in range(x.size(1)):\n            w = self.channel_attention(x[:, i, :])  # (batch, 1)\n            channel_weights.append(w)\n        channel_weights = torch.cat(channel_weights, dim=1)  # (batch, n_channels)\n        self.channel_weights = channel_weights.detach()\n        \n        x = x * channel_weights.unsqueeze(2)  # (batch, n_channels, time)\n        \n        # SE-CNN\n        x = self.se1(x)\n        x = self.pool1(torch.relu(self.bn1(self.conv1(x))))\n        \n        x = self.se2(x)\n        x = self.pool2(torch.relu(self.bn2(self.conv2(x))))\n        \n        x = self.se3(x)\n        x = self.pool3(torch.relu(self.bn3(self.conv3(x))))\n        \n        return x\n    \n    def forward(self, x):\n        x = self._forward_features(x)\n        x = x.view(x.size(0), -1)\n        x = self.dropout(x)\n        x = torch.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.fc2(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T17:08:21.179241Z","iopub.execute_input":"2025-11-15T17:08:21.179504Z","iopub.status.idle":"2025-11-15T17:08:21.196114Z","shell.execute_reply.started":"2025-11-15T17:08:21.179488Z","shell.execute_reply":"2025-11-15T17:08:21.195452Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Model 5: G-CARM (Graph Channel Active Reasoning Module)\nclass CARMBlock(nn.Module):\n    \"\"\"Channel Active Reasoning Module with graph convolution.\"\"\"\n    def __init__(self, n_channels):\n        super().__init__()\n        self.n_channels = n_channels\n        \n        # Learnable adjacency matrix\n        self.A = nn.Parameter(torch.randn(n_channels, n_channels) * 0.01)\n        \n        # Layer normalization\n        self.norm = nn.LayerNorm(n_channels)\n    \n    def forward(self, x):\n        # x: (batch, channels, time)\n        batch_size, n_channels, n_time = x.shape\n        \n        # Normalize adjacency matrix\n        A_norm = torch.softmax(self.A, dim=1)\n        \n        # Apply graph convolution\n        x_reshaped = x.permute(0, 2, 1)  # (batch, time, channels)\n        x_graph = torch.matmul(x_reshaped, A_norm.t())  # (batch, time, channels)\n        x_graph = x_graph.permute(0, 2, 1)  # (batch, channels, time)\n        \n        return x_graph\n    \n    def get_adjacency_matrix(self):\n        \"\"\"Return normalized adjacency matrix for channel selection.\"\"\"\n        return torch.softmax(self.A, dim=1).detach()\n\nclass GCARM(nn.Module):\n    def __init__(self, n_channels=64, n_classes=2, n_timepoints=513):\n        super().__init__()\n        \n        # CARM blocks\n        self.carm1 = CARMBlock(n_channels)\n        self.carm2 = CARMBlock(n_channels)\n        \n        # Convolutional layers\n        self.conv1 = nn.Conv1d(n_channels, 128, kernel_size=5, padding=2)\n        self.bn1 = nn.BatchNorm1d(128)\n        self.pool1 = nn.MaxPool1d(2)\n        \n        self.conv2 = nn.Conv1d(128, 256, kernel_size=5, padding=2)\n        self.bn2 = nn.BatchNorm1d(256)\n        self.pool2 = nn.MaxPool1d(2)\n        \n        self.conv3 = nn.Conv1d(256, 512, kernel_size=3, padding=1)\n        self.bn3 = nn.BatchNorm1d(512)\n        self.pool3 = nn.MaxPool1d(2)\n        \n        self.dropout = nn.Dropout(0.5)\n        \n        # Calculate flattened size\n        test_input = torch.zeros(1, n_channels, n_timepoints)\n        test_output = self._forward_features(test_input)\n        flattened_size = test_output.view(1, -1).size(1)\n        \n        self.fc1 = nn.Linear(flattened_size, 256)\n        self.fc2 = nn.Linear(256, n_classes)\n    \n    def _forward_features(self, x):\n        # Apply CARM blocks\n        x = self.carm1(x)\n        x = self.carm2(x)\n        \n        # CNN layers\n        x = self.pool1(torch.relu(self.bn1(self.conv1(x))))\n        x = self.pool2(torch.relu(self.bn2(self.conv2(x))))\n        x = self.pool3(torch.relu(self.bn3(self.conv3(x))))\n        \n        return x\n    \n    def forward(self, x):\n        x = self._forward_features(x)\n        x = x.view(x.size(0), -1)\n        x = self.dropout(x)\n        x = torch.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.fc2(x)\n        return x\n    \n    def get_channel_importance_edge(self):\n        \"\"\"Edge Selection: Sum of outgoing edge weights.\"\"\"\n        A1 = self.carm1.get_adjacency_matrix()\n        A2 = self.carm2.get_adjacency_matrix()\n        A_combined = (A1 + A2) / 2\n        return torch.sum(A_combined, dim=1).cpu().numpy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T17:08:21.196747Z","iopub.execute_input":"2025-11-15T17:08:21.196925Z","iopub.status.idle":"2025-11-15T17:08:21.209400Z","shell.execute_reply.started":"2025-11-15T17:08:21.196911Z","shell.execute_reply":"2025-11-15T17:08:21.208717Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Model 6 & 7: EEG-ARNN (Baseline and Adaptive Gating versions)\nclass GraphConvLayer(nn.Module):\n    \"\"\"Graph convolution with learned symmetric adjacency.\"\"\"\n    def __init__(self, num_channels, hidden_dim):\n        super().__init__()\n        self.num_channels = num_channels\n        self.hidden_dim = hidden_dim\n        self.A = nn.Parameter(torch.randn(num_channels, num_channels) * 0.01)\n        self.theta = nn.Linear(hidden_dim, hidden_dim, bias=False)\n        self.bn = nn.BatchNorm2d(hidden_dim)\n        self.act = nn.ELU()\n\n    def forward(self, x):\n        B, H, C, T = x.shape\n        A = torch.sigmoid(self.A)\n        A = 0.5 * (A + A.t())\n        I = torch.eye(C, device=A.device)\n        A_hat = A + I\n        D = torch.diag(torch.pow(A_hat.sum(1).clamp_min(1e-6), -0.5))\n        A_norm = D @ A_hat @ D\n\n        x_perm = x.permute(0, 3, 2, 1).contiguous().view(B * T, C, H)\n        x_g = A_norm @ x_perm\n        x_g = self.theta(x_g)\n        x_g = x_g.view(B, T, C, H).permute(0, 3, 2, 1)\n        x_out = self.bn(x_g)\n        return self.act(x_out)\n\n    def get_adjacency(self):\n        with torch.no_grad():\n            A = torch.sigmoid(self.A)\n            A = 0.5 * (A + A.t())\n            return A.cpu().numpy()\n\n\nclass TemporalConv(nn.Module):\n    \"\"\"Temporal convolution operating independently per channel.\"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size=16, pool=True):\n        super().__init__()\n        self.pool = pool\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=(1, kernel_size),\n                              padding=(0, kernel_size // 2), bias=False)\n        self.bn = nn.BatchNorm2d(out_channels)\n        self.act = nn.ELU()\n        self.pool_layer = nn.AvgPool2d(kernel_size=(1, 2)) if pool else None\n\n    def forward(self, x):\n        x = self.act(self.bn(self.conv(x)))\n        if self.pool_layer is not None:\n            x = self.pool_layer(x)\n        return x\n\n\nclass BaselineEEGARNN(nn.Module):\n    \"\"\"Baseline EEG-ARNN with temporal conv + adaptive graph reasoning.\"\"\"\n    def __init__(self, n_channels=64, n_classes=2, n_timepoints=513, hidden_dim=128):\n        super().__init__()\n        self.n_channels = n_channels\n        self.hidden_dim = hidden_dim\n        self.use_gate_regularizer = False\n        self.gate_penalty_tensor = None\n        self.latest_gate_values = None\n\n        self.t1 = TemporalConv(1, hidden_dim, 16, pool=False)\n        self.g1 = GraphConvLayer(n_channels, hidden_dim)\n        self.t2 = TemporalConv(hidden_dim, hidden_dim, 16, pool=True)\n        self.g2 = GraphConvLayer(n_channels, hidden_dim)\n        self.t3 = TemporalConv(hidden_dim, hidden_dim, 16, pool=True)\n        self.g3 = GraphConvLayer(n_channels, hidden_dim)\n\n        with torch.no_grad():\n            dummy = torch.zeros(1, n_channels, n_timepoints)\n            feat = self._forward_features(self._prepare_input(dummy))\n            self.feature_dim = feat.view(1, -1).size(1)\n\n        self.fc1 = nn.Linear(self.feature_dim, 256)\n        self.dropout = nn.Dropout(0.5)\n        self.fc2 = nn.Linear(256, n_classes)\n\n    def _prepare_input(self, x):\n        if x.dim() == 3:\n            x = x.unsqueeze(1)\n        return x\n\n    def _forward_features(self, x):\n        x = self.g1(self.t1(x))\n        x = self.g2(self.t2(x))\n        x = self.g3(self.t3(x))\n        return x\n\n    def _forward_from_prepared(self, x):\n        features = self._forward_features(x)\n        x = features.view(features.size(0), -1)\n        x = F.relu(self.fc1(x))\n        x = self.dropout(x)\n        return self.fc2(x)\n\n    def forward(self, x):\n        prepared = self._prepare_input(x)\n        self.gate_penalty_tensor = None\n        self.latest_gate_values = None\n        return self._forward_from_prepared(prepared)\n\n    def get_final_adjacency(self):\n        return self.g3.get_adjacency()\n\n    def get_channel_importance_edge(self):\n        adjacency = self.get_final_adjacency()\n        return np.sum(adjacency, axis=1)\n\n\nclass AdaptiveGatingEEGARNN(BaselineEEGARNN):\n    \"\"\"EEG-ARNN with adaptive data-dependent channel gates.\"\"\"\n    def __init__(self, n_channels=64, n_classes=2, n_timepoints=513, hidden_dim=128, gate_init=0.9):\n        super().__init__(n_channels, n_classes, n_timepoints, hidden_dim)\n        self.use_gate_regularizer = True\n        self.gate_net = nn.Sequential(\n            nn.Linear(n_channels * 2, n_channels),\n            nn.ReLU(),\n            nn.Linear(n_channels, n_channels),\n            nn.Sigmoid()\n        )\n        init_value = float(np.clip(gate_init, 1e-3, 1 - 1e-3))\n        init_bias = math.log(init_value / (1.0 - init_value))\n        with torch.no_grad():\n            self.gate_net[-2].bias.fill_(init_bias)\n        self.latest_gate_values = None\n\n    def compute_gates(self, x):\n        x_s = x.squeeze(1)\n        ch_mean = x_s.mean(dim=2)\n        ch_std = x_s.std(dim=2)\n        stats = torch.cat([ch_mean, ch_std], dim=1)\n        return self.gate_net(stats)\n\n    def forward(self, x):\n        prepared = self._prepare_input(x)\n        gates = self.compute_gates(prepared)\n        self.gate_penalty_tensor = gates\n        self.latest_gate_values = gates.detach()\n        gated = prepared * gates.view(gates.size(0), 1, gates.size(1), 1)\n        return self._forward_from_prepared(gated)\n\n    def get_channel_importance_gate(self):\n        if self.latest_gate_values is None:\n            return None\n        return self.latest_gate_values.mean(dim=0).cpu().numpy()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T17:08:25.678480Z","iopub.execute_input":"2025-11-15T17:08:25.679203Z","iopub.status.idle":"2025-11-15T17:08:25.696337Z","shell.execute_reply.started":"2025-11-15T17:08:25.679176Z","shell.execute_reply":"2025-11-15T17:08:25.695727Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"## Training Utilities","metadata":{}},{"cell_type":"code","source":"def train_epoch(model, dataloader, criterion, optimizer, device, l1_lambda=0.0):\n    \"\"\"Train for one epoch with optional gating regularization.\"\"\"\n    model.train()\n    total_loss = 0.0\n    correct = 0\n    total = 0\n\n    for X_batch, y_batch in dataloader:\n        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n\n        optimizer.zero_grad()\n        outputs = model(X_batch)\n        loss = criterion(outputs, y_batch)\n\n        gate_penalty = getattr(model, 'gate_penalty_tensor', None)\n        if l1_lambda > 0 and gate_penalty is not None:\n            loss = loss + l1_lambda * gate_penalty.abs().mean()\n\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n        _, predicted = torch.max(outputs.data, 1)\n        total += y_batch.size(0)\n        correct += (predicted == y_batch).sum().item()\n\n    denom = max(1, len(dataloader))\n    return total_loss / denom, correct / max(1, total)\n\n\ndef evaluate(model, dataloader, criterion, device):\n    \"\"\"Evaluate model.\"\"\"\n    model.eval()\n    total_loss = 0.0\n    correct = 0\n    total = 0\n\n    with torch.no_grad():\n        for X_batch, y_batch in dataloader:\n            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n            outputs = model(X_batch)\n            loss = criterion(outputs, y_batch)\n\n            total_loss += loss.item()\n            _, predicted = torch.max(outputs.data, 1)\n            total += y_batch.size(0)\n            correct += (predicted == y_batch).sum().item()\n\n    denom = max(1, len(dataloader))\n    return total_loss / denom, correct / max(1, total)\n\n\ndef train_pytorch_model(model, train_loader, val_loader, config, model_name=''):\n    \"\"\"Train a PyTorch model with scheduler + best checkpoint tracking.\"\"\"\n    device = config['device']\n    model = model.to(device)\n\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'], \n                          weight_decay=config['weight_decay'])\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, mode='min', factor=0.5, patience=config.get('scheduler_patience', 3), verbose=False\n    )\n\n    l1_lambda = config.get('gating', {}).get('l1_lambda', 0.0) if getattr(model, 'use_gate_regularizer', False) else 0.0\n    use_early_stopping = config.get('use_early_stopping', False) and config.get('patience') is not None\n    max_patience = config.get('patience', 0)\n    patience_counter = 0\n\n    best_state = deepcopy(model.state_dict())\n    best_val_acc = 0.0\n    best_val_loss = float('inf')\n\n    for epoch in range(config['epochs']):\n        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device, l1_lambda)\n        val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n        scheduler.step(val_loss)\n\n        improved = val_acc > best_val_acc or (val_acc == best_val_acc and val_loss < best_val_loss)\n        if improved:\n            best_state = deepcopy(model.state_dict())\n            best_val_acc = val_acc\n            best_val_loss = val_loss\n            patience_counter = 0\n        else:\n            patience_counter += 1\n\n        prefix = model_name if model_name else 'Model'\n        print(f\"[{prefix}] Epoch {epoch + 1}/{config['epochs']} - Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}\")\n\n        if use_early_stopping and patience_counter >= max_patience:\n            print(f\"Early stopping triggered for {prefix} at epoch {epoch + 1}\")\n            break\n\n    model.load_state_dict(best_state)\n    return best_state, best_val_acc\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T17:08:36.386863Z","iopub.execute_input":"2025-11-15T17:08:36.387554Z","iopub.status.idle":"2025-11-15T17:08:36.400533Z","shell.execute_reply.started":"2025-11-15T17:08:36.387529Z","shell.execute_reply":"2025-11-15T17:08:36.399690Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"## Main Training Loop","metadata":{}},{"cell_type":"code","source":"# Load data\nprint(\"Loading PhysioNet data...\")\nX, y, subject_labels = load_physionet_data(CONFIG['data_path'])\n\nprint(f\"\\nData loaded successfully!\")\nprint(f\"Total trials: {len(X)}\")\nprint(f\"Data shape: {X.shape}\")\nprint(f\"Labels: {np.unique(y, return_counts=True)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T17:08:36.401703Z","iopub.execute_input":"2025-11-15T17:08:36.402002Z","iopub.status.idle":"2025-11-15T17:08:53.524695Z","shell.execute_reply.started":"2025-11-15T17:08:36.401985Z","shell.execute_reply":"2025-11-15T17:08:53.523986Z"}},"outputs":[{"name":"stdout","text":"Loading PhysioNet data...\nDetected 51 preprocessed subject folders under /kaggle/input/eeg-preprocessed-data/derived/preprocessed\nLoaded 2966 trials from 51 subjects\nData shape: (2966, 64, 513)\nLabel distribution: [1489 1477]\n\nData loaded successfully!\nTotal trials: 2966\nData shape: (2966, 64, 513)\nLabels: (array([0, 1]), array([1489, 1477]))\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# Define models to train\nmodels_to_train = [\n    {'name': 'FBCSP', 'type': 'sklearn'},\n    {'name': 'CNN-SAE', 'type': 'pytorch'},\n    {'name': 'EEGNet', 'type': 'pytorch'},\n    {'name': 'ACS-SE-CNN', 'type': 'pytorch'},\n    {'name': 'G-CARM', 'type': 'pytorch'},\n    {'name': 'Baseline-EEG-ARNN', 'type': 'pytorch'},\n    {'name': 'Adaptive-Gating-EEG-ARNN', 'type': 'pytorch'},\n]\n\n# Initialize results storage\nall_results = []\n\n# Cross-validation\nskf = StratifiedKFold(n_splits=CONFIG['n_folds'], shuffle=True, random_state=CONFIG['random_seed'])\n\nprint(f\"\\nStarting {CONFIG['n_folds']}-fold cross-validation...\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T17:08:53.525768Z","iopub.execute_input":"2025-11-15T17:08:53.526157Z","iopub.status.idle":"2025-11-15T17:08:53.531078Z","shell.execute_reply.started":"2025-11-15T17:08:53.526138Z","shell.execute_reply":"2025-11-15T17:08:53.530427Z"}},"outputs":[{"name":"stdout","text":"\nStarting 3-fold cross-validation...\n\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"from torch.utils.data import Dataset\n\nclass EEGDataset(Dataset):\n    def __init__(self, X, y):\n        # (samples, channels, timepoints)\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.long)\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        return self.X[idx], self.y[idx]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T17:47:15.677505Z","iopub.execute_input":"2025-11-15T17:47:15.677749Z","iopub.status.idle":"2025-11-15T17:47:15.682587Z","shell.execute_reply.started":"2025-11-15T17:47:15.677733Z","shell.execute_reply":"2025-11-15T17:47:15.681735Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"X.shape \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T17:48:15.508352Z","iopub.execute_input":"2025-11-15T17:48:15.508656Z","iopub.status.idle":"2025-11-15T17:48:15.514006Z","shell.execute_reply.started":"2025-11-15T17:48:15.508635Z","shell.execute_reply":"2025-11-15T17:48:15.513457Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"(2966, 64, 513)"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"try:\n    EEGDataset\n    print(\"EEGDataset is already defined.\")\nexcept NameError:\n    print(\"EEGDataset is NOT defined.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T17:49:13.479247Z","iopub.execute_input":"2025-11-15T17:49:13.479519Z","iopub.status.idle":"2025-11-15T17:49:13.483786Z","shell.execute_reply.started":"2025-11-15T17:49:13.479499Z","shell.execute_reply":"2025-11-15T17:49:13.483074Z"}},"outputs":[{"name":"stdout","text":"EEGDataset is already defined.\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"import os,fnmatch,sys\nroot = os.getcwd()   # change if your code is in another dir\n\nbad = ['torch.F','torch.tanh(', 'torch.sigmoid(', 'torch.relu(']  # we only need torch.elu really\nmatches=[]\nfor path,dirs,files in os.walk(root):\n    for f in files:\n        if f.endswith('.py') or f.endswith('.ipynb'):\n            p=os.path.join(path,f)\n            try:\n                with open(p,'r',errors='ignore') as fh:\n                    txt=fh.read()\n                for token in bad:\n                    if token in txt:\n                        matches.append((p, token))\n            except:\n                pass\n\nif not matches:\n    print(\"No suspicious tokens found in files under\", root)\nelse:\n    for p,t in matches:\n        print(\"Found\",t,\"in\",p)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T18:26:45.974730Z","iopub.execute_input":"2025-11-15T18:26:45.975501Z","iopub.status.idle":"2025-11-15T18:26:45.982120Z","shell.execute_reply.started":"2025-11-15T18:26:45.975471Z","shell.execute_reply":"2025-11-15T18:26:45.981496Z"}},"outputs":[{"name":"stdout","text":"No suspicious tokens found in files under /kaggle/working\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"try:\n    F\n    print(\"✔ F is loaded and available.\")\nexcept NameError:\n    print(\"❌ F is NOT loaded. You must run the import cell.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T18:30:16.886607Z","iopub.execute_input":"2025-11-15T18:30:16.887205Z","iopub.status.idle":"2025-11-15T18:30:16.890551Z","shell.execute_reply.started":"2025-11-15T18:30:16.887182Z","shell.execute_reply":"2025-11-15T18:30:16.889907Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"from torch.utils.data import Dataset\nclass EEGDataset(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.long)\n    def __len__(self):\n        return len(self.X)\n    def __getitem__(self, idx):\n        return self.X[idx], self.y[idx]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T18:30:39.465141Z","iopub.execute_input":"2025-11-15T18:30:39.465735Z","iopub.status.idle":"2025-11-15T18:30:39.470188Z","shell.execute_reply.started":"2025-11-15T18:30:39.465715Z","shell.execute_reply":"2025-11-15T18:30:39.469451Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"try:\n    F\n    print(\"✔ F is loaded and available.\")\nexcept NameError:\n    print(\" F is NOT loaded. You must run the import cell.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T18:36:17.684580Z","iopub.execute_input":"2025-11-15T18:36:17.684878Z","iopub.status.idle":"2025-11-15T18:36:17.689409Z","shell.execute_reply.started":"2025-11-15T18:36:17.684857Z","shell.execute_reply":"2025-11-15T18:36:17.688512Z"}},"outputs":[{"name":"stdout","text":"✔ F is loaded and available.\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"import torch, gc\ntorch.cuda.synchronize()\ngc.collect()\ntorch.cuda.empty_cache()\nprint(\"Cache cleared. Free mem (MiB):\", torch.cuda.get_device_properties(0).total_memory//(1024**2) - torch.cuda.memory_allocated(0)//(1024**2))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T19:23:57.225563Z","iopub.execute_input":"2025-11-15T19:23:57.226221Z","iopub.status.idle":"2025-11-15T19:23:57.443081Z","shell.execute_reply.started":"2025-11-15T19:23:57.226196Z","shell.execute_reply":"2025-11-15T19:23:57.442426Z"}},"outputs":[{"name":"stdout","text":"Cache cleared. Free mem (MiB): 1286\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"\n# Train all models\nfor model_info in models_to_train:\n    model_name = model_info['name']\n    model_type = model_info['type']\n\n    print(f\"{'='*60}\")\n    print(f\"Training {model_name}\")\n    print(f\"{'='*60}\")\n\n    fold_accuracies = []\n\n    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n        print(f\"Fold {fold + 1}/{CONFIG['n_folds']}\")\n\n        X_train, X_val = X[train_idx], X[val_idx]\n        y_train, y_val = y[train_idx], y[val_idx]\n\n        if model_type == 'sklearn':\n            model = FBCSP(freq_bands=CONFIG['fbcsp_bands'],\n                          n_components=CONFIG['fbcsp_n_components'],\n                          sfreq=CONFIG['sfreq'])\n            model.fit(X_train, y_train)\n            val_acc = model.score(X_val, y_val)\n\n            model_path = os.path.join(CONFIG['models_dir'], f\"{model_name}_fold{fold+1}.pkl\")\n            with open(model_path, 'wb') as f:\n                pickle.dump(model, f)\n        else:\n            train_dataset = EEGDataset(X_train, y_train)\n            val_dataset = EEGDataset(X_val, y_val)\n\n            train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'],\n                                      shuffle=True, num_workers=0)\n            val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'],\n                                    shuffle=False, num_workers=0)\n\n            base_kwargs = {\n                'n_channels': CONFIG['n_channels'],\n                'n_classes': CONFIG['n_classes'],\n                'n_timepoints': CONFIG['n_timepoints'],\n            }\n\n            if model_name == 'CNN-SAE':\n                model = CNNSAE(**base_kwargs)\n            elif model_name == 'EEGNet':\n                model = EEGNet(**base_kwargs)\n            elif model_name == 'ACS-SE-CNN':\n                model = ACSECNN(**base_kwargs)\n            elif model_name == 'G-CARM':\n                model = GCARM(**base_kwargs)\n            elif model_name == 'Baseline-EEG-ARNN':\n                model = BaselineEEGARNN(hidden_dim=CONFIG['hidden_dim'], **base_kwargs)\n            elif model_name == 'Adaptive-Gating-EEG-ARNN':\n                model = AdaptiveGatingEEGARNN(hidden_dim=CONFIG['hidden_dim'],\n                                              gate_init=CONFIG['gating']['gate_init'],\n                                              **base_kwargs)\n            else:\n                raise ValueError(f\"Unknown model: {model_name}\")\n\n            best_state, val_acc = train_pytorch_model(model, train_loader, val_loader,\n                                                      CONFIG, model_name)\n            model_path = os.path.join(CONFIG['models_dir'], f\"{model_name}_fold{fold+1}.pt\")\n            torch.save(best_state, model_path)\n\n        fold_accuracies.append(val_acc)\n        print(f\"Fold {fold + 1} Validation Accuracy: {val_acc:.4f}\")\n\n    mean_acc = np.mean(fold_accuracies)\n    std_acc = np.std(fold_accuracies)\n\n    print(f\"{model_name} Results:\")\n    print(f\"Mean Accuracy: {mean_acc:.4f} +/- {std_acc:.4f}\")\n\n    all_results.append({\n        'model': model_name,\n        'mean_accuracy': mean_acc,\n        'std_accuracy': std_acc,\n        'fold_accuracies': fold_accuracies\n    })\n\nprint(f\"{'='*60}\")\nprint(\"All models trained successfully!\")\nprint(f\"{'='*60}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T18:57:22.865954Z","iopub.execute_input":"2025-11-15T18:57:22.866518Z","iopub.status.idle":"2025-11-15T19:15:06.359354Z","shell.execute_reply.started":"2025-11-15T18:57:22.866497Z","shell.execute_reply":"2025-11-15T19:15:06.357849Z"}},"outputs":[{"name":"stdout","text":"============================================================\nTraining FBCSP\n============================================================\nFold 1/3\nFold 1 Validation Accuracy: 0.6208\nFold 2/3\nFold 2 Validation Accuracy: 0.6077\nFold 3/3\nFold 3 Validation Accuracy: 0.6144\nFBCSP Results:\nMean Accuracy: 0.6143 +/- 0.0054\n============================================================\nTraining CNN-SAE\n============================================================\nFold 1/3\n[CNN-SAE] Epoch 1/20 - Train Acc: 0.6151 | Val Acc: 0.5025\n[CNN-SAE] Epoch 2/20 - Train Acc: 0.7951 | Val Acc: 0.5025\n[CNN-SAE] Epoch 3/20 - Train Acc: 0.8134 | Val Acc: 0.5905\n[CNN-SAE] Epoch 4/20 - Train Acc: 0.8417 | Val Acc: 0.5025\n[CNN-SAE] Epoch 5/20 - Train Acc: 0.8574 | Val Acc: 0.5025\n[CNN-SAE] Epoch 6/20 - Train Acc: 0.8604 | Val Acc: 0.5025\n[CNN-SAE] Epoch 7/20 - Train Acc: 0.8614 | Val Acc: 0.5025\n[CNN-SAE] Epoch 8/20 - Train Acc: 0.8761 | Val Acc: 0.5025\n[CNN-SAE] Epoch 9/20 - Train Acc: 0.8938 | Val Acc: 0.4975\n[CNN-SAE] Epoch 10/20 - Train Acc: 0.8973 | Val Acc: 0.5025\n[CNN-SAE] Epoch 11/20 - Train Acc: 0.8938 | Val Acc: 0.4975\n[CNN-SAE] Epoch 12/20 - Train Acc: 0.9155 | Val Acc: 0.4975\n[CNN-SAE] Epoch 13/20 - Train Acc: 0.9191 | Val Acc: 0.5025\n[CNN-SAE] Epoch 14/20 - Train Acc: 0.9226 | Val Acc: 0.4975\n[CNN-SAE] Epoch 15/20 - Train Acc: 0.9287 | Val Acc: 0.5652\n[CNN-SAE] Epoch 16/20 - Train Acc: 0.9403 | Val Acc: 0.5025\n[CNN-SAE] Epoch 17/20 - Train Acc: 0.9368 | Val Acc: 0.5025\n[CNN-SAE] Epoch 18/20 - Train Acc: 0.9393 | Val Acc: 0.4975\n[CNN-SAE] Epoch 19/20 - Train Acc: 0.9449 | Val Acc: 0.5430\n[CNN-SAE] Epoch 20/20 - Train Acc: 0.9454 | Val Acc: 0.4975\nFold 1 Validation Accuracy: 0.5905\nFold 2/3\n[CNN-SAE] Epoch 1/20 - Train Acc: 0.5321 | Val Acc: 0.4985\n[CNN-SAE] Epoch 2/20 - Train Acc: 0.7228 | Val Acc: 0.4985\n[CNN-SAE] Epoch 3/20 - Train Acc: 0.8073 | Val Acc: 0.4985\n[CNN-SAE] Epoch 4/20 - Train Acc: 0.8311 | Val Acc: 0.5015\n[CNN-SAE] Epoch 5/20 - Train Acc: 0.8386 | Val Acc: 0.4985\n[CNN-SAE] Epoch 6/20 - Train Acc: 0.8407 | Val Acc: 0.4985\n[CNN-SAE] Epoch 7/20 - Train Acc: 0.8528 | Val Acc: 0.4985\n[CNN-SAE] Epoch 8/20 - Train Acc: 0.8649 | Val Acc: 0.4985\n[CNN-SAE] Epoch 9/20 - Train Acc: 0.8700 | Val Acc: 0.4985\n[CNN-SAE] Epoch 10/20 - Train Acc: 0.8751 | Val Acc: 0.4985\n[CNN-SAE] Epoch 11/20 - Train Acc: 0.8832 | Val Acc: 0.4985\n[CNN-SAE] Epoch 12/20 - Train Acc: 0.8867 | Val Acc: 0.5015\n[CNN-SAE] Epoch 13/20 - Train Acc: 0.8902 | Val Acc: 0.4985\n[CNN-SAE] Epoch 14/20 - Train Acc: 0.8973 | Val Acc: 0.5015\n[CNN-SAE] Epoch 15/20 - Train Acc: 0.9079 | Val Acc: 0.4985\n[CNN-SAE] Epoch 16/20 - Train Acc: 0.9034 | Val Acc: 0.4985\n[CNN-SAE] Epoch 17/20 - Train Acc: 0.9044 | Val Acc: 0.4985\n[CNN-SAE] Epoch 18/20 - Train Acc: 0.8963 | Val Acc: 0.6289\n[CNN-SAE] Epoch 19/20 - Train Acc: 0.9140 | Val Acc: 0.4985\n[CNN-SAE] Epoch 20/20 - Train Acc: 0.9135 | Val Acc: 0.4995\nFold 2 Validation Accuracy: 0.6289\nFold 3/3\n[CNN-SAE] Epoch 1/20 - Train Acc: 0.6709 | Val Acc: 0.5020\n[CNN-SAE] Epoch 2/20 - Train Acc: 0.8145 | Val Acc: 0.4980\n[CNN-SAE] Epoch 3/20 - Train Acc: 0.8332 | Val Acc: 0.4980\n[CNN-SAE] Epoch 4/20 - Train Acc: 0.8463 | Val Acc: 0.5020\n[CNN-SAE] Epoch 5/20 - Train Acc: 0.8402 | Val Acc: 0.5061\n[CNN-SAE] Epoch 6/20 - Train Acc: 0.8620 | Val Acc: 0.5020\n[CNN-SAE] Epoch 7/20 - Train Acc: 0.8721 | Val Acc: 0.5020\n[CNN-SAE] Epoch 8/20 - Train Acc: 0.8862 | Val Acc: 0.4980\n[CNN-SAE] Epoch 9/20 - Train Acc: 0.8908 | Val Acc: 0.4980\n[CNN-SAE] Epoch 10/20 - Train Acc: 0.8953 | Val Acc: 0.4980\n[CNN-SAE] Epoch 11/20 - Train Acc: 0.9044 | Val Acc: 0.4980\n[CNN-SAE] Epoch 12/20 - Train Acc: 0.9110 | Val Acc: 0.4980\n[CNN-SAE] Epoch 13/20 - Train Acc: 0.9191 | Val Acc: 0.4980\n[CNN-SAE] Epoch 14/20 - Train Acc: 0.9161 | Val Acc: 0.4980\n[CNN-SAE] Epoch 15/20 - Train Acc: 0.9257 | Val Acc: 0.4980\n[CNN-SAE] Epoch 16/20 - Train Acc: 0.9333 | Val Acc: 0.5020\n[CNN-SAE] Epoch 17/20 - Train Acc: 0.9398 | Val Acc: 0.5030\n[CNN-SAE] Epoch 18/20 - Train Acc: 0.9424 | Val Acc: 0.4980\n[CNN-SAE] Epoch 19/20 - Train Acc: 0.9434 | Val Acc: 0.6923\n[CNN-SAE] Epoch 20/20 - Train Acc: 0.9520 | Val Acc: 0.4980\nFold 3 Validation Accuracy: 0.6923\nCNN-SAE Results:\nMean Accuracy: 0.6372 +/- 0.0420\n============================================================\nTraining EEGNet\n============================================================\nFold 1/3\n[EEGNet] Epoch 1/20 - Train Acc: 0.6808 | Val Acc: 0.5025\n[EEGNet] Epoch 2/20 - Train Acc: 0.7805 | Val Acc: 0.5379\n[EEGNet] Epoch 3/20 - Train Acc: 0.8053 | Val Acc: 0.8119\n[EEGNet] Epoch 4/20 - Train Acc: 0.8139 | Val Acc: 0.8231\n[EEGNet] Epoch 5/20 - Train Acc: 0.8240 | Val Acc: 0.8271\n[EEGNet] Epoch 6/20 - Train Acc: 0.8230 | Val Acc: 0.8372\n[EEGNet] Epoch 7/20 - Train Acc: 0.8392 | Val Acc: 0.8311\n[EEGNet] Epoch 8/20 - Train Acc: 0.8427 | Val Acc: 0.8261\n[EEGNet] Epoch 9/20 - Train Acc: 0.8528 | Val Acc: 0.8311\n[EEGNet] Epoch 10/20 - Train Acc: 0.8574 | Val Acc: 0.8332\n[EEGNet] Epoch 11/20 - Train Acc: 0.8569 | Val Acc: 0.8322\n[EEGNet] Epoch 12/20 - Train Acc: 0.8462 | Val Acc: 0.7927\n[EEGNet] Epoch 13/20 - Train Acc: 0.8746 | Val Acc: 0.8332\n[EEGNet] Epoch 14/20 - Train Acc: 0.8619 | Val Acc: 0.8372\n[EEGNet] Epoch 15/20 - Train Acc: 0.8670 | Val Acc: 0.8311\n[EEGNet] Epoch 16/20 - Train Acc: 0.8730 | Val Acc: 0.7968\n[EEGNet] Epoch 17/20 - Train Acc: 0.8660 | Val Acc: 0.8200\n[EEGNet] Epoch 18/20 - Train Acc: 0.8766 | Val Acc: 0.8049\n[EEGNet] Epoch 19/20 - Train Acc: 0.8857 | Val Acc: 0.8210\n[EEGNet] Epoch 20/20 - Train Acc: 0.8756 | Val Acc: 0.8291\nFold 1 Validation Accuracy: 0.8372\nFold 2/3\n[EEGNet] Epoch 1/20 - Train Acc: 0.7046 | Val Acc: 0.4985\n[EEGNet] Epoch 2/20 - Train Acc: 0.7967 | Val Acc: 0.6926\n[EEGNet] Epoch 3/20 - Train Acc: 0.8123 | Val Acc: 0.8069\n[EEGNet] Epoch 4/20 - Train Acc: 0.8164 | Val Acc: 0.8160\n[EEGNet] Epoch 5/20 - Train Acc: 0.8139 | Val Acc: 0.8311\n[EEGNet] Epoch 6/20 - Train Acc: 0.8331 | Val Acc: 0.8342\n[EEGNet] Epoch 7/20 - Train Acc: 0.8356 | Val Acc: 0.8261\n[EEGNet] Epoch 8/20 - Train Acc: 0.8326 | Val Acc: 0.7988\n[EEGNet] Epoch 9/20 - Train Acc: 0.8437 | Val Acc: 0.8281\n[EEGNet] Epoch 10/20 - Train Acc: 0.8432 | Val Acc: 0.8322\n[EEGNet] Epoch 11/20 - Train Acc: 0.8483 | Val Acc: 0.8291\n[EEGNet] Epoch 12/20 - Train Acc: 0.8548 | Val Acc: 0.8190\n[EEGNet] Epoch 13/20 - Train Acc: 0.8457 | Val Acc: 0.8180\n[EEGNet] Epoch 14/20 - Train Acc: 0.8442 | Val Acc: 0.8311\n[EEGNet] Epoch 15/20 - Train Acc: 0.8624 | Val Acc: 0.8119\n[EEGNet] Epoch 16/20 - Train Acc: 0.8634 | Val Acc: 0.8109\n[EEGNet] Epoch 17/20 - Train Acc: 0.8548 | Val Acc: 0.8291\n[EEGNet] Epoch 18/20 - Train Acc: 0.8599 | Val Acc: 0.8402\n[EEGNet] Epoch 19/20 - Train Acc: 0.8675 | Val Acc: 0.8433\n[EEGNet] Epoch 20/20 - Train Acc: 0.8563 | Val Acc: 0.8443\nFold 2 Validation Accuracy: 0.8443\nFold 3/3\n[EEGNet] Epoch 1/20 - Train Acc: 0.6931 | Val Acc: 0.4980\n[EEGNet] Epoch 2/20 - Train Acc: 0.7983 | Val Acc: 0.5638\n[EEGNet] Epoch 3/20 - Train Acc: 0.8210 | Val Acc: 0.7206\n[EEGNet] Epoch 4/20 - Train Acc: 0.8195 | Val Acc: 0.8158\n[EEGNet] Epoch 5/20 - Train Acc: 0.8337 | Val Acc: 0.8148\n[EEGNet] Epoch 6/20 - Train Acc: 0.8372 | Val Acc: 0.8107\n[EEGNet] Epoch 7/20 - Train Acc: 0.8347 | Val Acc: 0.8249\n[EEGNet] Epoch 8/20 - Train Acc: 0.8367 | Val Acc: 0.8279\n[EEGNet] Epoch 9/20 - Train Acc: 0.8453 | Val Acc: 0.7895\n[EEGNet] Epoch 10/20 - Train Acc: 0.8468 | Val Acc: 0.8259\n[EEGNet] Epoch 11/20 - Train Acc: 0.8504 | Val Acc: 0.8310\n[EEGNet] Epoch 12/20 - Train Acc: 0.8574 | Val Acc: 0.8330\n[EEGNet] Epoch 13/20 - Train Acc: 0.8554 | Val Acc: 0.8310\n[EEGNet] Epoch 14/20 - Train Acc: 0.8554 | Val Acc: 0.7935\n[EEGNet] Epoch 15/20 - Train Acc: 0.8620 | Val Acc: 0.8350\n[EEGNet] Epoch 16/20 - Train Acc: 0.8564 | Val Acc: 0.8391\n[EEGNet] Epoch 17/20 - Train Acc: 0.8655 | Val Acc: 0.8239\n[EEGNet] Epoch 18/20 - Train Acc: 0.8655 | Val Acc: 0.8229\n[EEGNet] Epoch 19/20 - Train Acc: 0.8655 | Val Acc: 0.8198\n[EEGNet] Epoch 20/20 - Train Acc: 0.8655 | Val Acc: 0.8370\nFold 3 Validation Accuracy: 0.8391\nEEGNet Results:\nMean Accuracy: 0.8402 +/- 0.0030\n============================================================\nTraining ACS-SE-CNN\n============================================================\nFold 1/3\n[ACS-SE-CNN] Epoch 1/20 - Train Acc: 0.5883 | Val Acc: 0.5025\n[ACS-SE-CNN] Epoch 2/20 - Train Acc: 0.8007 | Val Acc: 0.5025\n[ACS-SE-CNN] Epoch 3/20 - Train Acc: 0.8184 | Val Acc: 0.5025\n[ACS-SE-CNN] Epoch 4/20 - Train Acc: 0.8407 | Val Acc: 0.5025\n[ACS-SE-CNN] Epoch 5/20 - Train Acc: 0.8493 | Val Acc: 0.4975\n[ACS-SE-CNN] Epoch 6/20 - Train Acc: 0.8589 | Val Acc: 0.4975\n[ACS-SE-CNN] Epoch 7/20 - Train Acc: 0.8685 | Val Acc: 0.4975\n[ACS-SE-CNN] Epoch 8/20 - Train Acc: 0.8827 | Val Acc: 0.4975\n[ACS-SE-CNN] Epoch 9/20 - Train Acc: 0.8771 | Val Acc: 0.4975\n[ACS-SE-CNN] Epoch 10/20 - Train Acc: 0.8958 | Val Acc: 0.4975\n[ACS-SE-CNN] Epoch 11/20 - Train Acc: 0.9039 | Val Acc: 0.4975\n[ACS-SE-CNN] Epoch 12/20 - Train Acc: 0.9115 | Val Acc: 0.4975\n[ACS-SE-CNN] Epoch 13/20 - Train Acc: 0.9130 | Val Acc: 0.4975\n[ACS-SE-CNN] Epoch 14/20 - Train Acc: 0.9206 | Val Acc: 0.4975\n[ACS-SE-CNN] Epoch 15/20 - Train Acc: 0.9317 | Val Acc: 0.4975\n[ACS-SE-CNN] Epoch 16/20 - Train Acc: 0.9358 | Val Acc: 0.4975\n[ACS-SE-CNN] Epoch 17/20 - Train Acc: 0.9383 | Val Acc: 0.4975\n[ACS-SE-CNN] Epoch 18/20 - Train Acc: 0.9408 | Val Acc: 0.4975\n[ACS-SE-CNN] Epoch 19/20 - Train Acc: 0.9469 | Val Acc: 0.4975\n[ACS-SE-CNN] Epoch 20/20 - Train Acc: 0.9489 | Val Acc: 0.4975\nFold 1 Validation Accuracy: 0.5025\nFold 2/3\n[ACS-SE-CNN] Epoch 1/20 - Train Acc: 0.5387 | Val Acc: 0.4985\n[ACS-SE-CNN] Epoch 2/20 - Train Acc: 0.7299 | Val Acc: 0.4985\n[ACS-SE-CNN] Epoch 3/20 - Train Acc: 0.8037 | Val Acc: 0.4985\n[ACS-SE-CNN] Epoch 4/20 - Train Acc: 0.8235 | Val Acc: 0.5015\n[ACS-SE-CNN] Epoch 5/20 - Train Acc: 0.8285 | Val Acc: 0.4985\n[ACS-SE-CNN] Epoch 6/20 - Train Acc: 0.8452 | Val Acc: 0.4985\n[ACS-SE-CNN] Epoch 7/20 - Train Acc: 0.8548 | Val Acc: 0.4985\n[ACS-SE-CNN] Epoch 8/20 - Train Acc: 0.8477 | Val Acc: 0.4965\n[ACS-SE-CNN] Epoch 9/20 - Train Acc: 0.8655 | Val Acc: 0.4934\n[ACS-SE-CNN] Epoch 10/20 - Train Acc: 0.8766 | Val Acc: 0.4985\n[ACS-SE-CNN] Epoch 11/20 - Train Acc: 0.8842 | Val Acc: 0.5015\n[ACS-SE-CNN] Epoch 12/20 - Train Acc: 0.8902 | Val Acc: 0.4965\n[ACS-SE-CNN] Epoch 13/20 - Train Acc: 0.8933 | Val Acc: 0.4904\n[ACS-SE-CNN] Epoch 14/20 - Train Acc: 0.8998 | Val Acc: 0.5804\n[ACS-SE-CNN] Epoch 15/20 - Train Acc: 0.9024 | Val Acc: 0.5015\n[ACS-SE-CNN] Epoch 16/20 - Train Acc: 0.9100 | Val Acc: 0.5015\n[ACS-SE-CNN] Epoch 17/20 - Train Acc: 0.9095 | Val Acc: 0.5015\n[ACS-SE-CNN] Epoch 18/20 - Train Acc: 0.9135 | Val Acc: 0.5015\n[ACS-SE-CNN] Epoch 19/20 - Train Acc: 0.9181 | Val Acc: 0.4975\n[ACS-SE-CNN] Epoch 20/20 - Train Acc: 0.9191 | Val Acc: 0.5005\nFold 2 Validation Accuracy: 0.5804\nFold 3/3\n[ACS-SE-CNN] Epoch 1/20 - Train Acc: 0.7083 | Val Acc: 0.5020\n[ACS-SE-CNN] Epoch 2/20 - Train Acc: 0.8069 | Val Acc: 0.5020\n[ACS-SE-CNN] Epoch 3/20 - Train Acc: 0.8337 | Val Acc: 0.5547\n[ACS-SE-CNN] Epoch 4/20 - Train Acc: 0.8463 | Val Acc: 0.4980\n[ACS-SE-CNN] Epoch 5/20 - Train Acc: 0.8483 | Val Acc: 0.4980\n[ACS-SE-CNN] Epoch 6/20 - Train Acc: 0.8559 | Val Acc: 0.4980\n[ACS-SE-CNN] Epoch 7/20 - Train Acc: 0.8620 | Val Acc: 0.4980\n[ACS-SE-CNN] Epoch 8/20 - Train Acc: 0.8731 | Val Acc: 0.4980\n[ACS-SE-CNN] Epoch 9/20 - Train Acc: 0.8812 | Val Acc: 0.5020\n[ACS-SE-CNN] Epoch 10/20 - Train Acc: 0.8923 | Val Acc: 0.5020\n[ACS-SE-CNN] Epoch 11/20 - Train Acc: 0.8862 | Val Acc: 0.5020\n[ACS-SE-CNN] Epoch 12/20 - Train Acc: 0.9050 | Val Acc: 0.5020\n[ACS-SE-CNN] Epoch 13/20 - Train Acc: 0.9009 | Val Acc: 0.5020\n[ACS-SE-CNN] Epoch 14/20 - Train Acc: 0.9156 | Val Acc: 0.4980\n[ACS-SE-CNN] Epoch 15/20 - Train Acc: 0.9095 | Val Acc: 0.4980\n[ACS-SE-CNN] Epoch 16/20 - Train Acc: 0.9282 | Val Acc: 0.5020\n[ACS-SE-CNN] Epoch 17/20 - Train Acc: 0.9232 | Val Acc: 0.4980\n[ACS-SE-CNN] Epoch 18/20 - Train Acc: 0.9262 | Val Acc: 0.5020\n[ACS-SE-CNN] Epoch 19/20 - Train Acc: 0.9434 | Val Acc: 0.5081\n[ACS-SE-CNN] Epoch 20/20 - Train Acc: 0.9403 | Val Acc: 0.5020\nFold 3 Validation Accuracy: 0.5547\nACS-SE-CNN Results:\nMean Accuracy: 0.5459 +/- 0.0324\n============================================================\nTraining G-CARM\n============================================================\nFold 1/3\n[G-CARM] Epoch 1/20 - Train Acc: 0.4820 | Val Acc: 0.4975\n[G-CARM] Epoch 2/20 - Train Acc: 0.5033 | Val Acc: 0.4975\n[G-CARM] Epoch 3/20 - Train Acc: 0.4997 | Val Acc: 0.4975\n[G-CARM] Epoch 4/20 - Train Acc: 0.5104 | Val Acc: 0.4975\n[G-CARM] Epoch 5/20 - Train Acc: 0.4866 | Val Acc: 0.4975\n[G-CARM] Epoch 6/20 - Train Acc: 0.4896 | Val Acc: 0.4975\n[G-CARM] Epoch 7/20 - Train Acc: 0.5023 | Val Acc: 0.4975\n[G-CARM] Epoch 8/20 - Train Acc: 0.5048 | Val Acc: 0.4975\n[G-CARM] Epoch 9/20 - Train Acc: 0.4911 | Val Acc: 0.4975\n[G-CARM] Epoch 10/20 - Train Acc: 0.5073 | Val Acc: 0.4975\n[G-CARM] Epoch 11/20 - Train Acc: 0.4836 | Val Acc: 0.4975\n[G-CARM] Epoch 12/20 - Train Acc: 0.4937 | Val Acc: 0.4975\n[G-CARM] Epoch 13/20 - Train Acc: 0.4871 | Val Acc: 0.4975\n[G-CARM] Epoch 14/20 - Train Acc: 0.5119 | Val Acc: 0.4975\n[G-CARM] Epoch 15/20 - Train Acc: 0.4957 | Val Acc: 0.4975\n[G-CARM] Epoch 16/20 - Train Acc: 0.5245 | Val Acc: 0.4975\n[G-CARM] Epoch 17/20 - Train Acc: 0.5013 | Val Acc: 0.4975\n[G-CARM] Epoch 18/20 - Train Acc: 0.5033 | Val Acc: 0.4975\n[G-CARM] Epoch 19/20 - Train Acc: 0.5266 | Val Acc: 0.4975\n[G-CARM] Epoch 20/20 - Train Acc: 0.5003 | Val Acc: 0.4975\nFold 1 Validation Accuracy: 0.4975\nFold 2/3\n[G-CARM] Epoch 1/20 - Train Acc: 0.4836 | Val Acc: 0.4985\n[G-CARM] Epoch 2/20 - Train Acc: 0.4841 | Val Acc: 0.4985\n[G-CARM] Epoch 3/20 - Train Acc: 0.6950 | Val Acc: 0.4985\n[G-CARM] Epoch 4/20 - Train Acc: 0.8037 | Val Acc: 0.4985\n[G-CARM] Epoch 5/20 - Train Acc: 0.8058 | Val Acc: 0.5015\n[G-CARM] Epoch 6/20 - Train Acc: 0.8164 | Val Acc: 0.5015\n[G-CARM] Epoch 7/20 - Train Acc: 0.8366 | Val Acc: 0.4985\n[G-CARM] Epoch 8/20 - Train Acc: 0.8376 | Val Acc: 0.4985\n[G-CARM] Epoch 9/20 - Train Acc: 0.8361 | Val Acc: 0.4985\n[G-CARM] Epoch 10/20 - Train Acc: 0.8472 | Val Acc: 0.4985\n[G-CARM] Epoch 11/20 - Train Acc: 0.8392 | Val Acc: 0.5015\n[G-CARM] Epoch 12/20 - Train Acc: 0.8533 | Val Acc: 0.5015\n[G-CARM] Epoch 13/20 - Train Acc: 0.8493 | Val Acc: 0.4985\n[G-CARM] Epoch 14/20 - Train Acc: 0.8594 | Val Acc: 0.4985\n[G-CARM] Epoch 15/20 - Train Acc: 0.8665 | Val Acc: 0.4985\n[G-CARM] Epoch 16/20 - Train Acc: 0.8670 | Val Acc: 0.4985\n[G-CARM] Epoch 17/20 - Train Acc: 0.8655 | Val Acc: 0.5238\n[G-CARM] Epoch 18/20 - Train Acc: 0.8569 | Val Acc: 0.4985\n[G-CARM] Epoch 19/20 - Train Acc: 0.8680 | Val Acc: 0.4985\n[G-CARM] Epoch 20/20 - Train Acc: 0.8629 | Val Acc: 0.4985\nFold 2 Validation Accuracy: 0.5238\nFold 3/3\n[G-CARM] Epoch 1/20 - Train Acc: 0.5015 | Val Acc: 0.5020\n[G-CARM] Epoch 2/20 - Train Acc: 0.4863 | Val Acc: 0.5020\n[G-CARM] Epoch 3/20 - Train Acc: 0.6456 | Val Acc: 0.5020\n[G-CARM] Epoch 4/20 - Train Acc: 0.8023 | Val Acc: 0.5020\n[G-CARM] Epoch 5/20 - Train Acc: 0.8200 | Val Acc: 0.5020\n[G-CARM] Epoch 6/20 - Train Acc: 0.8215 | Val Acc: 0.5020\n[G-CARM] Epoch 7/20 - Train Acc: 0.8327 | Val Acc: 0.4980\n[G-CARM] Epoch 8/20 - Train Acc: 0.8337 | Val Acc: 0.5020\n[G-CARM] Epoch 9/20 - Train Acc: 0.8251 | Val Acc: 0.5020\n[G-CARM] Epoch 10/20 - Train Acc: 0.8418 | Val Acc: 0.5020\n[G-CARM] Epoch 11/20 - Train Acc: 0.8524 | Val Acc: 0.5020\n[G-CARM] Epoch 12/20 - Train Acc: 0.8407 | Val Acc: 0.5020\n[G-CARM] Epoch 13/20 - Train Acc: 0.8438 | Val Acc: 0.5020\n[G-CARM] Epoch 14/20 - Train Acc: 0.8620 | Val Acc: 0.5020\n[G-CARM] Epoch 15/20 - Train Acc: 0.8453 | Val Acc: 0.5020\n[G-CARM] Epoch 16/20 - Train Acc: 0.8600 | Val Acc: 0.5020\n[G-CARM] Epoch 17/20 - Train Acc: 0.8519 | Val Acc: 0.5020\n[G-CARM] Epoch 18/20 - Train Acc: 0.8579 | Val Acc: 0.5020\n[G-CARM] Epoch 19/20 - Train Acc: 0.8544 | Val Acc: 0.5020\n[G-CARM] Epoch 20/20 - Train Acc: 0.8559 | Val Acc: 0.5020\nFold 3 Validation Accuracy: 0.5020\nG-CARM Results:\nMean Accuracy: 0.5078 +/- 0.0115\n============================================================\nTraining Baseline-EEG-ARNN\n============================================================\nFold 1/3\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_48/2715062488.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Unknown model: {model_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             best_state, val_acc = train_pytorch_model(model, train_loader, val_loader,\n\u001b[0m\u001b[1;32m     61\u001b[0m                                                       CONFIG, model_name)\n\u001b[1;32m     62\u001b[0m             \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCONFIG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'models_dir'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"{model_name}_fold{fold+1}.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_48/220415915.py\u001b[0m in \u001b[0;36mtrain_pytorch_model\u001b[0;34m(model, train_loader, val_loader, config, model_name)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml1_lambda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_48/220415915.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, dataloader, criterion, optimizer, device, l1_lambda)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_48/3915871532.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgate_penalty_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_gate_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_from_prepared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepared\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_final_adjacency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_48/3915871532.py\u001b[0m in \u001b[0;36m_forward_from_prepared\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_forward_from_prepared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_48/3915871532.py\u001b[0m in \u001b[0;36m_forward_features\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_forward_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_48/3915871532.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mx_g\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_g\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mx_g\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_g\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mx_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_g\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0mused\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnormalization\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meval\u001b[0m \u001b[0mmode\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mbuffers\u001b[0m \u001b[0mare\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \"\"\"\n\u001b[0;32m--> 193\u001b[0;31m         return F.batch_norm(\n\u001b[0m\u001b[1;32m    194\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m             \u001b[0;31m# If buffers are not to be tracked, ensure that they won't be updated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2820\u001b[0m         \u001b[0m_verify_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2821\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2822\u001b[0;31m     return torch.batch_norm(\n\u001b[0m\u001b[1;32m   2823\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2824\u001b[0m         \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 514.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 365.12 MiB is free. Process 2582 has 15.53 GiB memory in use. Of the allocated memory 15.13 GiB is allocated by PyTorch, and 96.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 514.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 365.12 MiB is free. Process 2582 has 15.53 GiB memory in use. Of the allocated memory 15.13 GiB is allocated by PyTorch, and 96.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","output_type":"error"}],"execution_count":31},{"cell_type":"code","source":"import os\nprint(os.listdir(CONFIG['models_dir']))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T19:25:46.915139Z","iopub.execute_input":"2025-11-15T19:25:46.915719Z","iopub.status.idle":"2025-11-15T19:25:46.919936Z","shell.execute_reply.started":"2025-11-15T19:25:46.915692Z","shell.execute_reply":"2025-11-15T19:25:46.919176Z"}},"outputs":[{"name":"stdout","text":"['G-CARM_fold1.pt', 'FBCSP_fold2.pkl', 'G-CARM_fold2.pt', 'FBCSP_fold1.pkl', 'CNN-SAE_fold3.pt', 'ACS-SE-CNN_fold3.pt', 'ACS-SE-CNN_fold1.pt', 'FBCSP_fold3.pkl', 'EEGNet_fold1.pt', 'G-CARM_fold3.pt', 'EEGNet_fold3.pt', 'CNN-SAE_fold1.pt', 'ACS-SE-CNN_fold2.pt', 'CNN-SAE_fold2.pt', 'EEGNet_fold2.pt']\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"import torch, gc\ntorch.cuda.synchronize()\ngc.collect()\ntorch.cuda.empty_cache()\nprint(\"Cache cleared. Free mem (MiB):\", torch.cuda.get_device_properties(0).total_memory//(1024**2) - torch.cuda.memory_allocated(0)//(1024**2))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T19:31:00.753552Z","iopub.execute_input":"2025-11-15T19:31:00.754257Z","iopub.status.idle":"2025-11-15T19:31:00.968455Z","shell.execute_reply.started":"2025-11-15T19:31:00.754232Z","shell.execute_reply":"2025-11-15T19:31:00.967808Z"}},"outputs":[{"name":"stdout","text":"Cache cleared. Free mem (MiB): 16217\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"remaining_models = [\n    {'name': 'Baseline-EEG-ARNN', 'type': 'pytorch'},\n    {'name': 'Adaptive-Gating-EEG-ARNN', 'type': 'pytorch'},\n]\n\nfor model_info in remaining_models:\n    model_name = model_info['name']\n    model_type = model_info['type']\n\n    print(\"=\"*60)\n    print(f\"Resuming: Training {model_name}\")\n    print(\"=\"*60)\n\n    fold_accuracies = []\n\n    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n        print(f\"Fold {fold + 1}/{CONFIG['n_folds']}\")\n\n        X_train, X_val = X[train_idx], X[val_idx]\n        y_train, y_val = y[train_idx], y[val_idx]\n\n        train_dataset = EEGDataset(X_train, y_train)\n        val_dataset = EEGDataset(X_val, y_val)\n\n        # ↓↓↓ reduce batch size for safety ↓↓↓\n        safe_bs = max(4, CONFIG['batch_size'] // 2)\n\n        train_loader = DataLoader(train_dataset, batch_size=safe_bs,\n                                  shuffle=True, num_workers=0)\n        val_loader = DataLoader(val_dataset, batch_size=safe_bs,\n                                shuffle=False, num_workers=0)\n\n        base_kwargs = {\n            'n_channels': CONFIG['n_channels'],\n            'n_classes': CONFIG['n_classes'],\n            'n_timepoints': CONFIG['n_timepoints'],\n        }\n\n        if model_name == 'Baseline-EEG-ARNN':\n            model = BaselineEEGARNN(hidden_dim=CONFIG['hidden_dim'], **base_kwargs)\n        elif model_name == 'Adaptive-Gating-EEG-ARNN':\n            model = AdaptiveGatingEEGARNN(\n                hidden_dim=CONFIG['hidden_dim'],\n                gate_init=CONFIG['gating']['gate_init'],\n                **base_kwargs\n            )\n        else:\n            raise ValueError(\"Unexpected model name during resume.\")\n\n        best_state, val_acc = train_pytorch_model(model, train_loader, val_loader,\n                                                  CONFIG, model_name)\n        model_path = os.path.join(CONFIG['models_dir'], f\"{model_name}_fold{fold+1}.pt\")\n        torch.save(best_state, model_path)\n\n        print(f\"Fold {fold + 1} Accuracy: {val_acc:.4f}\")\n        fold_accuracies.append(val_acc)\n\n    print(f\"\\n{model_name} Results:\")\n    print(f\"Mean: {np.mean(fold_accuracies):.4f}  Std: {np.std(fold_accuracies):.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T19:31:04.473704Z","iopub.execute_input":"2025-11-15T19:31:04.474199Z","iopub.status.idle":"2025-11-15T20:41:26.358296Z","shell.execute_reply.started":"2025-11-15T19:31:04.474172Z","shell.execute_reply":"2025-11-15T20:41:26.357377Z"}},"outputs":[{"name":"stdout","text":"============================================================\nResuming: Training Baseline-EEG-ARNN\n============================================================\nFold 1/3\n[Baseline-EEG-ARNN] Epoch 1/20 - Train Acc: 0.4987 | Val Acc: 0.5025\n[Baseline-EEG-ARNN] Epoch 2/20 - Train Acc: 0.5215 | Val Acc: 0.5025\n[Baseline-EEG-ARNN] Epoch 3/20 - Train Acc: 0.5109 | Val Acc: 0.5106\n[Baseline-EEG-ARNN] Epoch 4/20 - Train Acc: 0.5210 | Val Acc: 0.5025\n[Baseline-EEG-ARNN] Epoch 5/20 - Train Acc: 0.5301 | Val Acc: 0.4975\n[Baseline-EEG-ARNN] Epoch 6/20 - Train Acc: 0.5119 | Val Acc: 0.5015\n[Baseline-EEG-ARNN] Epoch 7/20 - Train Acc: 0.4997 | Val Acc: 0.5025\n[Baseline-EEG-ARNN] Epoch 8/20 - Train Acc: 0.5099 | Val Acc: 0.5025\n[Baseline-EEG-ARNN] Epoch 9/20 - Train Acc: 0.4932 | Val Acc: 0.4874\n[Baseline-EEG-ARNN] Epoch 10/20 - Train Acc: 0.5099 | Val Acc: 0.4975\n[Baseline-EEG-ARNN] Epoch 11/20 - Train Acc: 0.4952 | Val Acc: 0.5086\n[Baseline-EEG-ARNN] Epoch 12/20 - Train Acc: 0.5392 | Val Acc: 0.5248\n[Baseline-EEG-ARNN] Epoch 13/20 - Train Acc: 0.5210 | Val Acc: 0.4944\n[Baseline-EEG-ARNN] Epoch 14/20 - Train Acc: 0.5367 | Val Acc: 0.5167\n[Baseline-EEG-ARNN] Epoch 15/20 - Train Acc: 0.5610 | Val Acc: 0.5005\n[Baseline-EEG-ARNN] Epoch 16/20 - Train Acc: 0.5559 | Val Acc: 0.5248\n[Baseline-EEG-ARNN] Epoch 17/20 - Train Acc: 0.5918 | Val Acc: 0.5137\n[Baseline-EEG-ARNN] Epoch 18/20 - Train Acc: 0.5893 | Val Acc: 0.5147\n[Baseline-EEG-ARNN] Epoch 19/20 - Train Acc: 0.6009 | Val Acc: 0.4975\n[Baseline-EEG-ARNN] Epoch 20/20 - Train Acc: 0.6318 | Val Acc: 0.5359\nFold 1 Accuracy: 0.5359\nFold 2/3\n[Baseline-EEG-ARNN] Epoch 1/20 - Train Acc: 0.5109 | Val Acc: 0.4985\n[Baseline-EEG-ARNN] Epoch 2/20 - Train Acc: 0.5164 | Val Acc: 0.4975\n[Baseline-EEG-ARNN] Epoch 3/20 - Train Acc: 0.5119 | Val Acc: 0.4944\n[Baseline-EEG-ARNN] Epoch 4/20 - Train Acc: 0.5119 | Val Acc: 0.4914\n[Baseline-EEG-ARNN] Epoch 5/20 - Train Acc: 0.5089 | Val Acc: 0.4975\n[Baseline-EEG-ARNN] Epoch 6/20 - Train Acc: 0.5169 | Val Acc: 0.4762\n[Baseline-EEG-ARNN] Epoch 7/20 - Train Acc: 0.5185 | Val Acc: 0.4965\n[Baseline-EEG-ARNN] Epoch 8/20 - Train Acc: 0.5104 | Val Acc: 0.5025\n[Baseline-EEG-ARNN] Epoch 9/20 - Train Acc: 0.5159 | Val Acc: 0.5005\n[Baseline-EEG-ARNN] Epoch 10/20 - Train Acc: 0.5099 | Val Acc: 0.4965\n[Baseline-EEG-ARNN] Epoch 11/20 - Train Acc: 0.5185 | Val Acc: 0.5046\n[Baseline-EEG-ARNN] Epoch 12/20 - Train Acc: 0.5094 | Val Acc: 0.5035\n[Baseline-EEG-ARNN] Epoch 13/20 - Train Acc: 0.5144 | Val Acc: 0.5066\n[Baseline-EEG-ARNN] Epoch 14/20 - Train Acc: 0.5175 | Val Acc: 0.4995\n[Baseline-EEG-ARNN] Epoch 15/20 - Train Acc: 0.5387 | Val Acc: 0.5015\n[Baseline-EEG-ARNN] Epoch 16/20 - Train Acc: 0.5164 | Val Acc: 0.5005\n[Baseline-EEG-ARNN] Epoch 17/20 - Train Acc: 0.5255 | Val Acc: 0.4914\n[Baseline-EEG-ARNN] Epoch 18/20 - Train Acc: 0.5185 | Val Acc: 0.4985\n[Baseline-EEG-ARNN] Epoch 19/20 - Train Acc: 0.5220 | Val Acc: 0.5076\n[Baseline-EEG-ARNN] Epoch 20/20 - Train Acc: 0.5296 | Val Acc: 0.4985\nFold 2 Accuracy: 0.5076\nFold 3/3\n[Baseline-EEG-ARNN] Epoch 1/20 - Train Acc: 0.4889 | Val Acc: 0.4980\n[Baseline-EEG-ARNN] Epoch 2/20 - Train Acc: 0.4904 | Val Acc: 0.5030\n[Baseline-EEG-ARNN] Epoch 3/20 - Train Acc: 0.5005 | Val Acc: 0.5142\n[Baseline-EEG-ARNN] Epoch 4/20 - Train Acc: 0.4990 | Val Acc: 0.4848\n[Baseline-EEG-ARNN] Epoch 5/20 - Train Acc: 0.5086 | Val Acc: 0.5010\n[Baseline-EEG-ARNN] Epoch 6/20 - Train Acc: 0.5046 | Val Acc: 0.5061\n[Baseline-EEG-ARNN] Epoch 7/20 - Train Acc: 0.5056 | Val Acc: 0.5010\n[Baseline-EEG-ARNN] Epoch 8/20 - Train Acc: 0.5152 | Val Acc: 0.5040\n[Baseline-EEG-ARNN] Epoch 9/20 - Train Acc: 0.5071 | Val Acc: 0.5030\n[Baseline-EEG-ARNN] Epoch 10/20 - Train Acc: 0.5157 | Val Acc: 0.5344\n[Baseline-EEG-ARNN] Epoch 11/20 - Train Acc: 0.5131 | Val Acc: 0.5071\n[Baseline-EEG-ARNN] Epoch 12/20 - Train Acc: 0.5157 | Val Acc: 0.5000\n[Baseline-EEG-ARNN] Epoch 13/20 - Train Acc: 0.5137 | Val Acc: 0.5020\n[Baseline-EEG-ARNN] Epoch 14/20 - Train Acc: 0.5177 | Val Acc: 0.5121\n[Baseline-EEG-ARNN] Epoch 15/20 - Train Acc: 0.5172 | Val Acc: 0.5010\n[Baseline-EEG-ARNN] Epoch 16/20 - Train Acc: 0.5152 | Val Acc: 0.5152\n[Baseline-EEG-ARNN] Epoch 17/20 - Train Acc: 0.5238 | Val Acc: 0.5132\n[Baseline-EEG-ARNN] Epoch 18/20 - Train Acc: 0.5212 | Val Acc: 0.4868\n[Baseline-EEG-ARNN] Epoch 19/20 - Train Acc: 0.5238 | Val Acc: 0.4879\n[Baseline-EEG-ARNN] Epoch 20/20 - Train Acc: 0.5248 | Val Acc: 0.5172\nFold 3 Accuracy: 0.5344\n\nBaseline-EEG-ARNN Results:\nMean: 0.5260  Std: 0.0130\n============================================================\nResuming: Training Adaptive-Gating-EEG-ARNN\n============================================================\nFold 1/3\n[Adaptive-Gating-EEG-ARNN] Epoch 1/20 - Train Acc: 0.5564 | Val Acc: 0.5025\n[Adaptive-Gating-EEG-ARNN] Epoch 2/20 - Train Acc: 0.7284 | Val Acc: 0.6502\n[Adaptive-Gating-EEG-ARNN] Epoch 3/20 - Train Acc: 0.7491 | Val Acc: 0.5056\n[Adaptive-Gating-EEG-ARNN] Epoch 4/20 - Train Acc: 0.7800 | Val Acc: 0.4975\n[Adaptive-Gating-EEG-ARNN] Epoch 5/20 - Train Acc: 0.7845 | Val Acc: 0.5025\n[Adaptive-Gating-EEG-ARNN] Epoch 6/20 - Train Acc: 0.7977 | Val Acc: 0.4975\n[Adaptive-Gating-EEG-ARNN] Epoch 7/20 - Train Acc: 0.8012 | Val Acc: 0.5774\n[Adaptive-Gating-EEG-ARNN] Epoch 13/20 - Train Acc: 0.8351 | Val Acc: 0.6916\n[Adaptive-Gating-EEG-ARNN] Epoch 14/20 - Train Acc: 0.8518 | Val Acc: 0.7007\n[Adaptive-Gating-EEG-ARNN] Epoch 15/20 - Train Acc: 0.8553 | Val Acc: 0.5521\n[Adaptive-Gating-EEG-ARNN] Epoch 16/20 - Train Acc: 0.8569 | Val Acc: 0.5217\n[Adaptive-Gating-EEG-ARNN] Epoch 17/20 - Train Acc: 0.8594 | Val Acc: 0.5774\n[Adaptive-Gating-EEG-ARNN] Epoch 18/20 - Train Acc: 0.8700 | Val Acc: 0.6987\n[Adaptive-Gating-EEG-ARNN] Epoch 19/20 - Train Acc: 0.8725 | Val Acc: 0.7108\n[Adaptive-Gating-EEG-ARNN] Epoch 20/20 - Train Acc: 0.8857 | Val Acc: 0.7644\nFold 1 Accuracy: 0.7644\nFold 2/3\n[Adaptive-Gating-EEG-ARNN] Epoch 1/20 - Train Acc: 0.6222 | Val Acc: 0.4985\n[Adaptive-Gating-EEG-ARNN] Epoch 2/20 - Train Acc: 0.7167 | Val Acc: 0.4985\n[Adaptive-Gating-EEG-ARNN] Epoch 3/20 - Train Acc: 0.7486 | Val Acc: 0.5268\n[Adaptive-Gating-EEG-ARNN] Epoch 4/20 - Train Acc: 0.7577 | Val Acc: 0.6138\n[Adaptive-Gating-EEG-ARNN] Epoch 5/20 - Train Acc: 0.7891 | Val Acc: 0.5561\n[Adaptive-Gating-EEG-ARNN] Epoch 6/20 - Train Acc: 0.7734 | Val Acc: 0.5683\n[Adaptive-Gating-EEG-ARNN] Epoch 7/20 - Train Acc: 0.7613 | Val Acc: 0.6997\n[Adaptive-Gating-EEG-ARNN] Epoch 8/20 - Train Acc: 0.7699 | Val Acc: 0.5015\n[Adaptive-Gating-EEG-ARNN] Epoch 9/20 - Train Acc: 0.7658 | Val Acc: 0.6593\n[Adaptive-Gating-EEG-ARNN] Epoch 10/20 - Train Acc: 0.7962 | Val Acc: 0.5015\n[Adaptive-Gating-EEG-ARNN] Epoch 11/20 - Train Acc: 0.7815 | Val Acc: 0.5177\n[Adaptive-Gating-EEG-ARNN] Epoch 12/20 - Train Acc: 0.7997 | Val Acc: 0.5298\n[Adaptive-Gating-EEG-ARNN] Epoch 13/20 - Train Acc: 0.8169 | Val Acc: 0.5521\n[Adaptive-Gating-EEG-ARNN] Epoch 14/20 - Train Acc: 0.8123 | Val Acc: 0.7947\n[Adaptive-Gating-EEG-ARNN] Epoch 15/20 - Train Acc: 0.8356 | Val Acc: 0.5025\n[Adaptive-Gating-EEG-ARNN] Epoch 16/20 - Train Acc: 0.8164 | Val Acc: 0.5015\n[Adaptive-Gating-EEG-ARNN] Epoch 17/20 - Train Acc: 0.8209 | Val Acc: 0.5116\n[Adaptive-Gating-EEG-ARNN] Epoch 18/20 - Train Acc: 0.8245 | Val Acc: 0.5137\n[Adaptive-Gating-EEG-ARNN] Epoch 19/20 - Train Acc: 0.8285 | Val Acc: 0.5106\n[Adaptive-Gating-EEG-ARNN] Epoch 20/20 - Train Acc: 0.8422 | Val Acc: 0.5126\nFold 2 Accuracy: 0.7947\nFold 3/3\n[Adaptive-Gating-EEG-ARNN] Epoch 1/20 - Train Acc: 0.5597 | Val Acc: 0.4980\n[Adaptive-Gating-EEG-ARNN] Epoch 2/20 - Train Acc: 0.7204 | Val Acc: 0.4980\n[Adaptive-Gating-EEG-ARNN] Epoch 3/20 - Train Acc: 0.7462 | Val Acc: 0.4980\n[Adaptive-Gating-EEG-ARNN] Epoch 4/20 - Train Acc: 0.7523 | Val Acc: 0.6113\n[Adaptive-Gating-EEG-ARNN] Epoch 5/20 - Train Acc: 0.7467 | Val Acc: 0.5294\n[Adaptive-Gating-EEG-ARNN] Epoch 6/20 - Train Acc: 0.7619 | Val Acc: 0.6690\n[Adaptive-Gating-EEG-ARNN] Epoch 7/20 - Train Acc: 0.7528 | Val Acc: 0.5020\n[Adaptive-Gating-EEG-ARNN] Epoch 8/20 - Train Acc: 0.7609 | Val Acc: 0.5020\n[Adaptive-Gating-EEG-ARNN] Epoch 9/20 - Train Acc: 0.7892 | Val Acc: 0.5020\n[Adaptive-Gating-EEG-ARNN] Epoch 10/20 - Train Acc: 0.7861 | Val Acc: 0.5020\n[Adaptive-Gating-EEG-ARNN] Epoch 11/20 - Train Acc: 0.8119 | Val Acc: 0.5455\n[Adaptive-Gating-EEG-ARNN] Epoch 12/20 - Train Acc: 0.8124 | Val Acc: 0.5020\n[Adaptive-Gating-EEG-ARNN] Epoch 13/20 - Train Acc: 0.8271 | Val Acc: 0.5020\n[Adaptive-Gating-EEG-ARNN] Epoch 14/20 - Train Acc: 0.8332 | Val Acc: 0.6063\n[Adaptive-Gating-EEG-ARNN] Epoch 15/20 - Train Acc: 0.8372 | Val Acc: 0.6457\n[Adaptive-Gating-EEG-ARNN] Epoch 16/20 - Train Acc: 0.8423 | Val Acc: 0.6559\n[Adaptive-Gating-EEG-ARNN] Epoch 17/20 - Train Acc: 0.8458 | Val Acc: 0.5972\n[Adaptive-Gating-EEG-ARNN] Epoch 18/20 - Train Acc: 0.8478 | Val Acc: 0.5344\n[Adaptive-Gating-EEG-ARNN] Epoch 19/20 - Train Acc: 0.8478 | Val Acc: 0.7905\n[Adaptive-Gating-EEG-ARNN] Epoch 20/20 - Train Acc: 0.8610 | Val Acc: 0.6447\nFold 3 Accuracy: 0.7905\n\nAdaptive-Gating-EEG-ARNN Results:\nMean: 0.7832  Std: 0.0134\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save results\nresults_df = pd.DataFrame(all_results)\nresults_df = results_df.sort_values('mean_accuracy', ascending=False)\nresults_df.to_csv(os.path.join(CONFIG['results_dir'], 'summary_all_models.csv'), index=False)\n\nprint(\"\\nFinal Results:\")\nprint(results_df[['model', 'mean_accuracy', 'std_accuracy']])\n\nprint(f\"\\nResults saved to {CONFIG['results_dir']}/summary_all_models.csv\")\nprint(f\"Models saved to {CONFIG['models_dir']}/\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T20:42:28.627860Z","iopub.execute_input":"2025-11-15T20:42:28.628455Z","iopub.status.idle":"2025-11-15T20:42:28.656717Z","shell.execute_reply.started":"2025-11-15T20:42:28.628429Z","shell.execute_reply":"2025-11-15T20:42:28.656074Z"}},"outputs":[{"name":"stdout","text":"\nFinal Results:\n        model  mean_accuracy  std_accuracy\n7      EEGNet       0.840188      0.002996\n2     CNN-SAE       0.697269      0.127215\n6     CNN-SAE       0.637240      0.041979\n0       FBCSP       0.614295      0.005367\n1       FBCSP       0.614295      0.005367\n3       FBCSP       0.614295      0.005367\n5       FBCSP       0.614295      0.005367\n4     CNN-SAE       0.561377      0.041023\n8  ACS-SE-CNN       0.545856      0.032388\n9      G-CARM       0.507753      0.011471\n\nResults saved to ./results/summary_all_models.csv\nModels saved to ./models/\n","output_type":"stream"}],"execution_count":40},{"cell_type":"markdown","source":"## Verification\n\nCheck that Adaptive-Gating-EEG-ARNN is the winner:","metadata":{}},{"cell_type":"code","source":"# Verify winner\nwinner = results_df.iloc[0]\nprint(f\"\\nWinner: {winner['model']}\")\nprint(f\"Accuracy: {winner['mean_accuracy']:.4f} +/- {winner['std_accuracy']:.4f}\")\n\nif winner['model'] == 'Adaptive-Gating-EEG-ARNN':\n    print(\"\\nSUCCESS: Adaptive-Gating-EEG-ARNN is the winner!\")\nelse:\n    print(f\"\\nWARNING: Expected Adaptive-Gating-EEG-ARNN to win, but {winner['model']} won instead.\")\n    print(\"This may indicate a hyperparameter tuning issue.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T20:42:52.752552Z","iopub.execute_input":"2025-11-15T20:42:52.752805Z","iopub.status.idle":"2025-11-15T20:42:52.758184Z","shell.execute_reply.started":"2025-11-15T20:42:52.752787Z","shell.execute_reply":"2025-11-15T20:42:52.757311Z"}},"outputs":[{"name":"stdout","text":"\nWinner: EEGNet\nAccuracy: 0.8402 +/- 0.0030\n\nWARNING: Expected Adaptive-Gating-EEG-ARNN to win, but EEGNet won instead.\nThis may indicate a hyperparameter tuning issue.\n","output_type":"stream"}],"execution_count":41},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}