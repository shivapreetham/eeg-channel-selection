{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PhysioNet Motor Imagery - EEG-ARNN Models\n",
    "\n",
    "## Baseline EEG-ARNN vs Adaptive Gating EEG-ARNN\n",
    "\n",
    "This notebook trains and evaluates:\n",
    "1. **Baseline EEG-ARNN** - Pure CNN-GCN architecture\n",
    "2. **Adaptive Gating EEG-ARNN** - Input-dependent channel gating\n",
    "\n",
    "## Configuration:\n",
    "- **30 epochs**, **0.002 LR**, **NO EARLY STOPPING**\n",
    "- **10 subjects**, **3-fold CV**\n",
    "- **Channel Selection**: ES/AS/GS at k=[10,15,20,25,30]\n",
    "\n",
    "## Metrics:\n",
    "- Accuracy, Precision, Recall, F1-Score, AUC-ROC, Specificity\n",
    "\n",
    "## Output:\n",
    "- `eeg_arnn_baseline_results.csv`\n",
    "- `eeg_arnn_adaptive_results.csv`\n",
    "- `eeg_arnn_baseline_retrain_results.csv`\n",
    "- `eeg_arnn_adaptive_retrain_results.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix\n",
    ")\n",
    "import gc\n",
    "\n",
    "import mne\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_context('notebook', font_scale=1.0)\n",
    "mne.set_log_level('WARNING')\n",
    "\n",
    "def set_seed(s=42):\n",
    "    random.seed(s)\n",
    "    np.random.seed(s)\n",
    "    torch.manual_seed(s)\n",
    "    torch.cuda.manual_seed_all(s)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "if os.path.exists('/kaggle/input'):\n",
    "    print(\"Running on Kaggle\")\n",
    "    kaggle_input = Path('/kaggle/input')\n",
    "    datasets = [d for d in kaggle_input.iterdir() if d.is_dir()]\n",
    "    print(f\"Available datasets: {[d.name for d in datasets]}\")\n",
    "\n",
    "    DATA_DIR = None\n",
    "    possible_names = ['physioneteegmi', 'eeg-motor-movementimagery-dataset']\n",
    "    for ds_name in possible_names:\n",
    "        test_path = kaggle_input / ds_name\n",
    "        if test_path.exists():\n",
    "            DATA_DIR = test_path\n",
    "            print(f\"Found dataset: {DATA_DIR}\")\n",
    "            break\n",
    "\n",
    "    if DATA_DIR is None and datasets:\n",
    "        DATA_DIR = datasets[0]\n",
    "        print(f\"Using first available dataset: {DATA_DIR}\")\n",
    "else:\n",
    "    print(\"Running locally\")\n",
    "    DATA_DIR = Path('data/physionet/files')\n",
    "\n",
    "CONFIG = {\n",
    "    'data': {\n",
    "        'raw_data_dir': DATA_DIR,\n",
    "        'selected_classes': [1, 2],\n",
    "        'tmin': -1.0,\n",
    "        'tmax': 5.0,\n",
    "        'baseline': (-0.5, 0)\n",
    "    },\n",
    "    'preprocessing': {\n",
    "        'l_freq': 0.5,\n",
    "        'h_freq': 40.0,\n",
    "        'notch_freq': 50.0,\n",
    "        'target_sfreq': 128.0,\n",
    "        'apply_car': True\n",
    "    },\n",
    "    'model': {\n",
    "        'hidden_dim': 16,\n",
    "        'epochs': 35,\n",
    "        'learning_rate': 0.002,\n",
    "        'batch_size': 32,\n",
    "        'n_folds': 5,\n",
    "        'patience': 999,\n",
    "        'adj_lr': 0.001\n",
    "    },\n",
    "    'gating': {\n",
    "        'l1_lambda': 1e-3,\n",
    "        'gate_init': 0.9\n",
    "    },\n",
    "    'channel_selection': {\n",
    "        'k_values': [10, 15, 20, 25, 30]\n",
    "    },\n",
    "    'output': {\n",
    "        'results_dir': Path('results'),\n",
    "    },\n",
    "    'max_subjects': 10,\n",
    "    'min_runs_per_subject': 8\n",
    "}\n",
    "\n",
    "CONFIG['output']['results_dir'].mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PhysioNet Motor Imagery - EEG-ARNN (OPTIMIZED SETTINGS)\")\n",
    "print(\"=\"*80)\n",
    "print(\"CRITICAL FIXES APPLIED:\")\n",
    "print(\"  1. MIN-MAX NORMALIZATION (was z-score)\")\n",
    "print(\"  2. CUSTOM ADJACENCY LEARNING (paper's method)\")\n",
    "print(\"  3. BATCH SIZE 20 (was 64)\")\n",
    "print(\"  4. HIDDEN DIM 16 (was 40)\")\n",
    "print(\"=\"*80)\n",
    "print(\"USER PREFERENCES:\")\n",
    "print(\"  - 35 epochs (faster training)\")\n",
    "print(\"  - 5-fold CV (balanced)\")\n",
    "print(\"  - LR 0.002 (slightly higher)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Training: {CONFIG['max_subjects']} subjects, {CONFIG['model']['n_folds']}-fold CV, {CONFIG['model']['epochs']} epochs\")\n",
    "print(f\"Learning rate: {CONFIG['model']['learning_rate']}, Adjacency LR: {CONFIG['model']['adj_lr']}\")\n",
    "print(f\"Channel selection k values: {CONFIG['channel_selection']['k_values']}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Cleaning - Remove Faulty Subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNOWN_BAD_SUBJECTS = [\n",
    "    'S088', 'S089', 'S092', 'S100', 'S104', 'S106', 'S107', 'S108', 'S109'\n",
    "]\n",
    "\n",
    "HIGH_ISSUE_SUBJECTS = [\n",
    "    'S003', 'S004', 'S009', 'S010', 'S012', 'S013', 'S017', 'S018', 'S019',\n",
    "    'S021', 'S022', 'S023', 'S024', 'S025', 'S026', 'S027', 'S028', 'S029'\n",
    "]\n",
    "\n",
    "EXCLUDED_SUBJECTS = set(KNOWN_BAD_SUBJECTS + HIGH_ISSUE_SUBJECTS)\n",
    "\n",
    "print(f\"Total excluded subjects: {len(EXCLUDED_SUBJECTS)}\")\n",
    "print(f\"Excluded subjects: {sorted(EXCLUDED_SUBJECTS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Loading and Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_raw(raw, config):\n",
    "    cleaned_names = {name: name.rstrip('.') for name in raw.ch_names}\n",
    "    raw.rename_channels(cleaned_names)\n",
    "    raw.pick_types(eeg=True)\n",
    "    raw.set_montage('standard_1020', on_missing='ignore', match_case=False)\n",
    "    \n",
    "    nyquist = raw.info['sfreq'] / 2.0\n",
    "    if config['preprocessing']['notch_freq'] < nyquist:\n",
    "        raw.notch_filter(freqs=config['preprocessing']['notch_freq'], verbose=False)\n",
    "    \n",
    "    raw.filter(\n",
    "        l_freq=config['preprocessing']['l_freq'],\n",
    "        h_freq=config['preprocessing']['h_freq'],\n",
    "        method='fir',\n",
    "        fir_design='firwin',\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    if config['preprocessing']['apply_car']:\n",
    "        raw.set_eeg_reference('average', projection=False, verbose=False)\n",
    "    \n",
    "    raw.resample(config['preprocessing']['target_sfreq'], npad='auto', verbose=False)\n",
    "    return raw\n",
    "\n",
    "\n",
    "def load_and_preprocess_edf(edf_path, config):\n",
    "    raw = mne.io.read_raw_edf(edf_path, preload=True, verbose='ERROR')\n",
    "    raw = preprocess_raw(raw, config)\n",
    "    \n",
    "    events, event_ids = mne.events_from_annotations(raw, verbose='ERROR')\n",
    "    \n",
    "    if len(events) == 0:\n",
    "        return None, None, raw.ch_names\n",
    "    \n",
    "    epochs = mne.Epochs(\n",
    "        raw,\n",
    "        events,\n",
    "        event_id=event_ids,\n",
    "        tmin=config['data']['tmin'],\n",
    "        tmax=config['data']['tmax'],\n",
    "        baseline=tuple(config['data']['baseline']),\n",
    "        preload=True,\n",
    "        verbose='ERROR'\n",
    "    )\n",
    "    \n",
    "    return epochs.get_data(), epochs.events[:, 2], raw.ch_names\n",
    "\n",
    "\n",
    "def filter_classes(x, y, selected_classes):\n",
    "    mask = np.isin(y, selected_classes)\n",
    "    y, x = y[mask], x[mask]\n",
    "    label_map = {old: new for new, old in enumerate(sorted(selected_classes))}\n",
    "    y = np.array([label_map[int(label)] for label in y], dtype=np.int64)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def normalize(x):\n",
    "    \"\"\"Min-max normalization matching the paper implementation - CRITICAL FIX\"\"\"\n",
    "    x_normalized = np.zeros_like(x)\n",
    "    for i in range(x.shape[0]):\n",
    "        x_min = np.min(x[i])\n",
    "        x_max = np.max(x[i])\n",
    "        x_normalized[i] = (x[i] - x_min) / (x_max - x_min + 1e-8)\n",
    "    return x_normalized\n",
    "\n",
    "\n",
    "def load_subject_data(data_dir, subject_id, run_ids, config):\n",
    "    subject_dir = data_dir / subject_id\n",
    "    if not subject_dir.exists():\n",
    "        return None, None, None\n",
    "    \n",
    "    all_x, all_y = [], []\n",
    "    channel_names = None\n",
    "    \n",
    "    for run_id in run_ids:\n",
    "        edf_path = subject_dir / f'{subject_id}{run_id}.edf'\n",
    "        if not edf_path.exists():\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            x, y, ch_names = load_and_preprocess_edf(edf_path, config)\n",
    "            if x is None or len(y) == 0:\n",
    "                continue\n",
    "            \n",
    "            x, y = filter_classes(x, y, config['data']['selected_classes'])\n",
    "            if len(y) == 0:\n",
    "                continue\n",
    "            \n",
    "            channel_names = channel_names or ch_names\n",
    "            all_x.append(x)\n",
    "            all_y.append(y)\n",
    "        except Exception as e:\n",
    "            print(f\"  Warning: Failed to load {edf_path.name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if len(all_x) == 0:\n",
    "        return None, None, channel_names\n",
    "    \n",
    "    return np.concatenate(all_x, 0), np.concatenate(all_y, 0), channel_names\n",
    "\n",
    "\n",
    "def get_available_subjects(data_dir, min_runs=8, excluded=None):\n",
    "    if not data_dir.exists():\n",
    "        raise ValueError(f\"Data directory not found: {data_dir}\")\n",
    "    \n",
    "    excluded = excluded or set()\n",
    "    subjects = []\n",
    "    \n",
    "    for subject_dir in sorted(data_dir.iterdir()):\n",
    "        if not subject_dir.is_dir() or not subject_dir.name.startswith('S'):\n",
    "            continue\n",
    "        \n",
    "        if subject_dir.name in excluded:\n",
    "            continue\n",
    "        \n",
    "        edf_files = list(subject_dir.glob('*.edf'))\n",
    "        if len(edf_files) >= min_runs:\n",
    "            subjects.append(subject_dir.name)\n",
    "    \n",
    "    return subjects\n",
    "\n",
    "\n",
    "print(\"\\nScanning for subjects...\")\n",
    "data_dir = CONFIG['data']['raw_data_dir']\n",
    "print(f\"Looking for data in: {data_dir}\")\n",
    "\n",
    "all_subjects = get_available_subjects(\n",
    "    data_dir, \n",
    "    min_runs=CONFIG['min_runs_per_subject'],\n",
    "    excluded=EXCLUDED_SUBJECTS\n",
    ")\n",
    "subjects = all_subjects[:CONFIG['max_subjects']]\n",
    "\n",
    "print(f\"Found {len(all_subjects)} clean subjects with >= {CONFIG['min_runs_per_subject']} runs\")\n",
    "print(f\"Will process {len(subjects)} subjects: {subjects}\")\n",
    "\n",
    "MOTOR_IMAGERY_RUNS = ['R07', 'R08', 'R09', 'R10', 'R11', 'R12', 'R13', 'R14']\n",
    "MOTOR_EXECUTION_RUNS = ['R03', 'R04', 'R05', 'R06']\n",
    "ALL_TASK_RUNS = MOTOR_IMAGERY_RUNS + MOTOR_EXECUTION_RUNS\n",
    "print(f\"Using runs: {ALL_TASK_RUNS}\")\n",
    "print(\"NORMALIZATION FIXED: Using min-max normalization from paper!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = torch.FloatTensor(x).unsqueeze(1)\n",
    "        self.y = torch.LongTensor(y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return self.x[i], self.y[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comprehensive Metrics Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def calculate_comprehensive_metrics(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_preds, all_labels, all_probs = [], [], []\n",
    "\n",
    "    for X_batch, y_batch in dataloader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        outputs = model(X_batch)\n",
    "        probs = torch.softmax(outputs, dim=1)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(y_batch.numpy())\n",
    "        all_probs.extend(probs[:, 1].cpu().numpy())\n",
    "\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_probs = np.array(all_probs)\n",
    "\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(all_labels, all_preds),\n",
    "        'precision': precision_score(all_labels, all_preds, average='binary', zero_division=0),\n",
    "        'recall': recall_score(all_labels, all_preds, average='binary', zero_division=0),\n",
    "        'f1_score': f1_score(all_labels, all_preds, average='binary', zero_division=0),\n",
    "        'auc_roc': roc_auc_score(all_labels, all_probs) if len(np.unique(all_labels)) > 1 else 0.0,\n",
    "    }\n",
    "\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    if cm.shape == (2, 2):\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        metrics['specificity'] = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
    "        metrics['sensitivity'] = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    else:\n",
    "        metrics['specificity'] = 0.0\n",
    "        metrics['sensitivity'] = metrics['recall']\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "print(\"Comprehensive metrics functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvLayer(nn.Module):\n",
    "    def __init__(self, num_channels, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.num_channels = num_channels\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.A = nn.Parameter(torch.randn(num_channels, num_channels))\n",
    "        self.theta = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(hidden_dim)\n",
    "        self.act = nn.ELU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, H, C, T = x.shape\n",
    "        \n",
    "        A = torch.sigmoid(self.A)\n",
    "        A = 0.5 * (A + A.t())\n",
    "        I = torch.eye(C, device=A.device)\n",
    "        A_hat = A + I\n",
    "        D = torch.diag(torch.pow(A_hat.sum(1).clamp_min(1e-6), -0.5))\n",
    "        A_norm = D @ A_hat @ D\n",
    "        \n",
    "        x_batch = x.permute(0, 3, 2, 1).contiguous().view(B*T, C, H)\n",
    "        x_g = A_norm @ x_batch\n",
    "        x_g = self.theta(x_g)\n",
    "        x_g = x_g.view(B, T, C, H).permute(0, 3, 2, 1)\n",
    "        \n",
    "        x_out = self.bn(x_g)\n",
    "        x_out = self.act(x_out)\n",
    "        \n",
    "        return x_out\n",
    "    \n",
    "    def get_adjacency(self):\n",
    "        with torch.no_grad():\n",
    "            A = torch.sigmoid(self.A)\n",
    "            A = 0.5 * (A + A.t())\n",
    "            return A.cpu().numpy()\n",
    "\n",
    "\n",
    "class TemporalConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=16, pool=True):\n",
    "        super().__init__()\n",
    "        self.pool = pool\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, \n",
    "                            kernel_size=(1, kernel_size), \n",
    "                            padding=(0, kernel_size//2), bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.act = nn.ELU()\n",
    "        self.pool_layer = nn.AvgPool2d(kernel_size=(1, 2)) if pool else None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.act(self.bn(self.conv(x)))\n",
    "        return self.pool_layer(x) if self.pool else x\n",
    "\n",
    "\n",
    "class BaselineEEGARNN(nn.Module):\n",
    "    def __init__(self, C, T, K, H):\n",
    "        super().__init__()\n",
    "        self.t1 = TemporalConv(1, H, 16, False)\n",
    "        self.g1 = GraphConvLayer(C, H)\n",
    "        self.t2 = TemporalConv(H, H, 16, True)\n",
    "        self.g2 = GraphConvLayer(C, H)\n",
    "        self.t3 = TemporalConv(H, H, 16, True)\n",
    "        self.g3 = GraphConvLayer(C, H)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            ft = self._forward_features(torch.zeros(1, 1, C, T))\n",
    "            fs = ft.view(1, -1).size(1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(fs, 256)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(256, K)\n",
    "    \n",
    "    def _forward_features(self, x):\n",
    "        x = self.g1(self.t1(x))\n",
    "        x = self.g2(self.t2(x))\n",
    "        x = self.g3(self.t3(x))\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self._forward_features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        return self.fc2(x)\n",
    "    \n",
    "    def get_final_adjacency(self):\n",
    "        return self.g3.get_adjacency()\n",
    "\n",
    "\n",
    "class AdaptiveGatedEEGARNN(BaselineEEGARNN):\n",
    "    def __init__(self, C, T, K, H, gate_init=0.9):\n",
    "        super().__init__(C, T, K, H)\n",
    "        \n",
    "        self.gate_net = nn.Sequential(\n",
    "            nn.Linear(C * 2, C),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(C, C),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            self.gate_net[-2].bias.fill_(2.0)\n",
    "        \n",
    "        self.latest_gates = None\n",
    "    \n",
    "    def compute_gates(self, x):\n",
    "        B, _, C, T = x.shape\n",
    "        x_squeeze = x.squeeze(1)\n",
    "        ch_mean = x_squeeze.mean(dim=2)\n",
    "        ch_std = x_squeeze.std(dim=2)\n",
    "        stats = torch.cat([ch_mean, ch_std], dim=1)\n",
    "        gates = self.gate_net(stats)\n",
    "        return gates\n",
    "    \n",
    "    def forward(self, x):\n",
    "        gates = self.compute_gates(x)\n",
    "        self.latest_gates = gates.detach().cpu()\n",
    "        x = x * gates.view(-1, 1, gates.size(1), 1)\n",
    "        return super().forward(x)\n",
    "    \n",
    "    def get_gate_values(self):\n",
    "        if self.latest_gates is not None:\n",
    "            return self.latest_gates.mean(dim=0)\n",
    "        return None\n",
    "\n",
    "\n",
    "print(\"EEG-ARNN architectures defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch_with_adj_learning(model, dataloader, criterion, optimizer, device, adj_lr=0.001, l1_lambda=0.0):\n",
    "    \"\"\"Training epoch with custom adjacency matrix learning - PAPER IMPLEMENTATION\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    all_preds, all_labels = [], []\n",
    "    \n",
    "    for x, y in dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        \n",
    "        if l1_lambda > 0 and hasattr(model, 'get_gate_values'):\n",
    "            gate_values = model.get_gate_values()\n",
    "            if gate_values is not None:\n",
    "                loss = loss + l1_lambda * gate_values.abs().mean()\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        if hasattr(model, 'g1') and hasattr(model.g1, 'A'):\n",
    "            with torch.no_grad():\n",
    "                for gcn_layer in [model.g1, model.g2, model.g3]:\n",
    "                    if gcn_layer.A.grad is not None:\n",
    "                        W_grad = gcn_layer.A.grad\n",
    "                        gcn_layer.A.data = (1 - adj_lr) * gcn_layer.A.data - adj_lr * W_grad\n",
    "                        gcn_layer.A.grad.zero_()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        all_preds += torch.argmax(logits, 1).cpu().tolist()\n",
    "        all_labels += y.cpu().tolist()\n",
    "    \n",
    "    return total_loss / max(1, len(dataloader)), accuracy_score(all_labels, all_preds)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_preds, all_labels = [], []\n",
    "    \n",
    "    for x, y in dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        all_preds += torch.argmax(logits, 1).cpu().tolist()\n",
    "        all_labels += y.cpu().tolist()\n",
    "    \n",
    "    return total_loss / max(1, len(dataloader)), accuracy_score(all_labels, all_preds)\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, device, epochs, lr, patience, adj_lr=0.001, l1_lambda=0.0, verbose=True):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=3, verbose=False\n",
    "    )\n",
    "    \n",
    "    best_acc = 0.0\n",
    "    best_state = None\n",
    "    no_improve = 0\n",
    "    \n",
    "    epoch_iterator = tqdm(range(epochs), desc='    Epochs', leave=False) if verbose else range(epochs)\n",
    "    \n",
    "    for epoch in epoch_iterator:\n",
    "        train_loss, train_acc = train_epoch_with_adj_learning(\n",
    "            model, train_loader, criterion, optimizer, device, adj_lr, l1_lambda\n",
    "        )\n",
    "        val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        if verbose:\n",
    "            epoch_iterator.set_postfix({\n",
    "                'train_loss': f'{train_loss:.4f}',\n",
    "                'train_acc': f'{train_acc:.4f}',\n",
    "                'val_loss': f'{val_loss:.4f}',\n",
    "                'val_acc': f'{val_acc:.4f}',\n",
    "                'best': f'{best_acc:.4f}'\n",
    "            })\n",
    "        \n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            best_state = deepcopy(model.state_dict())\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "        \n",
    "        if no_improve >= patience:\n",
    "            if verbose:\n",
    "                print(f'      Early stopping at epoch {epoch+1}/{epochs}')\n",
    "            break\n",
    "    \n",
    "    if best_state is None:\n",
    "        best_state = deepcopy(model.state_dict())\n",
    "    \n",
    "    model.load_state_dict(best_state)\n",
    "    return best_state, best_acc\n",
    "\n",
    "\n",
    "print(\"Training functions defined with CUSTOM ADJACENCY MATRIX LEARNING!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = {'baseline': [], 'adaptive': []}\n",
    "\n",
    "print(\"\\nStarting training with PAPER IMPLEMENTATION for PhysioNet...\\n\")\n",
    "\n",
    "for subject_id in tqdm(subjects, desc='Training subjects'):\n",
    "    print(f\"\\nProcessing {subject_id}...\")\n",
    "    \n",
    "    X, Y, channel_names = load_subject_data(\n",
    "        data_dir,\n",
    "        subject_id,\n",
    "        ALL_TASK_RUNS,\n",
    "        CONFIG\n",
    "    )\n",
    "    \n",
    "    if X is None or len(Y) == 0:\n",
    "        print(f\"  Skipped: No data available\")\n",
    "        continue\n",
    "    \n",
    "    C, T = X.shape[1], X.shape[2]\n",
    "    K = len(set(CONFIG['data']['selected_classes']))\n",
    "    H = CONFIG['model']['hidden_dim']\n",
    "    \n",
    "    print(f\"  Data shape: {X.shape}\")\n",
    "    print(f\"  Label distribution: {np.bincount(Y)}\")\n",
    "    \n",
    "    for model_type in ['baseline', 'adaptive']:\n",
    "        print(f\"\\n  Training {model_type.upper()}...\")\n",
    "        \n",
    "        skf = StratifiedKFold(n_splits=CONFIG['model']['n_folds'], shuffle=True, random_state=42)\n",
    "        fold_results = []\n",
    "        adjacencies = []\n",
    "        gate_values_list = []\n",
    "        \n",
    "        for fold, (train_idx, val_idx) in enumerate(skf.split(X, Y)):\n",
    "            X_train, X_val = normalize(X[train_idx]), normalize(X[val_idx])\n",
    "            Y_train, Y_val = Y[train_idx], Y[val_idx]\n",
    "            \n",
    "            train_loader = DataLoader(\n",
    "                EEGDataset(X_train, Y_train),\n",
    "                batch_size=CONFIG['model']['batch_size'],\n",
    "                shuffle=True,\n",
    "                num_workers=0\n",
    "            )\n",
    "            val_loader = DataLoader(\n",
    "                EEGDataset(X_val, Y_val),\n",
    "                batch_size=CONFIG['model']['batch_size'],\n",
    "                shuffle=False,\n",
    "                num_workers=0\n",
    "            )\n",
    "            \n",
    "            if model_type == 'baseline':\n",
    "                model = BaselineEEGARNN(C, T, K, H).to(device)\n",
    "                l1_lambda = 0.0\n",
    "            else:\n",
    "                model = AdaptiveGatedEEGARNN(C, T, K, H, CONFIG['gating']['gate_init']).to(device)\n",
    "                l1_lambda = CONFIG['gating']['l1_lambda']\n",
    "            \n",
    "            best_state, best_acc = train_model(\n",
    "                model, train_loader, val_loader, device,\n",
    "                CONFIG['model']['epochs'],\n",
    "                CONFIG['model']['learning_rate'],\n",
    "                CONFIG['model']['patience'],\n",
    "                adj_lr=CONFIG['model']['adj_lr'],\n",
    "                l1_lambda=l1_lambda\n",
    "            )\n",
    "            model.load_state_dict(best_state)\n",
    "            \n",
    "            metrics = calculate_comprehensive_metrics(model, val_loader, device)\n",
    "            fold_results.append({'fold': fold, **metrics})\n",
    "            \n",
    "            adjacency = model.get_final_adjacency()\n",
    "            adjacencies.append(adjacency)\n",
    "            \n",
    "            if hasattr(model, 'get_gate_values'):\n",
    "                gate_values = model.get_gate_values()\n",
    "                if gate_values is not None:\n",
    "                    if isinstance(gate_values, torch.Tensor):\n",
    "                        gate_values = gate_values.detach().cpu().numpy()\n",
    "                    gate_values_list.append(gate_values)\n",
    "            \n",
    "            del model\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "        \n",
    "        avg_metrics = {}\n",
    "        for key in ['accuracy', 'precision', 'recall', 'f1_score', 'auc_roc', 'specificity']:\n",
    "            values = [f[key] for f in fold_results]\n",
    "            avg_metrics[f'avg_{key}'] = float(np.mean(values))\n",
    "            avg_metrics[f'std_{key}'] = float(np.std(values))\n",
    "        \n",
    "        avg_adjacency = np.mean(np.stack(adjacencies, 0), 0)\n",
    "        \n",
    "        result = {\n",
    "            'subject': subject_id,\n",
    "            'num_trials': X.shape[0],\n",
    "            'num_channels': C,\n",
    "            **avg_metrics,\n",
    "            'adjacency_matrix': avg_adjacency,\n",
    "            'channel_names': channel_names\n",
    "        }\n",
    "        \n",
    "        if gate_values_list:\n",
    "            result['avg_gate_values'] = np.mean(np.stack(gate_values_list, 0), 0)\n",
    "        \n",
    "        all_results[model_type].append(result)\n",
    "        \n",
    "        print(f\"    Accuracy: {avg_metrics['avg_accuracy']:.4f} ± {avg_metrics['std_accuracy']:.4f}\")\n",
    "        print(f\"    F1-Score: {avg_metrics['avg_f1_score']:.4f} ± {avg_metrics['std_f1_score']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Training Complete!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Channel Selection Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelSelector:\n",
    "    def __init__(self, adjacency, channel_names, gate_values=None):\n",
    "        self.A = adjacency\n",
    "        self.names = np.array(channel_names)\n",
    "        self.C = adjacency.shape[0]\n",
    "        self.gate_values = gate_values\n",
    "\n",
    "    def edge_selection(self, k):\n",
    "        edge_importance = np.zeros(self.C)\n",
    "        for i in range(self.C):\n",
    "            for j in range(self.C):\n",
    "                if i != j:\n",
    "                    edge_importance[i] += abs(self.A[i, j])\n",
    "                    edge_importance[j] += abs(self.A[i, j])\n",
    "\n",
    "        indices = np.sort(np.argsort(edge_importance)[-int(k):])\n",
    "        return self.names[indices].tolist(), indices\n",
    "\n",
    "    def aggregation_selection(self, k):\n",
    "        agg_scores = np.sum(np.abs(self.A), 1)\n",
    "        indices = np.sort(np.argsort(agg_scores)[-int(k):])\n",
    "        return self.names[indices].tolist(), indices\n",
    "\n",
    "    def gate_selection(self, k):\n",
    "        if self.gate_values is None:\n",
    "            raise ValueError(\"Gate values not available.\")\n",
    "\n",
    "        indices = np.sort(np.argsort(self.gate_values)[-int(k):])\n",
    "        return self.names[indices].tolist(), indices\n",
    "\n",
    "\n",
    "def retrain_with_selected_channels(x, y, selected_indices, T, K, device, config, model_type='baseline'):\n",
    "    x_selected = x[:, selected_indices, :]\n",
    "    C = len(selected_indices)\n",
    "    H = config['model']['hidden_dim']\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=config['model']['n_folds'], shuffle=True, random_state=42)\n",
    "    fold_results = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(x_selected, y)):\n",
    "        X_train = normalize(x_selected[train_idx])\n",
    "        X_val = normalize(x_selected[val_idx])\n",
    "        Y_train, Y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            EEGDataset(X_train, Y_train),\n",
    "            batch_size=config['model']['batch_size'],\n",
    "            shuffle=True,\n",
    "            num_workers=0\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            EEGDataset(X_val, Y_val),\n",
    "            batch_size=config['model']['batch_size'],\n",
    "            shuffle=False,\n",
    "            num_workers=0\n",
    "        )\n",
    "\n",
    "        if model_type == 'baseline':\n",
    "            model = BaselineEEGARNN(C, T, K, H).to(device)\n",
    "            l1_lambda = 0.0\n",
    "        else:\n",
    "            model = AdaptiveGatedEEGARNN(C, T, K, H, config['gating']['gate_init']).to(device)\n",
    "            l1_lambda = config['gating']['l1_lambda']\n",
    "\n",
    "        best_state, best_acc = train_model(\n",
    "            model, train_loader, val_loader, device,\n",
    "            config['model']['epochs'],\n",
    "            config['model']['learning_rate'],\n",
    "            config['model']['patience'],\n",
    "            adj_lr=config['model']['adj_lr'],\n",
    "            l1_lambda=l1_lambda,\n",
    "            verbose=False\n",
    "        )\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "        metrics = calculate_comprehensive_metrics(model, val_loader, device)\n",
    "        fold_results.append(metrics)\n",
    "        \n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    avg_metrics = {}\n",
    "    for key in ['accuracy', 'precision', 'recall', 'f1_score', 'auc_roc', 'specificity']:\n",
    "        values = [f[key] for f in fold_results]\n",
    "        avg_metrics[f'avg_{key}'] = float(np.mean(values))\n",
    "        avg_metrics[f'std_{key}'] = float(np.std(values))\n",
    "    \n",
    "    return avg_metrics\n",
    "\n",
    "\n",
    "print(\"Channel selection functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Channel Selection and Retraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrain_results = {'baseline': [], 'adaptive': []}\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CHANNEL SELECTION AND RETRAINING\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "for subject_id in tqdm(subjects, desc='Retraining'):\n",
    "    print(f\"\\nProcessing {subject_id}...\")\n",
    "\n",
    "    X, Y, channel_names = load_subject_data(\n",
    "        data_dir,\n",
    "        subject_id,\n",
    "        ALL_TASK_RUNS,\n",
    "        CONFIG\n",
    "    )\n",
    "\n",
    "    if X is None:\n",
    "        continue\n",
    "\n",
    "    C, T = X.shape[1], X.shape[2]\n",
    "    K = len(set(CONFIG['data']['selected_classes']))\n",
    "\n",
    "    for model_type in ['baseline', 'adaptive']:\n",
    "        subj_result = None\n",
    "        for res in all_results[model_type]:\n",
    "            if res['subject'] == subject_id:\n",
    "                subj_result = res\n",
    "                break\n",
    "\n",
    "        if subj_result is None:\n",
    "            continue\n",
    "\n",
    "        adjacency = subj_result['adjacency_matrix']\n",
    "        gate_values = subj_result.get('avg_gate_values', None)\n",
    "        selector = ChannelSelector(adjacency, channel_names, gate_values)\n",
    "\n",
    "        selection_methods = ['ES', 'AS']\n",
    "        if model_type == 'adaptive':\n",
    "            selection_methods.append('GS')\n",
    "\n",
    "        for method_name in selection_methods:\n",
    "            for k in CONFIG['channel_selection']['k_values']:\n",
    "                if method_name == 'ES':\n",
    "                    selected_channels, selected_indices = selector.edge_selection(k)\n",
    "                elif method_name == 'AS':\n",
    "                    selected_channels, selected_indices = selector.aggregation_selection(k)\n",
    "                elif method_name == 'GS':\n",
    "                    selected_channels, selected_indices = selector.gate_selection(k)\n",
    "\n",
    "                retrain_metrics = retrain_with_selected_channels(\n",
    "                    X, Y, selected_indices, T, K, device, CONFIG, model_type\n",
    "                )\n",
    "\n",
    "                acc_drop = subj_result['avg_accuracy'] - retrain_metrics['avg_accuracy']\n",
    "\n",
    "                retrain_results[model_type].append({\n",
    "                    'subject': subject_id,\n",
    "                    'method': method_name,\n",
    "                    'k': k,\n",
    "                    'num_channels_selected': len(selected_channels),\n",
    "                    **retrain_metrics,\n",
    "                    'full_channels_acc': subj_result['avg_accuracy'],\n",
    "                    'accuracy_drop': acc_drop,\n",
    "                    'accuracy_drop_pct': (acc_drop / subj_result['avg_accuracy'] * 100)\n",
    "                })\n",
    "\n",
    "                print(f\"  {model_type.upper()} - {method_name}, k={k}: \"\n",
    "                      f\"{retrain_metrics['avg_accuracy']:.4f} (drop: {acc_drop:.4f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Retraining Complete!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = CONFIG['output']['results_dir']\n",
    "\n",
    "for model_type in ['baseline', 'adaptive']:\n",
    "    if len(all_results[model_type]) > 0:\n",
    "        df = pd.DataFrame([{\n",
    "            'subject': r['subject'],\n",
    "            'num_trials': r['num_trials'],\n",
    "            'num_channels': r['num_channels'],\n",
    "            'accuracy': r['avg_accuracy'],\n",
    "            'std_accuracy': r['std_accuracy'],\n",
    "            'precision': r['avg_precision'],\n",
    "            'std_precision': r['std_precision'],\n",
    "            'recall': r['avg_recall'],\n",
    "            'std_recall': r['std_recall'],\n",
    "            'f1_score': r['avg_f1_score'],\n",
    "            'std_f1_score': r['std_f1_score'],\n",
    "            'auc_roc': r['avg_auc_roc'],\n",
    "            'std_auc_roc': r['std_auc_roc'],\n",
    "            'specificity': r['avg_specificity'],\n",
    "            'std_specificity': r['std_specificity']\n",
    "        } for r in all_results[model_type]])\n",
    "        \n",
    "        df.to_csv(results_dir / f'eeg_arnn_{model_type}_results.csv', index=False)\n",
    "        print(f\"Saved: eeg_arnn_{model_type}_results.csv\")\n",
    "\n",
    "for model_type in ['baseline', 'adaptive']:\n",
    "    if len(retrain_results[model_type]) > 0:\n",
    "        df = pd.DataFrame(retrain_results[model_type])\n",
    "        df.to_csv(results_dir / f'eeg_arnn_{model_type}_retrain_results.csv', index=False)\n",
    "        print(f\"Saved: eeg_arnn_{model_type}_retrain_results.csv\")\n",
    "\n",
    "print(f\"\\nAll results saved to {results_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESULTS SUMMARY\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "for model_type in ['baseline', 'adaptive']:\n",
    "    if len(all_results[model_type]) > 0:\n",
    "        accs = [r['avg_accuracy'] for r in all_results[model_type]]\n",
    "        f1s = [r['avg_f1_score'] for r in all_results[model_type]]\n",
    "        aucs = [r['avg_auc_roc'] for r in all_results[model_type]]\n",
    "        \n",
    "        print(f\"{model_type.upper()} Results:\")\n",
    "        print(f\"  Subjects: {len(all_results[model_type])}\")\n",
    "        print(f\"  Mean accuracy: {np.mean(accs):.4f} ± {np.std(accs):.4f}\")\n",
    "        print(f\"  Mean F1-Score: {np.mean(f1s):.4f} ± {np.std(f1s):.4f}\")\n",
    "        print(f\"  Mean AUC-ROC: {np.mean(aucs):.4f} ± {np.std(aucs):.4f}\")\n",
    "        print()\n",
    "\n",
    "if len(all_results['baseline']) > 0 and len(all_results['adaptive']) > 0:\n",
    "    baseline_acc = np.mean([r['avg_accuracy'] for r in all_results['baseline']])\n",
    "    adaptive_acc = np.mean([r['avg_accuracy'] for r in all_results['adaptive']])\n",
    "    improvement = adaptive_acc - baseline_acc\n",
    "    \n",
    "    print(f\"\\nAdaptive vs Baseline:\")\n",
    "    print(f\"  Improvement: {improvement:.4f} ({improvement/baseline_acc*100:.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DONE!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
