{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13546721,"sourceType":"datasetVersion","datasetId":8603408}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Pipeline 1: Baseline Methods Evaluation\n\nThis notebook trains and evaluates 5 baseline methods on PhysioNet Motor Imagery:\n1. **FBCSP** - Filter Bank Common Spatial Patterns\n2. **CNN-SAE** - CNN with Spatial Attention\n3. **EEGNet** - Compact temporal convolutional network\n4. **ACS-SE-CNN** - Adaptive Channel Selection SE-CNN\n5. **G-CARM** - Graph Channel Active Reasoning Module\n\n**Plus: Retention Analysis**\n- Tests how baseline methods (EEGNet) perform with reduced channels\n- Uses variance-based channel selection (simple baseline)\n- k-values: [10, 15, 20, 25, 30, 35]\n- Provides comparison baseline for Pipeline 2 (adaptive gating)\n\n**Training Configuration:**\n- Epochs: 30 (with early stopping patience=5)\n- Learning rate: 0.002\n- Batch size: 64\n- 3-fold cross-validation\n\n**Expected Runtime:** ~7-8 hours on Kaggle GPU\n- Baseline training: ~3-4 hours\n- Retention analysis: ~3.5 hours (6 k-values Ã— 3 folds)\n\n**Outputs:**\n- `models/baseline_*.pt` - Trained model checkpoints\n- `results/baseline_methods_results.csv` - Complete metrics for all models\n- `results/baseline_methods_summary.csv` - Summary statistics\n- `results/baseline_retention_analysis.csv` - Retention curve data (for comparison with Pipeline 2)","metadata":{}},{"cell_type":"markdown","source":"## 1. Setup and Configuration","metadata":{}},{"cell_type":"code","source":"import os\nimport gc\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport mne\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nimport pickle\nfrom copy import deepcopy\nimport warnings\nwarnings.filterwarnings('ignore')\nmne.set_log_level('ERROR')\n\nprint(\"All imports successful!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T05:26:30.668649Z","iopub.execute_input":"2025-11-23T05:26:30.669053Z","iopub.status.idle":"2025-11-23T05:26:36.285623Z","shell.execute_reply.started":"2025-11-23T05:26:30.669032Z","shell.execute_reply":"2025-11-23T05:26:36.284904Z"}},"outputs":[{"name":"stdout","text":"All imports successful!\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Configuration\nCONFIG = {\n    'data_path': '/kaggle/input/eeg-preprocessed-data/derived',\n    'models_dir': './models',\n    'results_dir': './results',\n    \n    'n_folds': 3,\n    'random_seed': 42,\n    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n    \n    # Training hyperparameters\n    'batch_size': 64,\n    'epochs': 30,\n    'learning_rate': 0.002,\n    'weight_decay': 1e-4,\n    'patience': 5,\n    'scheduler_patience': 2,\n    'scheduler_factor': 0.5,\n    'use_early_stopping': True,\n    'min_lr': 1e-6,\n    \n    # Data parameters\n    'n_channels': 64,\n    'n_classes': 2,\n    'sfreq': 128,\n    'tmin': 0.0,\n    'tmax': 4.0,\n    'n_timepoints': 513,\n    'mi_runs': [7, 8, 11, 12],\n    \n    # FBCSP parameters\n    'fbcsp_bands': [(4, 8), (8, 12), (12, 16), (16, 20), (20, 24), (24, 28), (28, 32), (32, 36), (36, 40)],\n    'fbcsp_n_components': 4,\n}\n\nos.makedirs(CONFIG['models_dir'], exist_ok=True)\nos.makedirs(CONFIG['results_dir'], exist_ok=True)\n\nnp.random.seed(CONFIG['random_seed'])\ntorch.manual_seed(CONFIG['random_seed'])\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(CONFIG['random_seed'])\n\nprint(f\"Device: {CONFIG['device']}\")\nprint(f\"Epochs: {CONFIG['epochs']}\")\nprint(f\"Learning rate: {CONFIG['learning_rate']}\")\nprint(f\"Early stopping patience: {CONFIG['patience']}\")\n\n# Estimate runtime\nn_models = 5\ntotal_runs = n_models * CONFIG['n_folds']\nprint(f\"\\nEstimated training runs: {total_runs}\")\nprint(f\"Estimated runtime (~12 min/run): {total_runs * 12 / 60:.1f} hours\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T05:26:36.287167Z","iopub.execute_input":"2025-11-23T05:26:36.287627Z","iopub.status.idle":"2025-11-23T05:26:36.358801Z","shell.execute_reply.started":"2025-11-23T05:26:36.287606Z","shell.execute_reply":"2025-11-23T05:26:36.357923Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\nEpochs: 30\nLearning rate: 0.002\nEarly stopping patience: 5\n\nEstimated training runs: 15\nEstimated runtime (~12 min/run): 3.0 hours\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## 2. Data Loading","metadata":{}},{"cell_type":"code","source":"def load_physionet_data(data_path):\n    \"\"\"Load preprocessed PhysioNet data.\"\"\"\n    data_root = os.path.abspath(data_path)\n    if not os.path.isdir(data_root):\n        raise FileNotFoundError(f\"Data path not found: {data_root}\")\n\n    tmin = CONFIG['tmin']\n    tmax = CONFIG['tmax']\n    mi_runs = CONFIG['mi_runs']\n    event_id = {'T1': 1, 'T2': 2}\n    label_map = {1: 0, 2: 1}\n\n    preprocessed_dir = os.path.join(data_root, 'preprocessed')\n    if os.path.isdir(preprocessed_dir):\n        data_root = preprocessed_dir\n    \n    subject_dirs = [d for d in sorted(os.listdir(data_root))\n                    if os.path.isdir(os.path.join(data_root, d)) and d.upper().startswith('S')]\n\n    all_X, all_y, all_subjects = [], [], []\n    \n    print(f\"Loading data from {len(subject_dirs)} subjects...\")\n    \n    for subject_dir in subject_dirs:\n        subject_num = int(subject_dir[1:]) if len(subject_dir) > 1 else -1\n        subject_path = os.path.join(data_root, subject_dir)\n        \n        for run_id in mi_runs:\n            run_file = f\"{subject_dir}R{run_id:02d}_preproc_raw.fif\"\n            run_path = os.path.join(subject_path, run_file)\n            \n            if not os.path.exists(run_path):\n                continue\n            \n            try:\n                raw = mne.io.read_raw_fif(run_path, preload=True, verbose=False)\n                picks = mne.pick_types(raw.info, eeg=True, meg=False, stim=False, eog=False)\n                if len(picks) == 0:\n                    continue\n                \n                events, _ = mne.events_from_annotations(raw, event_id=event_id)\n                if len(events) == 0:\n                    continue\n                \n                epochs = mne.Epochs(raw, events, event_id=event_id, tmin=tmin, tmax=tmax,\n                                    baseline=None, preload=True, picks=picks, verbose=False)\n                \n                data = epochs.get_data()\n                labels = np.array([label_map.get(epochs.events[i, 2], -1) for i in range(len(epochs))])\n                valid = labels >= 0\n                \n                if np.any(valid):\n                    all_X.append(data[valid])\n                    all_y.append(labels[valid])\n                    all_subjects.append(np.full(np.sum(valid), subject_num))\n            except Exception as e:\n                continue\n    \n    X = np.concatenate(all_X, axis=0)\n    y = np.concatenate(all_y, axis=0)\n    subjects = np.concatenate(all_subjects, axis=0)\n    \n    print(f\"Loaded {len(X)} trials from {len(np.unique(subjects))} subjects\")\n    print(f\"Data shape: {X.shape}\")\n    print(f\"Labels: {np.bincount(y)}\")\n    \n    return X, y, subjects\n\n\nclass EEGDataset(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.long)\n    \n    def __len__(self):\n        return len(self.X)\n    \n    def __getitem__(self, idx):\n        return self.X[idx], self.y[idx]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T05:26:36.359571Z","iopub.execute_input":"2025-11-23T05:26:36.359829Z","iopub.status.idle":"2025-11-23T05:26:36.373598Z","shell.execute_reply.started":"2025-11-23T05:26:36.359810Z","shell.execute_reply":"2025-11-23T05:26:36.372994Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"## 3. Model Architectures","metadata":{}},{"cell_type":"code","source":"# FBCSP\nclass FBCSP:\n    def __init__(self, freq_bands, n_components=4, sfreq=128):\n        self.freq_bands = freq_bands\n        self.n_components = n_components\n        self.sfreq = sfreq\n        self.csp_list = []\n        self.classifier = None\n    \n    def fit(self, X, y):\n        from mne.decoding import CSP\n        all_features = []\n        \n        for low, high in self.freq_bands:\n            X_filtered = self._bandpass_filter(X, low, high)\n            csp = CSP(n_components=self.n_components, reg=None, log=True, norm_trace=False)\n            features = csp.fit_transform(X_filtered, y)\n            self.csp_list.append(csp)\n            all_features.append(features)\n        \n        all_features = np.concatenate(all_features, axis=1)\n        self.classifier = LinearDiscriminantAnalysis()\n        self.classifier.fit(all_features, y)\n        return self\n    \n    def predict(self, X):\n        all_features = []\n        for idx, (low, high) in enumerate(self.freq_bands):\n            X_filtered = self._bandpass_filter(X, low, high)\n            features = self.csp_list[idx].transform(X_filtered)\n            all_features.append(features)\n        all_features = np.concatenate(all_features, axis=1)\n        return self.classifier.predict(all_features)\n    \n    def score(self, X, y):\n        return np.mean(self.predict(X) == y)\n    \n    def _bandpass_filter(self, X, low, high):\n        from scipy.signal import butter, filtfilt\n        nyq = self.sfreq / 2\n        b, a = butter(4, [low / nyq, high / nyq], btype='band')\n        X_filtered = np.zeros_like(X)\n        for i in range(X.shape[0]):\n            for j in range(X.shape[1]):\n                X_filtered[i, j, :] = filtfilt(b, a, X[i, j, :])\n        return X_filtered","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T05:26:36.374317Z","iopub.execute_input":"2025-11-23T05:26:36.374561Z","iopub.status.idle":"2025-11-23T05:26:36.393028Z","shell.execute_reply.started":"2025-11-23T05:26:36.374533Z","shell.execute_reply":"2025-11-23T05:26:36.392385Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# CNN-SAE\nclass SpatialAttention(nn.Module):\n    def __init__(self, n_channels):\n        super().__init__()\n        self.attention = nn.Sequential(\n            nn.Linear(n_channels, n_channels // 4),\n            nn.ReLU(),\n            nn.Linear(n_channels // 4, n_channels),\n            nn.Sigmoid()\n        )\n    \n    def forward(self, x):\n        pooled = torch.mean(x, dim=2)\n        weights = self.attention(pooled)\n        return x * weights.unsqueeze(2)\n\n\nclass CNNSAE(nn.Module):\n    def __init__(self, n_channels=64, n_classes=2, n_timepoints=513):\n        super().__init__()\n        self.spatial_attention = SpatialAttention(n_channels)\n        self.conv1 = nn.Conv1d(n_channels, 64, kernel_size=5, padding=2)\n        self.bn1 = nn.BatchNorm1d(64)\n        self.pool1 = nn.MaxPool1d(2)\n        self.conv2 = nn.Conv1d(64, 128, kernel_size=5, padding=2)\n        self.bn2 = nn.BatchNorm1d(128)\n        self.pool2 = nn.MaxPool1d(2)\n        self.conv3 = nn.Conv1d(128, 256, kernel_size=3, padding=1)\n        self.bn3 = nn.BatchNorm1d(256)\n        self.pool3 = nn.MaxPool1d(2)\n        self.dropout = nn.Dropout(0.5)\n        \n        with torch.no_grad():\n            test_input = torch.zeros(1, n_channels, n_timepoints)\n            test_output = self._forward_features(test_input)\n            flattened_size = test_output.view(1, -1).size(1)\n        \n        self.fc1 = nn.Linear(flattened_size, 256)\n        self.fc2 = nn.Linear(256, n_classes)\n    \n    def _forward_features(self, x):\n        x = self.spatial_attention(x)\n        x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n        x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n        x = self.pool3(F.relu(self.bn3(self.conv3(x))))\n        return x\n    \n    def forward(self, x):\n        x = self._forward_features(x)\n        x = x.view(x.size(0), -1)\n        x = self.dropout(x)\n        x = F.relu(self.fc1(x))\n        x = self.dropout(x)\n        return self.fc2(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T05:26:36.394675Z","iopub.execute_input":"2025-11-23T05:26:36.394923Z","iopub.status.idle":"2025-11-23T05:26:36.411195Z","shell.execute_reply.started":"2025-11-23T05:26:36.394900Z","shell.execute_reply":"2025-11-23T05:26:36.410551Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# EEGNet\nclass EEGNet(nn.Module):\n    def __init__(self, n_channels=64, n_classes=2, n_timepoints=513, F1=8, D=2, F2=16):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, F1, (1, 64), padding=(0, 32), bias=False)\n        self.bn1 = nn.BatchNorm2d(F1)\n        self.conv2 = nn.Conv2d(F1, F1 * D, (n_channels, 1), groups=F1, bias=False)\n        self.bn2 = nn.BatchNorm2d(F1 * D)\n        self.pool1 = nn.AvgPool2d((1, 4))\n        self.dropout1 = nn.Dropout(0.5)\n        self.conv3 = nn.Conv2d(F1 * D, F2, (1, 16), padding=(0, 8), bias=False)\n        self.bn3 = nn.BatchNorm2d(F2)\n        self.pool2 = nn.AvgPool2d((1, 8))\n        self.dropout2 = nn.Dropout(0.5)\n        \n        with torch.no_grad():\n            test_input = torch.zeros(1, 1, n_channels, n_timepoints)\n            test_output = self._forward_features(test_input)\n            flattened_size = test_output.view(1, -1).size(1)\n        \n        self.fc = nn.Linear(flattened_size, n_classes)\n\n    def _forward_features(self, x):\n        x = self.bn1(self.conv1(x))\n        x = self.dropout1(self.pool1(F.elu(self.bn2(self.conv2(x)))))\n        x = self.dropout2(self.pool2(F.elu(self.bn3(self.conv3(x)))))\n        return x\n\n    def forward(self, x):\n        x = x.unsqueeze(1)\n        x = self._forward_features(x)\n        x = x.view(x.size(0), -1)\n        return self.fc(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T05:26:36.411845Z","iopub.execute_input":"2025-11-23T05:26:36.412071Z","iopub.status.idle":"2025-11-23T05:26:36.426089Z","shell.execute_reply.started":"2025-11-23T05:26:36.412056Z","shell.execute_reply":"2025-11-23T05:26:36.425427Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# ACS-SE-CNN\nclass SEBlock(nn.Module):\n    def __init__(self, channels, reduction=4):\n        super().__init__()\n        self.fc1 = nn.Linear(channels, max(1, channels // reduction))\n        self.fc2 = nn.Linear(max(1, channels // reduction), channels)\n    \n    def forward(self, x):\n        squeeze = torch.mean(x, dim=2)\n        excitation = F.relu(self.fc1(squeeze))\n        excitation = torch.sigmoid(self.fc2(excitation))\n        return x * excitation.unsqueeze(2)\n\n\nclass ACSECNN(nn.Module):\n    def __init__(self, n_channels=64, n_classes=2, n_timepoints=513):\n        super().__init__()\n        self.channel_attention = nn.Sequential(\n            nn.Linear(n_timepoints, 128),\n            nn.ReLU(),\n            nn.Linear(128, 1),\n            nn.Sigmoid()\n        )\n        self.se1 = SEBlock(n_channels)\n        self.se2 = SEBlock(128)\n        self.se3 = SEBlock(256)\n        self.conv1 = nn.Conv1d(n_channels, 128, kernel_size=5, padding=2)\n        self.bn1 = nn.BatchNorm1d(128)\n        self.pool1 = nn.MaxPool1d(2)\n        self.conv2 = nn.Conv1d(128, 256, kernel_size=5, padding=2)\n        self.bn2 = nn.BatchNorm1d(256)\n        self.pool2 = nn.MaxPool1d(2)\n        self.conv3 = nn.Conv1d(256, 512, kernel_size=3, padding=1)\n        self.bn3 = nn.BatchNorm1d(512)\n        self.pool3 = nn.MaxPool1d(2)\n        self.dropout = nn.Dropout(0.5)\n        \n        with torch.no_grad():\n            test_input = torch.zeros(1, n_channels, n_timepoints)\n            test_output = self._forward_features(test_input)\n            flattened_size = test_output.view(1, -1).size(1)\n        \n        self.fc1 = nn.Linear(flattened_size, 256)\n        self.fc2 = nn.Linear(256, n_classes)\n    \n    def _forward_features(self, x):\n        channel_weights = []\n        for i in range(x.size(1)):\n            w = self.channel_attention(x[:, i, :])\n            channel_weights.append(w)\n        channel_weights = torch.cat(channel_weights, dim=1)\n        x = x * channel_weights.unsqueeze(2)\n        x = self.se1(x)\n        x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n        x = self.se2(x)\n        x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n        x = self.se3(x)\n        x = self.pool3(F.relu(self.bn3(self.conv3(x))))\n        return x\n    \n    def forward(self, x):\n        x = self._forward_features(x)\n        x = x.view(x.size(0), -1)\n        x = self.dropout(x)\n        x = F.relu(self.fc1(x))\n        x = self.dropout(x)\n        return self.fc2(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T05:26:36.426779Z","iopub.execute_input":"2025-11-23T05:26:36.427005Z","iopub.status.idle":"2025-11-23T05:26:36.439733Z","shell.execute_reply.started":"2025-11-23T05:26:36.426989Z","shell.execute_reply":"2025-11-23T05:26:36.438980Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# G-CARM\nclass CARMBlock(nn.Module):\n    def __init__(self, n_channels):\n        super().__init__()\n        self.A = nn.Parameter(torch.randn(n_channels, n_channels) * 0.01)\n    \n    def forward(self, x):\n        A_norm = torch.softmax(self.A, dim=1)\n        x_reshaped = x.permute(0, 2, 1)\n        x_graph = torch.matmul(x_reshaped, A_norm.t())\n        return x_graph.permute(0, 2, 1)\n\n\nclass GCARM(nn.Module):\n    def __init__(self, n_channels=64, n_classes=2, n_timepoints=513):\n        super().__init__()\n        self.carm1 = CARMBlock(n_channels)\n        self.carm2 = CARMBlock(n_channels)\n        self.conv1 = nn.Conv1d(n_channels, 128, kernel_size=5, padding=2)\n        self.bn1 = nn.BatchNorm1d(128)\n        self.pool1 = nn.MaxPool1d(2)\n        self.conv2 = nn.Conv1d(128, 256, kernel_size=5, padding=2)\n        self.bn2 = nn.BatchNorm1d(256)\n        self.pool2 = nn.MaxPool1d(2)\n        self.conv3 = nn.Conv1d(256, 512, kernel_size=3, padding=1)\n        self.bn3 = nn.BatchNorm1d(512)\n        self.pool3 = nn.MaxPool1d(2)\n        self.dropout = nn.Dropout(0.5)\n        \n        with torch.no_grad():\n            test_input = torch.zeros(1, n_channels, n_timepoints)\n            test_output = self._forward_features(test_input)\n            flattened_size = test_output.view(1, -1).size(1)\n        \n        self.fc1 = nn.Linear(flattened_size, 256)\n        self.fc2 = nn.Linear(256, n_classes)\n    \n    def _forward_features(self, x):\n        x = self.carm1(x)\n        x = self.carm2(x)\n        x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n        x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n        x = self.pool3(F.relu(self.bn3(self.conv3(x))))\n        return x\n    \n    def forward(self, x):\n        x = self._forward_features(x)\n        x = x.view(x.size(0), -1)\n        x = self.dropout(x)\n        x = F.relu(self.fc1(x))\n        x = self.dropout(x)\n        return self.fc2(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T05:26:36.440465Z","iopub.execute_input":"2025-11-23T05:26:36.440628Z","iopub.status.idle":"2025-11-23T05:26:36.459539Z","shell.execute_reply.started":"2025-11-23T05:26:36.440614Z","shell.execute_reply":"2025-11-23T05:26:36.458745Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"## 4. Training Utilities","metadata":{}},{"cell_type":"code","source":"def train_epoch(model, dataloader, criterion, optimizer, device):\n    model.train()\n    total_loss, correct, total = 0.0, 0, 0\n    \n    for X_batch, y_batch in dataloader:\n        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n        optimizer.zero_grad()\n        outputs = model(X_batch)\n        loss = criterion(outputs, y_batch)\n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n        _, predicted = torch.max(outputs.data, 1)\n        total += y_batch.size(0)\n        correct += (predicted == y_batch).sum().item()\n    \n    return total_loss / max(1, len(dataloader)), correct / max(1, total)\n\n\ndef evaluate(model, dataloader, criterion, device):\n    model.eval()\n    total_loss, correct, total = 0.0, 0, 0\n    \n    with torch.no_grad():\n        for X_batch, y_batch in dataloader:\n            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n            outputs = model(X_batch)\n            loss = criterion(outputs, y_batch)\n            \n            total_loss += loss.item()\n            _, predicted = torch.max(outputs.data, 1)\n            total += y_batch.size(0)\n            correct += (predicted == y_batch).sum().item()\n    \n    return total_loss / max(1, len(dataloader)), correct / max(1, total)\n\n\ndef train_pytorch_model(model, train_loader, val_loader, config, model_name=''):\n    device = config['device']\n    model = model.to(device)\n    \n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'], \n                          weight_decay=config['weight_decay'])\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, mode='min', factor=config['scheduler_factor'], \n        patience=config['scheduler_patience'], min_lr=config['min_lr'], verbose=False\n    )\n    \n    best_state = deepcopy(model.state_dict())\n    best_val_acc = 0.0\n    best_val_loss = float('inf')\n    patience_counter = 0\n    \n    for epoch in range(config['epochs']):\n        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n        val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n        scheduler.step(val_loss)\n        \n        improved = val_acc > best_val_acc or (val_acc == best_val_acc and val_loss < best_val_loss)\n        if improved:\n            best_state = deepcopy(model.state_dict())\n            best_val_acc = val_acc\n            best_val_loss = val_loss\n            patience_counter = 0\n        else:\n            patience_counter += 1\n        \n        if epoch % 5 == 0 or improved:\n            print(f\"[{model_name}] Epoch {epoch+1}/{config['epochs']} - \"\n                  f\"Train: {train_acc:.4f} | Val: {val_acc:.4f} | Best: {best_val_acc:.4f}\")\n        \n        if config['use_early_stopping'] and patience_counter >= config['patience']:\n            print(f\"Early stopping at epoch {epoch+1}\")\n            break\n    \n    model.load_state_dict(best_state)\n    return best_state, best_val_acc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T05:26:36.460321Z","iopub.execute_input":"2025-11-23T05:26:36.460530Z","iopub.status.idle":"2025-11-23T05:26:36.479347Z","shell.execute_reply.started":"2025-11-23T05:26:36.460514Z","shell.execute_reply":"2025-11-23T05:26:36.478633Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"## 5. Load Data","metadata":{}},{"cell_type":"code","source":"print(\"Loading PhysioNet data...\")\nX, y, subjects = load_physionet_data(CONFIG['data_path'])\nprint(f\"\\nData ready for training!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T05:26:36.480106Z","iopub.execute_input":"2025-11-23T05:26:36.480309Z","iopub.status.idle":"2025-11-23T05:26:58.072784Z","shell.execute_reply.started":"2025-11-23T05:26:36.480289Z","shell.execute_reply":"2025-11-23T05:26:58.071907Z"}},"outputs":[{"name":"stdout","text":"Loading PhysioNet data...\nLoading data from 51 subjects...\nLoaded 2966 trials from 51 subjects\nData shape: (2966, 64, 513)\nLabels: [1489 1477]\n\nData ready for training!\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"## 6. Train All Baseline Models","metadata":{}},{"cell_type":"code","source":"# Define models\nmodels_to_train = [\n    {'name': 'FBCSP', 'type': 'sklearn'},\n    {'name': 'CNN-SAE', 'type': 'pytorch'},\n    {'name': 'EEGNet', 'type': 'pytorch'},\n    {'name': 'ACS-SE-CNN', 'type': 'pytorch'},\n    {'name': 'G-CARM', 'type': 'pytorch'},\n]\n\nall_results = []\nskf = StratifiedKFold(n_splits=CONFIG['n_folds'], shuffle=True, random_state=CONFIG['random_seed'])\n\nprint(f\"\\n{'='*60}\")\nprint(\"TRAINING BASELINE METHODS\")\nprint(f\"{'='*60}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T05:26:58.073727Z","iopub.execute_input":"2025-11-23T05:26:58.074175Z","iopub.status.idle":"2025-11-23T05:26:58.079487Z","shell.execute_reply.started":"2025-11-23T05:26:58.074153Z","shell.execute_reply":"2025-11-23T05:26:58.078866Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\nTRAINING BASELINE METHODS\n============================================================\n\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# Training loop\nfor model_info in models_to_train:\n    model_name = model_info['name']\n    model_type = model_info['type']\n    \n    print(f\"\\n{'='*60}\")\n    print(f\"Training {model_name}\")\n    print(f\"{'='*60}\\n\")\n    \n    fold_results = []\n    \n    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n        print(f\"\\nFold {fold + 1}/{CONFIG['n_folds']}\")\n        \n        X_train, X_val = X[train_idx], X[val_idx]\n        y_train, y_val = y[train_idx], y[val_idx]\n        \n        if model_type == 'sklearn':\n            model = FBCSP(freq_bands=CONFIG['fbcsp_bands'],\n                          n_components=CONFIG['fbcsp_n_components'],\n                          sfreq=CONFIG['sfreq'])\n            model.fit(X_train, y_train)\n            val_acc = model.score(X_val, y_val)\n            \n            model_path = os.path.join(CONFIG['models_dir'], f\"baseline_{model_name}_fold{fold+1}.pkl\")\n            with open(model_path, 'wb') as f:\n                pickle.dump(model, f)\n        else:\n            train_dataset = EEGDataset(X_train, y_train)\n            val_dataset = EEGDataset(X_val, y_val)\n            train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True)\n            val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'], shuffle=False)\n            \n            base_kwargs = {\n                'n_channels': CONFIG['n_channels'],\n                'n_classes': CONFIG['n_classes'],\n                'n_timepoints': CONFIG['n_timepoints'],\n            }\n            \n            if model_name == 'CNN-SAE':\n                model = CNNSAE(**base_kwargs)\n            elif model_name == 'EEGNet':\n                model = EEGNet(**base_kwargs)\n            elif model_name == 'ACS-SE-CNN':\n                model = ACSECNN(**base_kwargs)\n            elif model_name == 'G-CARM':\n                model = GCARM(**base_kwargs)\n            \n            best_state, val_acc = train_pytorch_model(model, train_loader, val_loader, CONFIG, model_name)\n            \n            model_path = os.path.join(CONFIG['models_dir'], f\"baseline_{model_name}_fold{fold+1}.pt\")\n            torch.save(best_state, model_path)\n            \n            del model\n            torch.cuda.empty_cache()\n            gc.collect()\n        \n        fold_results.append({'fold': fold + 1, 'accuracy': val_acc})\n        print(f\"Fold {fold + 1} Accuracy: {val_acc:.4f}\")\n    \n    fold_accs = [r['accuracy'] for r in fold_results]\n    mean_acc = np.mean(fold_accs)\n    std_acc = np.std(fold_accs)\n    \n    print(f\"\\n{model_name} Summary:\")\n    print(f\"Mean Accuracy: {mean_acc:.4f} +/- {std_acc:.4f}\")\n    \n    all_results.append({\n        'model': model_name,\n        'mean_accuracy': mean_acc,\n        'std_accuracy': std_acc,\n        'fold_results': fold_results\n    })\n\nprint(f\"\\n{'='*60}\")\nprint(\"ALL BASELINE MODELS TRAINED!\")\nprint(f\"{'='*60}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T05:26:58.080808Z","iopub.execute_input":"2025-11-23T05:26:58.081042Z","iopub.status.idle":"2025-11-23T05:44:07.486936Z","shell.execute_reply.started":"2025-11-23T05:26:58.081026Z","shell.execute_reply":"2025-11-23T05:44:07.486268Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\nTraining FBCSP\n============================================================\n\n\nFold 1/3\nFold 1 Accuracy: 0.6208\n\nFold 2/3\nFold 2 Accuracy: 0.6077\n\nFold 3/3\nFold 3 Accuracy: 0.6144\n\nFBCSP Summary:\nMean Accuracy: 0.6143 +/- 0.0054\n\n============================================================\nTraining CNN-SAE\n============================================================\n\n\nFold 1/3\n[CNN-SAE] Epoch 1/30 - Train: 0.5645 | Val: 0.5025 | Best: 0.5025\n[CNN-SAE] Epoch 6/30 - Train: 0.8518 | Val: 0.4975 | Best: 0.5025\nEarly stopping at epoch 6\nFold 1 Accuracy: 0.5025\n\nFold 2/3\n[CNN-SAE] Epoch 1/30 - Train: 0.5443 | Val: 0.5015 | Best: 0.5015\n[CNN-SAE] Epoch 5/30 - Train: 0.8366 | Val: 0.5096 | Best: 0.5096\n[CNN-SAE] Epoch 6/30 - Train: 0.8392 | Val: 0.4985 | Best: 0.5096\n[CNN-SAE] Epoch 9/30 - Train: 0.8624 | Val: 0.5116 | Best: 0.5116\n[CNN-SAE] Epoch 11/30 - Train: 0.8685 | Val: 0.5015 | Best: 0.5116\n[CNN-SAE] Epoch 13/30 - Train: 0.8811 | Val: 0.5430 | Best: 0.5430\n[CNN-SAE] Epoch 16/30 - Train: 0.8806 | Val: 0.5197 | Best: 0.5430\n[CNN-SAE] Epoch 18/30 - Train: 0.8857 | Val: 0.5753 | Best: 0.5753\n[CNN-SAE] Epoch 21/30 - Train: 0.8877 | Val: 0.6441 | Best: 0.6441\n[CNN-SAE] Epoch 26/30 - Train: 0.8907 | Val: 0.7139 | Best: 0.7139\n[CNN-SAE] Epoch 27/30 - Train: 0.8912 | Val: 0.7978 | Best: 0.7978\n[CNN-SAE] Epoch 28/30 - Train: 0.8897 | Val: 0.8190 | Best: 0.8190\n[CNN-SAE] Epoch 29/30 - Train: 0.8842 | Val: 0.8301 | Best: 0.8301\nFold 2 Accuracy: 0.8301\n\nFold 3/3\n[CNN-SAE] Epoch 1/30 - Train: 0.5308 | Val: 0.4980 | Best: 0.4980\n[CNN-SAE] Epoch 2/30 - Train: 0.7508 | Val: 0.5020 | Best: 0.5020\n[CNN-SAE] Epoch 6/30 - Train: 0.8453 | Val: 0.4980 | Best: 0.5020\nEarly stopping at epoch 7\nFold 3 Accuracy: 0.5020\n\nCNN-SAE Summary:\nMean Accuracy: 0.6116 +/- 0.1546\n\n============================================================\nTraining EEGNet\n============================================================\n\n\nFold 1/3\n[EEGNet] Epoch 1/30 - Train: 0.7481 | Val: 0.4975 | Best: 0.4975\n[EEGNet] Epoch 2/30 - Train: 0.8012 | Val: 0.5005 | Best: 0.5005\n[EEGNet] Epoch 3/30 - Train: 0.8220 | Val: 0.7058 | Best: 0.7058\n[EEGNet] Epoch 4/30 - Train: 0.8306 | Val: 0.7806 | Best: 0.7806\n[EEGNet] Epoch 5/30 - Train: 0.8376 | Val: 0.8231 | Best: 0.8231\n[EEGNet] Epoch 6/30 - Train: 0.8422 | Val: 0.8291 | Best: 0.8291\n[EEGNet] Epoch 10/30 - Train: 0.8720 | Val: 0.8311 | Best: 0.8311\n[EEGNet] Epoch 11/30 - Train: 0.8634 | Val: 0.8281 | Best: 0.8311\n[EEGNet] Epoch 14/30 - Train: 0.8741 | Val: 0.8332 | Best: 0.8332\n[EEGNet] Epoch 16/30 - Train: 0.8786 | Val: 0.8281 | Best: 0.8332\nEarly stopping at epoch 19\nFold 1 Accuracy: 0.8332\n\nFold 2/3\n[EEGNet] Epoch 1/30 - Train: 0.7542 | Val: 0.4985 | Best: 0.4985\n[EEGNet] Epoch 2/30 - Train: 0.8108 | Val: 0.6390 | Best: 0.6390\n[EEGNet] Epoch 3/30 - Train: 0.8184 | Val: 0.8301 | Best: 0.8301\n[EEGNet] Epoch 4/30 - Train: 0.8300 | Val: 0.8301 | Best: 0.8301\n[EEGNet] Epoch 5/30 - Train: 0.8351 | Val: 0.8311 | Best: 0.8311\n[EEGNet] Epoch 6/30 - Train: 0.8447 | Val: 0.8382 | Best: 0.8382\n[EEGNet] Epoch 9/30 - Train: 0.8614 | Val: 0.8382 | Best: 0.8382\n[EEGNet] Epoch 11/30 - Train: 0.8614 | Val: 0.8311 | Best: 0.8382\n[EEGNet] Epoch 12/30 - Train: 0.8584 | Val: 0.8392 | Best: 0.8392\n[EEGNet] Epoch 15/30 - Train: 0.8715 | Val: 0.8402 | Best: 0.8402\n[EEGNet] Epoch 16/30 - Train: 0.8725 | Val: 0.8382 | Best: 0.8402\n[EEGNet] Epoch 18/30 - Train: 0.8685 | Val: 0.8443 | Best: 0.8443\n[EEGNet] Epoch 20/30 - Train: 0.8720 | Val: 0.8473 | Best: 0.8473\n[EEGNet] Epoch 21/30 - Train: 0.8781 | Val: 0.8413 | Best: 0.8473\nEarly stopping at epoch 25\nFold 2 Accuracy: 0.8473\n\nFold 3/3\n[EEGNet] Epoch 1/30 - Train: 0.7310 | Val: 0.4980 | Best: 0.4980\n[EEGNet] Epoch 2/30 - Train: 0.8059 | Val: 0.5020 | Best: 0.5020\n[EEGNet] Epoch 3/30 - Train: 0.8291 | Val: 0.6468 | Best: 0.6468\n[EEGNet] Epoch 4/30 - Train: 0.8311 | Val: 0.6903 | Best: 0.6903\n[EEGNet] Epoch 5/30 - Train: 0.8251 | Val: 0.7682 | Best: 0.7682\n[EEGNet] Epoch 6/30 - Train: 0.8413 | Val: 0.7621 | Best: 0.7682\n[EEGNet] Epoch 7/30 - Train: 0.8498 | Val: 0.8411 | Best: 0.8411\n[EEGNet] Epoch 11/30 - Train: 0.8680 | Val: 0.7854 | Best: 0.8411\nEarly stopping at epoch 12\nFold 3 Accuracy: 0.8411\n\nEEGNet Summary:\nMean Accuracy: 0.8405 +/- 0.0058\n\n============================================================\nTraining ACS-SE-CNN\n============================================================\n\n\nFold 1/3\n[ACS-SE-CNN] Epoch 1/30 - Train: 0.4911 | Val: 0.4975 | Best: 0.4975\n[ACS-SE-CNN] Epoch 2/30 - Train: 0.5180 | Val: 0.5025 | Best: 0.5025\n[ACS-SE-CNN] Epoch 6/30 - Train: 0.6550 | Val: 0.5025 | Best: 0.5025\n[ACS-SE-CNN] Epoch 7/30 - Train: 0.7481 | Val: 0.5056 | Best: 0.5056\n[ACS-SE-CNN] Epoch 11/30 - Train: 0.8331 | Val: 0.4975 | Best: 0.5056\nEarly stopping at epoch 12\nFold 1 Accuracy: 0.5056\n\nFold 2/3\n[ACS-SE-CNN] Epoch 1/30 - Train: 0.5422 | Val: 0.5015 | Best: 0.5015\n[ACS-SE-CNN] Epoch 2/30 - Train: 0.7683 | Val: 0.5015 | Best: 0.5015\n[ACS-SE-CNN] Epoch 6/30 - Train: 0.8452 | Val: 0.4985 | Best: 0.5015\nEarly stopping at epoch 7\nFold 2 Accuracy: 0.5015\n\nFold 3/3\n[ACS-SE-CNN] Epoch 1/30 - Train: 0.5966 | Val: 0.4980 | Best: 0.4980\n[ACS-SE-CNN] Epoch 3/30 - Train: 0.8195 | Val: 0.5020 | Best: 0.5020\n[ACS-SE-CNN] Epoch 6/30 - Train: 0.8418 | Val: 0.4980 | Best: 0.5020\nEarly stopping at epoch 8\nFold 3 Accuracy: 0.5020\n\nACS-SE-CNN Summary:\nMean Accuracy: 0.5030 +/- 0.0018\n\n============================================================\nTraining G-CARM\n============================================================\n\n\nFold 1/3\n[G-CARM] Epoch 1/30 - Train: 0.5043 | Val: 0.5025 | Best: 0.5025\n[G-CARM] Epoch 6/30 - Train: 0.8462 | Val: 0.5025 | Best: 0.5025\nEarly stopping at epoch 6\nFold 1 Accuracy: 0.5025\n\nFold 2/3\n[G-CARM] Epoch 1/30 - Train: 0.4876 | Val: 0.4985 | Best: 0.4985\n[G-CARM] Epoch 6/30 - Train: 0.5023 | Val: 0.4985 | Best: 0.4985\nEarly stopping at epoch 6\nFold 2 Accuracy: 0.4985\n\nFold 3/3\n[G-CARM] Epoch 1/30 - Train: 0.5046 | Val: 0.4980 | Best: 0.4980\n[G-CARM] Epoch 6/30 - Train: 0.7740 | Val: 0.4980 | Best: 0.4980\nEarly stopping at epoch 6\nFold 3 Accuracy: 0.4980\n\nG-CARM Summary:\nMean Accuracy: 0.4997 +/- 0.0020\n\n============================================================\nALL BASELINE MODELS TRAINED!\n============================================================\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"## 7. Save Results","metadata":{}},{"cell_type":"markdown","source":"## 8. Retention Analysis\n\nTest how baseline methods perform with reduced channels using variance-based selection.\nThis provides a fair comparison baseline for the adaptive gating approach in Pipeline 2.","metadata":{}},{"cell_type":"code","source":"# Variance-based channel importance\ndef get_channel_importance_variance(X_train):\n    \"\"\"Compute channel importance based on temporal variance.\"\"\"\n    # X_train: (n_trials, n_channels, n_timepoints)\n    channel_variance = np.var(X_train, axis=(0, 2))  # variance across trials and time\n    return channel_variance\n\n\ndef select_top_k_channels(importance_scores, k):\n    \"\"\"Select top k channels based on importance scores.\"\"\"\n    top_k_indices = np.argsort(importance_scores)[-k:]\n    return sorted(top_k_indices)\n\n\ndef apply_channel_selection(X, selected_channels):\n    \"\"\"Apply channel selection to data.\"\"\"\n    return X[:, selected_channels, :]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T05:44:07.487605Z","iopub.execute_input":"2025-11-23T05:44:07.488019Z","iopub.status.idle":"2025-11-23T05:44:07.492844Z","shell.execute_reply.started":"2025-11-23T05:44:07.487993Z","shell.execute_reply":"2025-11-23T05:44:07.492131Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# Retention analysis configuration\nRETENTION_K_VALUES = [10, 15, 20, 25, 30, 35]\nRETENTION_MODEL = 'EEGNet'  # Use EEGNet as baseline representative\n\nprint(f\"Running retention analysis for {RETENTION_MODEL} with k-values: {RETENTION_K_VALUES}\")\nprint(f\"Estimated time: {len(RETENTION_K_VALUES) * CONFIG['n_folds'] * 12 / 60:.1f} hours\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T05:54:12.132891Z","iopub.execute_input":"2025-11-23T05:54:12.133488Z","iopub.status.idle":"2025-11-23T05:54:12.138145Z","shell.execute_reply.started":"2025-11-23T05:54:12.133462Z","shell.execute_reply":"2025-11-23T05:54:12.137296Z"}},"outputs":[{"name":"stdout","text":"Running retention analysis for EEGNet with k-values: [10, 15, 20, 25, 30, 35]\nEstimated time: 3.6 hours\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"**fixed multiple line string error**","metadata":{}},{"cell_type":"code","source":"# Run retention analysis\nretention_results = []\n\nfor k in RETENTION_K_VALUES:\n    print(f\"\\nTesting with k={k} channels:\", end=' ')\n    fold_accuracies = []\n    \n    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n        X_train, X_val = X[train_idx], X[val_idx]\n        y_train, y_val = y[train_idx], y[val_idx]\n        \n        # Compute channel importance on training data\n        importance_scores = get_channel_importance_variance(X_train)\n        selected_channels = select_top_k_channels(importance_scores, k)\n        \n        # Apply channel selection\n        X_train_selected = apply_channel_selection(X_train, selected_channels)\n        X_val_selected = apply_channel_selection(X_val, selected_channels)\n        \n        # Train model with selected channels\n        train_dataset = EEGDataset(X_train_selected, y_train)\n        val_dataset = EEGDataset(X_val_selected, y_val)\n        train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True)\n        val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'], shuffle=False)\n        \n        # Modify config for reduced channels\n        temp_config = CONFIG.copy()\n        temp_config['n_channels'] = k\n        \n        model = EEGNet(n_channels=k, n_classes=CONFIG['n_classes'], n_timepoints=CONFIG['n_timepoints'])\n        best_state, val_acc = train_pytorch_model(model, train_loader, val_loader, temp_config, f\"Retention-k{k}\")\n        \n        fold_accuracies.append(val_acc)\n        \n        del model\n        torch.cuda.empty_cache()\n        gc.collect()\n    \n    mean_acc = np.mean(fold_accuracies)\n    std_acc = np.std(fold_accuracies)\n    print(f\"{mean_acc:.4f} +/- {std_acc:.4f}\")\n    \n    retention_results.append({\n        'model': RETENTION_MODEL,\n        'k': k,\n        'mean_accuracy': mean_acc,\n        'std_accuracy': std_acc,\n        'fold_accuracies': fold_accuracies\n    })\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T05:55:28.210685Z","iopub.execute_input":"2025-11-23T05:55:28.211241Z","iopub.status.idle":"2025-11-23T05:57:33.776654Z","shell.execute_reply.started":"2025-11-23T05:55:28.211212Z","shell.execute_reply":"2025-11-23T05:57:33.775879Z"}},"outputs":[{"name":"stdout","text":"\nTesting with k=10 channels: [Retention-k10] Epoch 1/30 - Train: 0.7066 | Val: 0.5025 | Best: 0.5025\n[Retention-k10] Epoch 3/30 - Train: 0.7785 | Val: 0.7432 | Best: 0.7432\n[Retention-k10] Epoch 4/30 - Train: 0.7860 | Val: 0.8018 | Best: 0.8018\n[Retention-k10] Epoch 6/30 - Train: 0.7992 | Val: 0.7998 | Best: 0.8018\n[Retention-k10] Epoch 8/30 - Train: 0.8048 | Val: 0.8119 | Best: 0.8119\n[Retention-k10] Epoch 11/30 - Train: 0.8128 | Val: 0.7998 | Best: 0.8119\nEarly stopping at epoch 13\n[Retention-k10] Epoch 1/30 - Train: 0.7355 | Val: 0.5015 | Best: 0.5015\n[Retention-k10] Epoch 2/30 - Train: 0.7795 | Val: 0.5915 | Best: 0.5915\n[Retention-k10] Epoch 3/30 - Train: 0.7840 | Val: 0.7634 | Best: 0.7634\n[Retention-k10] Epoch 4/30 - Train: 0.7921 | Val: 0.7927 | Best: 0.7927\n[Retention-k10] Epoch 6/30 - Train: 0.8027 | Val: 0.8008 | Best: 0.8008\n[Retention-k10] Epoch 7/30 - Train: 0.8053 | Val: 0.8018 | Best: 0.8018\n[Retention-k10] Epoch 10/30 - Train: 0.7997 | Val: 0.8028 | Best: 0.8028\n[Retention-k10] Epoch 11/30 - Train: 0.8027 | Val: 0.7887 | Best: 0.8028\n[Retention-k10] Epoch 13/30 - Train: 0.8063 | Val: 0.8038 | Best: 0.8038\n[Retention-k10] Epoch 14/30 - Train: 0.8108 | Val: 0.8049 | Best: 0.8049\n[Retention-k10] Epoch 16/30 - Train: 0.8128 | Val: 0.7988 | Best: 0.8049\nEarly stopping at epoch 19\n[Retention-k10] Epoch 1/30 - Train: 0.7336 | Val: 0.5020 | Best: 0.5020\n[Retention-k10] Epoch 2/30 - Train: 0.7952 | Val: 0.7237 | Best: 0.7237\n[Retention-k10] Epoch 3/30 - Train: 0.7983 | Val: 0.7632 | Best: 0.7632\n[Retention-k10] Epoch 4/30 - Train: 0.7998 | Val: 0.7763 | Best: 0.7763\n[Retention-k10] Epoch 5/30 - Train: 0.7983 | Val: 0.7955 | Best: 0.7955\n[Retention-k10] Epoch 6/30 - Train: 0.8033 | Val: 0.7814 | Best: 0.7955\nEarly stopping at epoch 10\n0.8041 +/- 0.0067\n\nTesting with k=15 channels: [Retention-k15] Epoch 1/30 - Train: 0.7582 | Val: 0.5025 | Best: 0.5025\n[Retention-k15] Epoch 2/30 - Train: 0.7876 | Val: 0.5147 | Best: 0.5147\n[Retention-k15] Epoch 3/30 - Train: 0.8128 | Val: 0.8129 | Best: 0.8129\n[Retention-k15] Epoch 6/30 - Train: 0.8280 | Val: 0.7998 | Best: 0.8129\n[Retention-k15] Epoch 8/30 - Train: 0.8250 | Val: 0.8170 | Best: 0.8170\n[Retention-k15] Epoch 10/30 - Train: 0.8280 | Val: 0.8220 | Best: 0.8220\n[Retention-k15] Epoch 11/30 - Train: 0.8270 | Val: 0.8210 | Best: 0.8220\nEarly stopping at epoch 15\n[Retention-k15] Epoch 1/30 - Train: 0.7329 | Val: 0.4985 | Best: 0.4985\n[Retention-k15] Epoch 2/30 - Train: 0.7921 | Val: 0.5379 | Best: 0.5379\n[Retention-k15] Epoch 3/30 - Train: 0.7987 | Val: 0.7958 | Best: 0.7958\n[Retention-k15] Epoch 4/30 - Train: 0.8022 | Val: 0.8099 | Best: 0.8099\n[Retention-k15] Epoch 6/30 - Train: 0.8199 | Val: 0.8099 | Best: 0.8099\n[Retention-k15] Epoch 7/30 - Train: 0.8149 | Val: 0.8099 | Best: 0.8099\n[Retention-k15] Epoch 8/30 - Train: 0.8169 | Val: 0.8109 | Best: 0.8109\n[Retention-k15] Epoch 9/30 - Train: 0.8184 | Val: 0.8140 | Best: 0.8140\n[Retention-k15] Epoch 11/30 - Train: 0.8073 | Val: 0.8038 | Best: 0.8140\n[Retention-k15] Epoch 13/30 - Train: 0.8134 | Val: 0.8190 | Best: 0.8190\n[Retention-k15] Epoch 16/30 - Train: 0.8209 | Val: 0.8038 | Best: 0.8190\nEarly stopping at epoch 18\n[Retention-k15] Epoch 1/30 - Train: 0.7422 | Val: 0.5020 | Best: 0.5020\n[Retention-k15] Epoch 2/30 - Train: 0.7902 | Val: 0.5030 | Best: 0.5030\n[Retention-k15] Epoch 3/30 - Train: 0.8003 | Val: 0.7196 | Best: 0.7196\n[Retention-k15] Epoch 4/30 - Train: 0.8089 | Val: 0.7773 | Best: 0.7773\n[Retention-k15] Epoch 5/30 - Train: 0.8064 | Val: 0.8067 | Best: 0.8067\n[Retention-k15] Epoch 6/30 - Train: 0.8140 | Val: 0.7986 | Best: 0.8067\n[Retention-k15] Epoch 7/30 - Train: 0.8170 | Val: 0.8117 | Best: 0.8117\n[Retention-k15] Epoch 11/30 - Train: 0.8215 | Val: 0.7986 | Best: 0.8117\nEarly stopping at epoch 12\n0.8176 +/- 0.0043\n\nTesting with k=20 channels: [Retention-k20] Epoch 1/30 - Train: 0.7420 | Val: 0.4975 | Best: 0.4975\n[Retention-k20] Epoch 2/30 - Train: 0.7891 | Val: 0.6087 | Best: 0.6087\n[Retention-k20] Epoch 3/30 - Train: 0.8022 | Val: 0.8049 | Best: 0.8049\n[Retention-k20] Epoch 4/30 - Train: 0.8078 | Val: 0.8160 | Best: 0.8160\n[Retention-k20] Epoch 5/30 - Train: 0.8118 | Val: 0.8180 | Best: 0.8180\n[Retention-k20] Epoch 6/30 - Train: 0.8053 | Val: 0.8109 | Best: 0.8180\nEarly stopping at epoch 10\n[Retention-k20] Epoch 1/30 - Train: 0.6844 | Val: 0.5015 | Best: 0.5015\n[Retention-k20] Epoch 2/30 - Train: 0.7926 | Val: 0.5592 | Best: 0.5592\n[Retention-k20] Epoch 3/30 - Train: 0.8017 | Val: 0.8140 | Best: 0.8140\n[Retention-k20] Epoch 4/30 - Train: 0.8093 | Val: 0.8210 | Best: 0.8210\n[Retention-k20] Epoch 6/30 - Train: 0.8164 | Val: 0.8079 | Best: 0.8210\nEarly stopping at epoch 9\n[Retention-k20] Epoch 1/30 - Train: 0.6916 | Val: 0.5020 | Best: 0.5020\n[Retention-k20] Epoch 2/30 - Train: 0.7973 | Val: 0.7156 | Best: 0.7156\n[Retention-k20] Epoch 3/30 - Train: 0.8119 | Val: 0.8047 | Best: 0.8047\n[Retention-k20] Epoch 4/30 - Train: 0.8210 | Val: 0.8117 | Best: 0.8117\n[Retention-k20] Epoch 6/30 - Train: 0.8170 | Val: 0.8148 | Best: 0.8148\n[Retention-k20] Epoch 8/30 - Train: 0.8276 | Val: 0.8209 | Best: 0.8209\n[Retention-k20] Epoch 9/30 - Train: 0.8322 | Val: 0.8219 | Best: 0.8219\n[Retention-k20] Epoch 11/30 - Train: 0.8286 | Val: 0.8229 | Best: 0.8229\n[Retention-k20] Epoch 15/30 - Train: 0.8337 | Val: 0.8249 | Best: 0.8249\n[Retention-k20] Epoch 16/30 - Train: 0.8327 | Val: 0.8158 | Best: 0.8249\n[Retention-k20] Epoch 17/30 - Train: 0.8387 | Val: 0.8310 | Best: 0.8310\n[Retention-k20] Epoch 18/30 - Train: 0.8433 | Val: 0.8330 | Best: 0.8330\n[Retention-k20] Epoch 21/30 - Train: 0.8448 | Val: 0.8269 | Best: 0.8330\nEarly stopping at epoch 23\n0.8240 +/- 0.0065\n\nTesting with k=25 channels: [Retention-k25] Epoch 1/30 - Train: 0.7370 | Val: 0.5025 | Best: 0.5025\n[Retention-k25] Epoch 2/30 - Train: 0.8022 | Val: 0.7391 | Best: 0.7391\n[Retention-k25] Epoch 3/30 - Train: 0.8058 | Val: 0.8099 | Best: 0.8099\n[Retention-k25] Epoch 4/30 - Train: 0.8164 | Val: 0.8190 | Best: 0.8190\n[Retention-k25] Epoch 6/30 - Train: 0.8179 | Val: 0.7998 | Best: 0.8190\n[Retention-k25] Epoch 7/30 - Train: 0.8199 | Val: 0.8301 | Best: 0.8301\n[Retention-k25] Epoch 11/30 - Train: 0.8316 | Val: 0.7927 | Best: 0.8301\nEarly stopping at epoch 12\n[Retention-k25] Epoch 1/30 - Train: 0.7375 | Val: 0.4985 | Best: 0.4985\n[Retention-k25] Epoch 2/30 - Train: 0.7926 | Val: 0.8059 | Best: 0.8059\n[Retention-k25] Epoch 6/30 - Train: 0.8149 | Val: 0.7978 | Best: 0.8059\nEarly stopping at epoch 7\n[Retention-k25] Epoch 1/30 - Train: 0.7164 | Val: 0.4980 | Best: 0.4980\n[Retention-k25] Epoch 2/30 - Train: 0.8043 | Val: 0.5354 | Best: 0.5354\n[Retention-k25] Epoch 3/30 - Train: 0.8079 | Val: 0.7318 | Best: 0.7318\n[Retention-k25] Epoch 4/30 - Train: 0.8185 | Val: 0.8117 | Best: 0.8117\n[Retention-k25] Epoch 6/30 - Train: 0.8165 | Val: 0.7206 | Best: 0.8117\n[Retention-k25] Epoch 7/30 - Train: 0.8241 | Val: 0.8239 | Best: 0.8239\n[Retention-k25] Epoch 11/30 - Train: 0.8347 | Val: 0.7945 | Best: 0.8239\nEarly stopping at epoch 12\n0.8200 +/- 0.0103\n\nTesting with k=30 channels: [Retention-k30] Epoch 1/30 - Train: 0.7258 | Val: 0.4975 | Best: 0.4975\n[Retention-k30] Epoch 2/30 - Train: 0.7865 | Val: 0.7685 | Best: 0.7685\n[Retention-k30] Epoch 3/30 - Train: 0.8068 | Val: 0.8231 | Best: 0.8231\n[Retention-k30] Epoch 6/30 - Train: 0.8159 | Val: 0.7978 | Best: 0.8231\nEarly stopping at epoch 8\n[Retention-k30] Epoch 1/30 - Train: 0.6899 | Val: 0.5015 | Best: 0.5015\n[Retention-k30] Epoch 2/30 - Train: 0.7951 | Val: 0.7917 | Best: 0.7917\n[Retention-k30] Epoch 4/30 - Train: 0.7967 | Val: 0.7998 | Best: 0.7998\n[Retention-k30] Epoch 5/30 - Train: 0.8123 | Val: 0.8150 | Best: 0.8150\n[Retention-k30] Epoch 6/30 - Train: 0.8093 | Val: 0.8231 | Best: 0.8231\n[Retention-k30] Epoch 11/30 - Train: 0.8336 | Val: 0.7998 | Best: 0.8231\nEarly stopping at epoch 11\n[Retention-k30] Epoch 1/30 - Train: 0.7189 | Val: 0.5000 | Best: 0.5000\n[Retention-k30] Epoch 2/30 - Train: 0.8028 | Val: 0.5911 | Best: 0.5911\n[Retention-k30] Epoch 3/30 - Train: 0.8084 | Val: 0.8077 | Best: 0.8077\n[Retention-k30] Epoch 4/30 - Train: 0.8084 | Val: 0.8117 | Best: 0.8117\n[Retention-k30] Epoch 5/30 - Train: 0.8215 | Val: 0.8209 | Best: 0.8209\n[Retention-k30] Epoch 6/30 - Train: 0.8281 | Val: 0.8128 | Best: 0.8209\n[Retention-k30] Epoch 7/30 - Train: 0.8301 | Val: 0.8269 | Best: 0.8269\n[Retention-k30] Epoch 11/30 - Train: 0.8377 | Val: 0.8239 | Best: 0.8269\n[Retention-k30] Epoch 12/30 - Train: 0.8352 | Val: 0.8289 | Best: 0.8289\n[Retention-k30] Epoch 15/30 - Train: 0.8463 | Val: 0.8320 | Best: 0.8320\n[Retention-k30] Epoch 16/30 - Train: 0.8423 | Val: 0.8219 | Best: 0.8320\nEarly stopping at epoch 20\n0.8260 +/- 0.0042\n\nTesting with k=35 channels: [Retention-k35] Epoch 1/30 - Train: 0.7152 | Val: 0.4975 | Best: 0.4975\n[Retention-k35] Epoch 2/30 - Train: 0.7936 | Val: 0.5076 | Best: 0.5076\n[Retention-k35] Epoch 3/30 - Train: 0.7992 | Val: 0.7705 | Best: 0.7705\n[Retention-k35] Epoch 4/30 - Train: 0.8199 | Val: 0.8190 | Best: 0.8190\n[Retention-k35] Epoch 6/30 - Train: 0.8260 | Val: 0.8140 | Best: 0.8190\n[Retention-k35] Epoch 8/30 - Train: 0.8209 | Val: 0.8261 | Best: 0.8261\n[Retention-k35] Epoch 11/30 - Train: 0.8351 | Val: 0.8170 | Best: 0.8261\nEarly stopping at epoch 13\n[Retention-k35] Epoch 1/30 - Train: 0.7365 | Val: 0.5015 | Best: 0.5015\n[Retention-k35] Epoch 2/30 - Train: 0.8073 | Val: 0.7058 | Best: 0.7058\n[Retention-k35] Epoch 3/30 - Train: 0.8174 | Val: 0.8160 | Best: 0.8160\n[Retention-k35] Epoch 5/30 - Train: 0.8123 | Val: 0.8231 | Best: 0.8231\n[Retention-k35] Epoch 6/30 - Train: 0.8280 | Val: 0.7250 | Best: 0.8231\n[Retention-k35] Epoch 8/30 - Train: 0.8290 | Val: 0.8271 | Best: 0.8271\n[Retention-k35] Epoch 11/30 - Train: 0.8381 | Val: 0.7260 | Best: 0.8271\n[Retention-k35] Epoch 13/30 - Train: 0.8341 | Val: 0.8281 | Best: 0.8281\n[Retention-k35] Epoch 16/30 - Train: 0.8467 | Val: 0.8362 | Best: 0.8362\n[Retention-k35] Epoch 21/30 - Train: 0.8543 | Val: 0.8352 | Best: 0.8362\nEarly stopping at epoch 21\n[Retention-k35] Epoch 1/30 - Train: 0.6987 | Val: 0.4980 | Best: 0.4980\n[Retention-k35] Epoch 2/30 - Train: 0.7927 | Val: 0.5051 | Best: 0.5051\n[Retention-k35] Epoch 3/30 - Train: 0.8124 | Val: 0.8016 | Best: 0.8016\n[Retention-k35] Epoch 5/30 - Train: 0.8195 | Val: 0.8107 | Best: 0.8107\n[Retention-k35] Epoch 6/30 - Train: 0.8261 | Val: 0.8047 | Best: 0.8107\n[Retention-k35] Epoch 7/30 - Train: 0.8296 | Val: 0.8158 | Best: 0.8158\n[Retention-k35] Epoch 11/30 - Train: 0.8478 | Val: 0.8128 | Best: 0.8158\nEarly stopping at epoch 12\n0.8260 +/- 0.0083\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# Save retention results\nretention_df = pd.DataFrame(retention_results)\nretention_df.to_csv(os.path.join(CONFIG['results_dir'], 'baseline_retention_analysis.csv'), index=False)\n\nprint(\"Retention Analysis Results:\")\nprint(retention_df[['k', 'mean_accuracy', 'std_accuracy']])\n\nprint(\"Results saved to: results/baseline_retention_analysis.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T05:58:35.605392Z","iopub.execute_input":"2025-11-23T05:58:35.606033Z","iopub.status.idle":"2025-11-23T05:58:35.633431Z","shell.execute_reply.started":"2025-11-23T05:58:35.606009Z","shell.execute_reply":"2025-11-23T05:58:35.632641Z"}},"outputs":[{"name":"stdout","text":"Retention Analysis Results:\n    k  mean_accuracy  std_accuracy\n0  10       0.804110      0.006710\n1  15       0.817597      0.004322\n2  20       0.824008      0.006475\n3  25       0.819961      0.010288\n4  30       0.826030      0.004210\n5  35       0.826025      0.008332\nResults saved to: results/baseline_retention_analysis.csv\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# Prepare detailed results\ndetailed_results = []\nfor result in all_results:\n    for fold_result in result['fold_results']:\n        detailed_results.append({\n            'model': result['model'],\n            'fold': fold_result['fold'],\n            'accuracy': fold_result['accuracy']\n        })\n\ndetailed_df = pd.DataFrame(detailed_results)\ndetailed_df.to_csv(os.path.join(CONFIG['results_dir'], 'baseline_methods_results.csv'), index=False)\n\n# Prepare summary\nsummary_df = pd.DataFrame(all_results)[['model', 'mean_accuracy', 'std_accuracy']]\nsummary_df = summary_df.sort_values('mean_accuracy', ascending=False).reset_index(drop=True)\nsummary_df['rank'] = range(1, len(summary_df) + 1)\nsummary_df = summary_df[['rank', 'model', 'mean_accuracy', 'std_accuracy']]\nsummary_df.to_csv(os.path.join(CONFIG['results_dir'], 'baseline_methods_summary.csv'), index=False)\n\nprint(\"\\nResults saved to:\")\nprint(\"  - results/baseline_methods_results.csv\")\nprint(\"  - results/baseline_methods_summary.csv\")\n\nprint(\"\\nBaseline Methods Summary:\")\nprint(summary_df.to_string(index=False))\n\nprint(f\"\\n{'='*60}\")\nprint(\"PIPELINE 1 COMPLETE!\")\nprint(f\"{'='*60}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T05:58:44.663572Z","iopub.execute_input":"2025-11-23T05:58:44.663841Z","iopub.status.idle":"2025-11-23T05:58:44.678574Z","shell.execute_reply.started":"2025-11-23T05:58:44.663822Z","shell.execute_reply":"2025-11-23T05:58:44.678003Z"}},"outputs":[{"name":"stdout","text":"\nResults saved to:\n  - results/baseline_methods_results.csv\n  - results/baseline_methods_summary.csv\n\nBaseline Methods Summary:\n rank      model  mean_accuracy  std_accuracy\n    1     EEGNet       0.840526      0.005793\n    2      FBCSP       0.614295      0.005367\n    3    CNN-SAE       0.611561      0.154553\n    4 ACS-SE-CNN       0.503034      0.001799\n    5     G-CARM       0.499662      0.002037\n\n============================================================\nPIPELINE 1 COMPLETE!\n============================================================\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}