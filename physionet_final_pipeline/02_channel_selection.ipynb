{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PhysioNet EEG: Channel Selection Evaluation\n",
    "\n",
    "This notebook evaluates three channel selection methods:\n",
    "1. **Edge Selection (ES)**: Based on sum of outgoing edge weights from CARM adjacency matrix\n",
    "2. **Aggregation Selection (AS)**: Based on feature aggregation after CARM layers\n",
    "3. **Gate Selection (GS)**: Based on adaptive gate values (only for Adaptive-Gating-EEG-ARNN)\n",
    "\n",
    "**Expected Runtime**: 8-10 hours on Kaggle GPU\n",
    "\n",
    "**Input**: \n",
    "- `/kaggle/input/physionet-preprocessed/derived/` (preprocessed EEG data)\n",
    "- `models/` folder (trained models from notebook 01)\n",
    "\n",
    "**Output**: \n",
    "- `channel_selection_results.csv` - Results for all methods and k values\n",
    "- `retention_analysis.csv` - Performance vs number of channels retained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import mne\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import pickle\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "mne.set_log_level('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    'data_path': '/kaggle/input/eeg-preprocessed-data/derived',\n",
    "    'models_dir': './models',\n",
    "    'results_dir': './results',\n",
    "    \n",
    "    'n_folds': 3,\n",
    "    'random_seed': 42,\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    \n",
    "    'batch_size': 64,\n",
    "    'epochs': 20,\n",
    "    'learning_rate': 0.001,\n",
    "    'weight_decay': 1e-4,\n",
    "    'patience': 10,\n",
    "    \n",
    "    'n_channels': 64,\n",
    "    'n_classes': 2,\n",
    "    'sfreq': 128,\n",
    "    'n_timepoints': 513,\n",
    "    \n",
    "    # Channel selection k values\n",
    "    'k_values': [5, 10, 15, 20, 25, 30, 35],\n",
    "}\n",
    "\n",
    "np.random.seed(CONFIG['random_seed'])\n",
    "torch.manual_seed(CONFIG['random_seed'])\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(CONFIG['random_seed'])\n",
    "\n",
    "print(f\"Device: {CONFIG['device']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy Model Architectures and Data Loading from Notebook 01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading utilities (same as notebook 01)\n",
    "def load_physionet_data(data_path, subject_ids=None):\n",
    "    all_X = []\n",
    "    all_y = []\n",
    "    all_subjects = []\n",
    "    \n",
    "    if subject_ids is None:\n",
    "        files = [f for f in os.listdir(data_path) if f.endswith('.fif')]\n",
    "        subject_ids = sorted(list(set([int(f.split('_')[0][1:]) for f in files])))\n",
    "    \n",
    "    print(f\"Loading data from {len(subject_ids)} subjects...\")\n",
    "    \n",
    "    for subject_id in tqdm(subject_ids):\n",
    "        subject_runs = []\n",
    "        for run_id in [3, 4, 7, 8, 11, 12]:\n",
    "            filename = f\"S{subject_id:03d}_R{run_id:02d}.fif\"\n",
    "            filepath = os.path.join(data_path, filename)\n",
    "            \n",
    "            if not os.path.exists(filepath):\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                epochs = mne.read_epochs(filepath, preload=True, verbose=False)\n",
    "                subject_runs.append(epochs)\n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        if len(subject_runs) == 0:\n",
    "            continue\n",
    "        \n",
    "        epochs = mne.concatenate_epochs(subject_runs)\n",
    "        X = epochs.get_data()\n",
    "        \n",
    "        event_ids = epochs.event_id\n",
    "        valid_event_ids = {'T1': 1, 'T2': 2}\n",
    "        \n",
    "        event_name_to_label = {}\n",
    "        if 'T1' in event_ids:\n",
    "            event_name_to_label['T1'] = 0\n",
    "        if 'T2' in event_ids:\n",
    "            event_name_to_label['T2'] = 1\n",
    "        \n",
    "        event_code_to_label = {}\n",
    "        for name, label in event_name_to_label.items():\n",
    "            if name in valid_event_ids:\n",
    "                mne_code = valid_event_ids[name]\n",
    "                event_code_to_label[mne_code] = label\n",
    "        \n",
    "        y = np.array([event_code_to_label.get(epochs.events[i, -1], -1) \n",
    "                     for i in range(len(epochs))])\n",
    "        \n",
    "        valid_mask = y != -1\n",
    "        X = X[valid_mask]\n",
    "        y = y[valid_mask]\n",
    "        \n",
    "        if len(X) == 0:\n",
    "            continue\n",
    "        \n",
    "        all_X.append(X)\n",
    "        all_y.append(y)\n",
    "        all_subjects.append(np.full(len(y), subject_id))\n",
    "    \n",
    "    X = np.concatenate(all_X, axis=0)\n",
    "    y = np.concatenate(all_y, axis=0)\n",
    "    subject_labels = np.concatenate(all_subjects, axis=0)\n",
    "    \n",
    "    print(f\"Loaded {len(X)} trials from {len(np.unique(subject_labels))} subjects\")\n",
    "    \n",
    "    return X, y, subject_labels\n",
    "\n",
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.y = torch.LongTensor(y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model architectures (same as notebook 01 - only need the ones with channel selection)\n",
    "class CARMBlock(nn.Module):\n",
    "    def __init__(self, n_channels):\n",
    "        super().__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.A = nn.Parameter(torch.randn(n_channels, n_channels) * 0.01)\n",
    "        self.norm = nn.LayerNorm(n_channels)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, n_channels, n_time = x.shape\n",
    "        A_norm = torch.softmax(self.A, dim=1)\n",
    "        x_reshaped = x.permute(0, 2, 1)\n",
    "        x_graph = torch.matmul(x_reshaped, A_norm.t())\n",
    "        x_graph = x_graph.permute(0, 2, 1)\n",
    "        return x_graph\n",
    "    \n",
    "    def get_adjacency_matrix(self):\n",
    "        return torch.softmax(self.A, dim=1).detach()\n",
    "\n",
    "class TFEMBlock(nn.Module):\n",
    "    def __init__(self, n_channels, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.temporal_conv = nn.Conv1d(n_channels, hidden_dim, kernel_size=5, padding=2)\n",
    "        self.temporal_bn = nn.BatchNorm1d(hidden_dim)\n",
    "        self.freq_pool = nn.AdaptiveAvgPool1d(64)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.temporal_bn(self.temporal_conv(x)))\n",
    "        x = self.freq_pool(x)\n",
    "        return x\n",
    "\n",
    "class BaselineEEGARNN(nn.Module):\n",
    "    def __init__(self, n_channels=64, n_classes=2, n_timepoints=513, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.tfem = TFEMBlock(n_channels, hidden_dim)\n",
    "        self.carm = CARMBlock(hidden_dim)\n",
    "        self.lstm = nn.LSTM(hidden_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.fc1 = nn.Linear(hidden_dim * 2, 128)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(128, n_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.tfem(x)\n",
    "        x = self.carm(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = x[:, -1, :]\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    def get_channel_importance_edge(self):\n",
    "        A = self.carm.get_adjacency_matrix()\n",
    "        return torch.sum(A, dim=1).cpu().numpy()\n",
    "\n",
    "class AdaptiveGatingEEGARNN(nn.Module):\n",
    "    def __init__(self, n_channels=64, n_classes=2, n_timepoints=513, hidden_dim=128, gate_init=0.9):\n",
    "        super().__init__()\n",
    "        self.n_channels = n_channels\n",
    "        \n",
    "        self.gate_net = nn.Sequential(\n",
    "            nn.Linear(n_timepoints, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        for layer in self.gate_net:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.constant_(layer.bias, gate_init)\n",
    "        \n",
    "        self.tfem = TFEMBlock(n_channels, hidden_dim)\n",
    "        self.carm = CARMBlock(hidden_dim)\n",
    "        self.lstm = nn.LSTM(hidden_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.fc1 = nn.Linear(hidden_dim * 2, 128)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(128, n_classes)\n",
    "        \n",
    "        self.gate_values = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        gates = []\n",
    "        for i in range(self.n_channels):\n",
    "            g = self.gate_net(x[:, i, :])\n",
    "            gates.append(g)\n",
    "        gates = torch.cat(gates, dim=1)\n",
    "        self.gate_values = gates.detach()\n",
    "        \n",
    "        x = x * gates.unsqueeze(2)\n",
    "        x = self.tfem(x)\n",
    "        x = self.carm(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = x[:, -1, :]\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    def get_channel_importance_gate(self):\n",
    "        if self.gate_values is None:\n",
    "            return None\n",
    "        return torch.mean(self.gate_values, dim=0).cpu().numpy()\n",
    "    \n",
    "    def get_channel_importance_edge(self):\n",
    "        A = self.carm.get_adjacency_matrix()\n",
    "        return torch.sum(A, dim=1).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training utilities\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for X_batch, y_batch in dataloader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += y_batch.size(0)\n",
    "        correct += (predicted == y_batch).sum().item()\n",
    "    \n",
    "    return total_loss / len(dataloader), correct / total\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in dataloader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += y_batch.size(0)\n",
    "            correct += (predicted == y_batch).sum().item()\n",
    "    \n",
    "    return total_loss / len(dataloader), correct / total\n",
    "\n",
    "def train_pytorch_model(model, train_loader, val_loader, config):\n",
    "    device = config['device']\n",
    "    model = model.to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'], \n",
    "                          weight_decay=config['weight_decay'])\n",
    "    \n",
    "    best_val_acc = 0\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(config['epochs']):\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if patience_counter >= config['patience']:\n",
    "            break\n",
    "    \n",
    "    return model, best_val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Channel Selection Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_channel_importance_aggregation(model, dataloader, device):\n",
    "    \"\"\"\n",
    "    Aggregation Selection (AS): Compute channel importance based on\n",
    "    feature aggregation after CARM layers.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_activations = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, _ in dataloader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            \n",
    "            # Forward pass through TFEM and CARM\n",
    "            x = model.tfem(X_batch)\n",
    "            x = model.carm(x)\n",
    "            \n",
    "            # Aggregate over time dimension\n",
    "            activations = torch.mean(torch.abs(x), dim=2)  # (batch, channels)\n",
    "            all_activations.append(activations.cpu())\n",
    "    \n",
    "    # Average across all batches\n",
    "    all_activations = torch.cat(all_activations, dim=0)\n",
    "    channel_importance = torch.mean(all_activations, dim=0).numpy()\n",
    "    \n",
    "    return channel_importance\n",
    "\n",
    "def select_top_k_channels(importance_scores, k):\n",
    "    \"\"\"Select top k channels based on importance scores.\"\"\"\n",
    "    top_k_indices = np.argsort(importance_scores)[-k:]\n",
    "    return sorted(top_k_indices)\n",
    "\n",
    "def apply_channel_selection(X, selected_channels):\n",
    "    \"\"\"Apply channel selection to data.\"\"\"\n",
    "    return X[:, selected_channels, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading PhysioNet data...\")\n",
    "X, y, subject_labels = load_physionet_data(CONFIG['data_path'])\n",
    "print(f\"Data loaded: {X.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Channel Selection Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models to evaluate\n",
    "models_to_evaluate = [\n",
    "    {'name': 'Baseline-EEG-ARNN', 'methods': ['edge', 'aggregation']},\n",
    "    {'name': 'Adaptive-Gating-EEG-ARNN', 'methods': ['edge', 'aggregation', 'gate']},\n",
    "]\n",
    "\n",
    "# Results storage\n",
    "channel_selection_results = []\n",
    "retention_results = []\n",
    "\n",
    "skf = StratifiedKFold(n_splits=CONFIG['n_folds'], shuffle=True, random_state=CONFIG['random_seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main evaluation loop\n",
    "for model_info in models_to_evaluate:\n",
    "    model_name = model_info['name']\n",
    "    selection_methods = model_info['methods']\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Evaluating {model_name}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    for method in selection_methods:\n",
    "        print(f\"\\n{'-'*60}\")\n",
    "        print(f\"Channel Selection Method: {method.upper()}\")\n",
    "        print(f\"{'-'*60}\\n\")\n",
    "        \n",
    "        for k in CONFIG['k_values']:\n",
    "            print(f\"\\nEvaluating with k={k} channels...\")\n",
    "            \n",
    "            fold_accuracies = []\n",
    "            \n",
    "            for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "                print(f\"Fold {fold + 1}/{CONFIG['n_folds']}\", end=' ')\n",
    "                \n",
    "                X_train, X_val = X[train_idx], X[val_idx]\n",
    "                y_train, y_val = y[train_idx], y[val_idx]\n",
    "                \n",
    "                # Load pre-trained model from notebook 01\n",
    "                if model_name == 'Baseline-EEG-ARNN':\n",
    "                    model = BaselineEEGARNN(n_channels=CONFIG['n_channels'],\n",
    "                                           n_classes=CONFIG['n_classes'],\n",
    "                                           n_timepoints=CONFIG['n_timepoints'])\n",
    "                else:\n",
    "                    model = AdaptiveGatingEEGARNN(n_channels=CONFIG['n_channels'],\n",
    "                                                 n_classes=CONFIG['n_classes'],\n",
    "                                                 n_timepoints=CONFIG['n_timepoints'])\n",
    "                \n",
    "                model_path = os.path.join(CONFIG['models_dir'], f\"{model_name}_fold{fold+1}.pt\")\n",
    "                model.load_state_dict(torch.load(model_path))\n",
    "                model = model.to(CONFIG['device'])\n",
    "                model.eval()\n",
    "                \n",
    "                # Get channel importance scores\n",
    "                if method == 'edge':\n",
    "                    importance_scores = model.get_channel_importance_edge()\n",
    "                elif method == 'aggregation':\n",
    "                    train_dataset = EEGDataset(X_train, y_train)\n",
    "                    train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'],\n",
    "                                            shuffle=False, num_workers=0)\n",
    "                    importance_scores = get_channel_importance_aggregation(model, train_loader,\n",
    "                                                                          CONFIG['device'])\n",
    "                elif method == 'gate':\n",
    "                    # Run a forward pass to populate gate values\n",
    "                    train_dataset = EEGDataset(X_train, y_train)\n",
    "                    train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'],\n",
    "                                            shuffle=False, num_workers=0)\n",
    "                    with torch.no_grad():\n",
    "                        for X_batch, _ in train_loader:\n",
    "                            X_batch = X_batch.to(CONFIG['device'])\n",
    "                            _ = model(X_batch)\n",
    "                            break\n",
    "                    importance_scores = model.get_channel_importance_gate()\n",
    "                \n",
    "                # Select top k channels\n",
    "                selected_channels = select_top_k_channels(importance_scores, k)\n",
    "                \n",
    "                # Apply channel selection\n",
    "                X_train_selected = apply_channel_selection(X_train, selected_channels)\n",
    "                X_val_selected = apply_channel_selection(X_val, selected_channels)\n",
    "                \n",
    "                # Retrain model with selected channels\n",
    "                if model_name == 'Baseline-EEG-ARNN':\n",
    "                    new_model = BaselineEEGARNN(n_channels=k,\n",
    "                                               n_classes=CONFIG['n_classes'],\n",
    "                                               n_timepoints=CONFIG['n_timepoints'])\n",
    "                else:\n",
    "                    new_model = AdaptiveGatingEEGARNN(n_channels=k,\n",
    "                                                     n_classes=CONFIG['n_classes'],\n",
    "                                                     n_timepoints=CONFIG['n_timepoints'])\n",
    "                \n",
    "                train_dataset = EEGDataset(X_train_selected, y_train)\n",
    "                val_dataset = EEGDataset(X_val_selected, y_val)\n",
    "                \n",
    "                train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'],\n",
    "                                        shuffle=True, num_workers=0)\n",
    "                val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'],\n",
    "                                       shuffle=False, num_workers=0)\n",
    "                \n",
    "                new_model, val_acc = train_pytorch_model(new_model, train_loader, val_loader, CONFIG)\n",
    "                \n",
    "                fold_accuracies.append(val_acc)\n",
    "                print(f\"Acc: {val_acc:.4f}\")\n",
    "            \n",
    "            # Compute statistics\n",
    "            mean_acc = np.mean(fold_accuracies)\n",
    "            std_acc = np.std(fold_accuracies)\n",
    "            \n",
    "            print(f\"k={k}: {mean_acc:.4f} +/- {std_acc:.4f}\")\n",
    "            \n",
    "            # Store results\n",
    "            channel_selection_results.append({\n",
    "                'model': model_name,\n",
    "                'method': method,\n",
    "                'k': k,\n",
    "                'mean_accuracy': mean_acc,\n",
    "                'std_accuracy': std_acc,\n",
    "                'fold_accuracies': fold_accuracies\n",
    "            })\n",
    "\n",
    "print(f\"\\n\\n{'='*60}\")\n",
    "print(\"Channel selection evaluation complete!\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retention Analysis\n",
    "\n",
    "Evaluate performance as we gradually reduce the number of channels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retention analysis: Test with different numbers of channels\n",
    "retention_k_values = [5, 10, 15, 20, 25, 30, 35]\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Retention Analysis: Adaptive-Gating-EEG-ARNN with Gate Selection\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "for k in retention_k_values:\n",
    "    print(f\"\\nTesting with k={k} channels...\")\n",
    "    \n",
    "    fold_accuracies = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "        \n",
    "        # Load pre-trained model\n",
    "        model = AdaptiveGatingEEGARNN(n_channels=CONFIG['n_channels'],\n",
    "                                     n_classes=CONFIG['n_classes'],\n",
    "                                     n_timepoints=CONFIG['n_timepoints'])\n",
    "        model_path = os.path.join(CONFIG['models_dir'], f\"Adaptive-Gating-EEG-ARNN_fold{fold+1}.pt\")\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "        model = model.to(CONFIG['device'])\n",
    "        model.eval()\n",
    "        \n",
    "        # Get gate-based importance\n",
    "        train_dataset = EEGDataset(X_train, y_train)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'],\n",
    "                                shuffle=False, num_workers=0)\n",
    "        with torch.no_grad():\n",
    "            for X_batch, _ in train_loader:\n",
    "                X_batch = X_batch.to(CONFIG['device'])\n",
    "                _ = model(X_batch)\n",
    "                break\n",
    "        importance_scores = model.get_channel_importance_gate()\n",
    "        \n",
    "        # Select top k channels\n",
    "        selected_channels = select_top_k_channels(importance_scores, k)\n",
    "        \n",
    "        # Apply selection and retrain\n",
    "        X_train_selected = apply_channel_selection(X_train, selected_channels)\n",
    "        X_val_selected = apply_channel_selection(X_val, selected_channels)\n",
    "        \n",
    "        new_model = AdaptiveGatingEEGARNN(n_channels=k,\n",
    "                                         n_classes=CONFIG['n_classes'],\n",
    "                                         n_timepoints=CONFIG['n_timepoints'])\n",
    "        \n",
    "        train_dataset = EEGDataset(X_train_selected, y_train)\n",
    "        val_dataset = EEGDataset(X_val_selected, y_val)\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'],\n",
    "                                shuffle=True, num_workers=0)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'],\n",
    "                               shuffle=False, num_workers=0)\n",
    "        \n",
    "        new_model, val_acc = train_pytorch_model(new_model, train_loader, val_loader, CONFIG)\n",
    "        fold_accuracies.append(val_acc)\n",
    "    \n",
    "    mean_acc = np.mean(fold_accuracies)\n",
    "    std_acc = np.std(fold_accuracies)\n",
    "    \n",
    "    print(f\"k={k}: {mean_acc:.4f} +/- {std_acc:.4f}\")\n",
    "    \n",
    "    retention_results.append({\n",
    "        'k': k,\n",
    "        'mean_accuracy': mean_acc,\n",
    "        'std_accuracy': std_acc,\n",
    "        'fold_accuracies': fold_accuracies\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save channel selection results\n",
    "cs_df = pd.DataFrame(channel_selection_results)\n",
    "cs_df.to_csv(os.path.join(CONFIG['results_dir'], 'channel_selection_results.csv'), index=False)\n",
    "\n",
    "print(\"\\nChannel Selection Results:\")\n",
    "print(cs_df[['model', 'method', 'k', 'mean_accuracy', 'std_accuracy']])\n",
    "\n",
    "# Save retention analysis results\n",
    "retention_df = pd.DataFrame(retention_results)\n",
    "retention_df.to_csv(os.path.join(CONFIG['results_dir'], 'retention_analysis.csv'), index=False)\n",
    "\n",
    "print(\"\\nRetention Analysis Results:\")\n",
    "print(retention_df[['k', 'mean_accuracy', 'std_accuracy']])\n",
    "\n",
    "print(f\"\\nResults saved to {CONFIG['results_dir']}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Find the best channel selection method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best method for each model\n",
    "print(\"\\nBest Channel Selection Methods:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for model_name in ['Baseline-EEG-ARNN', 'Adaptive-Gating-EEG-ARNN']:\n",
    "    model_results = cs_df[cs_df['model'] == model_name]\n",
    "    best_result = model_results.loc[model_results['mean_accuracy'].idxmax()]\n",
    "    \n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"  Best Method: {best_result['method'].upper()}\")\n",
    "    print(f\"  Best k: {best_result['k']}\")\n",
    "    print(f\"  Accuracy: {best_result['mean_accuracy']:.4f} +/- {best_result['std_accuracy']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}