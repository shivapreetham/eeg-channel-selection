{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PhysioNet EEG: Channel Selection Evaluation\n",
    "\n",
    "This notebook evaluates three channel selection methods:\n",
    "1. **Edge Selection (ES)**: Based on sum of outgoing edge weights from CARM adjacency matrix\n",
    "2. **Aggregation Selection (AS)**: Based on feature aggregation after CARM layers\n",
    "3. **Gate Selection (GS)**: Based on adaptive gate values (only for Adaptive-Gating-EEG-ARNN)\n",
    "\n",
    "**Expected Runtime**: 8-10 hours on Kaggle GPU\n",
    "\n",
    "**Input**: \n",
    "- `/kaggle/input/physionet-preprocessed/derived/` (preprocessed EEG data)\n",
    "- `models/` folder (trained models from notebook 01)\n",
    "\n",
    "**Output**: \n",
    "- `channel_selection_results.csv` - Results for all methods and k values\n",
    "- `retention_analysis.csv` - Performance vs number of channels retained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import mne\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import pickle\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "mne.set_log_level('ERROR')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    'data_path': '/kaggle/input/eeg-preprocessed-data/derived',  # Change this for local testing\n",
    "    'output_dir': './',\n",
    "    'results_dir': './results',\n",
    "    'models_dir': './models',\n",
    "    'figures_dir': './figures',\n",
    "\n",
    "    'n_folds': 3,\n",
    "    'random_seed': 42,\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "\n",
    "    # Training hyperparameters\n",
    "    'batch_size': 64,\n",
    "    'epochs': 20,\n",
    "    'learning_rate': 0.001,\n",
    "    'weight_decay': 1e-4,\n",
    "    'patience': 10,\n",
    "    'scheduler_patience': 3,\n",
    "    'use_early_stopping': False,\n",
    "\n",
    "    # Data parameters\n",
    "    'n_channels': 64,\n",
    "    'n_classes': 2,\n",
    "    'sfreq': 128,\n",
    "    'tmin': 0.0,\n",
    "    'tmax': 4.0,\n",
    "    'n_timepoints': 513,  # 4 seconds at 128 Hz + 1\n",
    "    'hidden_dim': 128,\n",
    "    'mi_runs': [7, 8, 11, 12],\n",
    "\n",
    "    # FBCSP parameters\n",
    "    'fbcsp_bands': [(4, 8), (8, 12), (12, 16), (16, 20), (20, 24), (24, 28), (28, 32), (32, 36), (36, 40)],\n",
    "    'fbcsp_n_components': 4,\n",
    "\n",
    "    # Gating regularization\n",
    "    'gating': {\n",
    "        'gate_init': 0.9,\n",
    "        'l1_lambda': 1e-3,\n",
    "    },\n",
    "}\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(CONFIG['results_dir'], exist_ok=True)\n",
    "os.makedirs(CONFIG['models_dir'], exist_ok=True)\n",
    "os.makedirs(CONFIG['figures_dir'], exist_ok=True)\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(CONFIG['random_seed'])\n",
    "torch.manual_seed(CONFIG['random_seed'])\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(CONFIG['random_seed'])\n",
    "\n",
    "print(f\"Device: {CONFIG['device']}\")\n",
    "print(f\"Data path: {CONFIG['data_path']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy Model Architectures and Data Loading from Notebook 01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_physionet_data(data_path, subject_ids=None):\n",
    "    \"\"\"\n",
    "    Load preprocessed PhysioNet motor imagery data from the derived folder.\n",
    "    Supports both the newer folder structure (derived/preprocessed/S***/S***R**_preproc_raw.fif)\n",
    "    and the legacy flat directory containing epoch files.\n",
    "    \"\"\"\n",
    "    data_root = os.path.abspath(data_path)\n",
    "    if not os.path.isdir(data_root):\n",
    "        raise FileNotFoundError(f\"Data path not found: {data_root}\")\n",
    "\n",
    "    config = globals().get('CONFIG', {})\n",
    "    tmin = float(config.get('tmin', 0.0))\n",
    "    tmax = float(config.get('tmax', 4.0))\n",
    "    mi_runs = [int(r) for r in config.get('mi_runs', [7, 8, 11, 12])]\n",
    "    event_id = {'T1': 1, 'T2': 2}\n",
    "\n",
    "    def normalize_subject(value):\n",
    "        if value is None:\n",
    "            return None\n",
    "        if isinstance(value, str) and value.upper().startswith('S'):\n",
    "            value = value[1:]\n",
    "        try:\n",
    "            return int(value)\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    subject_filter = None\n",
    "    if subject_ids is not None:\n",
    "        subject_filter = set()\n",
    "        for sid in subject_ids:\n",
    "            norm = normalize_subject(sid)\n",
    "            if norm is not None:\n",
    "                subject_filter.add(norm)\n",
    "        if not subject_filter:\n",
    "            subject_filter = None\n",
    "\n",
    "    def aggregate_results(blocks_X, blocks_y, blocks_subjects):\n",
    "        X = np.concatenate(blocks_X, axis=0)\n",
    "        y = np.concatenate(blocks_y, axis=0)\n",
    "        subjects = np.concatenate(blocks_subjects, axis=0)\n",
    "        print(f\"Loaded {len(X)} trials from {len(np.unique(subjects))} subjects\")\n",
    "        print(f\"Data shape: {X.shape}\")\n",
    "        print(f\"Label distribution: {np.bincount(y)}\")\n",
    "        return X, y, subjects\n",
    "\n",
    "    subject_root = data_root\n",
    "    preprocessed_dir = os.path.join(data_root, 'preprocessed')\n",
    "    if os.path.isdir(preprocessed_dir):\n",
    "        subject_root = preprocessed_dir\n",
    "    subject_dirs = [d for d in sorted(os.listdir(subject_root))\n",
    "                    if os.path.isdir(os.path.join(subject_root, d)) and d.upper().startswith('S')]\n",
    "\n",
    "    all_X, all_y, all_subjects = [], [], []\n",
    "    if subject_dirs:\n",
    "        print(f\"Detected {len(subject_dirs)} preprocessed subject folders under {subject_root}\")\n",
    "        label_map = {event_id['T1']: 0, event_id['T2']: 1}\n",
    "        for subject_dir in subject_dirs:\n",
    "            subject_numeric = normalize_subject(subject_dir)\n",
    "            if subject_filter and subject_numeric not in subject_filter:\n",
    "                continue\n",
    "            subject_path = os.path.join(subject_root, subject_dir)\n",
    "            for run_id in mi_runs:\n",
    "                candidate_names = [\n",
    "                    f\"{subject_dir}R{run_id:02d}_preproc_raw.fif\",\n",
    "                    f\"{subject_dir}R{run_id:02d}_raw.fif\",\n",
    "                    f\"{subject_dir}R{run_id:02d}.fif\",\n",
    "                    f\"{subject_dir}_R{run_id:02d}.fif\",\n",
    "                ]\n",
    "                run_path = None\n",
    "                for name in candidate_names:\n",
    "                    candidate = os.path.join(subject_path, name)\n",
    "                    if os.path.exists(candidate):\n",
    "                        run_path = candidate\n",
    "                        break\n",
    "                if run_path is None:\n",
    "                    continue\n",
    "                try:\n",
    "                    raw = mne.io.read_raw_fif(run_path, preload=True, verbose=False)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading {run_path}: {e}\")\n",
    "                    continue\n",
    "                picks = mne.pick_types(raw.info, eeg=True, meg=False, stim=False, eog=False)\n",
    "                if len(picks) == 0:\n",
    "                    continue\n",
    "                try:\n",
    "                    events, _ = mne.events_from_annotations(raw, event_id=event_id)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error parsing annotations for {run_path}: {e}\")\n",
    "                    continue\n",
    "                if len(events) == 0:\n",
    "                    continue\n",
    "                try:\n",
    "                    epochs = mne.Epochs(raw, events, event_id=event_id, tmin=tmin, tmax=tmax,\n",
    "                                        baseline=None, preload=True, picks=picks, verbose=False)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error epoching {run_path}: {e}\")\n",
    "                    continue\n",
    "                data = epochs.get_data()\n",
    "                labels = epochs.events[:, 2]\n",
    "                mapped = np.array([label_map.get(lbl, -1) for lbl in labels])\n",
    "                valid_mask = mapped >= 0\n",
    "                if not np.any(valid_mask):\n",
    "                    continue\n",
    "                all_X.append(data[valid_mask])\n",
    "                all_y.append(mapped[valid_mask])\n",
    "                subj_label = subject_numeric if subject_numeric is not None else -1\n",
    "                all_subjects.append(np.full(np.sum(valid_mask), subj_label))\n",
    "        if all_X:\n",
    "            return aggregate_results(all_X, all_y, all_subjects)\n",
    "        print(\"No data loaded from preprocessed folders, falling back to legacy format...\")\n",
    "\n",
    "    # Legacy format fallback (flat directory with epoch files)\n",
    "    legacy_files = [f for f in os.listdir(data_root) if f.endswith('.fif')]\n",
    "    if not legacy_files:\n",
    "        raise ValueError(\n",
    "            \"No valid PhysioNet files found. Ensure the derived folder contains either \"\n",
    "            \"preprocessed subject subfolders or .fif epoch files.\"\n",
    "        )\n",
    "    if subject_filter:\n",
    "        filtered = []\n",
    "        for fname in legacy_files:\n",
    "            parts = fname.split('_')\n",
    "            if not parts:\n",
    "                continue\n",
    "            subj = normalize_subject(parts[0])\n",
    "            if subj is not None and subj in subject_filter:\n",
    "                filtered.append(fname)\n",
    "        legacy_files = filtered\n",
    "        if not legacy_files:\n",
    "            raise ValueError(\"No files matched the requested subject IDs in legacy format.\")\n",
    "\n",
    "    print(f\"Found {len(legacy_files)} legacy epoch files. Loading...\")\n",
    "    for fname in legacy_files:\n",
    "        filepath = os.path.join(data_root, fname)\n",
    "        try:\n",
    "            epochs = mne.read_epochs(filepath, preload=True, verbose=False)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {filepath}: {e}\")\n",
    "            continue\n",
    "        current_event_id = epochs.event_id\n",
    "        if not current_event_id:\n",
    "            continue\n",
    "        label_lookup = {}\n",
    "        if 'T1' in current_event_id:\n",
    "            label_lookup[current_event_id['T1']] = 0\n",
    "        if 'T2' in current_event_id:\n",
    "            label_lookup[current_event_id['T2']] = 1\n",
    "        if not label_lookup:\n",
    "            continue\n",
    "        labels = np.array([label_lookup.get(epochs.events[i, -1], -1) for i in range(len(epochs))])\n",
    "        valid = labels >= 0\n",
    "        if not np.any(valid):\n",
    "            continue\n",
    "        data = epochs.get_data()[valid]\n",
    "        labels = labels[valid]\n",
    "        subj = normalize_subject(fname.split('_')[0])\n",
    "        subj_arr = np.full(len(labels), subj if subj is not None else -1)\n",
    "        all_X.append(data)\n",
    "        all_y.append(labels)\n",
    "        all_subjects.append(subj_arr)\n",
    "    if not all_X:\n",
    "        raise ValueError(\"No valid trials were loaded from the provided PhysioNet files.\")\n",
    "    return aggregate_results(all_X, all_y, all_subjects)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EEG-ARNN Baseline + Adaptive Gating\n",
    "class GraphConvLayer(nn.Module):\n",
    "    \"\"\"Graph convolution with learnable adjacency.\"\"\"\n",
    "    def __init__(self, num_channels, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.num_channels = num_channels\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.A = nn.Parameter(torch.randn(num_channels, num_channels) * 0.01)\n",
    "        self.theta = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(hidden_dim)\n",
    "        self.act = nn.ELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, H, C, T = x.shape\n",
    "        A = torch.sigmoid(self.A)\n",
    "        A = 0.5 * (A + A.t())\n",
    "        I = torch.eye(C, device=A.device)\n",
    "        A_hat = A + I\n",
    "        D = torch.diag(torch.pow(A_hat.sum(1).clamp_min(1e-6), -0.5))\n",
    "        A_norm = D @ A_hat @ D\n",
    "\n",
    "        x_perm = x.permute(0, 3, 2, 1).contiguous().view(B * T, C, H)\n",
    "        x_g = A_norm @ x_perm\n",
    "        x_g = self.theta(x_g)\n",
    "        x_g = x_g.view(B, T, C, H).permute(0, 3, 2, 1)\n",
    "        x_out = self.bn(x_g)\n",
    "        return self.act(x_out)\n",
    "\n",
    "    def get_adjacency(self):\n",
    "        with torch.no_grad():\n",
    "            A = torch.sigmoid(self.A)\n",
    "            A = 0.5 * (A + A.t())\n",
    "            return A.cpu().numpy()\n",
    "\n",
    "\n",
    "class TemporalConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=16, pool=True):\n",
    "        super().__init__()\n",
    "        self.pool = pool\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=(1, kernel_size),\n",
    "                              padding=(0, kernel_size // 2), bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.act = nn.ELU()\n",
    "        self.pool_layer = nn.AvgPool2d(kernel_size=(1, 2)) if pool else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.act(self.bn(self.conv(x)))\n",
    "        if self.pool_layer is not None:\n",
    "            x = self.pool_layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class BaselineEEGARNN(nn.Module):\n",
    "    def __init__(self, n_channels=64, n_classes=2, n_timepoints=513, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.use_gate_regularizer = False\n",
    "        self.gate_penalty_tensor = None\n",
    "        self.latest_gate_values = None\n",
    "\n",
    "        self.t1 = TemporalConv(1, hidden_dim, 16, pool=False)\n",
    "        self.g1 = GraphConvLayer(n_channels, hidden_dim)\n",
    "        self.t2 = TemporalConv(hidden_dim, hidden_dim, 16, pool=True)\n",
    "        self.g2 = GraphConvLayer(n_channels, hidden_dim)\n",
    "        self.t3 = TemporalConv(hidden_dim, hidden_dim, 16, pool=True)\n",
    "        self.g3 = GraphConvLayer(n_channels, hidden_dim)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, n_channels, n_timepoints)\n",
    "            feat = self._forward_features(self._prepare_input(dummy))\n",
    "            self.feature_dim = feat.view(1, -1).size(1)\n",
    "\n",
    "        self.fc1 = nn.Linear(self.feature_dim, 256)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(256, n_classes)\n",
    "\n",
    "    def _prepare_input(self, x):\n",
    "        if x.dim() == 3:\n",
    "            x = x.unsqueeze(1)\n",
    "        return x\n",
    "\n",
    "    def _forward_features(self, x):\n",
    "        x = self.g1(self.t1(x))\n",
    "        x = self.g2(self.t2(x))\n",
    "        x = self.g3(self.t3(x))\n",
    "        return x\n",
    "\n",
    "    def _forward_from_prepared(self, x):\n",
    "        features = self._forward_features(x)\n",
    "        x = features.view(features.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        return self.fc2(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        prepared = self._prepare_input(x)\n",
    "        self.gate_penalty_tensor = None\n",
    "        self.latest_gate_values = None\n",
    "        return self._forward_from_prepared(prepared)\n",
    "\n",
    "    def get_final_adjacency(self):\n",
    "        return self.g3.get_adjacency()\n",
    "\n",
    "    def get_channel_importance_edge(self):\n",
    "        adjacency = self.get_final_adjacency()\n",
    "        return np.sum(adjacency, axis=1)\n",
    "\n",
    "\n",
    "class AdaptiveGatingEEGARNN(BaselineEEGARNN):\n",
    "    def __init__(self, n_channels=64, n_classes=2, n_timepoints=513, hidden_dim=128, gate_init=0.9):\n",
    "        super().__init__(n_channels, n_classes, n_timepoints, hidden_dim)\n",
    "        self.use_gate_regularizer = True\n",
    "        self.gate_net = nn.Sequential(\n",
    "            nn.Linear(n_channels * 2, n_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_channels, n_channels),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        init_value = float(np.clip(gate_init, 1e-3, 1 - 1e-3))\n",
    "        init_bias = math.log(init_value / (1.0 - init_value))\n",
    "        with torch.no_grad():\n",
    "            self.gate_net[-2].bias.fill_(init_bias)\n",
    "        self.latest_gate_values = None\n",
    "\n",
    "    def compute_gates(self, x):\n",
    "        x_s = x.squeeze(1)\n",
    "        ch_mean = x_s.mean(dim=2)\n",
    "        ch_std = x_s.std(dim=2)\n",
    "        stats = torch.cat([ch_mean, ch_std], dim=1)\n",
    "        return self.gate_net(stats)\n",
    "\n",
    "    def forward(self, x):\n",
    "        prepared = self._prepare_input(x)\n",
    "        gates = self.compute_gates(prepared)\n",
    "        self.gate_penalty_tensor = gates\n",
    "        self.latest_gate_values = gates.detach()\n",
    "        gated = prepared * gates.view(gates.size(0), 1, gates.size(1), 1)\n",
    "        return self._forward_from_prepared(gated)\n",
    "\n",
    "    def get_channel_importance_gate(self):\n",
    "        if self.latest_gate_values is None:\n",
    "            return None\n",
    "        return self.latest_gate_values.mean(dim=0).cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training utilities\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device, l1_lambda=0.0):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for X_batch, y_batch in dataloader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "\n",
    "        gate_penalty = getattr(model, 'gate_penalty_tensor', None)\n",
    "        if l1_lambda > 0 and gate_penalty is not None:\n",
    "            loss = loss + l1_lambda * gate_penalty.abs().mean()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += y_batch.size(0)\n",
    "        correct += (predicted == y_batch).sum().item()\n",
    "\n",
    "    denom = max(1, len(dataloader))\n",
    "    return total_loss / denom, correct / max(1, total)\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in dataloader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += y_batch.size(0)\n",
    "            correct += (predicted == y_batch).sum().item()\n",
    "\n",
    "    denom = max(1, len(dataloader))\n",
    "    return total_loss / denom, correct / max(1, total)\n",
    "\n",
    "\n",
    "def train_pytorch_model(model, train_loader, val_loader, config, model_name=''):\n",
    "    device = config['device']\n",
    "    model = model.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'], \n",
    "                          weight_decay=config['weight_decay'])\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=config.get('scheduler_patience', 3), verbose=False\n",
    "    )\n",
    "\n",
    "    l1_lambda = config.get('gating', {}).get('l1_lambda', 0.0) if getattr(model, 'use_gate_regularizer', False) else 0.0\n",
    "    use_early_stopping = config.get('use_early_stopping', False) and config.get('patience') is not None\n",
    "    max_patience = config.get('patience', 0)\n",
    "    patience_counter = 0\n",
    "\n",
    "    best_state = deepcopy(model.state_dict())\n",
    "    best_val_acc = 0.0\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(config['epochs']):\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device, l1_lambda)\n",
    "        val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        improved = val_acc > best_val_acc or (val_acc == best_val_acc and val_loss < best_val_loss)\n",
    "        if improved:\n",
    "            best_state = deepcopy(model.state_dict())\n",
    "            best_val_acc = val_acc\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        prefix = model_name if model_name else 'Model'\n",
    "        print(f\"[{prefix}] Epoch {epoch + 1}/{config['epochs']} - Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        if use_early_stopping and patience_counter >= max_patience:\n",
    "            print(f\"Early stopping triggered for {prefix} at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "    model.load_state_dict(best_state)\n",
    "    return best_state, best_val_acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Channel Selection Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_channel_importance_aggregation(model, dataloader, device):\n",
    "    \"\"\"Aggregation Selection (AS) using averaged feature activations.\"\"\"\n",
    "    model.eval()\n",
    "    channel_stats = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, _ in dataloader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            prepared = model._prepare_input(X_batch)\n",
    "            features = model._forward_features(prepared)\n",
    "            activations = torch.mean(torch.abs(features), dim=(1, 3))\n",
    "            channel_stats.append(activations.cpu())\n",
    "\n",
    "    if not channel_stats:\n",
    "        return np.zeros(model.n_channels)\n",
    "    stacked = torch.cat(channel_stats, dim=0)\n",
    "    return stacked.mean(dim=0).numpy()\n",
    "\n",
    "\n",
    "def compute_gate_importance(model, dataloader, device):\n",
    "    \"\"\"Average adaptive gate values across the entire dataset.\"\"\"\n",
    "    model.eval()\n",
    "    gate_batches = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, _ in dataloader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            _ = model(X_batch)\n",
    "            latest = getattr(model, 'latest_gate_values', None)\n",
    "            if latest is not None:\n",
    "                gate_batches.append(latest.cpu())\n",
    "\n",
    "    if not gate_batches:\n",
    "        return np.ones(model.n_channels) / model.n_channels\n",
    "    stacked = torch.cat(gate_batches, dim=0)\n",
    "    return stacked.mean(dim=0).numpy()\n",
    "\n",
    "\n",
    "def select_top_k_channels(importance_scores, k):\n",
    "    top_k_indices = np.argsort(importance_scores)[-k:]\n",
    "    return sorted(top_k_indices)\n",
    "\n",
    "\n",
    "def apply_channel_selection(X, selected_channels):\n",
    "    return X[:, selected_channels, :]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading PhysioNet data...\")\n",
    "X, y, subject_labels = load_physionet_data(CONFIG['data_path'])\n",
    "print(f\"Data loaded: {X.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Channel Selection Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models to evaluate\n",
    "models_to_evaluate = [\n",
    "    {'name': 'Baseline-EEG-ARNN', 'methods': ['edge', 'aggregation']},\n",
    "    {'name': 'Adaptive-Gating-EEG-ARNN', 'methods': ['edge', 'aggregation', 'gate']},\n",
    "]\n",
    "\n",
    "# Results storage\n",
    "channel_selection_results = []\n",
    "retention_results = []\n",
    "\n",
    "skf = StratifiedKFold(n_splits=CONFIG['n_folds'], shuffle=True, random_state=CONFIG['random_seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main evaluation loop\n",
    "base_kwargs = {\n",
    "    'n_channels': CONFIG['n_channels'],\n",
    "    'n_classes': CONFIG['n_classes'],\n",
    "    'n_timepoints': CONFIG['n_timepoints'],\n",
    "}\n",
    "\n",
    "def build_model(model_name, n_channels):\n",
    "    kwargs = dict(base_kwargs)\n",
    "    kwargs['n_channels'] = n_channels\n",
    "    if model_name == 'Baseline-EEG-ARNN':\n",
    "        return BaselineEEGARNN(hidden_dim=CONFIG['hidden_dim'], **kwargs)\n",
    "    return AdaptiveGatingEEGARNN(hidden_dim=CONFIG['hidden_dim'],\n",
    "                                 gate_init=CONFIG['gating']['gate_init'],\n",
    "                                 **kwargs)\n",
    "\n",
    "for model_info in models_to_evaluate:\n",
    "    model_name = model_info['name']\n",
    "    selection_methods = model_info['methods']\n",
    "\n",
    "    print(f\"\n",
    "{'='*60}\")\n",
    "    print(f\"Evaluating {model_name}\")\n",
    "    print(f\"{'='*60}\n",
    "\")\n",
    "\n",
    "    for method in selection_methods:\n",
    "        print(f\"\n",
    "{'-'*60}\")\n",
    "        print(f\"Channel Selection Method: {method.upper()}\")\n",
    "        print(f\"{'-'*60}\n",
    "\")\n",
    "\n",
    "        for k in CONFIG['k_values']:\n",
    "            print(f\"\n",
    "Evaluating with k={k} channels...\")\n",
    "            fold_accuracies = []\n",
    "\n",
    "            for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "                print(f\"Fold {fold + 1}/{CONFIG['n_folds']}\", end=' ')\n",
    "\n",
    "                X_train, X_val = X[train_idx], X[val_idx]\n",
    "                y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "                model = build_model(model_name, CONFIG['n_channels'])\n",
    "                model_path = os.path.join(CONFIG['models_dir'], f\"{model_name}_fold{fold+1}.pt\")\n",
    "                state_dict = torch.load(model_path, map_location=CONFIG['device'])\n",
    "                model.load_state_dict(state_dict)\n",
    "                model = model.to(CONFIG['device'])\n",
    "                model.eval()\n",
    "\n",
    "                if method == 'edge':\n",
    "                    importance_scores = model.get_channel_importance_edge()\n",
    "                elif method == 'aggregation':\n",
    "                    train_dataset = EEGDataset(X_train, y_train)\n",
    "                    train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'],\n",
    "                                              shuffle=False, num_workers=0)\n",
    "                    importance_scores = get_channel_importance_aggregation(model, train_loader,\n",
    "                                                                            CONFIG['device'])\n",
    "                else:\n",
    "                    train_dataset = EEGDataset(X_train, y_train)\n",
    "                    train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'],\n",
    "                                              shuffle=False, num_workers=0)\n",
    "                    importance_scores = compute_gate_importance(model, train_loader, CONFIG['device'])\n",
    "\n",
    "                selected_channels = select_top_k_channels(importance_scores, k)\n",
    "                X_train_selected = apply_channel_selection(X_train, selected_channels)\n",
    "                X_val_selected = apply_channel_selection(X_val, selected_channels)\n",
    "\n",
    "                new_model = build_model(model_name, k)\n",
    "                train_dataset = EEGDataset(X_train_selected, y_train)\n",
    "                val_dataset = EEGDataset(X_val_selected, y_val)\n",
    "\n",
    "                train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'],\n",
    "                                          shuffle=True, num_workers=0)\n",
    "                val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'],\n",
    "                                        shuffle=False, num_workers=0)\n",
    "\n",
    "                best_state, val_acc = train_pytorch_model(new_model, train_loader, val_loader,\n",
    "                                                          CONFIG, f\"{model_name}-{method}-k{k}\")\n",
    "                fold_accuracies.append(val_acc)\n",
    "                print(f\"Acc: {val_acc:.4f}\")\n",
    "\n",
    "            mean_acc = np.mean(fold_accuracies)\n",
    "            std_acc = np.std(fold_accuracies)\n",
    "\n",
    "            print(f\"k={k}: {mean_acc:.4f} +/- {std_acc:.4f}\")\n",
    "            channel_selection_results.append({\n",
    "                'model': model_name,\n",
    "                'method': method,\n",
    "                'k': k,\n",
    "                'mean_accuracy': mean_acc,\n",
    "                'std_accuracy': std_acc,\n",
    "                'fold_accuracies': fold_accuracies\n",
    "            })\n",
    "\n",
    "print(f\"\n",
    "\n",
    "{'='*60}\")\n",
    "print(\"Channel selection evaluation complete!\")\n",
    "print(f\"{'='*60}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retention Analysis\n",
    "\n",
    "Evaluate performance as we gradually reduce the number of channels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retention analysis: Adaptive gating with different channel budgets\n",
    "retention_k_values = [5, 10, 15, 20, 25, 30, 35]\n",
    "\n",
    "print(f\"\n",
    "{'='*60}\")\n",
    "print(\"Retention Analysis: Adaptive-Gating-EEG-ARNN with Gate Selection\")\n",
    "print(f\"{'='*60}\n",
    "\")\n",
    "\n",
    "for k in retention_k_values:\n",
    "    print(f\"\n",
    "Testing with k={k} channels...\")\n",
    "    fold_accuracies = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "        model = build_model('Adaptive-Gating-EEG-ARNN', CONFIG['n_channels'])\n",
    "        model_path = os.path.join(CONFIG['models_dir'], f\"Adaptive-Gating-EEG-ARNN_fold{fold+1}.pt\")\n",
    "        state_dict = torch.load(model_path, map_location=CONFIG['device'])\n",
    "        model.load_state_dict(state_dict)\n",
    "        model = model.to(CONFIG['device'])\n",
    "        model.eval()\n",
    "\n",
    "        train_dataset = EEGDataset(X_train, y_train)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'],\n",
    "                                  shuffle=False, num_workers=0)\n",
    "        importance_scores = compute_gate_importance(model, train_loader, CONFIG['device'])\n",
    "        selected_channels = select_top_k_channels(importance_scores, k)\n",
    "\n",
    "        X_train_selected = apply_channel_selection(X_train, selected_channels)\n",
    "        X_val_selected = apply_channel_selection(X_val, selected_channels)\n",
    "\n",
    "        new_model = build_model('Adaptive-Gating-EEG-ARNN', k)\n",
    "        train_dataset = EEGDataset(X_train_selected, y_train)\n",
    "        val_dataset = EEGDataset(X_val_selected, y_val)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'],\n",
    "                                  shuffle=True, num_workers=0)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'],\n",
    "                                shuffle=False, num_workers=0)\n",
    "\n",
    "        best_state, val_acc = train_pytorch_model(new_model, train_loader, val_loader,\n",
    "                                                  CONFIG, f\"Retention-k{k}\")\n",
    "        fold_accuracies.append(val_acc)\n",
    "        print(f\"Fold {fold + 1}/{CONFIG['n_folds']} Acc: {val_acc:.4f}\")\n",
    "\n",
    "    mean_acc = np.mean(fold_accuracies)\n",
    "    std_acc = np.std(fold_accuracies)\n",
    "\n",
    "    print(f\"k={k}: {mean_acc:.4f} +/- {std_acc:.4f}\")\n",
    "    retention_results.append({\n",
    "        'k': k,\n",
    "        'mean_accuracy': mean_acc,\n",
    "        'std_accuracy': std_acc,\n",
    "        'fold_accuracies': fold_accuracies\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save channel selection results\n",
    "cs_df = pd.DataFrame(channel_selection_results)\n",
    "cs_df.to_csv(os.path.join(CONFIG['results_dir'], 'channel_selection_results.csv'), index=False)\n",
    "\n",
    "print(\"\\nChannel Selection Results:\")\n",
    "print(cs_df[['model', 'method', 'k', 'mean_accuracy', 'std_accuracy']])\n",
    "\n",
    "# Save retention analysis results\n",
    "retention_df = pd.DataFrame(retention_results)\n",
    "retention_df.to_csv(os.path.join(CONFIG['results_dir'], 'retention_analysis.csv'), index=False)\n",
    "\n",
    "print(\"\\nRetention Analysis Results:\")\n",
    "print(retention_df[['k', 'mean_accuracy', 'std_accuracy']])\n",
    "\n",
    "print(f\"\\nResults saved to {CONFIG['results_dir']}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Find the best channel selection method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best method for each model\n",
    "print(\"\\nBest Channel Selection Methods:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for model_name in ['Baseline-EEG-ARNN', 'Adaptive-Gating-EEG-ARNN']:\n",
    "    model_results = cs_df[cs_df['model'] == model_name]\n",
    "    best_result = model_results.loc[model_results['mean_accuracy'].idxmax()]\n",
    "    \n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"  Best Method: {best_result['method'].upper()}\")\n",
    "    print(f\"  Best k: {best_result['k']}\")\n",
    "    print(f\"  Accuracy: {best_result['mean_accuracy']:.4f} +/- {best_result['std_accuracy']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}