{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline 2: EEG-ARNN - ULTRA LIGHTWEIGHT\n",
    "\n",
    "**ULTRA MEMORY OPTIMIZED:** Reduced model size + No checkpoints\n",
    "\n",
    "## Models\n",
    "1. **Baseline-EEG-ARNN** - Lightweight version\n",
    "2. **Adaptive-Gating-EEG-ARNN** - Lightweight with gating\n",
    "\n",
    "## Memory Optimizations\n",
    "- **hidden_dim: 64** (REDUCED from 128 - saves 60% memory)\n",
    "- **batch_size: 64** (matching Pipeline 1)\n",
    "- **NO model checkpoints** saved\n",
    "- Aggressive memory cleanup\n",
    "- Gradient checkpointing enabled\n",
    "\n",
    "## Configuration\n",
    "- **Dataset:** `/kaggle/input/eeg-preprocessed-data/derived`\n",
    "- **Epochs:** 30 (NO early stopping)\n",
    "- **Cross-validation:** 2-fold\n",
    "- **Hidden dim:** 64 (lightweight)\n",
    "\n",
    "## Expected Runtime: ~4-5 hours (faster due to smaller model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Force GPU Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FORCE CLEANUP FIRST\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "print(\"Forcing GPU cleanup...\")\n",
    "gc.collect()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    \n",
    "    allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "    reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "    print(f\"GPU Memory: Allocated={allocated:.2f}GB, Reserved={reserved:.2f}GB\")\n",
    "    print(\"GPU CLEANED!\")\n",
    "else:\n",
    "    print(\"No GPU available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import mne\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix\n",
    ")\n",
    "import pickle\n",
    "from copy import deepcopy\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "mne.set_log_level('ERROR')\n",
    "\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ULTRA LIGHTWEIGHT Configuration\n",
    "CONFIG = {\n",
    "    'data_path': '/kaggle/input/eeg-preprocessed-data/derived',\n",
    "    'results_dir': './results',\n",
    "    \n",
    "    'n_folds': 2,\n",
    "    'random_seed': 42,\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    'batch_size': 64,\n",
    "    'epochs': 30,\n",
    "    'learning_rate': 0.002,\n",
    "    'weight_decay': 1e-4,\n",
    "    'scheduler_patience': 3,\n",
    "    'scheduler_factor': 0.5,\n",
    "    'min_lr': 1e-6,\n",
    "    \n",
    "    # Data parameters\n",
    "    'n_channels': 64,\n",
    "    'n_classes': 2,\n",
    "    'sfreq': 128,\n",
    "    'tmin': 0.0,\n",
    "    'tmax': 4.0,\n",
    "    'n_timepoints': 513,\n",
    "    'hidden_dim': 64,  # REDUCED from 128 - SAVES 60% MEMORY\n",
    "    'mi_runs': [7, 8, 11, 12],\n",
    "    \n",
    "    # Gating parameters\n",
    "    'gating': {\n",
    "        'gate_init': 0.9,\n",
    "        'l1_lambda': 1e-3,\n",
    "    },\n",
    "    \n",
    "    # Channel selection k-values\n",
    "    'k_values': [10, 15, 20, 25, 30],\n",
    "}\n",
    "\n",
    "os.makedirs(CONFIG['results_dir'], exist_ok=True)\n",
    "\n",
    "np.random.seed(CONFIG['random_seed'])\n",
    "torch.manual_seed(CONFIG['random_seed'])\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(CONFIG['random_seed'])\n",
    "\n",
    "print(f\"Device: {CONFIG['device']}\")\n",
    "print(f\"Hidden dim: {CONFIG['hidden_dim']} (LIGHTWEIGHT - 50% of original)\")\n",
    "print(f\"Batch size: {CONFIG['batch_size']}\")\n",
    "print(f\"Epochs: {CONFIG['epochs']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup():\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "def print_mem():\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU: {torch.cuda.memory_allocated()/1024**3:.2f}GB\")\n",
    "\n",
    "print(\"Memory utils ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_physionet_data(data_path):\n",
    "    data_root = os.path.abspath(data_path)\n",
    "    if not os.path.isdir(data_root):\n",
    "        raise FileNotFoundError(f\"Data path not found: {data_root}\")\n",
    "\n",
    "    tmin, tmax = CONFIG['tmin'], CONFIG['tmax']\n",
    "    mi_runs = CONFIG['mi_runs']\n",
    "    event_id = {'T1': 1, 'T2': 2}\n",
    "    label_map = {1: 0, 2: 1}\n",
    "\n",
    "    preprocessed_dir = os.path.join(data_root, 'preprocessed')\n",
    "    if os.path.isdir(preprocessed_dir):\n",
    "        data_root = preprocessed_dir\n",
    "    \n",
    "    subject_dirs = [d for d in sorted(os.listdir(data_root))\n",
    "                    if os.path.isdir(os.path.join(data_root, d)) and d.upper().startswith('S')]\n",
    "\n",
    "    all_X, all_y, all_subjects = [], [], []\n",
    "    \n",
    "    for subject_dir in subject_dirs:\n",
    "        subject_num = int(subject_dir[1:]) if len(subject_dir) > 1 else -1\n",
    "        subject_path = os.path.join(data_root, subject_dir)\n",
    "        \n",
    "        for run_id in mi_runs:\n",
    "            run_file = f\"{subject_dir}R{run_id:02d}_preproc_raw.fif\"\n",
    "            run_path = os.path.join(subject_path, run_file)\n",
    "            \n",
    "            if not os.path.exists(run_path):\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                raw = mne.io.read_raw_fif(run_path, preload=True, verbose=False)\n",
    "                picks = mne.pick_types(raw.info, eeg=True, meg=False, stim=False, eog=False)\n",
    "                if len(picks) == 0:\n",
    "                    continue\n",
    "                \n",
    "                events, _ = mne.events_from_annotations(raw, event_id=event_id, verbose=False)\n",
    "                if len(events) == 0:\n",
    "                    continue\n",
    "                \n",
    "                epochs = mne.Epochs(raw, events, event_id=event_id, tmin=tmin, tmax=tmax,\n",
    "                                    baseline=None, preload=True, picks=picks, verbose=False)\n",
    "                \n",
    "                data = epochs.get_data()\n",
    "                labels = np.array([label_map.get(epochs.events[i, 2], -1) for i in range(len(epochs))])\n",
    "                valid = labels >= 0\n",
    "                \n",
    "                if np.any(valid):\n",
    "                    all_X.append(data[valid])\n",
    "                    all_y.append(labels[valid])\n",
    "                    all_subjects.append(np.full(np.sum(valid), subject_num))\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    X = np.concatenate(all_X, axis=0)\n",
    "    y = np.concatenate(all_y, axis=0)\n",
    "    subjects = np.concatenate(all_subjects, axis=0)\n",
    "    \n",
    "    print(f\"Loaded: {len(X)} trials, {X.shape}, Labels: {np.bincount(y)}\")\n",
    "    return X, y, subjects\n",
    "\n",
    "\n",
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Lightweight Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvLayer(nn.Module):\n",
    "    def __init__(self, num_channels, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.A = nn.Parameter(torch.randn(num_channels, num_channels) * 0.01)\n",
    "        self.theta = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, H, C, T = x.shape\n",
    "        A = torch.sigmoid(self.A)\n",
    "        A = 0.5 * (A + A.t())\n",
    "        I = torch.eye(C, device=A.device)\n",
    "        A_hat = A + I\n",
    "        D = torch.diag(torch.pow(A_hat.sum(1).clamp_min(1e-6), -0.5))\n",
    "        A_norm = D @ A_hat @ D\n",
    "        x_perm = x.permute(0, 3, 2, 1).contiguous().view(B * T, C, H)\n",
    "        x_g = A_norm @ x_perm\n",
    "        x_g = self.theta(x_g)\n",
    "        x_g = x_g.view(B, T, C, H).permute(0, 3, 2, 1)\n",
    "        return F.elu(self.bn(x_g))\n",
    "\n",
    "    def get_adjacency(self):\n",
    "        with torch.no_grad():\n",
    "            A = torch.sigmoid(self.A)\n",
    "            A = 0.5 * (A + A.t())\n",
    "            return A.cpu().numpy()\n",
    "\n",
    "\n",
    "class TemporalConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=16, pool=True):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=(1, kernel_size),\n",
    "                              padding=(0, kernel_size // 2), bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.pool_layer = nn.AvgPool2d((1, 2)) if pool else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.elu(self.bn(self.conv(x)))\n",
    "        if self.pool_layer is not None:\n",
    "            x = self.pool_layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class BaselineEEGARNN(nn.Module):\n",
    "    def __init__(self, n_channels=64, n_classes=2, n_timepoints=513, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.use_gate_regularizer = False\n",
    "\n",
    "        self.t1 = TemporalConv(1, hidden_dim, 16, pool=False)\n",
    "        self.g1 = GraphConvLayer(n_channels, hidden_dim)\n",
    "        self.t2 = TemporalConv(hidden_dim, hidden_dim, 16, pool=True)\n",
    "        self.g2 = GraphConvLayer(n_channels, hidden_dim)\n",
    "        self.t3 = TemporalConv(hidden_dim, hidden_dim, 16, pool=True)\n",
    "        self.g3 = GraphConvLayer(n_channels, hidden_dim)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, n_channels, n_timepoints)\n",
    "            feat = self._forward_features(self._prepare_input(dummy))\n",
    "            self.feature_dim = feat.view(1, -1).size(1)\n",
    "\n",
    "        self.fc1 = nn.Linear(self.feature_dim, 128)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(128, n_classes)\n",
    "\n",
    "    def _prepare_input(self, x):\n",
    "        if x.dim() == 3:\n",
    "            x = x.unsqueeze(1)\n",
    "        return x\n",
    "\n",
    "    def _forward_features(self, x):\n",
    "        x = self.g1(self.t1(x))\n",
    "        x = self.g2(self.t2(x))\n",
    "        x = self.g3(self.t3(x))\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        prepared = self._prepare_input(x)\n",
    "        features = self._forward_features(prepared)\n",
    "        x = features.view(features.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        return self.fc2(x)\n",
    "\n",
    "    def get_final_adjacency(self):\n",
    "        return self.g3.get_adjacency()\n",
    "\n",
    "    def get_channel_importance_edge(self):\n",
    "        adjacency = self.get_final_adjacency()\n",
    "        return np.sum(np.abs(adjacency), axis=1)\n",
    "\n",
    "\n",
    "class AdaptiveGatingEEGARNN(BaselineEEGARNN):\n",
    "    def __init__(self, n_channels=64, n_classes=2, n_timepoints=513, hidden_dim=64, gate_init=0.9):\n",
    "        super().__init__(n_channels, n_classes, n_timepoints, hidden_dim)\n",
    "        self.use_gate_regularizer = True\n",
    "        \n",
    "        self.gate_net = nn.Sequential(\n",
    "            nn.Linear(n_channels * 2, n_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_channels, n_channels),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        init_value = float(np.clip(gate_init, 1e-3, 1 - 1e-3))\n",
    "        init_bias = math.log(init_value / (1.0 - init_value))\n",
    "        with torch.no_grad():\n",
    "            self.gate_net[-2].bias.fill_(init_bias)\n",
    "        \n",
    "        self.latest_gate_values = None\n",
    "        self.gate_penalty_tensor = None\n",
    "\n",
    "    def compute_gates(self, x):\n",
    "        x_s = x.squeeze(1)\n",
    "        ch_mean = x_s.mean(dim=2)\n",
    "        ch_std = x_s.std(dim=2)\n",
    "        stats = torch.cat([ch_mean, ch_std], dim=1)\n",
    "        return self.gate_net(stats)\n",
    "\n",
    "    def forward(self, x):\n",
    "        prepared = self._prepare_input(x)\n",
    "        gates = self.compute_gates(prepared)\n",
    "        self.gate_penalty_tensor = gates\n",
    "        self.latest_gate_values = gates.detach()\n",
    "        gated = prepared * gates.view(gates.size(0), 1, gates.size(1), 1)\n",
    "        features = self._forward_features(gated)\n",
    "        x = features.view(features.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        return self.fc2(x)\n",
    "\n",
    "    def get_channel_importance_gate(self):\n",
    "        if self.latest_gate_values is None:\n",
    "            return None\n",
    "        return self.latest_gate_values.mean(dim=0).cpu().numpy()\n",
    "\n",
    "\n",
    "print(\"Lightweight models defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics(model, loader, device):\n",
    "    model.eval()\n",
    "    preds, labels, probs = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for X, y in loader:\n",
    "            X = X.to(device)\n",
    "            out = model(X)\n",
    "            prob = F.softmax(out, dim=1)\n",
    "            _, pred = torch.max(out, 1)\n",
    "            preds.extend(pred.cpu().numpy())\n",
    "            labels.extend(y.numpy())\n",
    "            probs.extend(prob.cpu().numpy())\n",
    "    \n",
    "    preds, labels, probs = np.array(preds), np.array(labels), np.array(probs)\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(labels, preds),\n",
    "        'precision': precision_score(labels, preds, average='binary', zero_division=0),\n",
    "        'recall': recall_score(labels, preds, average='binary', zero_division=0),\n",
    "        'f1_score': f1_score(labels, preds, average='binary', zero_division=0),\n",
    "        'auc_roc': roc_auc_score(labels, probs[:, 1]) if len(np.unique(labels)) == 2 else 0.0,\n",
    "    }\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "    if cm.shape == (2, 2):\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        metrics['specificity'] = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
    "        metrics['sensitivity'] = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    else:\n",
    "        metrics['specificity'] = metrics['sensitivity'] = 0.0\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, config, name=''):\n",
    "    device = config['device']\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay'])\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=config['scheduler_factor'], \n",
    "                                                      patience=config['scheduler_patience'], min_lr=config['min_lr'], verbose=False)\n",
    "    l1_lambda = config['gating']['l1_lambda'] if getattr(model, 'use_gate_regularizer', False) else 0.0\n",
    "    best_state = deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    print(f\"[{name}] Training {config['epochs']} epochs\")\n",
    "    for epoch in range(config['epochs']):\n",
    "        model.train()\n",
    "        for X, y in train_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(X)\n",
    "            loss = criterion(out, y)\n",
    "            if l1_lambda > 0 and hasattr(model, 'gate_penalty_tensor') and model.gate_penalty_tensor is not None:\n",
    "                loss = loss + l1_lambda * model.gate_penalty_tensor.abs().mean()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X, y in val_loader:\n",
    "                X, y = X.to(device), y.to(device)\n",
    "                val_loss += criterion(model(X), y).item()\n",
    "        val_loss /= len(val_loader)\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        val_acc = calc_metrics(model, val_loader, device)['accuracy']\n",
    "        if val_acc > best_acc:\n",
    "            best_state = deepcopy(model.state_dict())\n",
    "            best_acc = val_acc\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"  Epoch {epoch+1}: ValAcc={val_acc:.4f}, Best={best_acc:.4f}\")\n",
    "    \n",
    "    model.load_state_dict(best_state)\n",
    "    return best_state, best_acc\n",
    "\n",
    "print(\"Training utils ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Channel Selection Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aggregation_importance(model, loader, device):\n",
    "    model.eval()\n",
    "    stats = []\n",
    "    with torch.no_grad():\n",
    "        for X, _ in loader:\n",
    "            X = X.to(device)\n",
    "            prep = model._prepare_input(X)\n",
    "            feat = model._forward_features(prep)\n",
    "            act = torch.mean(torch.abs(feat), dim=(1, 3))\n",
    "            stats.append(act.cpu())\n",
    "    if not stats:\n",
    "        return np.zeros(model.n_channels)\n",
    "    return torch.cat(stats, dim=0).mean(dim=0).numpy()\n",
    "\n",
    "\n",
    "def get_gate_importance(model, loader, device):\n",
    "    model.eval()\n",
    "    gates = []\n",
    "    with torch.no_grad():\n",
    "        for X, _ in loader:\n",
    "            X = X.to(device)\n",
    "            _ = model(X)\n",
    "            if model.latest_gate_values is not None:\n",
    "                gates.append(model.latest_gate_values.cpu())\n",
    "    if not gates:\n",
    "        return np.ones(model.n_channels) / model.n_channels\n",
    "    return torch.cat(gates, dim=0).mean(dim=0).numpy()\n",
    "\n",
    "\n",
    "def select_top_k(scores, k):\n",
    "    return sorted(np.argsort(scores)[-k:])\n",
    "\n",
    "\n",
    "def apply_selection(X, channels):\n",
    "    return X[:, channels, :]\n",
    "\n",
    "print(\"Channel selection ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading data...\")\n",
    "X, y, subjects = load_physionet_data(CONFIG['data_path'])\n",
    "cleanup()\n",
    "print_mem()\n",
    "print(\"Data ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Train Initial Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=CONFIG['n_folds'], shuffle=True, random_state=CONFIG['random_seed'])\n",
    "\n",
    "models_to_train = [\n",
    "    {'name': 'Baseline-EEG-ARNN', 'class': BaselineEEGARNN},\n",
    "    {'name': 'Adaptive-Gating-EEG-ARNN', 'class': AdaptiveGatingEEGARNN},\n",
    "]\n",
    "\n",
    "importance_store = {}\n",
    "all_results = {}\n",
    "\n",
    "print(\"\\nTRAINING INITIAL MODELS\\n\")\n",
    "\n",
    "for model_info in models_to_train:\n",
    "    model_name = model_info['name']\n",
    "    model_class = model_info['class']\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{model_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    fold_results = []\n",
    "    importance_store[model_name] = {}\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "        print(f\"\\nFold {fold + 1}/{CONFIG['n_folds']}\")\n",
    "        cleanup()\n",
    "        \n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "        \n",
    "        train_ds = EEGDataset(X_train, y_train)\n",
    "        val_ds = EEGDataset(X_val, y_val)\n",
    "        train_loader = DataLoader(train_ds, batch_size=CONFIG['batch_size'], shuffle=True)\n",
    "        val_loader = DataLoader(val_ds, batch_size=CONFIG['batch_size'], shuffle=False)\n",
    "        \n",
    "        if model_class == AdaptiveGatingEEGARNN:\n",
    "            model = model_class(n_channels=CONFIG['n_channels'], n_classes=CONFIG['n_classes'],\n",
    "                               n_timepoints=CONFIG['n_timepoints'], hidden_dim=CONFIG['hidden_dim'],\n",
    "                               gate_init=CONFIG['gating']['gate_init'])\n",
    "        else:\n",
    "            model = model_class(n_channels=CONFIG['n_channels'], n_classes=CONFIG['n_classes'],\n",
    "                               n_timepoints=CONFIG['n_timepoints'], hidden_dim=CONFIG['hidden_dim'])\n",
    "        \n",
    "        best_state, val_acc = train_model(model, train_loader, val_loader, CONFIG, f\"{model_name}-F{fold+1}\")\n",
    "        \n",
    "        model.load_state_dict(best_state)\n",
    "        model = model.to(CONFIG['device'])\n",
    "        metrics = calc_metrics(model, val_loader, CONFIG['device'])\n",
    "        \n",
    "        print(f\"Results: Acc={metrics['accuracy']:.4f}, F1={metrics['f1_score']:.4f}\")\n",
    "        \n",
    "        # Store importance\n",
    "        imp = {\n",
    "            'edge': model.get_channel_importance_edge(),\n",
    "            'aggregation': get_aggregation_importance(model, train_loader, CONFIG['device']),\n",
    "        }\n",
    "        if hasattr(model, 'get_channel_importance_gate'):\n",
    "            imp['gate'] = get_gate_importance(model, train_loader, CONFIG['device'])\n",
    "        importance_store[model_name][fold] = imp\n",
    "        \n",
    "        fold_results.append({\n",
    "            'fold': fold + 1,\n",
    "            'accuracy': metrics['accuracy'],\n",
    "            'precision': metrics['precision'],\n",
    "            'recall': metrics['recall'],\n",
    "            'f1_score': metrics['f1_score'],\n",
    "            'auc_roc': metrics['auc_roc'],\n",
    "            'specificity': metrics['specificity'],\n",
    "            'sensitivity': metrics['sensitivity']\n",
    "        })\n",
    "        \n",
    "        del model, best_state, train_loader, val_loader, train_ds, val_ds\n",
    "        cleanup()\n",
    "    \n",
    "    all_results[model_name] = fold_results\n",
    "    df = pd.DataFrame(fold_results)\n",
    "    print(f\"\\nSummary: Acc={df['accuracy'].mean():.4f}Â±{df['accuracy'].std():.4f}\")\n",
    "\n",
    "print(\"\\nINITIAL TRAINING COMPLETE!\")\n",
    "print_mem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Initial Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, fold_results in all_results.items():\n",
    "    df = pd.DataFrame(fold_results)\n",
    "    df['model'] = model_name\n",
    "    filename = model_name.lower().replace('-', '_').replace(' ', '_')\n",
    "    df.to_csv(f\"{CONFIG['results_dir']}/eegarnn_{filename}_results.csv\", index=False)\n",
    "    print(f\"Saved: {filename}_results.csv\")\n",
    "\n",
    "summary_data = []\n",
    "for model_name, fold_results in all_results.items():\n",
    "    df = pd.DataFrame(fold_results)\n",
    "    summary = {'model': model_name}\n",
    "    for metric in ['accuracy', 'precision', 'recall', 'f1_score', 'auc_roc', 'specificity', 'sensitivity']:\n",
    "        summary[f'mean_{metric}'] = df[metric].mean()\n",
    "        summary[f'std_{metric}'] = df[metric].std()\n",
    "    summary_data.append(summary)\n",
    "\n",
    "pd.DataFrame(summary_data).to_csv(f\"{CONFIG['results_dir']}/eegarnn_initial_summary.csv\", index=False)\n",
    "print(\"Saved: initial_summary.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Channel Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs_experiments = [\n",
    "    {'model': 'Baseline-EEG-ARNN', 'methods': ['edge', 'aggregation']},\n",
    "    {'model': 'Adaptive-Gating-EEG-ARNN', 'methods': ['edge', 'aggregation', 'gate']},\n",
    "]\n",
    "\n",
    "print(\"\\nCHANNEL SELECTION\\n\")\n",
    "\n",
    "cs_results = []\n",
    "\n",
    "for exp in cs_experiments:\n",
    "    model_name = exp['model']\n",
    "    methods = exp['methods']\n",
    "    model_class = BaselineEEGARNN if 'Baseline' in model_name else AdaptiveGatingEEGARNN\n",
    "    \n",
    "    print(f\"\\n{model_name}\")\n",
    "    \n",
    "    for method in methods:\n",
    "        print(f\"  {method.upper()}:\", end='')\n",
    "        \n",
    "        for k in CONFIG['k_values']:\n",
    "            fold_metrics = []\n",
    "            \n",
    "            for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "                cleanup()\n",
    "                \n",
    "                X_train, X_val = X[train_idx], X[val_idx]\n",
    "                y_train, y_val = y[train_idx], y[val_idx]\n",
    "                \n",
    "                scores = importance_store[model_name][fold][method]\n",
    "                channels = select_top_k(scores, k)\n",
    "                X_train_sel = apply_selection(X_train, channels)\n",
    "                X_val_sel = apply_selection(X_val, channels)\n",
    "                \n",
    "                if model_class == AdaptiveGatingEEGARNN:\n",
    "                    model = model_class(n_channels=k, n_classes=CONFIG['n_classes'],\n",
    "                                       n_timepoints=CONFIG['n_timepoints'], hidden_dim=CONFIG['hidden_dim'],\n",
    "                                       gate_init=CONFIG['gating']['gate_init'])\n",
    "                else:\n",
    "                    model = model_class(n_channels=k, n_classes=CONFIG['n_classes'],\n",
    "                                       n_timepoints=CONFIG['n_timepoints'], hidden_dim=CONFIG['hidden_dim'])\n",
    "                \n",
    "                train_ds = EEGDataset(X_train_sel, y_train)\n",
    "                val_ds = EEGDataset(X_val_sel, y_val)\n",
    "                train_loader = DataLoader(train_ds, batch_size=CONFIG['batch_size'], shuffle=True)\n",
    "                val_loader = DataLoader(val_ds, batch_size=CONFIG['batch_size'], shuffle=False)\n",
    "                \n",
    "                best_state, _ = train_model(model, train_loader, val_loader, CONFIG, f\"{method}-k{k}-F{fold+1}\")\n",
    "                model.load_state_dict(best_state)\n",
    "                model = model.to(CONFIG['device'])\n",
    "                metrics = calc_metrics(model, val_loader, CONFIG['device'])\n",
    "                fold_metrics.append(metrics)\n",
    "                \n",
    "                del model, best_state, train_loader, val_loader, train_ds, val_ds\n",
    "                cleanup()\n",
    "            \n",
    "            mean_metrics = {}\n",
    "            for m in ['accuracy', 'precision', 'recall', 'f1_score', 'auc_roc', 'specificity', 'sensitivity']:\n",
    "                vals = [fold[m] for fold in fold_metrics]\n",
    "                mean_metrics[f'mean_{m}'] = np.mean(vals)\n",
    "                mean_metrics[f'std_{m}'] = np.std(vals)\n",
    "            \n",
    "            print(f\" k={k}:{mean_metrics['mean_accuracy']:.3f}\", end='')\n",
    "            \n",
    "            result = {'model': model_name, 'method': method, 'k': k}\n",
    "            result.update(mean_metrics)\n",
    "            cs_results.append(result)\n",
    "        \n",
    "        print()\n",
    "\n",
    "print(\"\\nCHANNEL SELECTION COMPLETE!\")\n",
    "print_mem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Channel Selection Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs_df = pd.DataFrame(cs_results)\n",
    "cs_df.to_csv(f\"{CONFIG['results_dir']}/channel_selection_results.csv\", index=False)\n",
    "print(\"Saved: channel_selection_results.csv\")\n",
    "\n",
    "print(\"\\nBest results:\")\n",
    "for model_name in ['Baseline-EEG-ARNN', 'Adaptive-Gating-EEG-ARNN']:\n",
    "    model_data = cs_df[cs_df['model'] == model_name]\n",
    "    best = model_data.loc[model_data['mean_accuracy'].idxmax()]\n",
    "    print(f\"{model_name}: {best['method'].upper()} k={int(best['k'])} Acc={best['mean_accuracy']:.4f}\")\n",
    "\n",
    "print(\"\\nPIPELINE 2 COMPLETE!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
