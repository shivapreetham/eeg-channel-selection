{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# PhysioNet Motor Imagery - Complete Model Comparison\n\n## Comprehensive Comparison of All 7 Models\n\nThis notebook compares:\n- **EEG-ARNN Models**: Baseline, Adaptive Gating (with ES/AS/GS channel selection)\n- **Legacy Methods**: FBCSP, CNN-SAE, EEGNet, ACS-SE-CNN, G-CARM (with channel selection)\n\n## Analyses:\n1. **Full-Channel Performance** - All 64 channels\n2. **Channel Selection Performance** - Top-k channels (k=10,15,20,25,30)\n3. **Accuracy Drop Analysis** - Robustness to channel reduction\n4. **Optimal k-Value** - Best accuracy/channel trade-off\n5. **Statistical Comparisons** - Significance tests\n6. **World-Class Visualizations** - Publication-ready figures\n\n## Input Files:\n**Full-channel results (7 files):**\n- `eeg_arnn_baseline_results.csv`, `eeg_arnn_adaptive_results.csv`\n- `legacy_fbcsp_results.csv`, `legacy_cnn_sae_results.csv`, `legacy_eegnet_results.csv`, `legacy_acs_se_cnn_results.csv`, `legacy_g_carm_results.csv`\n\n**Channel selection results (12 files):**\n- `eeg_arnn_baseline_retrain_results.csv`, `eeg_arnn_adaptive_retrain_results.csv`\n- `legacy_fbcsp_retrain_results.csv`, `legacy_cnn_sae_retrain_results.csv`, `legacy_eegnet_retrain_results.csv`, `legacy_acs_se_cnn_retrain_results.csv`, `legacy_g_carm_retrain_results.csv`\n\n## Output:\n- Comprehensive comparison tables\n- Statistical test results\n- 10+ publication-quality visualizations\n- Summary CSVs"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_context('notebook', font_scale=1.2)\n",
    "\n",
    "print(\"Libraries loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load All Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "results_dir = Path('results')\n\n# Full-channel results\nfull_channel_files = {\n    'Baseline EEG-ARNN': 'eeg_arnn_baseline_results.csv',\n    'Adaptive Gating EEG-ARNN': 'eeg_arnn_adaptive_results.csv',\n    'FBCSP': 'legacy_fbcsp_results.csv',\n    'CNN-SAE': 'legacy_cnn_sae_results.csv',\n    'EEGNet': 'legacy_eegnet_results.csv',\n    'ACS-SE-CNN': 'legacy_acs_se_cnn_results.csv',\n    'G-CARM': 'legacy_g_carm_results.csv'\n}\n\n# Channel selection (retrain) results\nretrain_files = {\n    'Baseline EEG-ARNN': 'eeg_arnn_baseline_retrain_results.csv',\n    'Adaptive Gating EEG-ARNN': 'eeg_arnn_adaptive_retrain_results.csv',\n    'FBCSP': 'legacy_fbcsp_retrain_results.csv',\n    'CNN-SAE': 'legacy_cnn_sae_retrain_results.csv',\n    'EEGNet': 'legacy_eegnet_retrain_results.csv',\n    'ACS-SE-CNN': 'legacy_acs_se_cnn_retrain_results.csv',\n    'G-CARM': 'legacy_g_carm_retrain_results.csv'\n}\n\n# Load full-channel results\nfull_results = {}\nfor model_name, filename in full_channel_files.items():\n    filepath = results_dir / filename\n    if filepath.exists():\n        full_results[model_name] = pd.read_csv(filepath)\n        print(f\"Loaded full-channel: {model_name:30s} ({len(full_results[model_name])} subjects)\")\n    else:\n        print(f\"Warning: {filename} not found\")\n\nprint()\n\n# Load channel selection results\nretrain_results = {}\nfor model_name, filename in retrain_files.items():\n    filepath = results_dir / filename\n    if filepath.exists():\n        retrain_results[model_name] = pd.read_csv(filepath)\n        n_rows = len(retrain_results[model_name])\n        print(f\"Loaded retrain: {model_name:30s} ({n_rows} rows)\")\n    else:\n        print(f\"Warning: {filename} not found\")\n\nprint(f\"\\nFull-channel models loaded: {len(full_results)}\")\nprint(f\"Channel selection models loaded: {len(retrain_results)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Aggregate Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Full-channel summary\nfull_summary_data = []\n\nfor model_name, df in full_results.items():\n    full_summary_data.append({\n        'Model': model_name,\n        'Channels': 64,\n        'Accuracy': df['accuracy'].mean(),\n        'Std_Acc': df['accuracy'].std(),\n        'Precision': df['precision'].mean(),\n        'Recall': df['recall'].mean(),\n        'F1-Score': df['f1_score'].mean(),\n        'AUC-ROC': df['auc_roc'].mean(),\n        'Specificity': df['specificity'].mean(),\n        'N_subjects': len(df)\n    })\n\nfull_summary_df = pd.DataFrame(full_summary_data).sort_values('Accuracy', ascending=False)\n\nprint(\"\\n\" + \"=\"*140)\nprint(\"FULL-CHANNEL PERFORMANCE (64 channels)\")\nprint(\"=\"*140)\nprint(full_summary_df.to_string(index=False, float_format='%.4f'))\nprint(\"=\"*140)\n\n# Best channel selection performance for each model\nbest_retrain_data = []\n\nfor model_name, df in retrain_results.items():\n    if len(df) == 0:\n        continue\n    \n    # Get unique methods for this model\n    methods = df['method'].unique() if 'method' in df.columns else [model_name.upper()]\n    \n    for method in methods:\n        method_df = df[df['method'] == method] if 'method' in df.columns else df\n        \n        # Find best k-value (highest accuracy)\n        best_row = method_df.loc[method_df.groupby('k')['avg_accuracy'].mean().idxmax()]\n        best_k = best_row['k'] if 'k' in best_row else method_df['k'].mode().iloc[0]\n        \n        best_k_df = method_df[method_df['k'] == best_k]\n        \n        best_retrain_data.append({\n            'Model': model_name,\n            'Method': method,\n            'Best_k': int(best_k),\n            'Accuracy': best_k_df['avg_accuracy'].mean(),\n            'Std_Acc': best_k_df['avg_accuracy'].std(),\n            'Acc_Drop': best_k_df['accuracy_drop'].mean(),\n            'F1-Score': best_k_df['avg_f1_score'].mean(),\n            'AUC-ROC': best_k_df['avg_auc_roc'].mean()\n        })\n\nbest_retrain_df = pd.DataFrame(best_retrain_data).sort_values('Accuracy', ascending=False)\n\nprint(\"\\n\" + \"=\"*140)\nprint(\"BEST CHANNEL SELECTION PERFORMANCE (optimal k per method)\")\nprint(\"=\"*140)\nprint(best_retrain_df.to_string(index=False, float_format='%.4f'))\nprint(\"=\"*140)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Statistical Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STATISTICAL SIGNIFICANCE TESTS (Paired t-test)\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "if 'Adaptive Gating EEG-ARNN' in results and 'Baseline EEG-ARNN' in results:\n",
    "    adaptive_acc = results['Adaptive Gating EEG-ARNN']['accuracy'].values\n",
    "    baseline_acc = results['Baseline EEG-ARNN']['accuracy'].values\n",
    "    \n",
    "    if len(adaptive_acc) == len(baseline_acc):\n",
    "        t_stat, p_value = stats.ttest_rel(adaptive_acc, baseline_acc)\n",
    "        improvement = adaptive_acc.mean() - baseline_acc.mean()\n",
    "        \n",
    "        print(\"Adaptive Gating vs Baseline EEG-ARNN:\")\n",
    "        print(f\"  Mean improvement: {improvement:.4f} ({improvement/baseline_acc.mean()*100:.2f}%)\")\n",
    "        print(f\"  t-statistic: {t_stat:.4f}\")\n",
    "        print(f\"  p-value: {p_value:.4f}\")\n",
    "        print(f\"  Significant: {'Yes' if p_value < 0.05 else 'No'} (alpha=0.05)\")\n",
    "        print()\n",
    "\n",
    "baseline_model = 'Baseline EEG-ARNN'\n",
    "if baseline_model in results:\n",
    "    baseline_acc = results[baseline_model]['accuracy'].values\n",
    "    \n",
    "    print(f\"\\nComparison against {baseline_model}:\\n\")\n",
    "    \n",
    "    for model_name, df in results.items():\n",
    "        if model_name == baseline_model:\n",
    "            continue\n",
    "        \n",
    "        model_acc = df['accuracy'].values\n",
    "        \n",
    "        if len(model_acc) == len(baseline_acc):\n",
    "            t_stat, p_value = stats.ttest_rel(model_acc, baseline_acc)\n",
    "            improvement = model_acc.mean() - baseline_acc.mean()\n",
    "            \n",
    "            print(f\"{model_name}:\")\n",
    "            print(f\"  Improvement: {improvement:+.4f} ({improvement/baseline_acc.mean()*100:+.2f}%)\")\n",
    "            print(f\"  p-value: {p_value:.4f}\")\n",
    "            print(f\"  Significant: {'Yes' if p_value < 0.05 else 'No'}\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Box Plot Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "fig.suptitle('Model Performance Comparison Across All Metrics', fontsize=16, fontweight='bold')\n",
    "\n",
    "metrics = ['accuracy', 'precision', 'recall', 'f1_score', 'auc_roc', 'specificity']\n",
    "metric_names = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC-ROC', 'Specificity']\n",
    "\n",
    "for idx, (metric, metric_name) in enumerate(zip(metrics, metric_names)):\n",
    "    ax = axes[idx // 3, idx % 3]\n",
    "    \n",
    "    data_for_plot = []\n",
    "    labels_for_plot = []\n",
    "    \n",
    "    for model_name, df in results.items():\n",
    "        if metric in df.columns:\n",
    "            data_for_plot.append(df[metric].values)\n",
    "            labels_for_plot.append(model_name.replace(' EEG-ARNN', '').replace(' ', '\\n'))\n",
    "    \n",
    "    bp = ax.boxplot(data_for_plot, labels=labels_for_plot, patch_artist=True)\n",
    "    \n",
    "    for patch in bp['boxes']:\n",
    "        patch.set_facecolor('skyblue')\n",
    "        patch.set_alpha(0.7)\n",
    "    \n",
    "    ax.set_ylabel(metric_name, fontsize=12)\n",
    "    ax.set_title(f'{metric_name} Distribution', fontsize=12, fontweight='bold')\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/model_comparison_boxplots.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Box plots saved to results/model_comparison_boxplots.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Bar Chart - Mean Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "model_names = list(results.keys())\n",
    "accuracies = [results[m]['accuracy'].mean() for m in model_names]\n",
    "std_accs = [results[m]['accuracy'].std() for m in model_names]\n",
    "\n",
    "x_pos = np.arange(len(model_names))\n",
    "bars = ax.bar(x_pos, accuracies, yerr=std_accs, capsize=5, alpha=0.8, \n",
    "              color=['#2ecc71' if 'Adaptive' in m else '#3498db' if 'Baseline' in m else '#95a5a6' \n",
    "                     for m in model_names])\n",
    "\n",
    "ax.set_xlabel('Model', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('Accuracy', fontsize=14, fontweight='bold')\n",
    "ax.set_title('Mean Accuracy Comparison Across All Models', fontsize=16, fontweight='bold')\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels([m.replace(' EEG-ARNN', '') for m in model_names], rotation=45, ha='right')\n",
    "ax.set_ylim([min(accuracies) - 0.05, max(accuracies) + 0.05])\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for i, (acc, std) in enumerate(zip(accuracies, std_accs)):\n",
    "    ax.text(i, acc + std + 0.01, f'{acc:.3f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/model_comparison_accuracy.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Bar chart saved to results/model_comparison_accuracy.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Radar Chart - Multi-Metric Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import pi\n",
    "\n",
    "metrics = ['accuracy', 'precision', 'recall', 'f1_score', 'auc_roc', 'specificity']\n",
    "metric_labels = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC-ROC', 'Specificity']\n",
    "\n",
    "num_vars = len(metrics)\n",
    "angles = [n / float(num_vars) * 2 * pi for n in range(num_vars)]\n",
    "angles += angles[:1]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 12), subplot_kw=dict(projection='polar'))\n",
    "\n",
    "colors = ['#2ecc71', '#3498db', '#e74c3c', '#f39c12', '#9b59b6', '#1abc9c', '#34495e']\n",
    "\n",
    "for idx, (model_name, df) in enumerate(results.items()):\n",
    "    values = [df[m].mean() for m in metrics]\n",
    "    values += values[:1]\n",
    "    \n",
    "    ax.plot(angles, values, 'o-', linewidth=2, label=model_name.replace(' EEG-ARNN', ''),\n",
    "            color=colors[idx % len(colors)])\n",
    "    ax.fill(angles, values, alpha=0.15, color=colors[idx % len(colors)])\n",
    "\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(metric_labels, size=12)\n",
    "ax.set_ylim(0, 1.0)\n",
    "ax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "ax.set_yticklabels(['0.2', '0.4', '0.6', '0.8', '1.0'], size=10)\n",
    "ax.grid(True)\n",
    "ax.set_title('Multi-Metric Performance Comparison', size=16, fontweight='bold', pad=20)\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1), fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/model_comparison_radar.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Radar chart saved to results/model_comparison_radar.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Ranking Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking_data = []\n",
    "\n",
    "for model_name, df in results.items():\n",
    "    ranking_data.append({\n",
    "        'Model': model_name,\n",
    "        'Mean_Accuracy': df['accuracy'].mean(),\n",
    "        'Mean_F1': df['f1_score'].mean(),\n",
    "        'Mean_AUC_ROC': df['auc_roc'].mean(),\n",
    "    })\n",
    "\n",
    "ranking_df = pd.DataFrame(ranking_data)\n",
    "ranking_df['Overall_Score'] = (\n",
    "    ranking_df['Mean_Accuracy'] * 0.4 + \n",
    "    ranking_df['Mean_F1'] * 0.3 + \n",
    "    ranking_df['Mean_AUC_ROC'] * 0.3\n",
    ")\n",
    "ranking_df = ranking_df.sort_values('Overall_Score', ascending=False).reset_index(drop=True)\n",
    "ranking_df['Rank'] = range(1, len(ranking_df) + 1)\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"MODEL RANKING (Overall Score = 0.4*Acc + 0.3*F1 + 0.3*AUC)\")\n",
    "print(\"=\"*100)\n",
    "print(ranking_df[['Rank', 'Model', 'Mean_Accuracy', 'Mean_F1', 'Mean_AUC_ROC', 'Overall_Score']].to_string(index=False))\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df.to_csv('results/final_comparison_summary.csv', index=False)\n",
    "ranking_df.to_csv('results/final_ranking.csv', index=False)\n",
    "\n",
    "print(\"\\nFinal summary saved:\")\n",
    "print(\"  - results/final_comparison_summary.csv\")\n",
    "print(\"  - results/final_ranking.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARISON COMPLETE!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}