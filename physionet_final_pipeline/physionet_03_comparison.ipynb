{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PhysioNet Motor Imagery - Complete Model Comparison\n",
    "\n",
    "## Compare All 7 Models\n",
    "\n",
    "This notebook loads results from:\n",
    "- **EEG-ARNN Models**: Baseline, Adaptive Gating\n",
    "- **Legacy Methods**: FBCSP, CNN-SAE, EEGNet, ACS-SE-CNN, G-CARM\n",
    "\n",
    "And generates comprehensive comparison tables and visualizations.\n",
    "\n",
    "## Input Files:\n",
    "- `eeg_arnn_baseline_results.csv`\n",
    "- `eeg_arnn_adaptive_results.csv`\n",
    "- `legacy_fbcsp_results.csv`\n",
    "- `legacy_cnn_sae_results.csv`\n",
    "- `legacy_eegnet_results.csv`\n",
    "- `legacy_acs_se_cnn_results.csv`\n",
    "- `legacy_g_carm_results.csv`\n",
    "\n",
    "## Output:\n",
    "- Comparison tables\n",
    "- Statistical tests\n",
    "- Visualizations\n",
    "- `final_comparison_summary.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_context('notebook', font_scale=1.2)\n",
    "\n",
    "print(\"Libraries loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load All Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = Path('results')\n",
    "\n",
    "model_files = {\n",
    "    'Baseline EEG-ARNN': 'eeg_arnn_baseline_results.csv',\n",
    "    'Adaptive Gating EEG-ARNN': 'eeg_arnn_adaptive_results.csv',\n",
    "    'FBCSP': 'legacy_fbcsp_results.csv',\n",
    "    'CNN-SAE': 'legacy_cnn_sae_results.csv',\n",
    "    'EEGNet': 'legacy_eegnet_results.csv',\n",
    "    'ACS-SE-CNN': 'legacy_acs_se_cnn_results.csv',\n",
    "    'G-CARM': 'legacy_g_carm_results.csv'\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for model_name, filename in model_files.items():\n",
    "    filepath = results_dir / filename\n",
    "    if filepath.exists():\n",
    "        results[model_name] = pd.read_csv(filepath)\n",
    "        print(f\"Loaded: {model_name} ({len(results[model_name])} subjects)\")\n",
    "    else:\n",
    "        print(f\"Warning: {filename} not found\")\n",
    "\n",
    "print(f\"\\nTotal models loaded: {len(results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Aggregate Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_data = []\n",
    "\n",
    "for model_name, df in results.items():\n",
    "    summary_data.append({\n",
    "        'Model': model_name,\n",
    "        'Accuracy': f\"{df['accuracy'].mean():.4f} ± {df['accuracy'].std():.4f}\",\n",
    "        'Precision': f\"{df['precision'].mean():.4f} ± {df['precision'].std():.4f}\",\n",
    "        'Recall': f\"{df['recall'].mean():.4f} ± {df['recall'].std():.4f}\",\n",
    "        'F1-Score': f\"{df['f1_score'].mean():.4f} ± {df['f1_score'].std():.4f}\",\n",
    "        'AUC-ROC': f\"{df['auc_roc'].mean():.4f} ± {df['auc_roc'].std():.4f}\",\n",
    "        'Specificity': f\"{df['specificity'].mean():.4f} ± {df['specificity'].std():.4f}\",\n",
    "        'N_subjects': len(df)\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "summary_df = summary_df.sort_values('Model')\n",
    "\n",
    "print(\"\\n\" + \"=\"*120)\n",
    "print(\"COMPREHENSIVE MODEL COMPARISON\")\n",
    "print(\"=\"*120)\n",
    "print(summary_df.to_string(index=False))\n",
    "print(\"=\"*120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Statistical Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STATISTICAL SIGNIFICANCE TESTS (Paired t-test)\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "if 'Adaptive Gating EEG-ARNN' in results and 'Baseline EEG-ARNN' in results:\n",
    "    adaptive_acc = results['Adaptive Gating EEG-ARNN']['accuracy'].values\n",
    "    baseline_acc = results['Baseline EEG-ARNN']['accuracy'].values\n",
    "    \n",
    "    if len(adaptive_acc) == len(baseline_acc):\n",
    "        t_stat, p_value = stats.ttest_rel(adaptive_acc, baseline_acc)\n",
    "        improvement = adaptive_acc.mean() - baseline_acc.mean()\n",
    "        \n",
    "        print(\"Adaptive Gating vs Baseline EEG-ARNN:\")\n",
    "        print(f\"  Mean improvement: {improvement:.4f} ({improvement/baseline_acc.mean()*100:.2f}%)\")\n",
    "        print(f\"  t-statistic: {t_stat:.4f}\")\n",
    "        print(f\"  p-value: {p_value:.4f}\")\n",
    "        print(f\"  Significant: {'Yes' if p_value < 0.05 else 'No'} (alpha=0.05)\")\n",
    "        print()\n",
    "\n",
    "baseline_model = 'Baseline EEG-ARNN'\n",
    "if baseline_model in results:\n",
    "    baseline_acc = results[baseline_model]['accuracy'].values\n",
    "    \n",
    "    print(f\"\\nComparison against {baseline_model}:\\n\")\n",
    "    \n",
    "    for model_name, df in results.items():\n",
    "        if model_name == baseline_model:\n",
    "            continue\n",
    "        \n",
    "        model_acc = df['accuracy'].values\n",
    "        \n",
    "        if len(model_acc) == len(baseline_acc):\n",
    "            t_stat, p_value = stats.ttest_rel(model_acc, baseline_acc)\n",
    "            improvement = model_acc.mean() - baseline_acc.mean()\n",
    "            \n",
    "            print(f\"{model_name}:\")\n",
    "            print(f\"  Improvement: {improvement:+.4f} ({improvement/baseline_acc.mean()*100:+.2f}%)\")\n",
    "            print(f\"  p-value: {p_value:.4f}\")\n",
    "            print(f\"  Significant: {'Yes' if p_value < 0.05 else 'No'}\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Box Plot Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "fig.suptitle('Model Performance Comparison Across All Metrics', fontsize=16, fontweight='bold')\n",
    "\n",
    "metrics = ['accuracy', 'precision', 'recall', 'f1_score', 'auc_roc', 'specificity']\n",
    "metric_names = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC-ROC', 'Specificity']\n",
    "\n",
    "for idx, (metric, metric_name) in enumerate(zip(metrics, metric_names)):\n",
    "    ax = axes[idx // 3, idx % 3]\n",
    "    \n",
    "    data_for_plot = []\n",
    "    labels_for_plot = []\n",
    "    \n",
    "    for model_name, df in results.items():\n",
    "        if metric in df.columns:\n",
    "            data_for_plot.append(df[metric].values)\n",
    "            labels_for_plot.append(model_name.replace(' EEG-ARNN', '').replace(' ', '\\n'))\n",
    "    \n",
    "    bp = ax.boxplot(data_for_plot, labels=labels_for_plot, patch_artist=True)\n",
    "    \n",
    "    for patch in bp['boxes']:\n",
    "        patch.set_facecolor('skyblue')\n",
    "        patch.set_alpha(0.7)\n",
    "    \n",
    "    ax.set_ylabel(metric_name, fontsize=12)\n",
    "    ax.set_title(f'{metric_name} Distribution', fontsize=12, fontweight='bold')\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/model_comparison_boxplots.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Box plots saved to results/model_comparison_boxplots.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Bar Chart - Mean Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "model_names = list(results.keys())\n",
    "accuracies = [results[m]['accuracy'].mean() for m in model_names]\n",
    "std_accs = [results[m]['accuracy'].std() for m in model_names]\n",
    "\n",
    "x_pos = np.arange(len(model_names))\n",
    "bars = ax.bar(x_pos, accuracies, yerr=std_accs, capsize=5, alpha=0.8, \n",
    "              color=['#2ecc71' if 'Adaptive' in m else '#3498db' if 'Baseline' in m else '#95a5a6' \n",
    "                     for m in model_names])\n",
    "\n",
    "ax.set_xlabel('Model', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('Accuracy', fontsize=14, fontweight='bold')\n",
    "ax.set_title('Mean Accuracy Comparison Across All Models', fontsize=16, fontweight='bold')\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels([m.replace(' EEG-ARNN', '') for m in model_names], rotation=45, ha='right')\n",
    "ax.set_ylim([min(accuracies) - 0.05, max(accuracies) + 0.05])\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for i, (acc, std) in enumerate(zip(accuracies, std_accs)):\n",
    "    ax.text(i, acc + std + 0.01, f'{acc:.3f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/model_comparison_accuracy.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Bar chart saved to results/model_comparison_accuracy.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Radar Chart - Multi-Metric Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import pi\n",
    "\n",
    "metrics = ['accuracy', 'precision', 'recall', 'f1_score', 'auc_roc', 'specificity']\n",
    "metric_labels = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC-ROC', 'Specificity']\n",
    "\n",
    "num_vars = len(metrics)\n",
    "angles = [n / float(num_vars) * 2 * pi for n in range(num_vars)]\n",
    "angles += angles[:1]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 12), subplot_kw=dict(projection='polar'))\n",
    "\n",
    "colors = ['#2ecc71', '#3498db', '#e74c3c', '#f39c12', '#9b59b6', '#1abc9c', '#34495e']\n",
    "\n",
    "for idx, (model_name, df) in enumerate(results.items()):\n",
    "    values = [df[m].mean() for m in metrics]\n",
    "    values += values[:1]\n",
    "    \n",
    "    ax.plot(angles, values, 'o-', linewidth=2, label=model_name.replace(' EEG-ARNN', ''),\n",
    "            color=colors[idx % len(colors)])\n",
    "    ax.fill(angles, values, alpha=0.15, color=colors[idx % len(colors)])\n",
    "\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(metric_labels, size=12)\n",
    "ax.set_ylim(0, 1.0)\n",
    "ax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "ax.set_yticklabels(['0.2', '0.4', '0.6', '0.8', '1.0'], size=10)\n",
    "ax.grid(True)\n",
    "ax.set_title('Multi-Metric Performance Comparison', size=16, fontweight='bold', pad=20)\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1), fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/model_comparison_radar.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Radar chart saved to results/model_comparison_radar.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Ranking Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking_data = []\n",
    "\n",
    "for model_name, df in results.items():\n",
    "    ranking_data.append({\n",
    "        'Model': model_name,\n",
    "        'Mean_Accuracy': df['accuracy'].mean(),\n",
    "        'Mean_F1': df['f1_score'].mean(),\n",
    "        'Mean_AUC_ROC': df['auc_roc'].mean(),\n",
    "    })\n",
    "\n",
    "ranking_df = pd.DataFrame(ranking_data)\n",
    "ranking_df['Overall_Score'] = (\n",
    "    ranking_df['Mean_Accuracy'] * 0.4 + \n",
    "    ranking_df['Mean_F1'] * 0.3 + \n",
    "    ranking_df['Mean_AUC_ROC'] * 0.3\n",
    ")\n",
    "ranking_df = ranking_df.sort_values('Overall_Score', ascending=False).reset_index(drop=True)\n",
    "ranking_df['Rank'] = range(1, len(ranking_df) + 1)\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"MODEL RANKING (Overall Score = 0.4*Acc + 0.3*F1 + 0.3*AUC)\")\n",
    "print(\"=\"*100)\n",
    "print(ranking_df[['Rank', 'Model', 'Mean_Accuracy', 'Mean_F1', 'Mean_AUC_ROC', 'Overall_Score']].to_string(index=False))\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df.to_csv('results/final_comparison_summary.csv', index=False)\n",
    "ranking_df.to_csv('results/final_ranking.csv', index=False)\n",
    "\n",
    "print(\"\\nFinal summary saved:\")\n",
    "print(\"  - results/final_comparison_summary.csv\")\n",
    "print(\"  - results/final_ranking.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARISON COMPLETE!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
