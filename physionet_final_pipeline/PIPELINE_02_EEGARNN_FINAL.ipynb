{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline 2: EEG-ARNN Methods - FINAL VERSION\n",
    "\n",
    "**COMPLETE TRAINING:** 30 epochs, NO early stopping, Comprehensive metrics\n",
    "\n",
    "## Models\n",
    "1. **Baseline-EEG-ARNN** - Pure CNN-GCN architecture\n",
    "2. **Adaptive-Gating-EEG-ARNN** - With data-dependent channel gating\n",
    "\n",
    "## Experiments\n",
    "### Part 1: Train Both Models\n",
    "- 2-fold cross-validation (matching Pipeline 1)\n",
    "- 30 epochs WITHOUT early stopping (full training)\n",
    "- Comprehensive metrics: Accuracy, Precision, Recall, F1, AUC-ROC, Specificity, Sensitivity\n",
    "\n",
    "### Part 2: Channel Selection\n",
    "**Baseline**: 2 methods × 5 k-values = 10 experiments\n",
    "- Edge Selection (ES)\n",
    "- Aggregation Selection (AS)\n",
    "\n",
    "**Adaptive**: 3 methods × 5 k-values = 15 experiments\n",
    "- Edge Selection (ES)\n",
    "- Aggregation Selection (AS)\n",
    "- Gate Selection (GS)\n",
    "\n",
    "k-values: [10, 15, 20, 25, 30]\n",
    "\n",
    "## Configuration\n",
    "- **Dataset:** `/kaggle/input/eeg-preprocessed-data/derived`\n",
    "- **Epochs:** 30 (NO early stopping)\n",
    "- **Cross-validation:** 2-fold\n",
    "- **Learning rate:** 0.002\n",
    "- **Batch size:** 64\n",
    "\n",
    "## Expected Runtime: ~6-7 hours on Kaggle GPU\n",
    "- Initial training: ~1 hour (2 models × 2 folds × 30 epochs)\n",
    "- Channel selection: ~5-6 hours (25 experiments × 2 folds)\n",
    "\n",
    "## Outputs (Matching Pipeline 1 format)\n",
    "```\n",
    "results/eegarnn_baseline_results.csv          - Per-fold results with ALL metrics\n",
    "results/eegarnn_adaptive_results.csv          - Per-fold results with ALL metrics\n",
    "results/eegarnn_baseline_summary.csv          - Summary statistics\n",
    "results/eegarnn_adaptive_summary.csv          - Summary statistics\n",
    "results/channel_selection_results.csv         - All selection methods\n",
    "results/channel_selection_summary.csv         - Summary by method\n",
    "results/training_histories.pkl                - Training curves\n",
    "plots/training_curves.png                     - Training visualization\n",
    "plots/model_comparison.png                    - Metrics comparison\n",
    "plots/channel_selection_curves.png            - Retention curves\n",
    "models/eegarnn_*.pt                           - Model checkpoints\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import mne\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix\n",
    ")\n",
    "import pickle\n",
    "from copy import deepcopy\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "mne.set_log_level('ERROR')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - COMPLETE TRAINING WITHOUT EARLY STOPPING\n",
    "CONFIG = {\n",
    "    'data_path': '/kaggle/input/eeg-preprocessed-data/derived',\n",
    "    'models_dir': './models',\n",
    "    'results_dir': './results',\n",
    "    'plots_dir': './plots',\n",
    "    \n",
    "    'n_folds': 2,\n",
    "    'random_seed': 42,\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    \n",
    "    # Training hyperparameters - NO EARLY STOPPING\n",
    "    'batch_size': 64,\n",
    "    'epochs': 30,  # Increased for full training\n",
    "    'learning_rate': 0.0015,\n",
    "    'weight_decay': 1e-4,\n",
    "    'scheduler_patience': 3,\n",
    "    'scheduler_factor': 0.5,\n",
    "    'use_early_stopping': False,  # DISABLED - train for full epochs\n",
    "    'min_lr': 1e-6,\n",
    "    \n",
    "    # Data parameters (matching Pipeline 1)\n",
    "    'n_channels': 64,\n",
    "    'n_classes': 2,\n",
    "    'sfreq': 128,\n",
    "    'tmin': 0.0,\n",
    "    'tmax': 4.0,\n",
    "    'n_timepoints': 513,\n",
    "    'hidden_dim': 64,\n",
    "    'mi_runs': [7, 8, 11, 12],\n",
    "    \n",
    "    # Gating parameters\n",
    "    'gating': {\n",
    "        'gate_init': 0.9,\n",
    "        'l1_lambda': 1e-3,\n",
    "    },\n",
    "    \n",
    "    # Channel selection k-values (STANDARDIZED - matching Pipeline 1)\n",
    "    'k_values': [10, 15, 20, 25, 30],\n",
    "}\n",
    "\n",
    "os.makedirs(CONFIG['models_dir'], exist_ok=True)\n",
    "os.makedirs(CONFIG['results_dir'], exist_ok=True)\n",
    "os.makedirs(CONFIG['plots_dir'], exist_ok=True)\n",
    "\n",
    "np.random.seed(CONFIG['random_seed'])\n",
    "torch.manual_seed(CONFIG['random_seed'])\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(CONFIG['random_seed'])\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "print(f\"Device: {CONFIG['device']}\")\n",
    "print(f\"Epochs: {CONFIG['epochs']} (NO EARLY STOPPING - FULL TRAINING)\")\n",
    "print(f\"Folds: {CONFIG['n_folds']}\")\n",
    "print(f\"Learning rate: {CONFIG['learning_rate']}\")\n",
    "print(f\"K-values: {CONFIG['k_values']}\")\n",
    "\n",
    "# Runtime estimates\n",
    "initial_runs = 2 * CONFIG['n_folds']  # 2 models × 2 folds\n",
    "# Baseline: 2 methods, Adaptive: 3 methods\n",
    "cs_runs = (2 + 3) * len(CONFIG['k_values']) * CONFIG['n_folds']  # 5 methods × 5 k × 2 folds = 50\n",
    "total_runs = initial_runs + cs_runs\n",
    "\n",
    "print(f\"\\nEstimated training runs:\")\n",
    "print(f\"  Initial: {initial_runs} runs × 30 epochs\")\n",
    "print(f\"  Channel selection: {cs_runs} runs × 30 epochs\")\n",
    "print(f\"  TOTAL: {total_runs} runs\")\n",
    "print(f\"\\nEstimated runtime (~7 min/run): {total_runs * 7 / 60:.1f} hours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading (EXACT MATCH with Pipeline 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_physionet_data(data_path):\n",
    "    \"\"\"Load preprocessed PhysioNet data - MATCHING PIPELINE 1.\"\"\"\n",
    "    data_root = os.path.abspath(data_path)\n",
    "    if not os.path.isdir(data_root):\n",
    "        raise FileNotFoundError(f\"Data path not found: {data_root}\")\n",
    "\n",
    "    tmin, tmax = CONFIG['tmin'], CONFIG['tmax']\n",
    "    mi_runs = CONFIG['mi_runs']\n",
    "    event_id = {'T1': 1, 'T2': 2}\n",
    "    label_map = {1: 0, 2: 1}\n",
    "\n",
    "    # Check for preprocessed subdirectory\n",
    "    preprocessed_dir = os.path.join(data_root, 'preprocessed')\n",
    "    if os.path.isdir(preprocessed_dir):\n",
    "        data_root = preprocessed_dir\n",
    "        print(f\"Using preprocessed data from: {data_root}\")\n",
    "    else:\n",
    "        print(f\"Using data from: {data_root}\")\n",
    "    \n",
    "    subject_dirs = [d for d in sorted(os.listdir(data_root))\n",
    "                    if os.path.isdir(os.path.join(data_root, d)) and d.upper().startswith('S')]\n",
    "\n",
    "    all_X, all_y, all_subjects = [], [], []\n",
    "    print(f\"\\nLoading data from {len(subject_dirs)} subjects...\")\n",
    "    \n",
    "    for subject_dir in subject_dirs:\n",
    "        subject_num = int(subject_dir[1:]) if len(subject_dir) > 1 else -1\n",
    "        subject_path = os.path.join(data_root, subject_dir)\n",
    "        \n",
    "        for run_id in mi_runs:\n",
    "            run_file = f\"{subject_dir}R{run_id:02d}_preproc_raw.fif\"\n",
    "            run_path = os.path.join(subject_path, run_file)\n",
    "            \n",
    "            if not os.path.exists(run_path):\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                # Load preprocessed data\n",
    "                raw = mne.io.read_raw_fif(run_path, preload=True, verbose=False)\n",
    "                \n",
    "                # Pick EEG channels only\n",
    "                picks = mne.pick_types(raw.info, eeg=True, meg=False, stim=False, eog=False)\n",
    "                if len(picks) == 0:\n",
    "                    continue\n",
    "                \n",
    "                # Extract events\n",
    "                events, _ = mne.events_from_annotations(raw, event_id=event_id, verbose=False)\n",
    "                if len(events) == 0:\n",
    "                    continue\n",
    "                \n",
    "                # Create epochs\n",
    "                epochs = mne.Epochs(\n",
    "                    raw, events, event_id=event_id, \n",
    "                    tmin=tmin, tmax=tmax,\n",
    "                    baseline=None,  # No baseline - data already preprocessed\n",
    "                    preload=True, \n",
    "                    picks=picks, \n",
    "                    verbose=False\n",
    "                )\n",
    "                \n",
    "                # Get data and labels\n",
    "                data = epochs.get_data()  # (n_epochs, n_channels, n_times)\n",
    "                labels = np.array([label_map.get(epochs.events[i, 2], -1) for i in range(len(epochs))])\n",
    "                valid = labels >= 0\n",
    "                \n",
    "                if np.any(valid):\n",
    "                    all_X.append(data[valid])\n",
    "                    all_y.append(labels[valid])\n",
    "                    all_subjects.append(np.full(np.sum(valid), subject_num))\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"  Warning: Failed to load {run_file}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    if len(all_X) == 0:\n",
    "        raise ValueError(\"No data loaded! Check data path and file format.\")\n",
    "    \n",
    "    X = np.concatenate(all_X, axis=0)\n",
    "    y = np.concatenate(all_y, axis=0)\n",
    "    subjects = np.concatenate(all_subjects, axis=0)\n",
    "    \n",
    "    print(f\"\\nData loaded successfully:\")\n",
    "    print(f\"  Total trials: {len(X)}\")\n",
    "    print(f\"  Unique subjects: {len(np.unique(subjects))}\")\n",
    "    print(f\"  Data shape: {X.shape} (trials, channels, timepoints)\")\n",
    "    print(f\"  Label distribution: {dict(zip(*np.unique(y, return_counts=True)))}\")\n",
    "    print(f\"  Class balance: {np.bincount(y)[0]}/{np.bincount(y)[1]} (class 0/class 1)\")\n",
    "    \n",
    "    return X, y, subjects\n",
    "\n",
    "\n",
    "class EEGDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for EEG data.\"\"\"\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph Convolution Layer\n",
    "class GraphConvLayer(nn.Module):\n",
    "    \"\"\"Graph Convolution Layer with learned adjacency matrix.\"\"\"\n",
    "    def __init__(self, num_channels, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.num_channels = num_channels\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Learnable adjacency matrix\n",
    "        self.A = nn.Parameter(torch.randn(num_channels, num_channels) * 0.01)\n",
    "        self.theta = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(hidden_dim)\n",
    "        self.act = nn.ELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass with graph convolution.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (B, H, C, T)\n",
    "        \n",
    "        Returns:\n",
    "            Output tensor of shape (B, H, C, T)\n",
    "        \"\"\"\n",
    "        B, H, C, T = x.shape\n",
    "        \n",
    "        # Normalize adjacency matrix\n",
    "        A = torch.sigmoid(self.A)\n",
    "        A = 0.5 * (A + A.t())  # Symmetrize\n",
    "        I = torch.eye(C, device=A.device)\n",
    "        A_hat = A + I\n",
    "        D = torch.diag(torch.pow(A_hat.sum(1).clamp_min(1e-6), -0.5))\n",
    "        A_norm = D @ A_hat @ D\n",
    "\n",
    "        # Apply graph convolution\n",
    "        x_perm = x.permute(0, 3, 2, 1).contiguous().view(B * T, C, H)\n",
    "        x_g = A_norm @ x_perm\n",
    "        x_g = self.theta(x_g)\n",
    "        x_g = x_g.view(B, T, C, H).permute(0, 3, 2, 1)\n",
    "        \n",
    "        return self.act(self.bn(x_g))\n",
    "\n",
    "    def get_adjacency(self):\n",
    "        \"\"\"Get the learned adjacency matrix.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            A = torch.sigmoid(self.A)\n",
    "            A = 0.5 * (A + A.t())\n",
    "            return A.cpu().numpy()\n",
    "\n",
    "\n",
    "# Temporal Convolution\n",
    "class TemporalConv(nn.Module):\n",
    "    \"\"\"Temporal Convolution Layer.\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=16, pool=True):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels, out_channels, \n",
    "            kernel_size=(1, kernel_size),\n",
    "            padding=(0, kernel_size // 2), \n",
    "            bias=False\n",
    "        )\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.act = nn.ELU()\n",
    "        self.pool_layer = nn.AvgPool2d(kernel_size=(1, 2)) if pool else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.act(self.bn(self.conv(x)))\n",
    "        if self.pool_layer is not None:\n",
    "            x = self.pool_layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "print(\"Basic layers defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline EEG-ARNN (without gating)\n",
    "class BaselineEEGARNN(nn.Module):\n",
    "    \"\"\"Baseline EEG-ARNN model without gating mechanism.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_channels=64, n_classes=2, n_timepoints=513, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.use_gate_regularizer = False\n",
    "\n",
    "        # Temporal-Graph layers\n",
    "        self.t1 = TemporalConv(1, hidden_dim, 16, pool=False)\n",
    "        self.g1 = GraphConvLayer(n_channels, hidden_dim)\n",
    "        self.t2 = TemporalConv(hidden_dim, hidden_dim, 16, pool=True)\n",
    "        self.g2 = GraphConvLayer(n_channels, hidden_dim)\n",
    "        self.t3 = TemporalConv(hidden_dim, hidden_dim, 16, pool=True)\n",
    "        self.g3 = GraphConvLayer(n_channels, hidden_dim)\n",
    "\n",
    "        # Calculate feature dimension\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, n_channels, n_timepoints)\n",
    "            feat = self._forward_features(self._prepare_input(dummy))\n",
    "            self.feature_dim = feat.view(1, -1).size(1)\n",
    "\n",
    "        # Classifier\n",
    "        self.fc1 = nn.Linear(self.feature_dim, 256)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(256, n_classes)\n",
    "\n",
    "    def _prepare_input(self, x):\n",
    "        \"\"\"Prepare input by adding channel dimension if needed.\"\"\"\n",
    "        if x.dim() == 3:\n",
    "            x = x.unsqueeze(1)  # (B, C, T) -> (B, 1, C, T)\n",
    "        return x\n",
    "\n",
    "    def _forward_features(self, x):\n",
    "        \"\"\"Extract features through temporal-graph layers.\"\"\"\n",
    "        x = self.g1(self.t1(x))\n",
    "        x = self.g2(self.t2(x))\n",
    "        x = self.g3(self.t3(x))\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass.\"\"\"\n",
    "        prepared = self._prepare_input(x)\n",
    "        features = self._forward_features(prepared)\n",
    "        x = features.view(features.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        return self.fc2(x)\n",
    "\n",
    "    def get_final_adjacency(self):\n",
    "        \"\"\"Get the final layer's adjacency matrix.\"\"\"\n",
    "        return self.g3.get_adjacency()\n",
    "\n",
    "    def get_channel_importance_edge(self):\n",
    "        \"\"\"Get channel importance based on edge connections.\"\"\"\n",
    "        adjacency = self.get_final_adjacency()\n",
    "        return np.sum(np.abs(adjacency), axis=1)\n",
    "\n",
    "\n",
    "print(\"Baseline EEG-ARNN defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adaptive Gating EEG-ARNN\n",
    "class AdaptiveGatingEEGARNN(BaselineEEGARNN):\n",
    "    \"\"\"EEG-ARNN with adaptive data-dependent channel gating.\n",
    "    \n",
    "    This is YOUR CONTRIBUTION - adaptive gating that learns which channels\n",
    "    are important based on the input data itself.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_channels=64, n_classes=2, n_timepoints=513, hidden_dim=128, gate_init=0.9):\n",
    "        super().__init__(n_channels, n_classes, n_timepoints, hidden_dim)\n",
    "        self.use_gate_regularizer = True\n",
    "        \n",
    "        # Adaptive gate network - learns to weight channels based on input statistics\n",
    "        self.gate_net = nn.Sequential(\n",
    "            nn.Linear(n_channels * 2, n_channels),  # 2 = mean + std\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_channels, n_channels),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Initialize gates to start high (most channels active initially)\n",
    "        init_value = float(np.clip(gate_init, 1e-3, 1 - 1e-3))\n",
    "        init_bias = math.log(init_value / (1.0 - init_value))\n",
    "        with torch.no_grad():\n",
    "            self.gate_net[-2].bias.fill_(init_bias)\n",
    "        \n",
    "        self.latest_gate_values = None\n",
    "        self.gate_penalty_tensor = None\n",
    "\n",
    "    def compute_gates(self, x):\n",
    "        \"\"\"Compute data-dependent channel gates.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (B, 1, C, T)\n",
    "            \n",
    "        Returns:\n",
    "            Gate values of shape (B, C) in range [0, 1]\n",
    "        \"\"\"\n",
    "        x_s = x.squeeze(1)  # (B, C, T)\n",
    "        ch_mean = x_s.mean(dim=2)  # (B, C)\n",
    "        ch_std = x_s.std(dim=2)    # (B, C)\n",
    "        stats = torch.cat([ch_mean, ch_std], dim=1)  # (B, C*2)\n",
    "        return self.gate_net(stats)  # (B, C)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass with adaptive gating.\"\"\"\n",
    "        prepared = self._prepare_input(x)\n",
    "        \n",
    "        # Compute gates based on input\n",
    "        gates = self.compute_gates(prepared)\n",
    "        self.gate_penalty_tensor = gates  # For L1 regularization\n",
    "        self.latest_gate_values = gates.detach()  # For channel selection\n",
    "        \n",
    "        # Apply gating\n",
    "        gated = prepared * gates.view(gates.size(0), 1, gates.size(1), 1)\n",
    "        \n",
    "        # Continue with normal forward pass\n",
    "        features = self._forward_features(gated)\n",
    "        x = features.view(features.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        return self.fc2(x)\n",
    "\n",
    "    def get_channel_importance_gate(self):\n",
    "        \"\"\"Get channel importance from gate values.\"\"\"\n",
    "        if self.latest_gate_values is None:\n",
    "            return None\n",
    "        return self.latest_gate_values.mean(dim=0).cpu().numpy()\n",
    "\n",
    "\n",
    "print(\"Adaptive Gating EEG-ARNN defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training and Evaluation Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_comprehensive_metrics(model, dataloader, device):\n",
    "    \"\"\"Calculate ALL metrics - matching Pipeline 1.\"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in dataloader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(y_batch.numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "    \n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_probs = np.array(all_probs)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(all_labels, all_preds),\n",
    "        'precision': precision_score(all_labels, all_preds, average='binary', zero_division=0),\n",
    "        'recall': recall_score(all_labels, all_preds, average='binary', zero_division=0),\n",
    "        'f1_score': f1_score(all_labels, all_preds, average='binary', zero_division=0),\n",
    "        'auc_roc': roc_auc_score(all_labels, all_probs[:, 1]) if len(np.unique(all_labels)) == 2 else 0.0,\n",
    "    }\n",
    "    \n",
    "    # Confusion matrix for specificity/sensitivity\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    if cm.shape == (2, 2):\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        metrics['specificity'] = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
    "        metrics['sensitivity'] = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    else:\n",
    "        metrics['specificity'] = 0.0\n",
    "        metrics['sensitivity'] = 0.0\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device, l1_lambda=0.0):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "    \n",
    "    for X_batch, y_batch in dataloader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        \n",
    "        # Add L1 regularization for gating\n",
    "        gate_penalty = getattr(model, 'gate_penalty_tensor', None)\n",
    "        if l1_lambda > 0 and gate_penalty is not None:\n",
    "            loss = loss + l1_lambda * gate_penalty.abs().mean()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += y_batch.size(0)\n",
    "        correct += (predicted == y_batch).sum().item()\n",
    "    \n",
    "    return total_loss / len(dataloader), correct / total\n",
    "\n",
    "\n",
    "def evaluate_epoch(model, dataloader, criterion, device):\n",
    "    \"\"\"Evaluate for one epoch.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in dataloader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += y_batch.size(0)\n",
    "            correct += (predicted == y_batch).sum().item()\n",
    "    \n",
    "    return total_loss / len(dataloader), correct / total\n",
    "\n",
    "\n",
    "def train_model_full(model, train_loader, val_loader, config, model_name=''):\n",
    "    \"\"\"Train model for FULL epochs without early stopping.\"\"\"\n",
    "    device = config['device']\n",
    "    model = model.to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(), \n",
    "        lr=config['learning_rate'], \n",
    "        weight_decay=config['weight_decay']\n",
    "    )\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, \n",
    "        mode='min', \n",
    "        factor=config['scheduler_factor'], \n",
    "        patience=config['scheduler_patience'], \n",
    "        min_lr=config['min_lr'], \n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    l1_lambda = config['gating']['l1_lambda'] if getattr(model, 'use_gate_regularizer', False) else 0.0\n",
    "    \n",
    "    best_state = deepcopy(model.state_dict())\n",
    "    best_val_acc = 0.0\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "    \n",
    "    print(f\"[{model_name}] Training for {config['epochs']} epochs (NO early stopping)\")\n",
    "    \n",
    "    for epoch in range(config['epochs']):\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device, l1_lambda)\n",
    "        val_loss, val_acc = evaluate_epoch(model, val_loader, criterion, device)\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc or (val_acc == best_val_acc and val_loss < best_val_loss):\n",
    "            best_state = deepcopy(model.state_dict())\n",
    "            best_val_acc = val_acc\n",
    "            best_val_loss = val_loss\n",
    "        \n",
    "        # Print progress\n",
    "        if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "            print(f\"  Epoch {epoch+1:2d}/{config['epochs']} - \"\n",
    "                  f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f} | \"\n",
    "                  f\"Best: {best_val_acc:.4f}\")\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(best_state)\n",
    "    \n",
    "    return best_state, best_val_acc, history\n",
    "\n",
    "\n",
    "print(\"Training utilities defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Channel Selection Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_channel_importance_aggregation(model, dataloader, device):\n",
    "    \"\"\"Aggregation Selection: based on feature activations.\"\"\"\n",
    "    model.eval()\n",
    "    channel_stats = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, _ in dataloader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            prepared = model._prepare_input(X_batch)\n",
    "            features = model._forward_features(prepared)\n",
    "            # Average over batch, hidden_dim, and time\n",
    "            activations = torch.mean(torch.abs(features), dim=(1, 3))  # (B, C)\n",
    "            channel_stats.append(activations.cpu())\n",
    "\n",
    "    if not channel_stats:\n",
    "        return np.zeros(model.n_channels)\n",
    "    \n",
    "    stacked = torch.cat(channel_stats, dim=0)  # (total_samples, C)\n",
    "    return stacked.mean(dim=0).numpy()\n",
    "\n",
    "\n",
    "def compute_gate_importance(model, dataloader, device):\n",
    "    \"\"\"Gate Selection: average gate values across dataset.\"\"\"\n",
    "    model.eval()\n",
    "    gate_batches = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, _ in dataloader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            _ = model(X_batch)  # Forward pass to compute gates\n",
    "            latest = getattr(model, 'latest_gate_values', None)\n",
    "            if latest is not None:\n",
    "                gate_batches.append(latest.cpu())\n",
    "\n",
    "    if not gate_batches:\n",
    "        return np.ones(model.n_channels) / model.n_channels\n",
    "    \n",
    "    stacked = torch.cat(gate_batches, dim=0)\n",
    "    return stacked.mean(dim=0).numpy()\n",
    "\n",
    "\n",
    "def select_top_k_channels(importance_scores, k):\n",
    "    \"\"\"Select top k channels based on importance scores.\"\"\"\n",
    "    top_k_indices = np.argsort(importance_scores)[-k:]\n",
    "    return sorted(top_k_indices)\n",
    "\n",
    "\n",
    "def apply_channel_selection(X, selected_channels):\n",
    "    \"\"\"Apply channel selection to data.\"\"\"\n",
    "    return X[:, selected_channels, :]\n",
    "\n",
    "\n",
    "print(\"Channel selection utilities defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"LOADING DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "X, y, subjects = load_physionet_data(CONFIG['data_path'])\n",
    "\n",
    "print(\"\\nData ready for training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train EEG-ARNN Models (FULL TRAINING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup cross-validation\n",
    "skf = StratifiedKFold(n_splits=CONFIG['n_folds'], shuffle=True, random_state=CONFIG['random_seed'])\n",
    "\n",
    "models_to_train = [\n",
    "    {'name': 'Baseline-EEG-ARNN', 'class': BaselineEEGARNN},\n",
    "    {'name': 'Adaptive-Gating-EEG-ARNN', 'class': AdaptiveGatingEEGARNN},\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING EEG-ARNN MODELS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Configuration: {CONFIG['epochs']} epochs, NO early stopping\")\n",
    "print(f\"Cross-validation: {CONFIG['n_folds']} folds\")\n",
    "print(\"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop with comprehensive metrics\n",
    "all_results = {}\n",
    "all_histories = {}\n",
    "\n",
    "for model_info in models_to_train:\n",
    "    model_name = model_info['name']\n",
    "    model_class = model_info['class']\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Training: {model_name}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    fold_results = []\n",
    "    fold_histories = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "        print(f\"\\nFold {fold + 1}/{CONFIG['n_folds']}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "        \n",
    "        print(f\"Train: {len(X_train)} samples, Val: {len(X_val)} samples\")\n",
    "        \n",
    "        # Create dataloaders\n",
    "        train_dataset = EEGDataset(X_train, y_train)\n",
    "        val_dataset = EEGDataset(X_val, y_val)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'], shuffle=False)\n",
    "        \n",
    "        # Build model\n",
    "        if model_class == AdaptiveGatingEEGARNN:\n",
    "            model = model_class(\n",
    "                n_channels=CONFIG['n_channels'],\n",
    "                n_classes=CONFIG['n_classes'],\n",
    "                n_timepoints=CONFIG['n_timepoints'],\n",
    "                hidden_dim=CONFIG['hidden_dim'],\n",
    "                gate_init=CONFIG['gating']['gate_init']\n",
    "            )\n",
    "        else:\n",
    "            model = model_class(\n",
    "                n_channels=CONFIG['n_channels'],\n",
    "                n_classes=CONFIG['n_classes'],\n",
    "                n_timepoints=CONFIG['n_timepoints'],\n",
    "                hidden_dim=CONFIG['hidden_dim']\n",
    "            )\n",
    "        \n",
    "        # Train model\n",
    "        best_state, val_acc, history = train_model_full(\n",
    "            model, train_loader, val_loader, CONFIG, \n",
    "            f\"{model_name}-Fold{fold+1}\"\n",
    "        )\n",
    "        \n",
    "        # Get comprehensive metrics\n",
    "        model.load_state_dict(best_state)\n",
    "        model = model.to(CONFIG['device'])\n",
    "        metrics = calculate_comprehensive_metrics(model, val_loader, CONFIG['device'])\n",
    "        \n",
    "        print(f\"\\nFold {fold + 1} Final Metrics:\")\n",
    "        print(f\"  Accuracy:    {metrics['accuracy']:.4f}\")\n",
    "        print(f\"  Precision:   {metrics['precision']:.4f}\")\n",
    "        print(f\"  Recall:      {metrics['recall']:.4f}\")\n",
    "        print(f\"  F1-Score:    {metrics['f1_score']:.4f}\")\n",
    "        print(f\"  AUC-ROC:     {metrics['auc_roc']:.4f}\")\n",
    "        print(f\"  Specificity: {metrics['specificity']:.4f}\")\n",
    "        print(f\"  Sensitivity: {metrics['sensitivity']:.4f}\")\n",
    "        \n",
    "        # Save model\n",
    "        model_path = os.path.join(CONFIG['models_dir'], f\"eegarnn_{model_name}_fold{fold+1}.pt\")\n",
    "        torch.save(best_state, model_path)\n",
    "        print(f\"\\nModel saved: {model_path}\")\n",
    "        \n",
    "        # Store results\n",
    "        fold_results.append({\n",
    "            'fold': fold + 1,\n",
    "            'accuracy': metrics['accuracy'],\n",
    "            'precision': metrics['precision'],\n",
    "            'recall': metrics['recall'],\n",
    "            'f1_score': metrics['f1_score'],\n",
    "            'auc_roc': metrics['auc_roc'],\n",
    "            'specificity': metrics['specificity'],\n",
    "            'sensitivity': metrics['sensitivity']\n",
    "        })\n",
    "        fold_histories.append(history)\n",
    "        \n",
    "        # Cleanup\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    # Store results\n",
    "    all_results[model_name] = fold_results\n",
    "    all_histories[model_name] = fold_histories\n",
    "    \n",
    "    # Print model summary\n",
    "    df_temp = pd.DataFrame(fold_results)\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"{model_name} - Summary Across Folds\")\n",
    "    print(\"=\"*80)\n",
    "    for metric in ['accuracy', 'precision', 'recall', 'f1_score', 'auc_roc', 'specificity', 'sensitivity']:\n",
    "        mean_val = df_temp[metric].mean()\n",
    "        std_val = df_temp[metric].std()\n",
    "        print(f\"  {metric:15s}: {mean_val:.4f} ± {std_val:.4f}\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"INITIAL TRAINING COMPLETE!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Initial Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save detailed results (per-fold)\n",
    "for model_name, fold_results in all_results.items():\n",
    "    df = pd.DataFrame(fold_results)\n",
    "    df['model'] = model_name\n",
    "    \n",
    "    # Reorder columns\n",
    "    cols = ['model', 'fold', 'accuracy', 'precision', 'recall', 'f1_score', \n",
    "            'auc_roc', 'specificity', 'sensitivity']\n",
    "    df = df[cols]\n",
    "    \n",
    "    filename = model_name.lower().replace('-', '_').replace(' ', '_')\n",
    "    filepath = os.path.join(CONFIG['results_dir'], f'eegarnn_{filename}_results.csv')\n",
    "    df.to_csv(filepath, index=False)\n",
    "    print(f\"Saved: {filepath}\")\n",
    "\n",
    "# Save summary statistics\n",
    "summary_data = []\n",
    "for model_name, fold_results in all_results.items():\n",
    "    df_temp = pd.DataFrame(fold_results)\n",
    "    summary = {'model': model_name}\n",
    "    \n",
    "    for metric in ['accuracy', 'precision', 'recall', 'f1_score', 'auc_roc', 'specificity', 'sensitivity']:\n",
    "        summary[f'mean_{metric}'] = df_temp[metric].mean()\n",
    "        summary[f'std_{metric}'] = df_temp[metric].std()\n",
    "    \n",
    "    summary_data.append(summary)\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "summary_df = summary_df.sort_values('mean_accuracy', ascending=False).reset_index(drop=True)\n",
    "summary_df['rank'] = range(1, len(summary_df) + 1)\n",
    "\n",
    "# Reorder columns\n",
    "cols = ['rank', 'model'] + [col for col in summary_df.columns if col not in ['rank', 'model']]\n",
    "summary_df = summary_df[cols]\n",
    "\n",
    "filepath = os.path.join(CONFIG['results_dir'], 'eegarnn_initial_summary.csv')\n",
    "summary_df.to_csv(filepath, index=False)\n",
    "print(f\"Saved: {filepath}\")\n",
    "\n",
    "# Save training histories\n",
    "filepath = os.path.join(CONFIG['results_dir'], 'training_histories.pkl')\n",
    "with open(filepath, 'wb') as f:\n",
    "    pickle.dump(all_histories, f)\n",
    "print(f\"Saved: {filepath}\")\n",
    "\n",
    "print(\"\\nInitial Results Summary:\")\n",
    "print(summary_df[['rank', 'model', 'mean_accuracy', 'mean_f1_score', 'mean_auc_roc']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Channel Selection Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Channel selection configuration\n",
    "cs_experiments = [\n",
    "    {'model': 'Baseline-EEG-ARNN', 'methods': ['edge', 'aggregation']},\n",
    "    {'model': 'Adaptive-Gating-EEG-ARNN', 'methods': ['edge', 'aggregation', 'gate']},\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CHANNEL SELECTION EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"k-values: {CONFIG['k_values']}\")\n",
    "print(f\"Baseline: 2 methods × {len(CONFIG['k_values'])} k-values = {2 * len(CONFIG['k_values'])} experiments\")\n",
    "print(f\"Adaptive: 3 methods × {len(CONFIG['k_values'])} k-values = {3 * len(CONFIG['k_values'])} experiments\")\n",
    "print(f\"Total: {(2 + 3) * len(CONFIG['k_values'])} experiments × {CONFIG['n_folds']} folds = {(2 + 3) * len(CONFIG['k_values']) * CONFIG['n_folds']} runs\")\n",
    "print(\"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run channel selection experiments\n",
    "channel_selection_results = []\n",
    "\n",
    "for exp in cs_experiments:\n",
    "    model_name = exp['model']\n",
    "    methods = exp['methods']\n",
    "    model_class = BaselineEEGARNN if 'Baseline' in model_name else AdaptiveGatingEEGARNN\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Methods: {', '.join([m.upper() for m in methods])}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for method in methods:\n",
    "        print(f\"\\n{'-'*80}\")\n",
    "        print(f\"Method: {method.upper()}\")\n",
    "        print(\"-\"*80)\n",
    "        \n",
    "        for k in CONFIG['k_values']:\n",
    "            print(f\"\\n  k={k} channels:\", end=' ')\n",
    "            fold_accuracies = []\n",
    "            fold_metrics_list = []\n",
    "            \n",
    "            for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "                X_train, X_val = X[train_idx], X[val_idx]\n",
    "                y_train, y_val = y[train_idx], y[val_idx]\n",
    "                \n",
    "                # Load trained model for channel importance\n",
    "                if model_class == AdaptiveGatingEEGARNN:\n",
    "                    model = model_class(\n",
    "                        n_channels=CONFIG['n_channels'],\n",
    "                        n_classes=CONFIG['n_classes'],\n",
    "                        n_timepoints=CONFIG['n_timepoints'],\n",
    "                        hidden_dim=CONFIG['hidden_dim'],\n",
    "                        gate_init=CONFIG['gating']['gate_init']\n",
    "                    )\n",
    "                else:\n",
    "                    model = model_class(\n",
    "                        n_channels=CONFIG['n_channels'],\n",
    "                        n_classes=CONFIG['n_classes'],\n",
    "                        n_timepoints=CONFIG['n_timepoints'],\n",
    "                        hidden_dim=CONFIG['hidden_dim']\n",
    "                    )\n",
    "                \n",
    "                model_path = os.path.join(CONFIG['models_dir'], f\"eegarnn_{model_name}_fold{fold+1}.pt\")\n",
    "                state_dict = torch.load(model_path, map_location=CONFIG['device'])\n",
    "                model.load_state_dict(state_dict)\n",
    "                model = model.to(CONFIG['device'])\n",
    "                model.eval()\n",
    "                \n",
    "                # Compute channel importance\n",
    "                if method == 'edge':\n",
    "                    importance_scores = model.get_channel_importance_edge()\n",
    "                elif method == 'aggregation':\n",
    "                    train_dataset = EEGDataset(X_train, y_train)\n",
    "                    train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=False)\n",
    "                    importance_scores = get_channel_importance_aggregation(model, train_loader, CONFIG['device'])\n",
    "                else:  # gate\n",
    "                    train_dataset = EEGDataset(X_train, y_train)\n",
    "                    train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=False)\n",
    "                    importance_scores = compute_gate_importance(model, train_loader, CONFIG['device'])\n",
    "                \n",
    "                # Select channels\n",
    "                selected_channels = select_top_k_channels(importance_scores, k)\n",
    "                X_train_selected = apply_channel_selection(X_train, selected_channels)\n",
    "                X_val_selected = apply_channel_selection(X_val, selected_channels)\n",
    "                \n",
    "                # Train new model with selected channels\n",
    "                if model_class == AdaptiveGatingEEGARNN:\n",
    "                    new_model = model_class(\n",
    "                        n_channels=k,\n",
    "                        n_classes=CONFIG['n_classes'],\n",
    "                        n_timepoints=CONFIG['n_timepoints'],\n",
    "                        hidden_dim=CONFIG['hidden_dim'],\n",
    "                        gate_init=CONFIG['gating']['gate_init']\n",
    "                    )\n",
    "                else:\n",
    "                    new_model = model_class(\n",
    "                        n_channels=k,\n",
    "                        n_classes=CONFIG['n_classes'],\n",
    "                        n_timepoints=CONFIG['n_timepoints'],\n",
    "                        hidden_dim=CONFIG['hidden_dim']\n",
    "                    )\n",
    "                \n",
    "                train_dataset = EEGDataset(X_train_selected, y_train)\n",
    "                val_dataset = EEGDataset(X_val_selected, y_val)\n",
    "                train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True)\n",
    "                val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'], shuffle=False)\n",
    "                \n",
    "                best_state, val_acc, _ = train_model_full(\n",
    "                    new_model, train_loader, val_loader, CONFIG, \n",
    "                    f\"{model_name}-{method}-k{k}-F{fold+1}\"\n",
    "                )\n",
    "                \n",
    "                # Get comprehensive metrics\n",
    "                new_model.load_state_dict(best_state)\n",
    "                new_model = new_model.to(CONFIG['device'])\n",
    "                metrics = calculate_comprehensive_metrics(new_model, val_loader, CONFIG['device'])\n",
    "                \n",
    "                fold_accuracies.append(metrics['accuracy'])\n",
    "                fold_metrics_list.append(metrics)\n",
    "                \n",
    "                del model, new_model\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "            \n",
    "            # Compute mean metrics\n",
    "            mean_metrics = {}\n",
    "            for metric_name in ['accuracy', 'precision', 'recall', 'f1_score', 'auc_roc', 'specificity', 'sensitivity']:\n",
    "                values = [m[metric_name] for m in fold_metrics_list]\n",
    "                mean_metrics[f'mean_{metric_name}'] = np.mean(values)\n",
    "                mean_metrics[f'std_{metric_name}'] = np.std(values)\n",
    "            \n",
    "            print(f\"{mean_metrics['mean_accuracy']:.4f} ± {mean_metrics['std_accuracy']:.4f}\")\n",
    "            \n",
    "            # Store results\n",
    "            result = {\n",
    "                'model': model_name,\n",
    "                'method': method,\n",
    "                'k': k,\n",
    "            }\n",
    "            result.update(mean_metrics)\n",
    "            channel_selection_results.append(result)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"CHANNEL SELECTION COMPLETE!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Channel Selection Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save detailed results\n",
    "cs_df = pd.DataFrame(channel_selection_results)\n",
    "\n",
    "# Reorder columns\n",
    "cols = ['model', 'method', 'k'] + [col for col in cs_df.columns if col not in ['model', 'method', 'k']]\n",
    "cs_df = cs_df[cols]\n",
    "\n",
    "filepath = os.path.join(CONFIG['results_dir'], 'channel_selection_results.csv')\n",
    "cs_df.to_csv(filepath, index=False)\n",
    "print(f\"Saved: {filepath}\")\n",
    "\n",
    "# Create summary by method\n",
    "print(\"\\nChannel Selection Results:\")\n",
    "print(cs_df[['model', 'method', 'k', 'mean_accuracy', 'std_accuracy', 'mean_f1_score']].to_string(index=False))\n",
    "\n",
    "# Best results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BEST CHANNEL SELECTION RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for model_name in ['Baseline-EEG-ARNN', 'Adaptive-Gating-EEG-ARNN']:\n",
    "    model_data = cs_df[cs_df['model'] == model_name]\n",
    "    best_row = model_data.loc[model_data['mean_accuracy'].idxmax()]\n",
    "    \n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"  Method: {best_row['method'].upper()}\")\n",
    "    print(f\"  k: {int(best_row['k'])}\")\n",
    "    print(f\"  Accuracy: {best_row['mean_accuracy']:.4f} ± {best_row['std_accuracy']:.4f}\")\n",
    "    print(f\"  F1-Score: {best_row['mean_f1_score']:.4f} ± {best_row['std_f1_score']:.4f}\")\n",
    "    print(f\"  AUC-ROC: {best_row['mean_auc_roc']:.4f} ± {best_row['std_auc_roc']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CREATING VISUALIZATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. Training Curves\n",
    "print(\"\\n1. Training curves...\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Training History - Baseline vs Adaptive Gating', fontsize=16, fontweight='bold')\n",
    "\n",
    "colors = {'Baseline-EEG-ARNN': 'blue', 'Adaptive-Gating-EEG-ARNN': 'red'}\n",
    "\n",
    "for model_name, histories in all_histories.items():\n",
    "    color = colors[model_name]\n",
    "    \n",
    "    # Average across folds\n",
    "    train_loss = np.mean([h['train_loss'] for h in histories], axis=0)\n",
    "    val_loss = np.mean([h['val_loss'] for h in histories], axis=0)\n",
    "    train_acc = np.mean([h['train_acc'] for h in histories], axis=0)\n",
    "    val_acc = np.mean([h['val_acc'] for h in histories], axis=0)\n",
    "    \n",
    "    epochs = range(1, len(train_loss) + 1)\n",
    "    \n",
    "    # Loss plots\n",
    "    axes[0, 0].plot(epochs, train_loss, f'{color}--', alpha=0.7, linewidth=2, label=f'{model_name} (train)')\n",
    "    axes[0, 0].plot(epochs, val_loss, f'{color}-', linewidth=2, label=f'{model_name} (val)')\n",
    "    \n",
    "    # Accuracy plots\n",
    "    axes[0, 1].plot(epochs, train_acc, f'{color}--', alpha=0.7, linewidth=2, label=f'{model_name} (train)')\n",
    "    axes[0, 1].plot(epochs, val_acc, f'{color}-', linewidth=2, label=f'{model_name} (val)')\n",
    "\n",
    "axes[0, 0].set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Loss', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_title('Training & Validation Loss', fontsize=13, fontweight='bold')\n",
    "axes[0, 0].legend(fontsize=10)\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0, 1].set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Accuracy', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_title('Training & Validation Accuracy', fontsize=13, fontweight='bold')\n",
    "axes[0, 1].legend(fontsize=10)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Metrics comparison\n",
    "comparison_data = []\n",
    "for model_name, fold_results in all_results.items():\n",
    "    df_temp = pd.DataFrame(fold_results)\n",
    "    comparison_data.append({\n",
    "        'Model': 'Baseline' if 'Baseline' in model_name else 'Adaptive',\n",
    "        'Accuracy': df_temp['accuracy'].mean(),\n",
    "        'Precision': df_temp['precision'].mean(),\n",
    "        'Recall': df_temp['recall'].mean(),\n",
    "        'F1-Score': df_temp['f1_score'].mean(),\n",
    "        'AUC-ROC': df_temp['auc_roc'].mean()\n",
    "    })\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "x = np.arange(len(df_comparison))\n",
    "width = 0.15\n",
    "\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC-ROC']\n",
    "colors_bar = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    offset = width * (i - 2)\n",
    "    axes[1, 0].bar(x + offset, df_comparison[metric], width, label=metric, color=colors_bar[i], alpha=0.8)\n",
    "\n",
    "axes[1, 0].set_xlabel('Model', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Score', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_title('Model Performance Comparison', fontsize=13, fontweight='bold')\n",
    "axes[1, 0].set_xticks(x)\n",
    "axes[1, 0].set_xticklabels(df_comparison['Model'])\n",
    "axes[1, 0].legend(fontsize=9)\n",
    "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "axes[1, 0].set_ylim([0, 1.0])\n",
    "\n",
    "# Improvement over baseline\n",
    "if len(df_comparison) == 2:\n",
    "    improvements = []\n",
    "    for metric in metrics:\n",
    "        baseline_val = df_comparison.loc[df_comparison['Model'] == 'Baseline', metric].values[0]\n",
    "        adaptive_val = df_comparison.loc[df_comparison['Model'] == 'Adaptive', metric].values[0]\n",
    "        improvement = ((adaptive_val - baseline_val) / baseline_val) * 100\n",
    "        improvements.append(improvement)\n",
    "    \n",
    "    bars = axes[1, 1].barh(metrics, improvements, color=['green' if x > 0 else 'red' for x in improvements], alpha=0.7)\n",
    "    axes[1, 1].axvline(0, color='black', linewidth=0.8)\n",
    "    axes[1, 1].set_xlabel('Improvement (%)', fontsize=12, fontweight='bold')\n",
    "    axes[1, 1].set_title('Adaptive Gating Improvement over Baseline', fontsize=13, fontweight='bold')\n",
    "    axes[1, 1].grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, improvement in zip(bars, improvements):\n",
    "        width = bar.get_width()\n",
    "        axes[1, 1].text(width, bar.get_y() + bar.get_height()/2, \n",
    "                       f'{improvement:+.1f}%', \n",
    "                       ha='left' if width > 0 else 'right',\n",
    "                       va='center', fontsize=10, fontweight='bold')\n",
    "else:\n",
    "    axes[1, 1].text(0.5, 0.5, 'Improvement plot\\nrequires 2 models', \n",
    "                   ha='center', va='center', transform=axes[1, 1].transAxes)\n",
    "\n",
    "plt.tight_layout()\n",
    "filepath = os.path.join(CONFIG['plots_dir'], 'training_curves.png')\n",
    "plt.savefig(filepath, dpi=300, bbox_inches='tight')\n",
    "print(f\"Saved: {filepath}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Channel Selection Curves\n",
    "print(\"\\n2. Channel selection curves...\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.suptitle('Channel Selection Performance', fontsize=16, fontweight='bold')\n",
    "\n",
    "colors_methods = {'edge': 'blue', 'aggregation': 'green', 'gate': 'red'}\n",
    "markers_methods = {'edge': 'o', 'aggregation': 's', 'gate': '^'}\n",
    "\n",
    "for idx, model_name in enumerate(['Baseline-EEG-ARNN', 'Adaptive-Gating-EEG-ARNN']):\n",
    "    ax = axes[idx]\n",
    "    model_data = cs_df[cs_df['model'] == model_name]\n",
    "    \n",
    "    methods = model_data['method'].unique()\n",
    "    \n",
    "    for method in sorted(methods):\n",
    "        method_data = model_data[model_data['method'] == method].sort_values('k')\n",
    "        ax.errorbar(\n",
    "            method_data['k'], \n",
    "            method_data['mean_accuracy'], \n",
    "            yerr=method_data['std_accuracy'],\n",
    "            label=method.upper(), \n",
    "            marker=markers_methods.get(method, 'o'),\n",
    "            color=colors_methods.get(method, 'gray'), \n",
    "            capsize=5, \n",
    "            linewidth=2, \n",
    "            markersize=10,\n",
    "            alpha=0.8\n",
    "        )\n",
    "    \n",
    "    # Add full-channel baseline\n",
    "    full_acc = pd.DataFrame(all_results[model_name])['accuracy'].mean()\n",
    "    ax.axhline(full_acc, color='black', linestyle='--', linewidth=2, alpha=0.5, label='Full (64 ch)')\n",
    "    \n",
    "    ax.set_xlabel('Number of Channels (k)', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Accuracy', fontsize=12, fontweight='bold')\n",
    "    title = 'Baseline' if 'Baseline' in model_name else 'Adaptive Gating'\n",
    "    ax.set_title(f'{title}\\nChannel Selection Methods', fontsize=13, fontweight='bold')\n",
    "    ax.legend(fontsize=10, loc='lower right')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim([0.5, 1.0])\n",
    "    ax.set_xticks(CONFIG['k_values'])\n",
    "\n",
    "plt.tight_layout()\n",
    "filepath = os.path.join(CONFIG['plots_dir'], 'channel_selection_curves.png')\n",
    "plt.savefig(filepath, dpi=300, bbox_inches='tight')\n",
    "print(f\"Saved: {filepath}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Retention Analysis\n",
    "print(\"\\n3. Retention analysis...\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.suptitle('Performance Retention with Channel Reduction', fontsize=16, fontweight='bold')\n",
    "\n",
    "for idx, model_name in enumerate(['Baseline-EEG-ARNN', 'Adaptive-Gating-EEG-ARNN']):\n",
    "    ax = axes[idx]\n",
    "    model_data = cs_df[cs_df['model'] == model_name]\n",
    "    \n",
    "    # Get full-channel performance\n",
    "    full_acc = pd.DataFrame(all_results[model_name])['accuracy'].mean()\n",
    "    \n",
    "    # For each method, plot retention percentage\n",
    "    methods = model_data['method'].unique()\n",
    "    \n",
    "    for method in sorted(methods):\n",
    "        method_data = model_data[model_data['method'] == method].sort_values('k')\n",
    "        retention = (method_data['mean_accuracy'] / full_acc) * 100\n",
    "        \n",
    "        ax.plot(\n",
    "            method_data['k'], \n",
    "            retention,\n",
    "            label=method.upper(), \n",
    "            marker=markers_methods.get(method, 'o'),\n",
    "            color=colors_methods.get(method, 'gray'), \n",
    "            linewidth=2, \n",
    "            markersize=10,\n",
    "            alpha=0.8\n",
    "        )\n",
    "    \n",
    "    ax.axhline(100, color='black', linestyle='--', linewidth=2, alpha=0.5, label='100% (baseline)')\n",
    "    ax.axhline(90, color='orange', linestyle=':', linewidth=1.5, alpha=0.5, label='90% threshold')\n",
    "    \n",
    "    ax.set_xlabel('Number of Channels (k)', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Retention (%)', fontsize=12, fontweight='bold')\n",
    "    title = 'Baseline' if 'Baseline' in model_name else 'Adaptive Gating'\n",
    "    ax.set_title(f'{title}\\nPerformance Retention', fontsize=13, fontweight='bold')\n",
    "    ax.legend(fontsize=10, loc='lower left')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim([70, 105])\n",
    "    ax.set_xticks(CONFIG['k_values'])\n",
    "\n",
    "plt.tight_layout()\n",
    "filepath = os.path.join(CONFIG['plots_dir'], 'retention_analysis.png')\n",
    "plt.savefig(filepath, dpi=300, bbox_inches='tight')\n",
    "print(f\"Saved: {filepath}\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nVisualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PIPELINE 2 - COMPLETE SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. Initial Performance\n",
    "print(\"\\n1. INITIAL MODEL PERFORMANCE (Full 64 Channels):\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for model_name, fold_results in all_results.items():\n",
    "    df_temp = pd.DataFrame(fold_results)\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"  Accuracy:    {df_temp['accuracy'].mean():.4f} ± {df_temp['accuracy'].std():.4f}\")\n",
    "    print(f\"  Precision:   {df_temp['precision'].mean():.4f} ± {df_temp['precision'].std():.4f}\")\n",
    "    print(f\"  Recall:      {df_temp['recall'].mean():.4f} ± {df_temp['recall'].std():.4f}\")\n",
    "    print(f\"  F1-Score:    {df_temp['f1_score'].mean():.4f} ± {df_temp['f1_score'].std():.4f}\")\n",
    "    print(f\"  AUC-ROC:     {df_temp['auc_roc'].mean():.4f} ± {df_temp['auc_roc'].std():.4f}\")\n",
    "    print(f\"  Specificity: {df_temp['specificity'].mean():.4f} ± {df_temp['specificity'].std():.4f}\")\n",
    "    print(f\"  Sensitivity: {df_temp['sensitivity'].mean():.4f} ± {df_temp['sensitivity'].std():.4f}\")\n",
    "\n",
    "# 2. Channel Selection Results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"2. CHANNEL SELECTION RESULTS:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for model_name in ['Baseline-EEG-ARNN', 'Adaptive-Gating-EEG-ARNN']:\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    model_data = cs_df[cs_df['model'] == model_name]\n",
    "    \n",
    "    for method in sorted(model_data['method'].unique()):\n",
    "        print(f\"\\n  {method.upper()} Method:\")\n",
    "        method_data = model_data[model_data['method'] == method].sort_values('k')\n",
    "        for _, row in method_data.iterrows():\n",
    "            print(f\"    k={int(row['k']):2d}: {row['mean_accuracy']:.4f} ± {row['std_accuracy']:.4f}\")\n",
    "\n",
    "# 3. Key Findings\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"3. KEY FINDINGS:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Best initial model\n",
    "best_initial = None\n",
    "best_initial_acc = 0\n",
    "for model_name, fold_results in all_results.items():\n",
    "    acc = pd.DataFrame(fold_results)['accuracy'].mean()\n",
    "    if acc > best_initial_acc:\n",
    "        best_initial_acc = acc\n",
    "        best_initial = model_name\n",
    "\n",
    "print(f\"\\na) Best Initial Model:\")\n",
    "print(f\"   {best_initial}: {best_initial_acc:.4f}\")\n",
    "\n",
    "# Best channel selection\n",
    "best_cs = cs_df.loc[cs_df['mean_accuracy'].idxmax()]\n",
    "print(f\"\\nb) Best Channel Selection:\")\n",
    "print(f\"   Model: {best_cs['model']}\")\n",
    "print(f\"   Method: {best_cs['method'].upper()}\")\n",
    "print(f\"   k: {int(best_cs['k'])}\")\n",
    "print(f\"   Accuracy: {best_cs['mean_accuracy']:.4f} ± {best_cs['std_accuracy']:.4f}\")\n",
    "\n",
    "# Retention at k=30\n",
    "print(f\"\\nc) Performance Retention at k=30:\")\n",
    "for model_name in ['Baseline-EEG-ARNN', 'Adaptive-Gating-EEG-ARNN']:\n",
    "    model_data = cs_df[cs_df['model'] == model_name]\n",
    "    k30_data = model_data[model_data['k'] == 30]\n",
    "    \n",
    "    if len(k30_data) > 0:\n",
    "        best_k30 = k30_data.loc[k30_data['mean_accuracy'].idxmax()]\n",
    "        full_acc = pd.DataFrame(all_results[model_name])['accuracy'].mean()\n",
    "        retention = (best_k30['mean_accuracy'] / full_acc) * 100\n",
    "        \n",
    "        print(f\"\\n   {model_name}:\")\n",
    "        print(f\"     Method: {best_k30['method'].upper()}\")\n",
    "        print(f\"     Accuracy: {best_k30['mean_accuracy']:.4f} (vs {full_acc:.4f} full)\")\n",
    "        print(f\"     Retention: {retention:.1f}%\")\n",
    "\n",
    "# Comparison\n",
    "if len(all_results) == 2:\n",
    "    baseline_acc = pd.DataFrame(all_results['Baseline-EEG-ARNN'])['accuracy'].mean()\n",
    "    adaptive_acc = pd.DataFrame(all_results['Adaptive-Gating-EEG-ARNN'])['accuracy'].mean()\n",
    "    improvement = ((adaptive_acc - baseline_acc) / baseline_acc) * 100\n",
    "    \n",
    "    print(f\"\\nd) Adaptive Gating Improvement:\")\n",
    "    print(f\"   Baseline: {baseline_acc:.4f}\")\n",
    "    print(f\"   Adaptive: {adaptive_acc:.4f}\")\n",
    "    print(f\"   Improvement: {improvement:+.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PIPELINE 2 COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nAll results saved to:\")\n",
    "print(\"  results/eegarnn_*_results.csv          - Detailed per-fold results\")\n",
    "print(\"  results/eegarnn_initial_summary.csv    - Initial training summary\")\n",
    "print(\"  results/channel_selection_results.csv  - Channel selection results\")\n",
    "print(\"  results/training_histories.pkl         - Training curves data\")\n",
    "\n",
    "print(\"\\nAll visualizations saved to:\")\n",
    "print(\"  plots/training_curves.png              - Training history\")\n",
    "print(\"  plots/channel_selection_curves.png     - Channel selection performance\")\n",
    "print(\"  plots/retention_analysis.png           - Performance retention\")\n",
    "\n",
    "print(\"\\nAll models saved to:\")\n",
    "print(\"  models/eegarnn_*.pt                    - Trained model checkpoints\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Ready for comparison with Pipeline 1!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
