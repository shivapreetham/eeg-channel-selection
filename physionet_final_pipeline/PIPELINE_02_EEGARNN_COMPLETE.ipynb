{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline 2: EEG-ARNN Methods with Channel Selection\n",
    "\n",
    "This notebook implements the complete evaluation of EEG-ARNN methods with adaptive gating.\n",
    "\n",
    "## Models\n",
    "1. **Baseline-EEG-ARNN** - Without adaptive gating\n",
    "2. **Adaptive-Gating-EEG-ARNN** - With adaptive data-dependent channel gating (YOUR CONTRIBUTION!)\n",
    "\n",
    "## Experiments\n",
    "### Part 1: Train Both Models\n",
    "- 2-fold cross-validation\n",
    "- 20 epochs with early stopping\n",
    "\n",
    "### Part 2: Channel Selection (3 methods × 5 k-values)\n",
    "1. **Edge Selection (ES)** - Graph adjacency-based\n",
    "2. **Aggregation Selection (AS)** - Feature activation-based  \n",
    "3. **Gate Selection (GS)** - Adaptive gating-based (Adaptive-Gating only)\n",
    "\n",
    "k-values tested: [10, 20, 30, 40, 50]\n",
    "\n",
    "### Part 3: Retention Analysis\n",
    "- Tests performance degradation with channel reduction\n",
    "- Uses Gate Selection (best method)\n",
    "- k-values: [10, 15, 20, 25, 30, 35]\n",
    "\n",
    "## Configuration\n",
    "- **Epochs:** 20 (optimized for speed)\n",
    "- **Cross-validation:** 2-fold (faster than 3-fold)\n",
    "- **Learning rate:** 0.002\n",
    "- **Batch size:** 64\n",
    "\n",
    "## Expected Runtime: ~12-13 hours on Kaggle GPU\n",
    "- Initial training: ~40 min (2 models × 2 folds)\n",
    "- Channel selection: ~10 hours (60 experiments)\n",
    "- Retention: ~2 hours (12 experiments)\n",
    "\n",
    "## Outputs\n",
    "```\n",
    "results/eegarnn_baseline_results.csv        - Baseline-EEG-ARNN (2 folds)\n",
    "results/eegarnn_adaptive_results.csv        - Adaptive-Gating-EEG-ARNN (2 folds)\n",
    "results/channel_selection_results.csv       - All selection methods\n",
    "results/retention_analysis.csv              - Retention curve (Gate-based)\n",
    "results/eegarnn_complete_summary.csv        - Final summary\n",
    "models/eegarnn_*.pt                         - Model checkpoints\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import mne\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from copy import deepcopy\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "mne.set_log_level('ERROR')\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - OPTIMIZED FOR SPEED\n",
    "CONFIG = {\n",
    "    'data_path': '/kaggle/input/eeg-preprocessed-data/derived',\n",
    "    'models_dir': './models',\n",
    "    'results_dir': './results',\n",
    "    \n",
    "    'n_folds': 2,  # Reduced from 3 for faster runtime\n",
    "    'random_seed': 42,\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    'batch_size': 64,\n",
    "    'epochs': 20,  # Optimized for speed\n",
    "    'learning_rate': 0.002,\n",
    "    'weight_decay': 1e-4,\n",
    "    'patience': 5,\n",
    "    'scheduler_patience': 2,\n",
    "    'scheduler_factor': 0.5,\n",
    "    'use_early_stopping': True,\n",
    "    'min_lr': 1e-6,\n",
    "    \n",
    "    # Data parameters\n",
    "    'n_channels': 64,\n",
    "    'n_classes': 2,\n",
    "    'sfreq': 128,\n",
    "    'tmin': 0.0,\n",
    "    'tmax': 4.0,\n",
    "    'n_timepoints': 513,\n",
    "    'hidden_dim': 128,\n",
    "    'mi_runs': [7, 8, 11, 12],\n",
    "    \n",
    "    # Gating parameters\n",
    "    'gating': {\n",
    "        'gate_init': 0.9,\n",
    "        'l1_lambda': 1e-3,\n",
    "    },\n",
    "    \n",
    "    # Channel selection k-values\n",
    "    'k_values': [10, 20, 30, 40, 50],\n",
    "    'retention_k_values': [10, 15, 20, 25, 30, 35],\n",
    "}\n",
    "\n",
    "os.makedirs(CONFIG['models_dir'], exist_ok=True)\n",
    "os.makedirs(CONFIG['results_dir'], exist_ok=True)\n",
    "\n",
    "np.random.seed(CONFIG['random_seed'])\n",
    "torch.manual_seed(CONFIG['random_seed'])\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(CONFIG['random_seed'])\n",
    "\n",
    "print(f\"Device: {CONFIG['device']}\")\n",
    "print(f\"Epochs: {CONFIG['epochs']}\")\n",
    "print(f\"Folds: {CONFIG['n_folds']}\")\n",
    "print(f\"Learning rate: {CONFIG['learning_rate']}\")\n",
    "\n",
    "# Runtime estimates\n",
    "initial_runs = 2 * CONFIG['n_folds']\n",
    "cs_runs = 2 * 3 * len(CONFIG['k_values']) * CONFIG['n_folds']\n",
    "retention_runs = len(CONFIG['retention_k_values']) * CONFIG['n_folds']\n",
    "total_runs = initial_runs + cs_runs + retention_runs\n",
    "\n",
    "print(f\"\\nEstimated training runs:\")\n",
    "print(f\"  Initial: {initial_runs}\")\n",
    "print(f\"  Channel selection: {cs_runs}\")\n",
    "print(f\"  Retention: {retention_runs}\")\n",
    "print(f\"  TOTAL: {total_runs} runs\")\n",
    "print(f\"\\nEstimated runtime (~10 min/run): {total_runs * 10 / 60:.1f} hours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_physionet_data(data_path):\n",
    "    \"\"\"Load preprocessed PhysioNet data.\"\"\"\n",
    "    data_root = os.path.abspath(data_path)\n",
    "    if not os.path.isdir(data_root):\n",
    "        raise FileNotFoundError(f\"Data path not found: {data_root}\")\n",
    "\n",
    "    tmin, tmax = CONFIG['tmin'], CONFIG['tmax']\n",
    "    mi_runs = CONFIG['mi_runs']\n",
    "    event_id = {'T1': 1, 'T2': 2}\n",
    "    label_map = {1: 0, 2: 1}\n",
    "\n",
    "    preprocessed_dir = os.path.join(data_root, 'preprocessed')\n",
    "    if os.path.isdir(preprocessed_dir):\n",
    "        data_root = preprocessed_dir\n",
    "    \n",
    "    subject_dirs = [d for d in sorted(os.listdir(data_root))\n",
    "                    if os.path.isdir(os.path.join(data_root, d)) and d.upper().startswith('S')]\n",
    "\n",
    "    all_X, all_y, all_subjects = [], [], []\n",
    "    print(f\"Loading data from {len(subject_dirs)} subjects...\")\n",
    "    \n",
    "    for subject_dir in subject_dirs:\n",
    "        subject_num = int(subject_dir[1:]) if len(subject_dir) > 1 else -1\n",
    "        subject_path = os.path.join(data_root, subject_dir)\n",
    "        \n",
    "        for run_id in mi_runs:\n",
    "            run_file = f\"{subject_dir}R{run_id:02d}_preproc_raw.fif\"\n",
    "            run_path = os.path.join(subject_path, run_file)\n",
    "            \n",
    "            if not os.path.exists(run_path):\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                raw = mne.io.read_raw_fif(run_path, preload=True, verbose=False)\n",
    "                picks = mne.pick_types(raw.info, eeg=True, meg=False, stim=False, eog=False)\n",
    "                if len(picks) == 0:\n",
    "                    continue\n",
    "                \n",
    "                events, _ = mne.events_from_annotations(raw, event_id=event_id)\n",
    "                if len(events) == 0:\n",
    "                    continue\n",
    "                \n",
    "                epochs = mne.Epochs(raw, events, event_id=event_id, tmin=tmin, tmax=tmax,\n",
    "                                    baseline=None, preload=True, picks=picks, verbose=False)\n",
    "                \n",
    "                data = epochs.get_data()\n",
    "                labels = np.array([label_map.get(epochs.events[i, 2], -1) for i in range(len(epochs))])\n",
    "                valid = labels >= 0\n",
    "                \n",
    "                if np.any(valid):\n",
    "                    all_X.append(data[valid])\n",
    "                    all_y.append(labels[valid])\n",
    "                    all_subjects.append(np.full(np.sum(valid), subject_num))\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    X = np.concatenate(all_X, axis=0)\n",
    "    y = np.concatenate(all_y, axis=0)\n",
    "    subjects = np.concatenate(all_subjects, axis=0)\n",
    "    \n",
    "    print(f\"Loaded {len(X)} trials from {len(np.unique(subjects))} subjects\")\n",
    "    print(f\"Data shape: {X.shape}\")\n",
    "    print(f\"Labels: {np.bincount(y)}\")\n",
    "    \n",
    "    return X, y, subjects\n",
    "\n",
    "\n",
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph Convolution Layer\n",
    "class GraphConvLayer(nn.Module):\n",
    "    def __init__(self, num_channels, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.A = nn.Parameter(torch.randn(num_channels, num_channels) * 0.01)\n",
    "        self.theta = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(hidden_dim)\n",
    "        self.act = nn.ELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, H, C, T = x.shape\n",
    "        A = torch.sigmoid(self.A)\n",
    "        A = 0.5 * (A + A.t())\n",
    "        I = torch.eye(C, device=A.device)\n",
    "        A_hat = A + I\n",
    "        D = torch.diag(torch.pow(A_hat.sum(1).clamp_min(1e-6), -0.5))\n",
    "        A_norm = D @ A_hat @ D\n",
    "\n",
    "        x_perm = x.permute(0, 3, 2, 1).contiguous().view(B * T, C, H)\n",
    "        x_g = A_norm @ x_perm\n",
    "        x_g = self.theta(x_g)\n",
    "        x_g = x_g.view(B, T, C, H).permute(0, 3, 2, 1)\n",
    "        return self.act(self.bn(x_g))\n",
    "\n",
    "    def get_adjacency(self):\n",
    "        with torch.no_grad():\n",
    "            A = torch.sigmoid(self.A)\n",
    "            A = 0.5 * (A + A.t())\n",
    "            return A.cpu().numpy()\n",
    "\n",
    "\n",
    "# Temporal Convolution\n",
    "class TemporalConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=16, pool=True):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=(1, kernel_size),\n",
    "                              padding=(0, kernel_size // 2), bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.act = nn.ELU()\n",
    "        self.pool_layer = nn.AvgPool2d(kernel_size=(1, 2)) if pool else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.act(self.bn(self.conv(x)))\n",
    "        if self.pool_layer is not None:\n",
    "            x = self.pool_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline EEG-ARNN (without gating)\n",
    "class BaselineEEGARNN(nn.Module):\n",
    "    def __init__(self, n_channels=64, n_classes=2, n_timepoints=513, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.use_gate_regularizer = False\n",
    "\n",
    "        self.t1 = TemporalConv(1, hidden_dim, 16, pool=False)\n",
    "        self.g1 = GraphConvLayer(n_channels, hidden_dim)\n",
    "        self.t2 = TemporalConv(hidden_dim, hidden_dim, 16, pool=True)\n",
    "        self.g2 = GraphConvLayer(n_channels, hidden_dim)\n",
    "        self.t3 = TemporalConv(hidden_dim, hidden_dim, 16, pool=True)\n",
    "        self.g3 = GraphConvLayer(n_channels, hidden_dim)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, n_channels, n_timepoints)\n",
    "            feat = self._forward_features(self._prepare_input(dummy))\n",
    "            self.feature_dim = feat.view(1, -1).size(1)\n",
    "\n",
    "        self.fc1 = nn.Linear(self.feature_dim, 256)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(256, n_classes)\n",
    "\n",
    "    def _prepare_input(self, x):\n",
    "        if x.dim() == 3:\n",
    "            x = x.unsqueeze(1)\n",
    "        return x\n",
    "\n",
    "    def _forward_features(self, x):\n",
    "        x = self.g1(self.t1(x))\n",
    "        x = self.g2(self.t2(x))\n",
    "        x = self.g3(self.t3(x))\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        prepared = self._prepare_input(x)\n",
    "        features = self._forward_features(prepared)\n",
    "        x = features.view(features.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        return self.fc2(x)\n",
    "\n",
    "    def get_final_adjacency(self):\n",
    "        return self.g3.get_adjacency()\n",
    "\n",
    "    def get_channel_importance_edge(self):\n",
    "        adjacency = self.get_final_adjacency()\n",
    "        return np.sum(adjacency, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adaptive Gating EEG-ARNN (YOUR CONTRIBUTION!)\n",
    "class AdaptiveGatingEEGARNN(BaselineEEGARNN):\n",
    "    def __init__(self, n_channels=64, n_classes=2, n_timepoints=513, hidden_dim=128, gate_init=0.9):\n",
    "        super().__init__(n_channels, n_classes, n_timepoints, hidden_dim)\n",
    "        self.use_gate_regularizer = True\n",
    "        \n",
    "        # Adaptive gate network\n",
    "        self.gate_net = nn.Sequential(\n",
    "            nn.Linear(n_channels * 2, n_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_channels, n_channels),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Initialize gates to start high\n",
    "        init_value = float(np.clip(gate_init, 1e-3, 1 - 1e-3))\n",
    "        init_bias = math.log(init_value / (1.0 - init_value))\n",
    "        with torch.no_grad():\n",
    "            self.gate_net[-2].bias.fill_(init_bias)\n",
    "        \n",
    "        self.latest_gate_values = None\n",
    "        self.gate_penalty_tensor = None\n",
    "\n",
    "    def compute_gates(self, x):\n",
    "        \"\"\"Compute data-dependent channel gates.\"\"\"\n",
    "        x_s = x.squeeze(1)\n",
    "        ch_mean = x_s.mean(dim=2)\n",
    "        ch_std = x_s.std(dim=2)\n",
    "        stats = torch.cat([ch_mean, ch_std], dim=1)\n",
    "        return self.gate_net(stats)\n",
    "\n",
    "    def forward(self, x):\n",
    "        prepared = self._prepare_input(x)\n",
    "        gates = self.compute_gates(prepared)\n",
    "        self.gate_penalty_tensor = gates\n",
    "        self.latest_gate_values = gates.detach()\n",
    "        gated = prepared * gates.view(gates.size(0), 1, gates.size(1), 1)\n",
    "        \n",
    "        features = self._forward_features(gated)\n",
    "        x = features.view(features.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        return self.fc2(x)\n",
    "\n",
    "    def get_channel_importance_gate(self):\n",
    "        \"\"\"Get channel importance from gate values.\"\"\"\n",
    "        if self.latest_gate_values is None:\n",
    "            return None\n",
    "        return self.latest_gate_values.mean(dim=0).cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer, device, l1_lambda=0.0):\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "    \n",
    "    for X_batch, y_batch in dataloader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        \n",
    "        # Add gating regularization if applicable\n",
    "        gate_penalty = getattr(model, 'gate_penalty_tensor', None)\n",
    "        if l1_lambda > 0 and gate_penalty is not None:\n",
    "            loss = loss + l1_lambda * gate_penalty.abs().mean()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += y_batch.size(0)\n",
    "        correct += (predicted == y_batch).sum().item()\n",
    "    \n",
    "    return total_loss / max(1, len(dataloader)), correct / max(1, total)\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in dataloader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += y_batch.size(0)\n",
    "            correct += (predicted == y_batch).sum().item()\n",
    "    \n",
    "    return total_loss / max(1, len(dataloader)), correct / max(1, total)\n",
    "\n",
    "\n",
    "def train_pytorch_model(model, train_loader, val_loader, config, model_name=''):\n",
    "    device = config['device']\n",
    "    model = model.to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'], \n",
    "                          weight_decay=config['weight_decay'])\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=config['scheduler_factor'], \n",
    "        patience=config['scheduler_patience'], min_lr=config['min_lr'], verbose=False\n",
    "    )\n",
    "    \n",
    "    l1_lambda = config['gating']['l1_lambda'] if getattr(model, 'use_gate_regularizer', False) else 0.0\n",
    "    \n",
    "    best_state = deepcopy(model.state_dict())\n",
    "    best_val_acc = 0.0\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(config['epochs']):\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device, l1_lambda)\n",
    "        val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        improved = val_acc > best_val_acc or (val_acc == best_val_acc and val_loss < best_val_loss)\n",
    "        if improved:\n",
    "            best_state = deepcopy(model.state_dict())\n",
    "            best_val_acc = val_acc\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if epoch % 5 == 0 or improved:\n",
    "            print(f\"[{model_name}] Epoch {epoch+1}/{config['epochs']} - \"\n",
    "                  f\"Train: {train_acc:.4f} | Val: {val_acc:.4f} | Best: {best_val_acc:.4f}\")\n",
    "        \n",
    "        if config['use_early_stopping'] and patience_counter >= config['patience']:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    model.load_state_dict(best_state)\n",
    "    return best_state, best_val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading PhysioNet data...\")\n",
    "X, y, subjects = load_physionet_data(CONFIG['data_path'])\n",
    "print(f\"\\nData ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train EEG-ARNN Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup cross-validation\n",
    "skf = StratifiedKFold(n_splits=CONFIG['n_folds'], shuffle=True, random_state=CONFIG['random_seed'])\n",
    "\n",
    "models_to_train = [\n",
    "    {'name': 'Baseline-EEG-ARNN', 'class': BaselineEEGARNN},\n",
    "    {'name': 'Adaptive-Gating-EEG-ARNN', 'class': AdaptiveGatingEEGARNN},\n",
    "]\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"TRAINING EEG-ARNN MODELS\")\n",
    "print(f\"{'='*60}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "all_results = {}\n",
    "\n",
    "for model_info in models_to_train:\n",
    "    model_name = model_info['name']\n",
    "    model_class = model_info['class']\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training {model_name}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    fold_results = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "        print(f\"\\nFold {fold + 1}/{CONFIG['n_folds']}\")\n",
    "        \n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "        \n",
    "        train_dataset = EEGDataset(X_train, y_train)\n",
    "        val_dataset = EEGDataset(X_val, y_val)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'], shuffle=False)\n",
    "        \n",
    "        # Build model\n",
    "        if model_class == AdaptiveGatingEEGARNN:\n",
    "            model = model_class(\n",
    "                n_channels=CONFIG['n_channels'],\n",
    "                n_classes=CONFIG['n_classes'],\n",
    "                n_timepoints=CONFIG['n_timepoints'],\n",
    "                hidden_dim=CONFIG['hidden_dim'],\n",
    "                gate_init=CONFIG['gating']['gate_init']\n",
    "            )\n",
    "        else:\n",
    "            model = model_class(\n",
    "                n_channels=CONFIG['n_channels'],\n",
    "                n_classes=CONFIG['n_classes'],\n",
    "                n_timepoints=CONFIG['n_timepoints'],\n",
    "                hidden_dim=CONFIG['hidden_dim']\n",
    "            )\n",
    "        \n",
    "        best_state, val_acc = train_pytorch_model(model, train_loader, val_loader, CONFIG, model_name)\n",
    "        \n",
    "        # Save model\n",
    "        model_path = os.path.join(CONFIG['models_dir'], f\"eegarnn_{model_name}_fold{fold+1}.pt\")\n",
    "        torch.save(best_state, model_path)\n",
    "        \n",
    "        fold_results.append({'fold': fold + 1, 'accuracy': val_acc})\n",
    "        print(f\"Fold {fold + 1} Accuracy: {val_acc:.4f}\")\n",
    "        \n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    # Store results\n",
    "    all_results[model_name] = fold_results\n",
    "    mean_acc = np.mean([r['accuracy'] for r in fold_results])\n",
    "    std_acc = np.std([r['accuracy'] for r in fold_results])\n",
    "    \n",
    "    print(f\"\\n{model_name} Summary:\")\n",
    "    print(f\"Mean Accuracy: {mean_acc:.4f} +/- {std_acc:.4f}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"EEG-ARNN MODELS TRAINED!\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save initial results\n",
    "for model_name, fold_results in all_results.items():\n",
    "    df = pd.DataFrame(fold_results)\n",
    "    df['model'] = model_name\n",
    "    filename = model_name.lower().replace('-', '_').replace(' ', '_')\n",
    "    df.to_csv(os.path.join(CONFIG['results_dir'], f'eegarnn_{filename}_results.csv'), index=False)\n",
    "    print(f\"Saved: results/eegarnn_{filename}_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Channel Selection Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_channel_importance_aggregation(model, dataloader, device):\n",
    "    \"\"\"Aggregation Selection: based on feature activations.\"\"\"\n",
    "    model.eval()\n",
    "    channel_stats = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, _ in dataloader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            prepared = model._prepare_input(X_batch)\n",
    "            features = model._forward_features(prepared)\n",
    "            activations = torch.mean(torch.abs(features), dim=(1, 3))\n",
    "            channel_stats.append(activations.cpu())\n",
    "\n",
    "    if not channel_stats:\n",
    "        return np.zeros(model.n_channels)\n",
    "    stacked = torch.cat(channel_stats, dim=0)\n",
    "    return stacked.mean(dim=0).numpy()\n",
    "\n",
    "\n",
    "def compute_gate_importance(model, dataloader, device):\n",
    "    \"\"\"Gate Selection: average gate values across dataset.\"\"\"\n",
    "    model.eval()\n",
    "    gate_batches = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, _ in dataloader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            _ = model(X_batch)\n",
    "            latest = getattr(model, 'latest_gate_values', None)\n",
    "            if latest is not None:\n",
    "                gate_batches.append(latest.cpu())\n",
    "\n",
    "    if not gate_batches:\n",
    "        return np.ones(model.n_channels) / model.n_channels\n",
    "    stacked = torch.cat(gate_batches, dim=0)\n",
    "    return stacked.mean(dim=0).numpy()\n",
    "\n",
    "\n",
    "def select_top_k_channels(importance_scores, k):\n",
    "    \"\"\"Select top k channels.\"\"\"\n",
    "    top_k_indices = np.argsort(importance_scores)[-k:]\n",
    "    return sorted(top_k_indices)\n",
    "\n",
    "\n",
    "def apply_channel_selection(X, selected_channels):\n",
    "    \"\"\"Apply channel selection to data.\"\"\"\n",
    "    return X[:, selected_channels, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Channel Selection Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Channel selection configuration\n",
    "cs_experiments = [\n",
    "    {'model': 'Baseline-EEG-ARNN', 'methods': ['edge', 'aggregation']},\n",
    "    {'model': 'Adaptive-Gating-EEG-ARNN', 'methods': ['edge', 'aggregation', 'gate']},\n",
    "]\n",
    "\n",
    "channel_selection_results = []\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"CHANNEL SELECTION EVALUATION\")\n",
    "print(f\"{'='*60}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run channel selection experiments\n",
    "for exp in cs_experiments:\n",
    "    model_name = exp['model']\n",
    "    methods = exp['methods']\n",
    "    model_class = BaselineEEGARNN if 'Baseline' in model_name else AdaptiveGatingEEGARNN\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{model_name}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    for method in methods:\n",
    "        print(f\"\\nMethod: {method.upper()}\")\n",
    "        \n",
    "        for k in CONFIG['k_values']:\n",
    "            print(f\"  k={k}:\", end=' ')\n",
    "            fold_accuracies = []\n",
    "            \n",
    "            for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "                X_train, X_val = X[train_idx], X[val_idx]\n",
    "                y_train, y_val = y[train_idx], y[val_idx]\n",
    "                \n",
    "                # Load trained model\n",
    "                if model_class == AdaptiveGatingEEGARNN:\n",
    "                    model = model_class(\n",
    "                        n_channels=CONFIG['n_channels'],\n",
    "                        n_classes=CONFIG['n_classes'],\n",
    "                        n_timepoints=CONFIG['n_timepoints'],\n",
    "                        hidden_dim=CONFIG['hidden_dim'],\n",
    "                        gate_init=CONFIG['gating']['gate_init']\n",
    "                    )\n",
    "                else:\n",
    "                    model = model_class(\n",
    "                        n_channels=CONFIG['n_channels'],\n",
    "                        n_classes=CONFIG['n_classes'],\n",
    "                        n_timepoints=CONFIG['n_timepoints'],\n",
    "                        hidden_dim=CONFIG['hidden_dim']\n",
    "                    )\n",
    "                \n",
    "                model_path = os.path.join(CONFIG['models_dir'], f\"eegarnn_{model_name}_fold{fold+1}.pt\")\n",
    "                state_dict = torch.load(model_path, map_location=CONFIG['device'])\n",
    "                model.load_state_dict(state_dict)\n",
    "                model = model.to(CONFIG['device'])\n",
    "                model.eval()\n",
    "                \n",
    "                # Compute importance\n",
    "                if method == 'edge':\n",
    "                    importance_scores = model.get_channel_importance_edge()\n",
    "                elif method == 'aggregation':\n",
    "                    train_dataset = EEGDataset(X_train, y_train)\n",
    "                    train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=False)\n",
    "                    importance_scores = get_channel_importance_aggregation(model, train_loader, CONFIG['device'])\n",
    "                else:  # gate\n",
    "                    train_dataset = EEGDataset(X_train, y_train)\n",
    "                    train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=False)\n",
    "                    importance_scores = compute_gate_importance(model, train_loader, CONFIG['device'])\n",
    "                \n",
    "                # Select channels\n",
    "                selected_channels = select_top_k_channels(importance_scores, k)\n",
    "                X_train_selected = apply_channel_selection(X_train, selected_channels)\n",
    "                X_val_selected = apply_channel_selection(X_val, selected_channels)\n",
    "                \n",
    "                # Train new model\n",
    "                if model_class == AdaptiveGatingEEGARNN:\n",
    "                    new_model = model_class(\n",
    "                        n_channels=k,\n",
    "                        n_classes=CONFIG['n_classes'],\n",
    "                        n_timepoints=CONFIG['n_timepoints'],\n",
    "                        hidden_dim=CONFIG['hidden_dim'],\n",
    "                        gate_init=CONFIG['gating']['gate_init']\n",
    "                    )\n",
    "                else:\n",
    "                    new_model = model_class(\n",
    "                        n_channels=k,\n",
    "                        n_classes=CONFIG['n_classes'],\n",
    "                        n_timepoints=CONFIG['n_timepoints'],\n",
    "                        hidden_dim=CONFIG['hidden_dim']\n",
    "                    )\n",
    "                \n",
    "                train_dataset = EEGDataset(X_train_selected, y_train)\n",
    "                val_dataset = EEGDataset(X_val_selected, y_val)\n",
    "                train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True)\n",
    "                val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'], shuffle=False)\n",
    "                \n",
    "                best_state, val_acc = train_pytorch_model(new_model, train_loader, val_loader, \n",
    "                                                          CONFIG, f\"{model_name}-{method}-k{k}\")\n",
    "                fold_accuracies.append(val_acc)\n",
    "                \n",
    "                del model, new_model\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "            \n",
    "            mean_acc = np.mean(fold_accuracies)\n",
    "            std_acc = np.std(fold_accuracies)\n",
    "            print(f\"{mean_acc:.4f} +/- {std_acc:.4f}\")\n",
    "            \n",
    "            channel_selection_results.append({\n",
    "                'model': model_name,\n",
    "                'method': method,\n",
    "                'k': k,\n",
    "                'mean_accuracy': mean_acc,\n",
    "                'std_accuracy': std_acc,\n",
    "                'fold_accuracies': fold_accuracies\n",
    "            })\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"CHANNEL SELECTION COMPLETE!\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save channel selection results\n",
    "cs_df = pd.DataFrame(channel_selection_results)\n",
    "cs_df.to_csv(os.path.join(CONFIG['results_dir'], 'channel_selection_results.csv'), index=False)\n",
    "\n",
    "print(\"\\nChannel Selection Results:\")\n",
    "print(cs_df[['model', 'method', 'k', 'mean_accuracy', 'std_accuracy']])\n",
    "print(\"\\nSaved: results/channel_selection_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Retention Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retention analysis using Gate Selection\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"RETENTION ANALYSIS: Adaptive-Gating-EEG-ARNN (Gate Selection)\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "retention_results = []\n",
    "\n",
    "for k in CONFIG['retention_k_values']:\n",
    "    print(f\"k={k}:\", end=' ')\n",
    "    fold_accuracies = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "        \n",
    "        # Load trained model\n",
    "        model = AdaptiveGatingEEGARNN(\n",
    "            n_channels=CONFIG['n_channels'],\n",
    "            n_classes=CONFIG['n_classes'],\n",
    "            n_timepoints=CONFIG['n_timepoints'],\n",
    "            hidden_dim=CONFIG['hidden_dim'],\n",
    "            gate_init=CONFIG['gating']['gate_init']\n",
    "        )\n",
    "        model_path = os.path.join(CONFIG['models_dir'], f\"eegarnn_Adaptive-Gating-EEG-ARNN_fold{fold+1}.pt\")\n",
    "        state_dict = torch.load(model_path, map_location=CONFIG['device'])\n",
    "        model.load_state_dict(state_dict)\n",
    "        model = model.to(CONFIG['device'])\n",
    "        model.eval()\n",
    "        \n",
    "        # Compute gate importance\n",
    "        train_dataset = EEGDataset(X_train, y_train)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=False)\n",
    "        importance_scores = compute_gate_importance(model, train_loader, CONFIG['device'])\n",
    "        selected_channels = select_top_k_channels(importance_scores, k)\n",
    "        \n",
    "        # Apply selection\n",
    "        X_train_selected = apply_channel_selection(X_train, selected_channels)\n",
    "        X_val_selected = apply_channel_selection(X_val, selected_channels)\n",
    "        \n",
    "        # Train new model\n",
    "        new_model = AdaptiveGatingEEGARNN(\n",
    "            n_channels=k,\n",
    "            n_classes=CONFIG['n_classes'],\n",
    "            n_timepoints=CONFIG['n_timepoints'],\n",
    "            hidden_dim=CONFIG['hidden_dim'],\n",
    "            gate_init=CONFIG['gating']['gate_init']\n",
    "        )\n",
    "        \n",
    "        train_dataset = EEGDataset(X_train_selected, y_train)\n",
    "        val_dataset = EEGDataset(X_val_selected, y_val)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'], shuffle=False)\n",
    "        \n",
    "        best_state, val_acc = train_pytorch_model(new_model, train_loader, val_loader, \n",
    "                                                  CONFIG, f\"Retention-k{k}\")\n",
    "        fold_accuracies.append(val_acc)\n",
    "        \n",
    "        del model, new_model\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    mean_acc = np.mean(fold_accuracies)\n",
    "    std_acc = np.std(fold_accuracies)\n",
    "    print(f\"{mean_acc:.4f} +/- {std_acc:.4f}\")\n",
    "    \n",
    "    retention_results.append({\n",
    "        'k': k,\n",
    "        'mean_accuracy': mean_acc,\n",
    "        'std_accuracy': std_acc,\n",
    "        'fold_accuracies': fold_accuracies\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save retention results\n",
    "retention_df = pd.DataFrame(retention_results)\n",
    "retention_df.to_csv(os.path.join(CONFIG['results_dir'], 'retention_analysis.csv'), index=False)\n",
    "\n",
    "print(\"\\nRetention Analysis Results:\")\n",
    "print(retention_df[['k', 'mean_accuracy', 'std_accuracy']])\n",
    "print(\"\\nSaved: results/retention_analysis.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create complete summary\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"COMPLETE SUMMARY\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Model comparison\n",
    "print(\"1. EEG-ARNN Model Comparison:\")\n",
    "for model_name, fold_results in all_results.items():\n",
    "    accs = [r['accuracy'] for r in fold_results]\n",
    "    print(f\"   {model_name}: {np.mean(accs):.4f} +/- {np.std(accs):.4f}\")\n",
    "\n",
    "# Best channel selection\n",
    "print(\"\\n2. Best Channel Selection Method:\")\n",
    "best_cs = cs_df.loc[cs_df['mean_accuracy'].idxmax()]\n",
    "print(f\"   Model: {best_cs['model']}\")\n",
    "print(f\"   Method: {best_cs['method'].upper()}\")\n",
    "print(f\"   k: {best_cs['k']}\")\n",
    "print(f\"   Accuracy: {best_cs['mean_accuracy']:.4f} +/- {best_cs['std_accuracy']:.4f}\")\n",
    "\n",
    "# Retention insights\n",
    "print(\"\\n3. Retention Analysis (Gate Selection):\")\n",
    "for _, row in retention_df.iterrows():\n",
    "    print(f\"   k={row['k']}: {row['mean_accuracy']:.4f} +/- {row['std_accuracy']:.4f}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"PIPELINE 2 COMPLETE!\")\n",
    "print(f\"{'='*60}\")\n",
    "print(\"\\nAll results saved to results/ directory\")\n",
    "print(\"Ready for comparison with Pipeline 1 baseline methods!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
