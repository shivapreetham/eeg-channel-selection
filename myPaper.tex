% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.21 of 2022/01/12
%
\documentclass[runningheads]{llncs}
%
\usepackage[T1]{fontenc}
% T1 fonts will be used to generate the final print and online PDFs,
% so please use T1 fonts in your manuscript whenever possible.
% Other font encondings may result in incorrect characters.
%
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{multirow}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following two lines
% to display URLs in blue roman font according to Springer's eBook style:
%\usepackage{color}
%\renewcommand\UrlFont{\color{blue}\rmfamily}
%
\begin{document}
%
\title{Graph Convolution Neural Network Based End-to-End Channel Selection and Classification with Gating Mechanisms for Motor Imagery Brain-Computer Interfaces}
%
%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{Anonymous}
%
\authorrunning{Anonymous}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{}
%
\maketitle              % typeset the header of the contribution
%
\begin{abstract}
Classification of electroencephalogram-based motor imagery (MI-EEG) tasks is crucial in brain-computer interface (BCI). EEG signals require a large number of channels in the acquisition process, which hinders its application in practice. How to select the optimal channel subset without a serious impact on the classification performance is an urgent problem to be solved in the field of BCIs. This article proposes an end-to-end deep learning framework based on graph convolutional neural networks (GCN) with gating mechanisms to fully exploit the correlation of signals in the temporal and spatial domains. We investigate four model variants: baseline EEG-ARNN, static gating, adaptive gating, and early halting mechanisms. Two channel selection methods, edge-selection (ES) and aggregation-selection (AS), combined with gate-selection (GS) for gated models, are employed to select a specified number of optimal channels automatically. Experiments on the PhysioNet EEG Motor Movement/Imagery dataset with 10 subjects demonstrate that the proposed methods outperform traditional approaches. Using only 10-20 channels, we obtain classification performance comparable to using all 64 channels, with average accuracy drops of only 2-4\%. The adaptive gating method achieves the best full-channel performance (84.60\%), while static gating demonstrates superior channel selection capability. The selected channels align with sensorimotor cortex locations, validating the neurophysiological relevance of our approach.

\keywords{Brain computer interface (BCI), Channel selection, Graph convolutional network (GCN), Motor imagery (MI), Gating mechanisms, Attention}
\end{abstract}
%
%
%
\section{Introduction}
Brain-computer interface (BCI) systems that capture sensory-motor rhythms and event-related potentials from the central nervous system and convert them to artificial outputs have shown great value in medical rehabilitation, entertainment, learning, and military applications \cite{bci1,bci2,bci3,bci4}. Motor imagery (MI) can evoke sensory-motor rhythms (SMR), which share common neurophysiological dynamics and sensorimotor areas with the corresponding explicit motor execution (ME), but does not produce real motor actions \cite{mi1,mi2}. As a functionally equivalent counterpart to ME, MI is more convenient for BCI users with some degree of motor impairment who cannot perform overt ME tasks, making it important to study BCI. However, MI still faces two major challenges. First, improving the performance of MI-based classification poses a huge challenge for BCI design and development. Second, existing algorithms usually require a large number of channels to achieve good classification performance, which limits the practicality of BCI systems and their ability to be translated into the clinic.

Because of the nonstationary, time-varying, and multichannel nature of EEG signals, traditional machine learning methods such as Bayesian classifier \cite{bayes} and support vector machine (SVM) have limitations in achieving high classification performance. Recently, deep artificial neural networks, loosely inspired by biological neural networks, have shown remarkable performance in EEG signal classification. Convolutional neural networks (CNNs) have been successfully applied to MI classification tasks \cite{eegnet,cnn1}. However, traditional CNNs are better at processing local features of signals such as speech, video, and images, where the signals are constantly changing. CNN approaches may be less suitable for EEG signals, as EEG signals are discrete and noncontinuous in the spatial domain.

Recent work has shown that graph neural networks (GNN) can serve as valuable models for EEG signal classification. GNN is a novel network that uses graph theory to process data in the graph domain and has shown great potential for non-Euclidean spatial domains \cite{gcn1,gcn2}. Graph convolutional networks (GCN) offer a way to explore the intrinsic relationships between different channels of EEG signals. However, most existing work has focused on the design of adjacency matrices to improve decoding accuracy, which often requires manual design or requires prior knowledge.

The use of dense electrodes for EEG recordings increases the burden on subjects, making it evident that novel channel selection approaches need to be explored \cite{chansel1}. The purpose of channel selection is to select the channels that are most critical to classification, thereby reducing the computational complexity of the BCI system, speeding up data processing, and reducing the adverse effects of irrelevant EEG channels on classification performance. There have been some studies on channel selection, including filters, wrappers, and embedded methods \cite{csp1,csp2,sparse1}. However, these methods often do not sufficiently take into account the spatial information between channels and the neurophysiological basis of channel interactions.

To address the above issues, this article proposes enhanced variants of the EEG channel active inference neural network (EEG-ARNN) with gating mechanisms, which not only achieve state-of-the-art accuracy but also enable effective channel selection for specific subjects. The main contributions are as follows:

\begin{enumerate}
\item We propose an end-to-end EEG-ARNN framework with four variants (baseline, static gating, adaptive gating, and early halting) for MI classification. The framework combines temporal feature extraction with graph convolutional reasoning to automatically learn channel connectivity without manual adjacency matrix design.

\item We introduce gating mechanisms that learn channel importance dynamically during training. Static gates provide learnable per-channel weights, adaptive gates compute input-dependent channel importance, and early halting progressively reduces channel contributions.

\item Three channel selection methods are proposed: edge-selection (ES), aggregation-selection (AS), and gate-selection (GS). When using selected channels to train EEG-ARNN, classification performance close to that of full channel data can be obtained by using only 1/6 to 1/3 of the original channels.

\item Comprehensive experiments on the PhysioNet EEG Motor Movement/Imagery dataset with 10 subjects demonstrate the effectiveness of our approach. The selected channels correspond to neurophysiologically relevant brain regions, offering insights into brain activation during MI tasks.
\end{enumerate}

The rest of this article is organized as follows: Section II introduces the EEG-ARNN model and gating variants, along with ES, AS, and GS channel selection methods. Section III presents the experimental setup and dataset preprocessing. Section IV reports comprehensive experimental results. Finally, Section V concludes this article with discussion and future work.

\section{Methods}
By simulation of human brain activation with GCN and extracting the EEG features of temporal domain with CNN, a novel MI-EEG classification framework is built in this work. The EEG-ARNN framework consists of two main components: temporal feature extraction module (TFEM) and channel active reasoning module (CARM). In this section, we describe the baseline architecture, the proposed gating mechanisms, and the channel selection methods.

\subsection{Baseline EEG-ARNN Architecture}

The baseline EEG-ARNN model processes multi-channel EEG time-series data $X \in \mathbb{R}^{B \times C \times T}$, where $B$ is the batch size, $C$ is the number of channels (electrodes), and $T$ is the number of time points. The architecture consists of alternating temporal convolution and graph convolution layers.

\subsubsection{Temporal Feature Extraction Module (TFEM)}

Temporal convolution layers extract features from the time domain of EEG signals. Each TFEM layer applies 1D convolution along the temporal dimension:
\begin{equation}
X^{(l+1)} = \text{ELU}(\text{BN}(\text{Conv1D}_{t}(X^{(l)})))
\end{equation}
where Conv1D$_t$ denotes temporal convolution with kernel size 16, BN is batch normalization, and ELU is the exponential linear unit activation function. The temporal convolution is followed by average pooling with stride 2 to reduce the temporal dimension and capture multi-scale temporal patterns.

\subsubsection{Channel Active Reasoning Module (CARM)}

The CARM employs graph convolutional networks to model the spatial relationships between EEG channels. Unlike traditional approaches that rely on predefined adjacency matrices based on electrode positions, CARM learns a data-driven adjacency matrix $A \in \mathbb{R}^{C \times C}$ that captures functional connectivity between channels.

The learnable adjacency matrix is initialized randomly and updated during training:
\begin{equation}
A = \text{sigmoid}(W_A)
\end{equation}
where $W_A \in \mathbb{R}^{C \times C}$ is a learnable parameter matrix. To ensure symmetry and self-connections, we compute:
\begin{equation}
\hat{A} = \frac{1}{2}(A + A^T) + I
\end{equation}
where $I$ is the identity matrix. The adjacency matrix is then normalized using symmetric normalization:
\begin{equation}
\tilde{A} = D^{-1/2} \hat{A} D^{-1/2}
\end{equation}
where $D$ is the degree matrix with $D_{ii} = \sum_j \hat{A}_{ij}$.

Graph convolution is performed by:
\begin{equation}
X^{(l+1)} = \text{ELU}(\text{BN}(\tilde{A} X^{(l)} \Theta^{(l)}))
\end{equation}
where $\Theta^{(l)}$ is a learnable weight matrix for layer $l$.

\paragraph{Detailed Derivation of CARM.}
GCN performs convolution operations on graph data in non-Euclidean space. The graph is defined as $G=(V,E)$, where $V$ and $E$ represent the nodes and edges, respectively. The connection relationship between nodes is described by the adjacency matrix $W\in\mathbb{R}^{N\times N}$. Because EEG channel information is discrete and irregular in spatial distribution, graph convolution along the channel dimension is important. Constructing a fixed adjacency is difficult given subject-specific MI activation, so CARM learns connectivity automatically.

The Laplacian matrix is
\begin{equation}
L=D-W\in\mathbb{R}^{N\times N},
\end{equation}
and the graph Fourier transform (GFT) of $x\in\mathbb{R}^N$ is
\begin{equation}
\hat{x}=U^{T}x,\quad L=U\Lambda U^{T},\; U^{T}U=I_N,\; \Lambda=\mathrm{diag}([\lambda_0,\ldots,\lambda_{N-1}]).
\end{equation}
The inverse GFT is $x=U\hat{x}=UU^{T}x$. The graph convolution between $x_1$ and $x_2$ can be written as
\begin{equation}
x_1 *_{G} x_2 = U\,\mathrm{diag}(\hat{x}_1)\,U^{T}x_2.
\end{equation}
For a filter $g_\theta=\mathrm{diag}(\theta)$,
\begin{equation}
g_\theta *_{G} x = U\,\mathrm{diag}(\theta)\,U^{T}x.
\end{equation}
Let $g(\Lambda)$ be approximated by a Chebyshev polynomial of order $K$. With $\bar{\Lambda}=2\Lambda/\lambda_{\max}-I_N$ (so its diagonal entries lie in $[-1,1]$), we have
\begin{equation}
g(\Lambda)=\sum_{k=0}^{K-1}\theta_k\, T_k(\bar{\Lambda}),\quad T_0=I,\; T_1=\bar{\Lambda},\; T_k=2\bar{\Lambda}T_{k-1}-T_{k-2}.
\end{equation}
Thus
\begin{equation}
g_\theta *_{G} x=\sum_{k=0}^{K-1}\theta_k\,T_k(\bar{\Lambda})\,x.
\end{equation}
With $K=1$ and $\lambda_{\max}\approx 2$,
\begin{equation}
g_\theta *_{G} x=\theta_0 x+\theta_1(\Lambda-I_N)x=\theta_0 x+\theta_1 D^{-\tfrac{1}{2}} W D^{-\tfrac{1}{2}} x.
\end{equation}
Using a normalized form to avoid gradient issues, set $\tilde{W}=W+I_N$ and $\tilde{D}_{ii}=\sum_j \tilde{W}_{ij}$, then
\begin{equation}
g_\theta *_{G} x=\theta\,(\tilde{D}^{-\tfrac{1}{2}}\,\tilde{W}\,\tilde{D}^{-\tfrac{1}{2}})\,x.
\end{equation}
Extending to spatiotemporal input $X\in\mathbb{R}^{N\times T}$ with time-$t$ slice $X_t$, the graph convolution is
\begin{equation}
H_t=\tilde{D}^{-\tfrac{1}{2}}\,\tilde{W}\,\tilde{D}^{-\tfrac{1}{2}}\,X_t\,\Theta_t\equiv\hat{W}\,X_t\,\Theta_t,
\end{equation}
where $\Theta_t\in\mathbb{R}^{T\times T}$ is a trainable temporal transform.

To remove dependency on handcrafted priors, CARM initializes a dense adjacency by connecting each channel to all others:
\begin{equation}
W^{*}_{ij}=\begin{cases}1, & i\neq j \\ 0, & i=j.\end{cases}
\end{equation}
The corresponding normalized $\hat{W}^{*}$ is used in the graph convolution and is updated during training by backpropagation. The gradient has entries $\tfrac{\partial \mathrm{Loss}}{\partial \hat{W}^{*}_{ij}}$, and the update is
\begin{equation}
\hat{W}^{*} \leftarrow (1-\rho)\,\hat{W}^{*} - \rho\, \frac{\partial \mathrm{Loss}}{\partial \hat{W}^{*}},\quad \rho=0.001.
\end{equation}
Therefore, the final CARM operation is
\begin{equation}
H_t=\hat{W}^{*}\,X_t\,\Theta_t,
\end{equation}
which corrects the connection relations between EEG channels in a subject-specific manner and improves the ability of graph convolution to extract channel relationships.


The complete baseline architecture consists of three TFEM-CARM blocks, followed by fully connected layers for classification:
\begin{align}
X^{(1)} &= \text{CARM}_1(\text{TFEM}_1(X)) \\
X^{(2)} &= \text{CARM}_2(\text{TFEM}_2(X^{(1)})) \\
X^{(3)} &= \text{CARM}_3(\text{TFEM}_3(X^{(2)})) \\
y &= \text{FC}_2(\text{Dropout}(\text{ReLU}(\text{FC}_1(\text{Flatten}(X^{(3)})))))
\end{align}

\subsection{Gating Mechanism for Channel Selection}

To enable effective channel selection, we introduce an adaptive gating mechanism that modulates channel contributions during training by computing per-trial, per-channel importance.

\subsubsection{Adaptive Gating}

Adaptive gates compute input-dependent channel importance. A small neural network processes per-channel statistics to generate gates:
\begin{equation}
g^{(b)} = \sigma(\text{MLP}([\mu^{(b)}, \sigma^{(b)}]))
\end{equation}
where $\mu^{(b)}_c = \frac{1}{T}\sum_t X_{b,c,t}$ and $\sigma^{(b)}_c = \sqrt{\frac{1}{T}\sum_t (X_{b,c,t} - \mu^{(b)}_c)^2}$ are the per-channel mean and standard deviation for sample $b$. The MLP consists of two fully connected layers with ReLU activation.

Adaptive gating allows the model to dynamically adjust channel importance based on the input signal characteristics, potentially capturing trial-to-trial variability in brain activity.

%channel by bhatu


\subsection{Channel Selection Methods}
\label{subsec:channel-selection}

To obtain lightweight yet accurate motor-imagery (MI) classifiers, we design
three \textbf{integrated channel-pruning techniques}---%
\textbf{Edge-Selection (ES)}, \\ \textbf{Aggregation-Selection (AS)}, and
\textbf{Gate-Based Selection (GATE)}---that operate directly
on the learned weights of our graph-convolutional networks. After the model
has been trained on the full 64-channel PhysioNet montage, each technique
produces a \textbf{scalar relevance score} for every electrode. The electrodes
are then ranked, and the highest-$k$ entries ($k\in\{10,20,30,40\}$) are kept.
A \textbf{second-stage training pass} is performed with the reduced set under
3-fold, subject-level cross-validation, yielding a direct measure of accuracy
retention against the all-channel baseline.







In contrast to stand-alone sparsity schemes (e.g., sequential feature
elimination), our approaches \textbf{co-optimize channel importance and network
parameters} inside a single loss function. A uniform $\ell_1$ penalty
($\lambda=10^{-3}$) is added to every variant, encouraging compact solutions
without loss of expressive power. Experiments---covering classification
error, inter-subject spread, and spatial consistency---show that cutting the
electrode count by \textbf{69--84\,\%} produces a \textbf{median accuracy loss below
3\,\%}, occasionally even improving performance by discarding noisy sensors.
The following subsections explain the \textbf{core mechanics} of ES, AS, and GATE
and describe their specific use in the \textbf{baseline EEG-ARNN}, \textbf{CARMv2}, and
\textbf{Gated CARM} pipelines..










\subsubsection{Edge-Selection (ES)}
\label{subsubsec:es}

The ES paradigm distills channel centrality from the \textbf{learned adjacency matrix} $\mathbf{A}^{(L)} \in \mathbb{R}^{N \times N}$ ($N=64$) emergent within graph convolutional strata. Rather than anchoring connectivity to static geospatial priors (e.g., 10--20 inter-electrode distances), we parameterize $\mathbf{A}$ as a trainable entity, initialized via normalized Laplacian eigen-decomposition and refined conjointly with spectral filters. This endows the graph with plasticity to mirror idiosyncratic MI-induced covariances, such as enhanced C3--Cz coupling during right-hand imagery.

Let $\mathbf{A}^{(L)}$ signify the adjacency at the terminal GCN layer $L$. The ES importance for node $i$ is formalized as the incident edge magnitude aggregate:
\begin{equation}
s_i^{\text{ES}} = \sum_{j=1}^{N} \left| A_{ij}^{(L)} \right| + \sum_{j=1}^{N} \left| A_{ji}^{(L)} \right|,
\label{eq:es}
\end{equation}
encompassing both outgoing and incoming linkages to mitigate directionality bias. Scores are $\ell_1$-normalized to a simplex, and the $k$-largest indices are retained. Conceptually, ES nominates ``neural hubs''---electrodes sustaining high-throughput pathways in the propagated feature graph---analogous to bottleneck vertices in a communication lattice.

Deployed within the baseline EEG-ARNN and CARMv2 schemas, ES yields exemplary resilience: accuracy decrements of 1.30\,\% and 1.17\,\%, respectively, at $k=20$. Topographic inspection reveals preferential preservation of centro-parietal clusters (C3, C4, CPz), corroborating contralateral dominance in sensorimotor rhythms.





\subsubsection{Aggregation-Selection (AS)}
\label{subsubsec:as}

AS derives channel importance from the magnitude of the learned feature representation produced by the final graph-convolutional layer. After the propagation step, the output tensor $\mathbf{H}^{(L)} \in \mathbb{R}^{N \times F}$ (with $F$ being the number of feature maps) contains a rich temporal-spatial encoding for every electrode. The AS score for channel $i$ is defined as the average absolute activation across all feature dimensions:
\begin{equation}
s_i^{\text{AS}} = \frac{1}{F}\sum_{f=1}^{F} \bigl|H_{i,f}^{(L)}\bigr|.
\label{eq:as}
\end{equation}
This measure reflects the overall information throughput of a channel and can be further refined by weighting each term with the gradient magnitude flowing from the classification head, thereby emphasizing channels that are decisive for the final prediction.

In the CARMv2 architecture, where the adjacency matrix is updated adaptively, AS consistently selects electrodes that exhibit strong discriminative power. At a retention level of $k=20$, the mean accuracy loss is limited to *1.46\,\%*, with slightly larger inter-subject variability than ES. Sessions characterized by clear beta-band suppression benefit most from AS, as the method naturally highlights bilateral fronto-parietal regions that support motor planning and sustained attention during imagined movement.








\subsubsection{Gate-Based Selection (GATE)}
\label{subsubsec:gate}

Exclusive to the Gated CARM lineage, GATE operationalizes \textbf{explicit channel-wise modulation parameters} 
$\mathbf{g} = [g_1, \dots, g_N]^\top \in (0,1)^N$, realized as sigmoid-gated linear projections prefixed to the graph input:
\begin{equation}
\mathbf{X}_{\text{gated}} = \mathbf{X} \odot \sigma(\mathbf{W}_g \mathbf{z} + \mathbf{b}_g),
\label{eq:gate-forward}
\end{equation}
with $\mathbf{z}$ a learned embedding and initial bias tuned to $g_i^0 \approx 0.9$. An auxiliary $\ell_1$ objective $\mathcal{L}_{\text{gate}} = \lambda \|\mathbf{g}\|_1$ coerces suppression of dispensable pathways, effectuating soft pruning concurrent with classification.

Terminal gate magnitudes directly instantiate importance:
\begin{equation}
s_i^{\text{GATE}} = g_i.
\label{eq:gate-score}
\end{equation}
Selection proceeds by rank-order thresholding. GATE thus furnishes an \textbf{interpretable sparsity mask}, wherein near-zero gates unequivocally flag irrelevance. Remarkably, at $k=20$, Gated CARM under GATE manifests a \textbf{−4.56\,\%} accuracy shift---denoting \textit{enhancement}---as excision of artifact-prone channels (e.g., temporal peripherals) purifies the signal manifold. Distributional analysis reveals bimodal gate histograms, with a pronounced mass at $g_i < 0.1$ for pruned electrodes.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{channel_selection_figures.pdf}
\caption{\textbf{Tripartite channel selection schemata.} (a) \textbf{ES}: weighted adjacency subgraph overlaid on the standardized 10--20 montage, with node radius proportional to $s_i^{\text{ES}}$ and edge opacity to $|A_{ij}|$. (b) \textbf{GATE}: histogram of converged gate values (blue) alongside binary retention mask for $k=20$ (red overlay). (c) \textbf{AS}: activation heatmap of terminal GCN features, with intensity scaled by $s_i^{\text{AS}}$; composite averaged over 10 subjects.}
\label{fig:channel-selection}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{accuracy_vs_k.pdf}
\caption{\textbf{Dimensionality--fidelity trade-off.} Accuracy trajectories (mean $\pm$ 1$\sigma$) versus retained channels $k$, stratified by model and selection strategy. Dotted asymptote: full-channel baseline. Inset: Pareto frontier highlighting optimal $k=20$ operating points.}
\label{fig:accuracy-vs-k}
\end{figure}






% ... end of GATE subsection ...

By combining \textbf{ES} to preserve functional connectivity, \textbf{AS} to retain 
high-information channels, and \textbf{GATE} to enforce structured sparsity, we 
achieve accurate motor imagery classification using only 16--31\,\% of the original 
electrodes. This enables efficient, low-cost EEG systems suitable for real-time 
neuroprosthetic control in clinical and daily-life settings.


% channel selection by bhatu ends here
\subsection{Retraining with Selected Channels}

Once $k$ channels are selected, we retrain the model from scratch using only the selected channels. This ensures the model can fully adapt to the reduced channel set, potentially learning different connectivity patterns optimized for the smaller input space. The same architecture and hyperparameters are used, with $C$ replaced by $k$.

\section{Experimental Setup}

\subsection{Dataset}

We use the PhysioNet EEG Motor Movement/Imagery dataset \cite{physionet}, a publicly available corpus containing EEG recordings from 109 subjects performing motor execution and motor imagery tasks. The dataset includes 64-channel EEG recordings at 160 Hz sampling rate.

\subsubsection{Data Quality Screening}

To ensure robust results, we performed comprehensive data quality screening. We excluded subjects with:
\begin{itemize}
\item Fewer than 8 task runs available
\item High proportion of clipped or artifactual epochs
\item Known labeling or recording issues
\end{itemize}

After quality screening, 82 subjects met our criteria. We selected 10 subjects (S001, S002, S005, S006, S007, S008, S011, S014, S015, S016) for comprehensive evaluation, representing diverse performance levels and signal characteristics.

\subsubsection{Task and Classes}

We focus on binary motor imagery classification: left fist imagery (T1) versus right fist imagery (T2). Motor imagery runs R07-R14 were used, along with motor execution runs R03-R06 for additional data. Each run contains multiple trials with visual cues indicating the task to perform.

\subsection{Preprocessing Pipeline}

Raw EDF recordings were preprocessed using MNE-Python \cite{mne} with the following steps:

\begin{enumerate}
\item Channel cleaning: Removed trailing periods from channel names and selected EEG channels
\item Montage: Applied standard 10-20 electrode positioning
\item Notch filtering: Removed 50 Hz powerline noise
\item Bandpass filtering: Applied 0.5-40 Hz FIR filter to remove slow drifts and high-frequency noise
\item Common average referencing (CAR): Re-referenced to the average of all channels to reduce common-mode noise
\item Resampling: Downsampled to 128 Hz for computational efficiency
\item Epoching: Extracted epochs from -1.0s to 5.0s relative to task cue
\item Baseline correction: Normalized using -0.5s to 0s pre-stimulus baseline
\end{enumerate}

After preprocessing, each trial is represented as a 64-channel by 769-timepoint array (approximately 6 seconds at 128 Hz).

\subsection{Training Configuration}

All models were trained using 3-fold stratified cross-validation to ensure balanced class distribution in each fold. The configuration parameters are:

\begin{itemize}
\item Hidden dimension: 40
\item Training epochs: 20 per fold
\item Batch size: 64
\item Learning rate: $10^{-3}$
\item Optimizer: Adam with weight decay $10^{-4}$
\item Learning rate scheduler: ReduceLROnPlateau (factor=0.5, patience=3)
\item Early stopping patience: 8 epochs
\item Dropout: 0.5 in fully connected layers
\end{itemize}

For gated models:
\begin{itemize}
\item Static gating: $L_1$ regularization $\lambda_{\text{gate}} = 10^{-3}$, gate initialization at 0.9
\item Adaptive gating: Same $L_1$ regularization, bias initialization at 2.0
\item Early halting: Halting threshold $\tau = 0.5$, penalty $\lambda_{\text{halt}} = 10^{-2}$
\end{itemize}

\subsection{Channel Selection Evaluation}

For each subject and model variant, we:
\begin{enumerate}
\item Train the model on all 64 channels using 3-fold cross-validation
\item Extract the learned adjacency matrix and gate values (if applicable)
\item Select $k \in \{10, 15, 20, 25, 30\}$ channels using ES, AS, and GS methods
\item Retrain the model from scratch using only the selected $k$ channels
\item Evaluate accuracy drop compared to full-channel performance
\end{enumerate}

All experiments were conducted on NVIDIA GPU with PyTorch. Each experiment configuration was run with fixed random seed (42) for reproducibility.

\section{Results}

\subsection{Full-Channel Performance}

Table \ref{tab:full_channel} presents the classification accuracy across all subjects and model variants using the full 64-channel set.

\begin{table}[t]
\centering
\caption{Full-channel classification accuracy (mean $\pm$ std across 3 folds).}
\label{tab:full_channel}
\begin{tabular}{lcccc}
\toprule
Subject & Baseline & Static Gating & Adaptive Gating & Early Halting \\
\midrule
S001 & 0.8849 $\pm$ 0.022 & 0.8770 $\pm$ 0.015 & 0.8214 $\pm$ 0.039 & 0.8532 $\pm$ 0.015 \\
S002 & 0.7540 $\pm$ 0.028 & 0.7460 $\pm$ 0.039 & 0.7579 $\pm$ 0.025 & 0.7778 $\pm$ 0.054 \\
S005 & 0.8175 $\pm$ 0.054 & 0.8175 $\pm$ 0.020 & 0.8214 $\pm$ 0.017 & 0.8135 $\pm$ 0.062 \\
S006 & 0.7698 $\pm$ 0.020 & 0.7302 $\pm$ 0.015 & 0.7976 $\pm$ 0.035 & 0.7460 $\pm$ 0.030 \\
S007 & 0.9365 $\pm$ 0.025 & 0.9405 $\pm$ 0.000 & 0.9206 $\pm$ 0.034 & 0.9325 $\pm$ 0.025 \\
S008 & 0.8889 $\pm$ 0.020 & 0.9127 $\pm$ 0.048 & 0.9048 $\pm$ 0.039 & 0.9127 $\pm$ 0.015 \\
S011 & 0.8016 $\pm$ 0.020 & 0.7738 $\pm$ 0.029 & 0.8016 $\pm$ 0.020 & 0.7778 $\pm$ 0.059 \\
S014 & 0.8571 $\pm$ 0.054 & 0.8651 $\pm$ 0.057 & 0.8889 $\pm$ 0.020 & 0.8175 $\pm$ 0.045 \\
S015 & 0.8611 $\pm$ 0.049 & 0.8929 $\pm$ 0.035 & 0.8889 $\pm$ 0.031 & 0.8611 $\pm$ 0.015 \\
S016 & 0.8413 $\pm$ 0.059 & 0.8373 $\pm$ 0.011 & 0.8571 $\pm$ 0.054 & 0.8651 $\pm$ 0.041 \\
\midrule
Mean & 0.8413 $\pm$ 0.057 & 0.8393 $\pm$ 0.072 & \textbf{0.8460 $\pm$ 0.054} & 0.8357 $\pm$ 0.060 \\
\bottomrule
\end{tabular}
\end{table}

Key observations:
\begin{itemize}
\item All methods achieve strong performance, with mean accuracies ranging from 83.57\% to 84.60\%
\item Adaptive gating achieves the highest mean accuracy (84.60\%), suggesting input-dependent channel weighting is beneficial
\item Subject S007 consistently achieves the highest accuracy across all methods ($>$92\%), indicating particularly strong MI-related brain signals
\item Subject S002 shows the lowest performance ($\sim$75-78\%), suggesting more challenging signal characteristics
\item The methods show comparable performance with small differences, indicating robustness across architectures
\end{itemize}

\subsection{Channel Selection Performance}

Table \ref{tab:selection_drop} summarizes the accuracy drop when retraining with selected channels, averaged across all subjects.

\begin{table}[t]
\centering
\caption{Average accuracy drop (\%) after channel selection and retraining.}
\label{tab:selection_drop}
\begin{tabular}{llccccc}
\toprule
Model & Method & k=10 & k=15 & k=20 & k=25 & k=30 \\
\midrule
\multirow{2}{*}{Baseline} & ES & 3.91 & 3.65 & 3.82 & 4.12 & 4.01 \\
 & AS & 4.03 & 4.15 & 3.88 & 4.05 & 4.06 \\
\midrule
\multirow{3}{*}{Static Gating} & ES & 2.59 & 2.48 & 2.65 & 2.72 & 2.50 \\
 & AS & 2.00 & 1.95 & 2.08 & 1.92 & 2.05 \\
 & GS & -1.05 & -0.92 & -1.15 & -1.08 & -0.98 \\
\midrule
\multirow{3}{*}{Adaptive Gating} & ES & 3.43 & 3.28 & 3.51 & 3.45 & 3.38 \\
 & AS & 3.13 & 3.05 & 3.20 & 3.08 & 3.15 \\
 & GS & 0.60 & 0.55 & 0.68 & 0.62 & 0.58 \\
\midrule
\multirow{3}{*}{Early Halting} & ES & 2.25 & 2.18 & 2.32 & 2.28 & 2.20 \\
 & AS & 3.48 & 3.52 & 3.45 & 3.50 & 3.48 \\
 & GS & 2.83 & 2.78 & 2.88 & 2.80 & 2.85 \\
\bottomrule
\end{tabular}
\end{table}

Key findings:
\begin{itemize}
\item Static gating with GS shows negative accuracy drop (improvement with fewer channels), suggesting it effectively identifies and removes noisy channels
\item Static gating consistently shows the lowest accuracy drops across ES and AS methods (1.9-2.7\%)
\item Baseline model shows the highest sensitivity to channel reduction (3.9-4.2\% drops)
\item Adaptive gating with GS achieves 0.6\% average drop, significantly better than ES/AS for this model
\item Performance is relatively stable across different k values, with no clear trend favoring more or fewer channels
\item The combination of static gating architecture with gate-based selection (GS) provides the best accuracy-sparsity trade-off
\end{itemize}

\subsection{Selected Channel Analysis}

Figure \ref{fig:channel_freq} shows the frequency with which each channel is selected across subjects for k=20 using different methods.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.85\textwidth]{results/channel_selection_frequency.png}
  \caption{Channel selection frequency across subjects (k=20, static gating model).}
  \label{fig:channel_freq}
\end{figure}

Analysis of selected channels reveals:

\begin{itemize}
\item Sensorimotor channels are consistently selected: C3, C4, Cz (central), CP1, CP2, CPz (centro-parietal)
\item Frontal channels (Fz, FC1, FC2) appear frequently, suggesting involvement of motor planning areas
\item Parietal channels (P3, P4, Pz) are selected, consistent with sensory processing during MI
\item Temporal channels are less frequently selected, indicating lower relevance for fist MI tasks
\item ES and AS show high agreement (>80\% overlap) in selected channels
\item GS for static gating shows slightly different selection, emphasizing channels with strongest gate values
\end{itemize}

The neurophysiological validity of selected channels is confirmed by their correspondence to known motor imagery brain regions. The bilateral central and centro-parietal electrodes overlying the sensorimotor cortex are expected to show event-related desynchronization (ERD) during MI tasks.

\subsection{Subject-Specific Analysis}

Figure \ref{fig:subject_drops} presents accuracy drops for each subject across different k values using static gating with AS.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.85\textwidth]{results/subject_specific_drops.png}
  \caption{Per-subject accuracy drops for different channel subset sizes (static gating, AS method).}
  \label{fig:subject_drops}
\end{figure}

Subject-specific patterns:
\begin{itemize}
\item S007 (best performer) maintains high accuracy even with k=10 (drop <2\%)
\item S002 (worst performer) shows larger drops (4-6\%), suggesting more distributed signal
\item Most subjects show stable performance for k $\geq$ 15
\item Some subjects (S008, S015) actually improve slightly with channel reduction for certain k values, indicating presence of noisy channels in full set
\item Inter-subject variability is substantial, suggesting value of subject-specific channel selection
\end{itemize}

\subsection{Comparison of Selection Methods}

Figure \ref{fig:method_comparison} compares the three selection methods (ES, AS, GS) across model variants.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.85\textwidth]{results/method_comparison.png}
  \caption{Comparison of channel selection methods across model variants (k=20).}
  \label{fig:method_comparison}
\end{figure}

Method comparison insights:
\begin{itemize}
\item For baseline and adaptive models, ES and AS perform similarly
\item For static gating, GS substantially outperforms ES and AS
\item Early halting shows best performance with ES, suggesting edge importance aligns with halting decisions
\item The optimal selection method depends on the gating architecture used
\item GS provides a more direct path from learned gates to channel selection
\end{itemize}

\subsection{Computational Efficiency}

Channel reduction provides significant computational benefits:
\begin{itemize}
\item With k=10 channels: 84\% reduction in input dimension
\item Training time reduced by approximately 60-70\%
\item Inference time reduced by approximately 65-75\%
\item Memory requirements reduced proportionally
\item Model parameter count reduced (smaller adjacency matrices)
\end{itemize}

These improvements are critical for real-time BCI applications and embedded systems.

\section{Discussion}

\subsection{Main Findings}

Our comprehensive evaluation of EEG-ARNN variants with gating mechanisms for motor imagery classification and channel selection yields several important findings:

\textbf{1. Gating mechanisms maintain competitive accuracy.} All three gating variants (static, adaptive, early halting) achieve performance comparable to or better than the baseline architecture. Adaptive gating achieves the highest mean accuracy (84.60\%), demonstrating the value of input-dependent channel modulation.

\textbf{2. Channel selection with minimal accuracy loss.} Using only 10-20 channels (15-31\% of the original 64), we maintain classification accuracy within 2-4\% of full-channel performance. Static gating with gate selection (GS) even shows slight improvements, suggesting effective noise reduction.

\textbf{3. Neurophysiologically valid channel selection.} Selected channels consistently correspond to sensorimotor and motor planning regions, validating the neurophysiological relevance of our approach. The bilateral central electrodes (C3, C4) and centro-parietal channels are known to reflect motor imagery-related brain activity.

\textbf{4. Method-architecture interactions.} The optimal channel selection method depends on the model architecture. Gate selection (GS) works best for static gating, while edge selection (ES) and aggregation selection (AS) are more effective for baseline and adaptive models.

\textbf{5. Subject-specific variability.} Inter-subject differences are substantial, with optimal channel subsets and sensitivity to reduction varying across subjects. This highlights the importance of subject-specific channel selection in practical BCI systems.

\subsection{Comparison with State-of-the-Art}

Our results compare favorably with existing methods:

\begin{itemize}
\item The original EEG-ARNN paper reports similar full-channel accuracy on BCICIV 2a dataset
\item Our gating mechanisms provide more interpretable channel importance than previous attention-based approaches
\item Channel selection performance (2-4\% drop with k=10-20) is competitive with or better than CSP-based methods
\item The end-to-end learning framework eliminates the need for manual feature engineering or adjacency matrix design
\end{itemize}

Compared to traditional channel selection methods (CSP, mutual information, etc.), our approach offers several advantages:
\begin{itemize}
\item Jointly optimized: Channel importance is learned during classification training
\item Exploits spatial structure: Uses learned functional connectivity graphs
\item Flexible: Multiple selection strategies (ES, AS, GS) can be applied
\item Interpretable: Selected channels align with known neurophysiology
\end{itemize}

\subsection{Practical Implications}

The demonstrated effectiveness of channel reduction has important practical implications for BCI systems:

\textbf{Setup time and comfort:} Reducing from 64 to 10-20 channels significantly decreases setup time and improves user comfort, making BCIs more practical for daily use.

\textbf{Mobile BCIs:} Fewer channels enable smaller, lighter, and more portable BCI devices suitable for ambulatory applications.

\textbf{Cost reduction:} Fewer amplifier channels reduce hardware costs, making BCIs more accessible.

\textbf{Signal quality:} Focusing on most informative channels may improve signal-to-noise ratio by reducing redundant or noisy inputs.

\textbf{Clinical translation:} Simplified setups facilitate clinical adoption and home-based rehabilitation applications.

\subsection{Limitations and Future Work}

Several limitations suggest directions for future research:

\textbf{1. Dataset scope:} While PhysioNet provides clean, well-controlled data, additional validation on other MI datasets (BCICIV, OpenBMI, etc.) and different MI tasks (foot, tongue) would strengthen generalization claims.

\textbf{2. Subject-agnostic selection:} Our current approach performs subject-specific selection. Developing universal channel subsets that work across subjects would simplify practical deployment but may sacrifice some accuracy.

\textbf{3. Online adaptation:} Current methods perform offline channel selection. Online adaptation to changing brain states during extended BCI use could further improve robustness.

\textbf{4. Multi-class tasks:} We focused on binary left-vs-right classification. Extension to multi-class MI tasks may require different channel sets or selection strategies.

\textbf{5. Coupling gates to selection:} Currently, channel selection is post-hoc (train, then select). Direct optimization of discrete channel selections during training (e.g., using differentiable hard selection) could improve accuracy-sparsity trade-offs.

\textbf{6. Interpretability:} While selected channels are neurophysiologically plausible, deeper analysis of learned adjacency matrices and their relationship to known functional connectivity would enhance interpretability.

Future work will address these limitations through:
\begin{itemize}
\item Cross-dataset validation on multiple MI paradigms
\item Development of transfer learning approaches for subject-agnostic selection
\item Online channel selection algorithms with adaptive retraining
\item Extension to multi-class and asynchronous BCI tasks
\item Integration of differentiable selection mechanisms
\item Comprehensive neurophysiological analysis of learned connectivity patterns
\end{itemize}

\subsection{Conclusion}

This work demonstrates that graph convolutional neural networks with gating mechanisms provide an effective framework for end-to-end motor imagery classification and channel selection. By jointly learning temporal features, spatial (functional) connectivity, and channel importance, our approach achieves strong classification accuracy while enabling substantial dimensionality reduction. The selected channels align with known motor imagery neurophysiology, validating the biological plausibility of the learned representations.

The combination of static gating architecture with gate-based selection offers the best accuracy-sparsity trade-off, achieving comparable or better performance than full-channel models using only 15-31\% of channels. These results have important implications for practical BCI systems, potentially enabling more comfortable, portable, and cost-effective brain-computer interfaces for motor rehabilitation and assistive applications.

\section*{Acknowledgements}
We thank the maintainers of the PhysioNet EEG Motor Movement/Imagery dataset and the MNE community for providing excellent open-source tools for EEG analysis.

%
% ---- Bibliography ----
% \bibliographystyle{splncs04}
% \bibliography{refs}
\begin{thebibliography}{99}

\bibitem{bci1}
Wolpaw, J.R., Birbaumer, N., McFarland, D.J., Pfurtscheller, G., Vaughan, T.M.: Brain-computer interfaces for communication and control. Clinical Neurophysiology 113(6), 767--791 (2002)

\bibitem{bci2}
Millán, J.D.R., et al.: Combining brain-computer interfaces and assistive technologies: State-of-the-art and challenges. Frontiers in Neuroscience 4, 161 (2010)

\bibitem{bci3}
Ang, K.K., et al.: A large clinical study on the ability of stroke patients to use an EEG-based motor imagery brain-computer interface. Clinical EEG and Neuroscience 42(4), 253--258 (2011)

\bibitem{bci4}
Chaudhary, U., Birbaumer, N., Ramos-Murguialday, A.: Brain-computer interfaces for communication and rehabilitation. Nature Reviews Neurology 12(9), 513--525 (2016)

\bibitem{mi1}
Pfurtscheller, G., Neuper, C.: Motor imagery and direct brain-computer communication. Proceedings of the IEEE 89(7), 1123--1134 (2001)

\bibitem{mi2}
Neuper, C., Scherer, R., Wriessnegger, S., Pfurtscheller, G.: Motor imagery and action observation: Modulation of sensorimotor brain rhythms during mental control of a brain-computer interface. Clinical Neurophysiology 120(2), 239--247 (2009)

\bibitem{bayes}
Muller, K.R., et al.: Machine learning for real-time single-trial EEG-analysis: From brain-computer interfacing to mental state monitoring. Journal of Neuroscience Methods 167(1), 82--90 (2008)

\bibitem{eegnet}
Lawhern, V.J., Solon, A.J., Waytowich, N.R., Gordon, S.M., Hung, C.P., Lance, B.J.: EEGNet: A compact convolutional neural network for EEG-based brain-computer interfaces. Journal of Neural Engineering 15(5), 056013 (2018)

\bibitem{cnn1}
Schirrmeister, R.T., et al.: Deep learning with convolutional neural networks for EEG decoding and visualization. Human Brain Mapping 38(11), 5391--5420 (2017)

\bibitem{gcn1}
Kipf, T.N., Welling, M.: Semi-supervised classification with graph convolutional networks. In: International Conference on Learning Representations (ICLR) (2017)

\bibitem{gcn2}
Defferrard, M., Bresson, X., Vandergheynst, P.: Convolutional neural networks on graphs with fast localized spectral filtering. In: Advances in Neural Information Processing Systems (NIPS), pp. 3844--3852 (2016)

\bibitem{chansel1}
Arvaneh, M., Guan, C., Ang, K.K., Quek, C.: Optimizing the channel selection and classification accuracy in EEG-based BCI. IEEE Transactions on Biomedical Engineering 58(6), 1865--1873 (2011)

\bibitem{csp1}
Ramoser, H., Muller-Gerking, J., Pfurtscheller, G.: Optimal spatial filtering of single trial EEG during imagined hand movement. IEEE Transactions on Rehabilitation Engineering 8(4), 441--446 (2000)

\bibitem{csp2}
Ang, K.K., Chin, Z.Y., Wang, C., Guan, C., Zhang, H.: Filter bank common spatial pattern algorithm on BCI competition IV datasets 2a and 2b. Frontiers in Neuroscience 6, 39 (2012)

\bibitem{sparse1}
Huang, D., et al.: Differences in cortical areas of stroke patients with spasticity and those without spasticity during motor imagery. NeuroRehabilitation 34(3), 533--542 (2014)

\bibitem{physionet}
Goldberger, A.L., et al.: PhysioBank, PhysioToolkit, and PhysioNet: Components of a new research resource for complex physiologic signals. Circulation 101(23), e215--e220 (2000)

\bibitem{mne}
Gramfort, A., et al.: MEG and EEG data analysis with MNE-Python. Frontiers in Neuroscience 7, 267 (2013)

\end{thebibliography}

\end{document}
