{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PhysioNet - CARMv2 (Standalone)\n",
    "Standalone training with upgraded CARMv2. No imports from project modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SHIVAPREETHAM ROHITH\\AppData\\Roaming\\Python\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json, random, warnings\n",
    "from pathlib import Path\n",
    "import numpy as np, pandas as pd\n",
    "import torch, torch.nn as nn, torch.nn.functional as F, torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm.auto import tqdm\n",
    "import mne\n",
    "import matplotlib.pyplot as plt, seaborn as sns\n",
    "warnings.filterwarnings('ignore'); sns.set_context('notebook', font_scale=1.0)\n",
    "def seed(s=42):\n",
    "    random.seed(s); np.random.seed(s); torch.manual_seed(s); torch.cuda.manual_seed_all(s)\n",
    "    torch.backends.cudnn.deterministic=True; torch.backends.cudnn.benchmark=False\n",
    "seed(42); device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'); print('Device:', device)\n",
    "EXPERIMENT_CONFIG={\n",
    " 'data':{'preprocessed_dir':Path('data/physionet/derived/preprocessed'),'index_file':Path('data/physionet/derived/physionet_preprocessed_index.csv'),'selected_classes':[1,2],'tmin':-1.0,'tmax':5.0,'baseline':(-0.5,0)},\n",
    " 'model':{'hidden_dim':40,'epochs':10,'learning_rate':1e-3,'batch_size':32,'n_folds':2,'patience':8},\n",
    " 'carmv2':{'topk_k':8,'lambda_feat':0.3,'hop_alpha':0.5,'edge_dropout':0.1,'use_pairnorm':True,'use_residual':True,'low_rank_r':0},\n",
    " 'output':{'results_dir':Path('results'),'results_file':'trial_carmv2_subject_results.csv','channel_selection_file':'trial_carmv2_channel_selection_results.csv','comparison_file':'trial_carmv2_vs_baseline.csv','results_summary_figure':'trial_carmv2_results_summary.png','adjacency_prefix':'trial_carmv2_adjacency'},\n",
    " 'max_subjects':5,'min_runs_per_subject':10}\n",
    "EXPERIMENT_CONFIG['output']['results_dir'].mkdir(exist_ok=True,parents=True)\n",
    "print(json.dumps(EXPERIMENT_CONFIG,indent=2,default=str))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGDataset(Dataset):\n",
    "    def __init__(self,x,y):\n",
    "        self.x=torch.FloatTensor(x).unsqueeze(1)\n",
    "        self.y=torch.LongTensor(y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self,i):\n",
    "        return self.x[i],self.y[i]\n",
    "\n",
    "def _row_path_value(row):\n",
    "    for c in ['preprocessed_path','preprocessed_file','file','filepath','path','fif_path','raw_path']:\n",
    "        if c in row and pd.notnull(row[c]):\n",
    "            v=str(row[c])\n",
    "            if v.lower().endswith('.fif'):\n",
    "                return v\n",
    "    return None\n",
    "\n",
    "def load_preprocessed_data(fif,tmin,tmax,baseline):\n",
    "    raw=mne.io.read_raw_fif(fif,preload=True,verbose='ERROR')\n",
    "    try:\n",
    "        ev=mne.find_events(raw,verbose='ERROR')\n",
    "        ids={f'T{i}':i for i in np.unique(ev[:,2])}\n",
    "        assert len(ev)>0\n",
    "    except Exception:\n",
    "        ev,ids=mne.events_from_annotations(raw,verbose='ERROR')\n",
    "    if len(ev)==0:\n",
    "        return None,None,raw.ch_names\n",
    "    ep=mne.Epochs(raw,ev,event_id=ids,tmin=tmin,tmax=tmax,baseline=baseline,preload=True,verbose='ERROR')\n",
    "    return ep.get_data(), ep.events[:,2], raw.ch_names\n",
    "\n",
    "def filter_classes(x,y,cls):\n",
    "    m=np.isin(y,cls)\n",
    "    y=y[m]\n",
    "    x=x[m]\n",
    "    mapd={o:n for n,o in enumerate(sorted(cls))}\n",
    "    y=np.array([mapd[int(a)] for a in y],dtype=np.int64)\n",
    "    return x,y\n",
    "\n",
    "def normalize(x):\n",
    "    mu=x.mean(axis=(0,2),keepdims=True)\n",
    "    sd=x.std(axis=(0,2),keepdims=True)+1e-8\n",
    "    return (x-mu)/sd\n",
    "\n",
    "def load_subject(index_df,subject,cfg):\n",
    "    df=index_df[(index_df['subject']==subject)&(index_df['status']=='success')].copy()\n",
    "    xs,ys=[],[]\n",
    "    chn=None\n",
    "    tmin=cfg['data']['tmin']\n",
    "    tmax=cfg['data']['tmax']\n",
    "    base=tuple(cfg['data']['baseline'])\n",
    "    cls=cfg['data']['selected_classes']\n",
    "    for _,r in df.iterrows():\n",
    "        p=_row_path_value(r)\n",
    "        if not p:\n",
    "            continue\n",
    "        x,y,ch=load_preprocessed_data(p,tmin,tmax,base)\n",
    "        if x is None:\n",
    "            continue\n",
    "        x,y=filter_classes(x,y,cls)\n",
    "        if len(y)==0:\n",
    "            continue\n",
    "        chn=chn or ch\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    if len(xs)==0:\n",
    "        return None,None,chn\n",
    "    return np.concatenate(xs,0), np.concatenate(ys,0), chn\n",
    "\n",
    "def subjects_with_min_runs(index_df,min_runs):\n",
    "    df=index_df[index_df['status']=='success'].copy()\n",
    "    counts=df.groupby('subject')['run'].count() if 'run' in df.columns else df.groupby('subject').size()\n",
    "    return list(counts[counts>=int(min_runs)].index)\n",
    "\n",
    "index_df=pd.read_csv(EXPERIMENT_CONFIG['data']['index_file'])\n",
    "subjects=subjects_with_min_runs(index_df,EXPERIMENT_CONFIG['min_runs_per_subject'])[:EXPERIMENT_CONFIG['max_subjects']]\n",
    "print('Subjects:',subjects)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairnorm(x,node_dim=2,eps=1e-6):\n",
    "    m=x.mean(dim=node_dim,keepdim=True)\n",
    "    xc=x-m\n",
    "    v=(xc*xc).mean(dim=node_dim,keepdim=True)\n",
    "    return xc/torch.sqrt(v+eps)\n",
    "\n",
    "def build_feat_topk_adj(x,k):\n",
    "    B,H,C,T=x.shape\n",
    "    E=x.permute(2,1,0,3).contiguous().view(C,H,B*T).mean(2)\n",
    "    En=F.normalize(E,p=2,dim=1)\n",
    "    S=(En@En.t()).clamp_min(0.0)\n",
    "    k=max(1,min(int(k),C))\n",
    "    vals,idx=torch.topk(S,k,dim=1)\n",
    "    M=torch.zeros_like(S)\n",
    "    M.scatter_(1,idx,1.0)\n",
    "    A=S*M\n",
    "    A=torch.softmax(A,1)\n",
    "    A=0.5*(A+A.t())\n",
    "    return A\n",
    "\n",
    "class CARMv2(nn.Module):\n",
    "    def __init__(self,C,H,cfg):\n",
    "        super().__init__()\n",
    "        self.C=C\n",
    "        self.H=H\n",
    "        self.k=int(cfg['topk_k'])\n",
    "        self.lf=float(cfg['lambda_feat'])\n",
    "        self.ha=float(cfg['hop_alpha'])\n",
    "        self.ed=float(cfg['edge_dropout'])\n",
    "        self.pn=bool(cfg['use_pairnorm'])\n",
    "        self.res=bool(cfg['use_residual'])\n",
    "        r=int(cfg['low_rank_r'])\n",
    "        if r>0:\n",
    "            self.B=nn.Parameter(torch.empty(C,r))\n",
    "            nn.init.xavier_uniform_(self.B)\n",
    "            self.W=None\n",
    "        else:\n",
    "            self.W=nn.Parameter(torch.empty(C,C))\n",
    "            nn.init.xavier_uniform_(self.W)\n",
    "            self.B=None\n",
    "        self.th=nn.Linear(H,H,bias=False)\n",
    "        self.bn=nn.BatchNorm2d(H)\n",
    "        self.act=nn.ELU()\n",
    "        self.last=None\n",
    "    \n",
    "    def _learned(self,dev):\n",
    "        W=self.W if self.B is None else (self.B@self.B.t())\n",
    "        A=torch.sigmoid(W)\n",
    "        A=0.5*(A+A.t())\n",
    "        I=torch.eye(self.C,device=dev,dtype=A.dtype)\n",
    "        At=A+I\n",
    "        d=torch.pow(At.sum(1).clamp_min(1e-6),-0.5)\n",
    "        D=torch.diag(d)\n",
    "        return D@At@D\n",
    "    \n",
    "    def forward(self,x):\n",
    "        B,H,C,T=x.shape\n",
    "        Al=self._learned(x.device)\n",
    "        A2=Al@Al\n",
    "        Ah=(1-self.ha)*Al+self.ha*A2\n",
    "        Af=build_feat_topk_adj(x,self.k)\n",
    "        A=(1-self.lf)*Ah+self.lf*Af\n",
    "        if self.training and self.ed>0:\n",
    "            M=(torch.rand_like(A)>self.ed).float()\n",
    "            A=0.5*((A*M)+(A*M).t())\n",
    "            A=A+torch.eye(C,device=A.device,dtype=A.dtype)\n",
    "        d=torch.pow(A.sum(1).clamp_min(1e-6),-0.5)\n",
    "        D=torch.diag(d)\n",
    "        A=D@A@D\n",
    "        xb=x.permute(0,3,2,1).contiguous().view(B*T,C,H)\n",
    "        xg=A@xb\n",
    "        xg=self.th(xg)\n",
    "        xg=xg.view(B,T,C,H).permute(0,3,2,1)\n",
    "        out=xg+x if self.res else xg\n",
    "        out=pairnorm(out,2) if self.pn else out\n",
    "        out=self.bn(out)\n",
    "        out=self.act(out)\n",
    "        self.last={'learned':Al.detach().cpu().numpy(),'effective':A.detach().cpu().numpy()}\n",
    "        return out\n",
    "    \n",
    "    def get_adjs(self):\n",
    "        return self.last or {}\n",
    "\n",
    "class TFEM(nn.Module):\n",
    "    def __init__(self,i,o,k=16,pool=True):\n",
    "        super().__init__()\n",
    "        self.pool=pool\n",
    "        self.cv=nn.Conv2d(i,o,kernel_size=(1,k),padding=(0,k//2),bias=False)\n",
    "        self.bn=nn.BatchNorm2d(o)\n",
    "        self.act=nn.ELU()\n",
    "        self.pl=nn.AvgPool2d(kernel_size=(1,2)) if pool else None\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x=self.act(self.bn(self.cv(x)))\n",
    "        return self.pl(x) if self.pool else x\n",
    "\n",
    "class EEGARNN_CARMv2(nn.Module):\n",
    "    def __init__(self,C,T,K,H,cfg):\n",
    "        super().__init__()\n",
    "        self.t1=TFEM(1,H,16,False)\n",
    "        self.g1=CARMv2(C,H,cfg)\n",
    "        self.t2=TFEM(H,H,16,True)\n",
    "        self.g2=CARMv2(C,H,cfg)\n",
    "        self.t3=TFEM(H,H,16,True)\n",
    "        self.g3=CARMv2(C,H,cfg)\n",
    "        with torch.no_grad():\n",
    "            ft=self._f(torch.zeros(1,1,C,T))\n",
    "            fs=ft.view(1,-1).size(1)\n",
    "        self.fc1=nn.Linear(fs,256)\n",
    "        self.do=nn.Dropout(0.5)\n",
    "        self.fc2=nn.Linear(256,K)\n",
    "    \n",
    "    def _f(self,x):\n",
    "        x=self.g1(self.t1(x))\n",
    "        x=self.g2(self.t2(x))\n",
    "        x=self.g3(self.t3(x))\n",
    "        return x\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x=self._f(x)\n",
    "        x=x.view(x.size(0),-1)\n",
    "        x=F.relu(self.fc1(x))\n",
    "        x=self.do(x)\n",
    "        return self.fc2(x)\n",
    "    \n",
    "    def get_final_adjs(self):\n",
    "        return self.g3.get_adjs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(m,ld,crit,opt,dev):\n",
    "    m.train()\n",
    "    tl=0.0\n",
    "    ap,al=[],[]\n",
    "    for x,y in ld:\n",
    "        x,y=x.to(dev),y.to(dev)\n",
    "        opt.zero_grad()\n",
    "        lg=m(x)\n",
    "        ls=crit(lg,y)\n",
    "        ls.backward()\n",
    "        opt.step()\n",
    "        tl+=ls.item()\n",
    "        ap+=torch.argmax(lg,1).cpu().tolist()\n",
    "        al+=y.cpu().tolist()\n",
    "    return tl/max(1,len(ld)), accuracy_score(al,ap)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(m,ld,crit,dev):\n",
    "    m.eval()\n",
    "    tl=0.0\n",
    "    ap,al=[],[]\n",
    "    for x,y in ld:\n",
    "        x,y=x.to(dev),y.to(dev)\n",
    "        lg=m(x)\n",
    "        ls=crit(lg,y)\n",
    "        tl+=ls.item()\n",
    "        ap+=torch.argmax(lg,1).cpu().tolist()\n",
    "        al+=y.cpu().tolist()\n",
    "    return tl/max(1,len(ld)), accuracy_score(al,ap), ap, al\n",
    "\n",
    "def train_model(m,tr,va,dev,ep,lr,pt):\n",
    "    crit=nn.CrossEntropyLoss()\n",
    "    opt=optim.Adam(m.parameters(),lr=lr,weight_decay=1e-4)\n",
    "    try:\n",
    "        sch=optim.lr_scheduler.ReduceLROnPlateau(opt,mode='min',factor=0.5,patience=3,verbose=False)\n",
    "    except TypeError:\n",
    "        sch=optim.lr_scheduler.ReduceLROnPlateau(opt,mode='min',factor=0.5,patience=3)\n",
    "    best_acc=0.0\n",
    "    best=None\n",
    "    noimp=0\n",
    "    hist={'train_loss':[],'train_acc':[],'val_loss':[],'val_acc':[]}\n",
    "    for _ in range(ep):\n",
    "        tl,ta=train_epoch(m,tr,crit,opt,dev)\n",
    "        vl,va_acc,_,_=evaluate(m,va,crit,dev)\n",
    "        hist['train_loss']+=[tl]\n",
    "        hist['train_acc']+=[ta]\n",
    "        hist['val_loss']+=[vl]\n",
    "        hist['val_acc']+=[va_acc]\n",
    "        try:\n",
    "            sch.step(vl)\n",
    "        except Exception:\n",
    "            pass\n",
    "        if va_acc>best_acc:\n",
    "            best_acc=va_acc\n",
    "            best={k:v.detach().cpu() if hasattr(v,'detach') else v for k,v in m.state_dict().items()}\n",
    "            noimp=0\n",
    "        else:\n",
    "            noimp+=1\n",
    "        if noimp>=pt:\n",
    "            break\n",
    "    if best is None:\n",
    "        best={k:v.detach().cpu() if hasattr(v,'detach') else v for k,v in m.state_dict().items()}\n",
    "    m.load_state_dict(best)\n",
    "    return hist,best\n",
    "\n",
    "def cross_validate_subject(x,y,chn,T,K,dev,cfg):\n",
    "    C=x.shape[1]\n",
    "    skf=StratifiedKFold(n_splits=int(cfg['model']['n_folds']),shuffle=True,random_state=42)\n",
    "    bs=int(cfg['model']['batch_size'])\n",
    "    ep=int(cfg['model']['epochs'])\n",
    "    lr=float(cfg['model']['learning_rate'])\n",
    "    pt=int(cfg['model']['patience'])\n",
    "    folds=[]\n",
    "    adjs=[]\n",
    "    for f,(tr,va) in enumerate(skf.split(x,y)):\n",
    "        Xtr,Xva=normalize(x[tr]),normalize(x[va])\n",
    "        Ytr,Yva=y[tr],y[va]\n",
    "        trl=DataLoader(EEGDataset(Xtr,Ytr),batch_size=bs,shuffle=True,num_workers=0)\n",
    "        val=DataLoader(EEGDataset(Xva,Yva),batch_size=bs,shuffle=False,num_workers=0)\n",
    "        m=EEGARNN_CARMv2(C,T,K,cfg['model']['hidden_dim'],cfg['carmv2']).to(dev)\n",
    "        h,b=train_model(m,trl,val,dev,ep,lr,pt)\n",
    "        m.load_state_dict(b)\n",
    "        _,acc,_,_=evaluate(m,val,nn.CrossEntropyLoss(),dev)\n",
    "        ad=m.get_final_adjs().get('learned',None)\n",
    "        adjs.append(ad)\n",
    "        folds.append({'fold':f,'val_acc':acc,'history':h})\n",
    "    av=float(np.mean([z['val_acc'] for z in folds]))\n",
    "    sd=float(np.std([z['val_acc'] for z in folds]))\n",
    "    A=np.mean(np.stack([a for a in adjs if a is not None],0),0) if any(a is not None for a in adjs) else None\n",
    "    return {'fold_results':folds,'avg_accuracy':av,'std_accuracy':sd,'adjacency_matrix':A,'channel_names':chn}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelSelector:\n",
    "    def __init__(self,A,names):\n",
    "        self.A=A\n",
    "        self.names=np.array(names)\n",
    "        self.C=A.shape[0]\n",
    "    \n",
    "    def edge_selection(self,k):\n",
    "        E=[]\n",
    "        for i in range(self.C):\n",
    "            for j in range(i+1,self.C):\n",
    "                E.append((i,j,abs(self.A[i,j])+abs(self.A[j,i])))\n",
    "        E.sort(key=lambda t:t[2],reverse=True)\n",
    "        top=E[:int(k)]\n",
    "        idx=sorted(set([i for i,_,_ in top]+[j for _,j,_ in top]))\n",
    "        return self.names[idx].tolist(), np.array(idx)\n",
    "    \n",
    "    def aggregation_selection(self,k):\n",
    "        s=np.sum(np.abs(self.A),1)\n",
    "        idx=np.sort(np.argsort(s)[-int(k):])\n",
    "        return self.names[idx].tolist(), idx\n",
    "\n",
    "def viz_adj(A,names,path=None):\n",
    "    import seaborn as sns, matplotlib.pyplot as plt\n",
    "    plt.figure(figsize=(10,8))\n",
    "    sns.heatmap(A,xticklabels=names,yticklabels=names,cmap='RdYlGn',center=0,square=True,linewidths=0.4)\n",
    "    plt.title('Adjacency (CARMv2)')\n",
    "    plt.tight_layout()\n",
    "    if path:\n",
    "        plt.savefig(path,dpi=200,bbox_inches='tight')\n",
    "        plt.close()\n",
    "        return path\n",
    "\n",
    "results_dir=EXPERIMENT_CONFIG['output']['results_dir']\n",
    "all_results=[]\n",
    "recs=[]\n",
    "\n",
    "for subj in tqdm(subjects,desc='Training (CARMv2)'):\n",
    "    X,Y,names=load_subject(index_df,subj,EXPERIMENT_CONFIG)\n",
    "    if X is None or len(Y)==0:\n",
    "        print('Skip',subj)\n",
    "        continue\n",
    "    C,T=X.shape[1],X.shape[2]\n",
    "    K=len(set(EXPERIMENT_CONFIG['data']['selected_classes']))\n",
    "    res=cross_validate_subject(X,Y,names,T,K,device,EXPERIMENT_CONFIG)\n",
    "    all_results.append({\n",
    "        'subject':subj,\n",
    "        'num_trials':X.shape[0],\n",
    "        'num_channels':C,\n",
    "        'carmv2_acc':res['avg_accuracy'],\n",
    "        'carmv2_std':res['std_accuracy'],\n",
    "        'adjacency_matrix':res['adjacency_matrix'],\n",
    "        'channel_names':res['channel_names'],\n",
    "        'fold_results':res['fold_results']\n",
    "    })\n",
    "    recs.append({\n",
    "        'subject':subj,\n",
    "        'num_trials':X.shape[0],\n",
    "        'num_channels':C,\n",
    "        'carmv2_acc':res['avg_accuracy'],\n",
    "        'carmv2_std':res['std_accuracy']\n",
    "    })\n",
    "\n",
    "rdf=pd.DataFrame.from_records(recs)\n",
    "print('Subjects trained:',len(rdf))\n",
    "\n",
    "if len(rdf)>0:\n",
    "    print(f\"Mean acc: {rdf['carmv2_acc'].mean():.4f} ± {rdf['carmv2_acc'].std():.4f}\")\n",
    "    print('Best:',rdf.loc[rdf['carmv2_acc'].idxmax(),'subject'],'Worst:',rdf.loc[rdf['carmv2_acc'].idxmin(),'subject'])\n",
    "    p=results_dir/EXPERIMENT_CONFIG['output']['results_file']\n",
    "    rdf.to_csv(p,index=False)\n",
    "    print('Saved results to',p)\n",
    "    cfgp=results_dir/'experiment_config_carmv2.json'\n",
    "    open(cfgp,'w').write(json.dumps(EXPERIMENT_CONFIG,indent=2,default=str))\n",
    "    print('Saved config to',cfgp)\n",
    "    bi=int(np.argmax([r['carmv2_acc'] for r in all_results]))\n",
    "    br=all_results[bi]\n",
    "    A=br['adjacency_matrix']\n",
    "    names=br['channel_names']\n",
    "    if A is not None and names is not None:\n",
    "        ap=results_dir/f\"{EXPERIMENT_CONFIG['output']['adjacency_prefix']}_{br['subject']}.png\"\n",
    "        viz_adj(A,names,ap)\n",
    "        print('Adjacency saved to',ap)\n",
    "        sel=[]\n",
    "        cs=ChannelSelector(A,names)\n",
    "        for m in ['ES','AS']:\n",
    "            for k in [10,15,20]:\n",
    "                n,i=(cs.edge_selection(k) if m=='ES' else cs.aggregation_selection(k))\n",
    "                sel.append({'subject':br['subject'],'method':m,'k':k,'channels':n})\n",
    "                print(f'Selected ({m},k={k}):',n)\n",
    "        chp=results_dir/EXPERIMENT_CONFIG['output']['channel_selection_file']\n",
    "        pd.DataFrame(sel).to_csv(chp,index=False)\n",
    "        print('Channel selection saved to',chp)\n",
    "    fig,ax=plt.subplots(2,2,figsize=(10,8))\n",
    "    ax[0,0].hist(rdf['carmv2_acc'],bins=15,color='steelblue',alpha=0.8)\n",
    "    ax[0,0].set_title('Acc Dist')\n",
    "    ax[0,1].scatter(rdf['num_trials'],rdf['carmv2_acc'])\n",
    "    ax[0,1].set_title('#Trials vs Acc')\n",
    "    top=rdf.nlargest(min(10,len(rdf)),'carmv2_acc')\n",
    "    ax[1,0].barh(range(len(top)),top['carmv2_acc'])\n",
    "    ax[1,0].set_yticks(range(len(top)))\n",
    "    ax[1,0].set_yticklabels(top['subject'])\n",
    "    ax[1,0].invert_yaxis()\n",
    "    ax[1,0].set_title('Top Subjects')\n",
    "    srt=rdf.sort_values('carmv2_acc')\n",
    "    ax[1,1].plot(range(len(srt)),srt['carmv2_acc'],marker='o')\n",
    "    ax[1,1].set_title('Ranking')\n",
    "    plt.tight_layout()\n",
    "    fp=results_dir/EXPERIMENT_CONFIG['output']['results_summary_figure']\n",
    "    plt.savefig(fp,dpi=200,bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print('Saved summary to',fp)\n",
    "\n",
    "bp=results_dir/'subject_results.csv'\n",
    "cp=results_dir/EXPERIMENT_CONFIG['output']['results_file']\n",
    "if Path(bp).exists() and Path(cp).exists():\n",
    "    base=pd.read_csv(bp)\n",
    "    carm=pd.read_csv(cp)\n",
    "    m=base[['subject','all_channels_acc','all_channels_std']].merge(carm[['subject','carmv2_acc','carmv2_std']],on='subject',how='inner')\n",
    "    m['accuracy_delta']=m['carmv2_acc']-m['all_channels_acc']\n",
    "    mp=results_dir/EXPERIMENT_CONFIG['output']['comparison_file']\n",
    "    m.rename(columns={'all_channels_acc':'baseline_acc','all_channels_std':'baseline_std'},inplace=True)\n",
    "    m.to_csv(mp,index=False)\n",
    "    print('Saved comparison to',mp)\n",
    "else:\n",
    "    print('No baseline/carmv2 results yet for comparison')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
