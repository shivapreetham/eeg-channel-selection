{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BCI Competition IV: CNN vs GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mne\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load BCI Competition IV Dataset 2a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\contextlib.py:144: RuntimeWarning: Channel names are not unique, found duplicates for: {'EEG'}. Applying running numbers for duplicates.\n",
      "  next(self.gen)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 1: 288 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\contextlib.py:144: RuntimeWarning: Channel names are not unique, found duplicates for: {'EEG'}. Applying running numbers for duplicates.\n",
      "  next(self.gen)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 2: 288 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\contextlib.py:144: RuntimeWarning: Channel names are not unique, found duplicates for: {'EEG'}. Applying running numbers for duplicates.\n",
      "  next(self.gen)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 3: 288 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\contextlib.py:144: RuntimeWarning: Channel names are not unique, found duplicates for: {'EEG'}. Applying running numbers for duplicates.\n",
      "  next(self.gen)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 4: 288 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\contextlib.py:144: RuntimeWarning: Channel names are not unique, found duplicates for: {'EEG'}. Applying running numbers for duplicates.\n",
      "  next(self.gen)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 5: 288 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\contextlib.py:144: RuntimeWarning: Channel names are not unique, found duplicates for: {'EEG'}. Applying running numbers for duplicates.\n",
      "  next(self.gen)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 6: 288 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\contextlib.py:144: RuntimeWarning: Channel names are not unique, found duplicates for: {'EEG'}. Applying running numbers for duplicates.\n",
      "  next(self.gen)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 7: 288 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\contextlib.py:144: RuntimeWarning: Channel names are not unique, found duplicates for: {'EEG'}. Applying running numbers for duplicates.\n",
      "  next(self.gen)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 8: 288 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\contextlib.py:144: RuntimeWarning: Channel names are not unique, found duplicates for: {'EEG'}. Applying running numbers for duplicates.\n",
      "  next(self.gen)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 9: 288 epochs\n",
      "\n",
      "Total: (2592, 25, 1001) - 4 classes\n"
     ]
    }
   ],
   "source": [
    "data_list = []\n",
    "labels_list = []\n",
    "\n",
    "for subject in range(1, 10):\n",
    "    train_file = f'BCI_2a/A0{subject}T.gdf'\n",
    "    raw = mne.io.read_raw_gdf(train_file, preload=True, verbose=False)\n",
    "    events, event_id = mne.events_from_annotations(raw, verbose=False)\n",
    "    \n",
    "    mi_event_ids = {k: v for k, v in event_id.items() if k in ['769', '770', '771', '772']}\n",
    "    \n",
    "    if len(mi_event_ids) > 0:\n",
    "        epochs = mne.Epochs(raw, events, event_id=mi_event_ids,\n",
    "                           tmin=0, tmax=4, baseline=None, preload=True, verbose=False)\n",
    "        \n",
    "        data = epochs.get_data()\n",
    "        labels = epochs.events[:, -1]\n",
    "        label_mapping = {v: i for i, (k, v) in enumerate(sorted(mi_event_ids.items(), key=lambda x: x[1]))}\n",
    "        labels = np.array([label_mapping[l] for l in labels])\n",
    "        \n",
    "        data_list.append(data)\n",
    "        labels_list.append(labels)\n",
    "        print(f'Subject {subject}: {data.shape[0]} epochs')\n",
    "\n",
    "X = np.concatenate(data_list, axis=0)\n",
    "y = np.concatenate(labels_list, axis=0)\n",
    "\n",
    "print(f'\\nTotal: {X.shape} - 4 classes')\n",
    "num_channels = X.shape[1]\n",
    "num_timepoints = X.shape[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (2073, 25, 1001), Test: (519, 25, 1001)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train.reshape(-1, num_timepoints)).reshape(-1, num_channels, num_timepoints)\n",
    "X_test = scaler.transform(X_test.reshape(-1, num_timepoints)).reshape(-1, num_channels, num_timepoints)\n",
    "\n",
    "y_train_cat = keras.utils.to_categorical(y_train, 4)\n",
    "y_test_cat = keras.utils.to_categorical(y_test, 4)\n",
    "\n",
    "print(f'Train: {X_train.shape}, Test: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Graph Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjacency: (25, 25), Laplacian: (25, 25)\n"
     ]
    }
   ],
   "source": [
    "def compute_adjacency_from_pearson(data):\n",
    "    n_epochs, n_channels, n_timepoints = data.shape\n",
    "    data_reshaped = data.transpose(1, 0, 2).reshape(n_channels, -1)\n",
    "    correlation_matrix = np.corrcoef(data_reshaped)\n",
    "    adjacency = np.abs(correlation_matrix)\n",
    "    np.fill_diagonal(adjacency, 1.0)\n",
    "    return adjacency.astype(np.float32)\n",
    "\n",
    "def compute_laplacian(adjacency):\n",
    "    D = np.sum(adjacency, axis=1)\n",
    "    D_sqrt_inv = np.diag(1.0 / np.sqrt(D + 1e-6))\n",
    "    L = np.eye(len(adjacency)) - D_sqrt_inv @ adjacency @ D_sqrt_inv\n",
    "    lambda_max = np.linalg.eigvalsh(L)[-1]\n",
    "    L_rescaled = (2.0 / lambda_max) * L - np.eye(len(L))\n",
    "    return L_rescaled.astype(np.float32)\n",
    "\n",
    "adjacency = compute_adjacency_from_pearson(X_train)\n",
    "L_rescaled = compute_laplacian(adjacency)\n",
    "print(f'Adjacency: {adjacency.shape}, Laplacian: {L_rescaled.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chebyshev Graph Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChebyshevGraphConv(layers.Layer):\n",
    "    def __init__(self, num_filters, K=3, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_filters = num_filters\n",
    "        self.K = K\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.theta = [self.add_weight(shape=(input_shape[0][-1], self.num_filters),\n",
    "                                     initializer='glorot_uniform', name=f'theta_{k}')\n",
    "                     for k in range(self.K)]\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, L_rescaled = inputs\n",
    "        Tx_0 = x\n",
    "        Tx_1 = tf.matmul(L_rescaled, x)\n",
    "        out = tf.matmul(Tx_0, self.theta[0])\n",
    "        if self.K > 1:\n",
    "            out += tf.matmul(Tx_1, self.theta[1])\n",
    "        for k in range(2, self.K):\n",
    "            Tx_2 = 2 * tf.matmul(L_rescaled, Tx_1) - Tx_0\n",
    "            out += tf.matmul(Tx_2, self.theta[k])\n",
    "            Tx_0, Tx_1 = Tx_1, Tx_2\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN: 648,132 parameters\n"
     ]
    }
   ],
   "source": [
    "def create_cnn(input_shape, num_classes):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = layers.Permute((2, 1))(inputs)\n",
    "    \n",
    "    x = layers.Conv1D(64, 50, activation='relu', padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    \n",
    "    x = layers.Conv1D(128, 25, activation='relu', padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    \n",
    "    x = layers.Conv1D(256, 10, activation='relu', padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    \n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "cnn_model = create_cnn((num_channels, num_timepoints), 4)\n",
    "print(f'CNN: {cnn_model.count_params():,} parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GCN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\SHIVAPREETHAM ROHITH\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:232: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "GCN: 507,716 parameters\n"
     ]
    }
   ],
   "source": [
    "def create_gcn(input_shape, num_classes, num_channels, L_rescaled):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = layers.Permute((2, 1))(inputs)\n",
    "    \n",
    "    x = layers.Conv1D(64, 50, activation='relu', padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    \n",
    "    x = layers.Conv1D(128, 25, activation='relu', padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    cnn_features = x\n",
    "    \n",
    "    channel_input = layers.Input(shape=(num_channels, num_timepoints))\n",
    "    channel_avg = layers.Lambda(lambda x: tf.reduce_mean(x, axis=-1, keepdims=True))(channel_input)\n",
    "    \n",
    "    L_tensor = tf.constant(L_rescaled, dtype=tf.float32)\n",
    "    graph_features = ChebyshevGraphConv(64, K=2)([channel_avg, L_tensor])\n",
    "    graph_features = layers.Flatten()(graph_features)\n",
    "    \n",
    "    combined = layers.Concatenate()([cnn_features, graph_features])\n",
    "    x = layers.Dense(128, activation='relu')(combined)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = keras.Model([inputs, channel_input], outputs)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "gcn_model = create_gcn((num_channels, num_timepoints), 4, num_channels, L_rescaled)\n",
    "print(f'GCN: {gcn_model.count_params():,} parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    }
   ],
   "source": [
    "cnn_history = cnn_model.fit(\n",
    "    X_train, y_train_cat,\n",
    "    validation_split=0.2,\n",
    "    epochs=30,\n",
    "    batch_size=64,\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),\n",
    "        keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcn_history = gcn_model.fit(\n",
    "    [X_train, X_train], y_train_cat,\n",
    "    validation_split=0.2,\n",
    "    epochs=30,\n",
    "    batch_size=64,\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),\n",
    "        keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_loss, cnn_acc = cnn_model.evaluate(X_test, y_test_cat, verbose=0)\n",
    "gcn_loss, gcn_acc = gcn_model.evaluate([X_test, X_test], y_test_cat, verbose=0)\n",
    "\n",
    "print('='*70)\n",
    "print('FINAL RESULTS')\n",
    "print('='*70)\n",
    "print(f'CNN Accuracy: {cnn_acc*100:.2f}%')\n",
    "print(f'GCN Accuracy: {gcn_acc*100:.2f}%')\n",
    "print('='*70)\n",
    "\n",
    "if gcn_acc > cnn_acc:\n",
    "    print(f'WINNER: GCN (+{(gcn_acc-cnn_acc)*100:.2f}%)')\n",
    "else:\n",
    "    print(f'WINNER: CNN (+{(cnn_acc-gcn_acc)*100:.2f}%)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
